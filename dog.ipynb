{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import unitary_group\n",
    "from skimage.measure import block_reduce\n",
    "from opt_einsum import contract\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from src.QDDPM_torch import DiffusionModel, QDDPM, naturalDistance\n",
    "import src.ImageEncode as ie\n",
    "rc('text', usetex=False)\n",
    "rc('axes', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Torch will use CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Torch will use CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_encode(image_values: np.array, npixels: int):\n",
    "    '''\n",
    "    Args:\n",
    "        image_values: 2-dimensional array (non hairy) of image data\n",
    "    '''\n",
    "    norm: float = np.linalg.norm(image_values)\n",
    "    normalized_image_values: np.array = image_values.flatten()/norm\n",
    "    return normalized_image_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.33\n",
      "0.66\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "pixelValues = []\n",
    "for i in range(4):\n",
    "    value = 0.33 * i\n",
    "    pixelValues.append(value)\n",
    "    print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.33]\n",
      " [0.66 0.99]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.26726124 0.53452248 0.80178373]\n"
     ]
    }
   ],
   "source": [
    "values = np.array([pixelValues[0:2], pixelValues[2:4]])\n",
    "print(values)\n",
    "normalized = ie.normalize(values)\n",
    "plt.imshow(values, cmap='grey', interpolation='nearest')\n",
    "plt.show()\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training(values: np.array, n_train: int, scale: float, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    n = values.size\n",
    "    noise = abs(np.random.randn(n_train,n))+0j*np.random.randn(n_train,n) \n",
    "    print(noise.shape)\n",
    "    print(values.shape)\n",
    "    states = (noise*scale) + values\n",
    "    states/=np.tile(np.linalg.norm(states, axis=1).reshape(1,n_train), (n,1)).T\n",
    "    return states\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4)\n",
      "(4,)\n",
      "[[0.44259474+0.j 0.38482286+0.j 0.66548664+0.j 0.46168041+0.j]\n",
      " [0.7514234 +0.j 0.20481689+0.j 0.47856725+0.j 0.40544581+0.j]\n",
      " [0.32429462+0.j 0.19918448+0.j 0.32138155+0.j 0.86710578+0.j]\n",
      " ...\n",
      " [0.22174245+0.j 0.48802167+0.j 0.80101346+0.j 0.26653812+0.j]\n",
      " [0.20544193+0.j 0.55450863+0.j 0.61566867+0.j 0.52083192+0.j]\n",
      " [0.00428428+0.j 0.40977478+0.j 0.87967187+0.j 0.24133726+0.j]]\n"
     ]
    }
   ],
   "source": [
    "a = generate_training(normalized, 400, 2)\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4)\n",
      "(4,)\n",
      "tensor([[0.6464+0.j, 0.5237+0.j, 0.3415+0.j, 0.4373+0.j],\n",
      "        [0.0067+0.j, 0.8688+0.j, 0.2650+0.j, 0.4183+0.j],\n",
      "        [0.3313+0.j, 0.0974+0.j, 0.1614+0.j, 0.9245+0.j],\n",
      "        ...,\n",
      "        [0.2273+0.j, 0.1022+0.j, 0.6904+0.j, 0.6792+0.j],\n",
      "        [0.3447+0.j, 0.2543+0.j, 0.1964+0.j, 0.8820+0.j],\n",
      "        [0.3278+0.j, 0.5786+0.j, 0.2133+0.j, 0.7158+0.j]],\n",
      "       dtype=torch.complex128)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#generate diffusion data\n",
    "n = 2\n",
    "T = 20\n",
    "Ndata = 400\n",
    "\n",
    "diff_hs = np.linspace(0.5, 4., T)\n",
    "\n",
    "model_diff = DiffusionModel(n, T, Ndata)\n",
    "X = torch.from_numpy(generate_training(normalized, Ndata, 2))\n",
    "\n",
    "Xout = np.zeros((T+1, Ndata, 2**n), dtype = np.complex64)\n",
    "Xout[0] = X\n",
    "print(X)\n",
    "\n",
    "for t in range(1, T+1):\n",
    "    Xout[t] = model_diff.set_diffusionData_t(t, X, diff_hs[:t], seed = t).numpy()\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_t(model, t, inputs_T, params_tot, Ndata, epochs):\n",
    "    '''\n",
    "    the trianing for the backward PQC at step t\n",
    "    input_tplus1: the output from step t+1, as the role of input at step t\n",
    "    Args:\n",
    "    model: the QDDPM model\n",
    "    t: the diffusion step\n",
    "    inputs_T: the input data at step t=T\n",
    "    params_tot: collection of PQC parameters before step t\n",
    "    Ndata: number of samples in dataset\n",
    "    epochs: the number of iterations\n",
    "    '''\n",
    "    input_tplus1 = model.prepareInput_t(inputs_T, params_tot, t, Ndata) # prepare input\n",
    "    states_diff = model.states_diff\n",
    "    loss_hist = [] # record of training history\n",
    "\n",
    "    # initialize parameters\n",
    "    np.random.seed()\n",
    "    params_t = torch.tensor(np.random.normal(size=2 * model.n_tot * model.L), requires_grad=True)\n",
    "    # set optimizer and learning rate decay\n",
    "    optimizer = torch.optim.Adam([params_t], lr=0.0005)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for step in range(epochs):\n",
    "        indices = np.random.choice(states_diff.shape[1], size=Ndata, replace=False)\n",
    "        true_data = states_diff[t, indices]\n",
    "\n",
    "        output_t = model.backwardOutput_t(input_tplus1, params_t)\n",
    "        loss = naturalDistance(output_t, true_data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_hist.append(loss) # record the current loss\n",
    "        \n",
    "        if step%100 == 0:\n",
    "            loss_value = loss_hist[-1]\n",
    "            print(\"Step %s, loss: %s, time elapsed: %s seconds\"%(step, loss_value, time.time() - t0))\n",
    "\n",
    "    return params_t, torch.stack(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 0.1265239715576172 seconds\n",
      "Step 100, loss: tensor(0.0279, grad_fn=<SubBackward0>), time elapsed: 8.191352367401123 seconds\n",
      "Step 200, loss: tensor(0.0176, grad_fn=<SubBackward0>), time elapsed: 14.751994371414185 seconds\n",
      "Step 300, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 21.049241542816162 seconds\n",
      "Step 400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 27.31992530822754 seconds\n",
      "Step 500, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 33.919762134552 seconds\n",
      "Step 600, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 40.55551314353943 seconds\n",
      "Step 700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 48.21543622016907 seconds\n",
      "Step 800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 55.93238592147827 seconds\n",
      "Step 900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 63.850862979888916 seconds\n",
      "Step 1000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 70.50420212745667 seconds\n",
      "Step 1100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 76.79943823814392 seconds\n",
      "Step 1200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 83.04411292076111 seconds\n",
      "Step 1300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 91.2685067653656 seconds\n",
      "Step 1400, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 100.58124446868896 seconds\n",
      "Step 1500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 107.59106111526489 seconds\n",
      "Step 1600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 114.46603012084961 seconds\n",
      "Step 1700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 121.61914253234863 seconds\n",
      "Step 1800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 129.7094759941101 seconds\n",
      "Step 1900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 138.0612621307373 seconds\n",
      "Step 2000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 145.57809162139893 seconds\n",
      "19\n",
      "Step 0, loss: tensor(0.0617, grad_fn=<SubBackward0>), time elapsed: 0.06374287605285645 seconds\n",
      "Step 100, loss: tensor(0.0415, grad_fn=<SubBackward0>), time elapsed: 6.530721187591553 seconds\n",
      "Step 200, loss: tensor(0.0238, grad_fn=<SubBackward0>), time elapsed: 12.91403079032898 seconds\n",
      "Step 300, loss: tensor(0.0170, grad_fn=<SubBackward0>), time elapsed: 19.38406538963318 seconds\n",
      "Step 400, loss: tensor(0.0126, grad_fn=<SubBackward0>), time elapsed: 27.516260862350464 seconds\n",
      "Step 500, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 35.248475551605225 seconds\n",
      "Step 600, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 43.30463981628418 seconds\n",
      "Step 700, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 50.238754749298096 seconds\n",
      "Step 800, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 56.82747673988342 seconds\n",
      "Step 900, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 63.199742794036865 seconds\n",
      "Step 1000, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 70.87632632255554 seconds\n",
      "Step 1100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 77.93238663673401 seconds\n",
      "Step 1200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 84.14853262901306 seconds\n",
      "Step 1300, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 90.3933002948761 seconds\n",
      "Step 1400, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 97.01416254043579 seconds\n",
      "Step 1500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 103.27602624893188 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 109.55982804298401 seconds\n",
      "Step 1700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 115.78084468841553 seconds\n",
      "Step 1800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 122.03357410430908 seconds\n",
      "Step 1900, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 128.206524848938 seconds\n",
      "Step 2000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 134.50614166259766 seconds\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0882, grad_fn=<SubBackward0>), time elapsed: 0.060056447982788086 seconds\n",
      "Step 100, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 6.38100004196167 seconds\n",
      "Step 200, loss: tensor(0.0374, grad_fn=<SubBackward0>), time elapsed: 12.73757815361023 seconds\n",
      "Step 300, loss: tensor(0.0287, grad_fn=<SubBackward0>), time elapsed: 19.539886713027954 seconds\n",
      "Step 400, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 25.944641828536987 seconds\n",
      "Step 500, loss: tensor(0.0108, grad_fn=<SubBackward0>), time elapsed: 32.34673523902893 seconds\n",
      "Step 600, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 38.75867056846619 seconds\n",
      "Step 700, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 45.111488342285156 seconds\n",
      "Step 800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 51.564852237701416 seconds\n",
      "Step 900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 57.91825556755066 seconds\n",
      "Step 1000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 64.22706890106201 seconds\n",
      "Step 1100, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 70.9252233505249 seconds\n",
      "Step 1200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 77.19632267951965 seconds\n",
      "Step 1300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 83.47900819778442 seconds\n",
      "Step 1400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 90.0304479598999 seconds\n",
      "Step 1500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 97.6184310913086 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 104.61102366447449 seconds\n",
      "Step 1700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 110.8814628124237 seconds\n",
      "Step 1800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 117.11610198020935 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 123.7895302772522 seconds\n",
      "Step 2000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 130.03507804870605 seconds\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1438, grad_fn=<SubBackward0>), time elapsed: 0.052785396575927734 seconds\n",
      "Step 100, loss: tensor(0.1030, grad_fn=<SubBackward0>), time elapsed: 6.403523683547974 seconds\n",
      "Step 200, loss: tensor(0.0643, grad_fn=<SubBackward0>), time elapsed: 15.08870530128479 seconds\n",
      "Step 300, loss: tensor(0.0359, grad_fn=<SubBackward0>), time elapsed: 23.874300718307495 seconds\n",
      "Step 400, loss: tensor(0.0213, grad_fn=<SubBackward0>), time elapsed: 31.593012809753418 seconds\n",
      "Step 500, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 38.628345012664795 seconds\n",
      "Step 600, loss: tensor(0.0099, grad_fn=<SubBackward0>), time elapsed: 45.298229694366455 seconds\n",
      "Step 700, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 53.52444076538086 seconds\n",
      "Step 800, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 61.53108215332031 seconds\n",
      "Step 900, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 70.42421698570251 seconds\n",
      "Step 1000, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 78.56546545028687 seconds\n",
      "Step 1100, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 85.28494477272034 seconds\n",
      "Step 1200, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 92.1960871219635 seconds\n",
      "Step 1300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 98.92434525489807 seconds\n",
      "Step 1400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 105.64222598075867 seconds\n",
      "Step 1500, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 112.35811448097229 seconds\n",
      "Step 1600, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 119.53216004371643 seconds\n",
      "Step 1700, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 126.13780045509338 seconds\n",
      "Step 1800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 132.634583234787 seconds\n",
      "Step 1900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 139.33344984054565 seconds\n",
      "Step 2000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 145.96628546714783 seconds\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0604, grad_fn=<SubBackward0>), time elapsed: 0.060369014739990234 seconds\n",
      "Step 100, loss: tensor(0.0378, grad_fn=<SubBackward0>), time elapsed: 6.784485816955566 seconds\n",
      "Step 200, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 13.259727716445923 seconds\n",
      "Step 300, loss: tensor(0.0105, grad_fn=<SubBackward0>), time elapsed: 19.615729808807373 seconds\n",
      "Step 400, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 26.469371557235718 seconds\n",
      "Step 500, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 32.73752284049988 seconds\n",
      "Step 600, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 39.093562841415405 seconds\n",
      "Step 700, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 46.380794286727905 seconds\n",
      "Step 800, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 53.736241817474365 seconds\n",
      "Step 900, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 60.23522400856018 seconds\n",
      "Step 1000, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 66.4764723777771 seconds\n",
      "Step 1100, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 72.86908674240112 seconds\n",
      "Step 1200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 80.26409339904785 seconds\n",
      "Step 1300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 88.25893449783325 seconds\n",
      "Step 1400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 94.52748441696167 seconds\n",
      "Step 1500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 100.79333329200745 seconds\n",
      "Step 1600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 107.19058895111084 seconds\n",
      "Step 1700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 113.58123660087585 seconds\n",
      "Step 1800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 119.84538793563843 seconds\n",
      "Step 1900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 126.2595272064209 seconds\n",
      "Step 2000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 132.7487931251526 seconds\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 0.06231045722961426 seconds\n",
      "Step 100, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 7.002143621444702 seconds\n",
      "Step 200, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 13.301380634307861 seconds\n",
      "Step 300, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 19.666154384613037 seconds\n",
      "Step 400, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 25.926978826522827 seconds\n",
      "Step 500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 32.28629541397095 seconds\n",
      "Step 600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 38.601757526397705 seconds\n",
      "Step 700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 44.90562844276428 seconds\n",
      "Step 800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 51.1822075843811 seconds\n",
      "Step 900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 57.78496217727661 seconds\n",
      "Step 1000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 64.00706315040588 seconds\n",
      "Step 1100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 70.29481959342957 seconds\n",
      "Step 1200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 76.51907229423523 seconds\n",
      "Step 1300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 82.8186149597168 seconds\n",
      "Step 1400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 90.20116591453552 seconds\n",
      "Step 1500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 97.33862471580505 seconds\n",
      "Step 1600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 103.58408808708191 seconds\n",
      "Step 1700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 109.79068851470947 seconds\n",
      "Step 1800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 116.47363996505737 seconds\n",
      "Step 1900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 123.19270968437195 seconds\n",
      "Step 2000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 129.50692486763 seconds\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 0.0622098445892334 seconds\n",
      "Step 100, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 6.451179504394531 seconds\n",
      "Step 200, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 15.262205600738525 seconds\n",
      "Step 300, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 23.771364212036133 seconds\n",
      "Step 400, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 31.302667140960693 seconds\n",
      "Step 500, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 37.7254753112793 seconds\n",
      "Step 600, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 44.56735706329346 seconds\n",
      "Step 700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 51.12166166305542 seconds\n",
      "Step 800, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 57.52760696411133 seconds\n",
      "Step 900, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 63.95318651199341 seconds\n",
      "Step 1000, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 70.28772807121277 seconds\n",
      "Step 1100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 76.71575856208801 seconds\n",
      "Step 1200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 83.05202746391296 seconds\n",
      "Step 1300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 89.43848633766174 seconds\n",
      "Step 1400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 96.21784710884094 seconds\n",
      "Step 1500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 102.45180034637451 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 108.97590637207031 seconds\n",
      "Step 1700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 115.28071904182434 seconds\n",
      "Step 1800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 121.73963403701782 seconds\n",
      "Step 1900, loss: tensor(0.0001, grad_fn=<SubBackward0>), time elapsed: 128.07357025146484 seconds\n",
      "Step 2000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 134.47720885276794 seconds\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0701, grad_fn=<SubBackward0>), time elapsed: 0.05347466468811035 seconds\n",
      "Step 100, loss: tensor(0.0388, grad_fn=<SubBackward0>), time elapsed: 6.350348234176636 seconds\n",
      "Step 200, loss: tensor(0.0203, grad_fn=<SubBackward0>), time elapsed: 12.70088505744934 seconds\n",
      "Step 300, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 19.569835901260376 seconds\n",
      "Step 400, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 25.88131284713745 seconds\n",
      "Step 500, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 32.383443117141724 seconds\n",
      "Step 600, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 38.897037744522095 seconds\n",
      "Step 700, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 45.416983127593994 seconds\n",
      "Step 800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 51.90297722816467 seconds\n",
      "Step 900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 58.33354210853577 seconds\n",
      "Step 1000, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 64.9000186920166 seconds\n",
      "Step 1100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 71.72151613235474 seconds\n",
      "Step 1200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 78.1721785068512 seconds\n",
      "Step 1300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 84.62685036659241 seconds\n",
      "Step 1400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 91.08209300041199 seconds\n",
      "Step 1500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 97.54747176170349 seconds\n",
      "Step 1600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 103.96485686302185 seconds\n",
      "Step 1700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 110.28696012496948 seconds\n",
      "Step 1800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 116.68193364143372 seconds\n",
      "Step 1900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 123.68641519546509 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 129.9816963672638 seconds\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 0.06269574165344238 seconds\n",
      "Step 100, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 6.548895835876465 seconds\n",
      "Step 200, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 13.045855522155762 seconds\n",
      "Step 300, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 19.613996267318726 seconds\n",
      "Step 400, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 26.109642028808594 seconds\n",
      "Step 500, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 32.532493114471436 seconds\n",
      "Step 600, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 39.031572103500366 seconds\n",
      "Step 700, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 45.56524467468262 seconds\n",
      "Step 800, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 52.37634897232056 seconds\n",
      "Step 900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 58.80705761909485 seconds\n",
      "Step 1000, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 65.31394457817078 seconds\n",
      "Step 1100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 71.72427892684937 seconds\n",
      "Step 1200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 78.2259259223938 seconds\n",
      "Step 1300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 84.65829253196716 seconds\n",
      "Step 1400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 91.06398630142212 seconds\n",
      "Step 1500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 97.4909336566925 seconds\n",
      "Step 1600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 104.4488787651062 seconds\n",
      "Step 1700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 110.87799382209778 seconds\n",
      "Step 1800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 117.24821400642395 seconds\n",
      "Step 1900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 123.58102607727051 seconds\n",
      "Step 2000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 130.1518406867981 seconds\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0851, grad_fn=<SubBackward0>), time elapsed: 0.060997724533081055 seconds\n",
      "Step 100, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 6.560189723968506 seconds\n",
      "Step 200, loss: tensor(0.0345, grad_fn=<SubBackward0>), time elapsed: 13.210543632507324 seconds\n",
      "Step 300, loss: tensor(0.0264, grad_fn=<SubBackward0>), time elapsed: 19.716657638549805 seconds\n",
      "Step 400, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 26.589908123016357 seconds\n",
      "Step 500, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 32.913654804229736 seconds\n",
      "Step 600, loss: tensor(0.0124, grad_fn=<SubBackward0>), time elapsed: 39.41118502616882 seconds\n",
      "Step 700, loss: tensor(0.0086, grad_fn=<SubBackward0>), time elapsed: 45.958611249923706 seconds\n",
      "Step 800, loss: tensor(0.0093, grad_fn=<SubBackward0>), time elapsed: 52.431028842926025 seconds\n",
      "Step 900, loss: tensor(0.0086, grad_fn=<SubBackward0>), time elapsed: 58.920973777770996 seconds\n",
      "Step 1000, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 65.3191168308258 seconds\n",
      "Step 1100, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 71.75868344306946 seconds\n",
      "Step 1200, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 78.19970870018005 seconds\n",
      "Step 1300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 84.92465353012085 seconds\n",
      "Step 1400, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 91.26459980010986 seconds\n",
      "Step 1500, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 97.58011031150818 seconds\n",
      "Step 1600, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 104.07420182228088 seconds\n",
      "Step 1700, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 110.5442373752594 seconds\n",
      "Step 1800, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 117.05303621292114 seconds\n",
      "Step 1900, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 123.37392497062683 seconds\n",
      "Step 2000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 129.8377285003662 seconds\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0915, grad_fn=<SubBackward0>), time elapsed: 0.06491208076477051 seconds\n",
      "Step 100, loss: tensor(0.0700, grad_fn=<SubBackward0>), time elapsed: 6.96681547164917 seconds\n",
      "Step 200, loss: tensor(0.0572, grad_fn=<SubBackward0>), time elapsed: 13.612115144729614 seconds\n",
      "Step 300, loss: tensor(0.0440, grad_fn=<SubBackward0>), time elapsed: 20.00333547592163 seconds\n",
      "Step 400, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 26.45742106437683 seconds\n",
      "Step 500, loss: tensor(0.0204, grad_fn=<SubBackward0>), time elapsed: 32.92093586921692 seconds\n",
      "Step 600, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 39.41432809829712 seconds\n",
      "Step 700, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 45.88918709754944 seconds\n",
      "Step 800, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 52.40475535392761 seconds\n",
      "Step 900, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 59.295165061950684 seconds\n",
      "Step 1000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 65.63811016082764 seconds\n",
      "Step 1100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 72.0023443698883 seconds\n",
      "Step 1200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 78.3766176700592 seconds\n",
      "Step 1300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 84.85468339920044 seconds\n",
      "Step 1400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 91.24515748023987 seconds\n",
      "Step 1500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 97.62743759155273 seconds\n",
      "Step 1600, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 104.11756730079651 seconds\n",
      "Step 1700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 110.62170720100403 seconds\n",
      "Step 1800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 117.36115503311157 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 123.80483365058899 seconds\n",
      "Step 2000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 130.19357109069824 seconds\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1967, grad_fn=<SubBackward0>), time elapsed: 0.05969953536987305 seconds\n",
      "Step 100, loss: tensor(0.1560, grad_fn=<SubBackward0>), time elapsed: 6.510388374328613 seconds\n",
      "Step 200, loss: tensor(0.1126, grad_fn=<SubBackward0>), time elapsed: 13.10195016860962 seconds\n",
      "Step 300, loss: tensor(0.0787, grad_fn=<SubBackward0>), time elapsed: 19.536025524139404 seconds\n",
      "Step 400, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 25.91976833343506 seconds\n",
      "Step 500, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 32.369464635849 seconds\n",
      "Step 600, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 39.166470527648926 seconds\n",
      "Step 700, loss: tensor(0.0360, grad_fn=<SubBackward0>), time elapsed: 45.67563796043396 seconds\n",
      "Step 800, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 52.34950137138367 seconds\n",
      "Step 900, loss: tensor(0.0240, grad_fn=<SubBackward0>), time elapsed: 58.9062135219574 seconds\n",
      "Step 1000, loss: tensor(0.0206, grad_fn=<SubBackward0>), time elapsed: 65.41519069671631 seconds\n",
      "Step 1100, loss: tensor(0.0100, grad_fn=<SubBackward0>), time elapsed: 71.89413475990295 seconds\n",
      "Step 1200, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 78.33200693130493 seconds\n",
      "Step 1300, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 84.76355648040771 seconds\n",
      "Step 1400, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 91.11219191551208 seconds\n",
      "Step 1500, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 97.95207262039185 seconds\n",
      "Step 1600, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 104.29645895957947 seconds\n",
      "Step 1700, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 110.73130440711975 seconds\n",
      "Step 1800, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 117.13229632377625 seconds\n",
      "Step 1900, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 123.52478194236755 seconds\n",
      "Step 2000, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 129.8593578338623 seconds\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1864, grad_fn=<SubBackward0>), time elapsed: 0.0638570785522461 seconds\n",
      "Step 100, loss: tensor(0.1172, grad_fn=<SubBackward0>), time elapsed: 6.388741731643677 seconds\n",
      "Step 200, loss: tensor(0.0946, grad_fn=<SubBackward0>), time elapsed: 12.907294034957886 seconds\n",
      "Step 300, loss: tensor(0.0631, grad_fn=<SubBackward0>), time elapsed: 19.70526671409607 seconds\n",
      "Step 400, loss: tensor(0.0418, grad_fn=<SubBackward0>), time elapsed: 26.161539316177368 seconds\n",
      "Step 500, loss: tensor(0.0273, grad_fn=<SubBackward0>), time elapsed: 32.54377818107605 seconds\n",
      "Step 600, loss: tensor(0.0231, grad_fn=<SubBackward0>), time elapsed: 38.99195408821106 seconds\n",
      "Step 700, loss: tensor(0.0198, grad_fn=<SubBackward0>), time elapsed: 45.3940007686615 seconds\n",
      "Step 800, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 51.84110641479492 seconds\n",
      "Step 900, loss: tensor(0.0105, grad_fn=<SubBackward0>), time elapsed: 58.3245804309845 seconds\n",
      "Step 1000, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 64.86768817901611 seconds\n",
      "Step 1100, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 71.78361630439758 seconds\n",
      "Step 1200, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 78.28232073783875 seconds\n",
      "Step 1300, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 84.7640585899353 seconds\n",
      "Step 1400, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 91.42038369178772 seconds\n",
      "Step 1500, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 97.92520236968994 seconds\n",
      "Step 1600, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 104.35178256034851 seconds\n",
      "Step 1700, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 110.76660799980164 seconds\n",
      "Step 1800, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 117.20033264160156 seconds\n",
      "Step 1900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 124.111656665802 seconds\n",
      "Step 2000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 130.36530900001526 seconds\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.2045, grad_fn=<SubBackward0>), time elapsed: 0.06388688087463379 seconds\n",
      "Step 100, loss: tensor(0.1316, grad_fn=<SubBackward0>), time elapsed: 6.52377724647522 seconds\n",
      "Step 200, loss: tensor(0.0988, grad_fn=<SubBackward0>), time elapsed: 12.999982118606567 seconds\n",
      "Step 300, loss: tensor(0.0666, grad_fn=<SubBackward0>), time elapsed: 19.498276948928833 seconds\n",
      "Step 400, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 25.86614489555359 seconds\n",
      "Step 500, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 32.23680114746094 seconds\n",
      "Step 600, loss: tensor(0.0391, grad_fn=<SubBackward0>), time elapsed: 38.62253141403198 seconds\n",
      "Step 700, loss: tensor(0.0246, grad_fn=<SubBackward0>), time elapsed: 45.050517559051514 seconds\n",
      "Step 800, loss: tensor(0.0178, grad_fn=<SubBackward0>), time elapsed: 51.823615312576294 seconds\n",
      "Step 900, loss: tensor(0.0198, grad_fn=<SubBackward0>), time elapsed: 58.22442817687988 seconds\n",
      "Step 1000, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 64.62903070449829 seconds\n",
      "Step 1100, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 71.07796216011047 seconds\n",
      "Step 1200, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 77.6635103225708 seconds\n",
      "Step 1300, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 84.22349667549133 seconds\n",
      "Step 1400, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 90.72434639930725 seconds\n",
      "Step 1500, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 97.23032021522522 seconds\n",
      "Step 1600, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 104.15780401229858 seconds\n",
      "Step 1700, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 110.6326847076416 seconds\n",
      "Step 1800, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 117.06404972076416 seconds\n",
      "Step 1900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 123.43580412864685 seconds\n",
      "Step 2000, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 129.93986797332764 seconds\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.3595, grad_fn=<SubBackward0>), time elapsed: 0.04940390586853027 seconds\n",
      "Step 100, loss: tensor(0.2765, grad_fn=<SubBackward0>), time elapsed: 6.500809907913208 seconds\n",
      "Step 200, loss: tensor(0.2064, grad_fn=<SubBackward0>), time elapsed: 12.98439645767212 seconds\n",
      "Step 300, loss: tensor(0.1634, grad_fn=<SubBackward0>), time elapsed: 19.462690114974976 seconds\n",
      "Step 400, loss: tensor(0.1223, grad_fn=<SubBackward0>), time elapsed: 26.407310009002686 seconds\n",
      "Step 500, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 35.02383613586426 seconds\n",
      "Step 600, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 43.85630917549133 seconds\n",
      "Step 700, loss: tensor(0.0391, grad_fn=<SubBackward0>), time elapsed: 51.426804304122925 seconds\n",
      "Step 800, loss: tensor(0.0254, grad_fn=<SubBackward0>), time elapsed: 57.721402168273926 seconds\n",
      "Step 900, loss: tensor(0.0197, grad_fn=<SubBackward0>), time elapsed: 64.09952473640442 seconds\n",
      "Step 1000, loss: tensor(0.0171, grad_fn=<SubBackward0>), time elapsed: 70.44490337371826 seconds\n",
      "Step 1100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 76.843416929245 seconds\n",
      "Step 1200, loss: tensor(0.0108, grad_fn=<SubBackward0>), time elapsed: 83.28503847122192 seconds\n",
      "Step 1300, loss: tensor(0.0102, grad_fn=<SubBackward0>), time elapsed: 89.97155022621155 seconds\n",
      "Step 1400, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 96.32795548439026 seconds\n",
      "Step 1500, loss: tensor(0.0080, grad_fn=<SubBackward0>), time elapsed: 102.7187135219574 seconds\n",
      "Step 1600, loss: tensor(0.0075, grad_fn=<SubBackward0>), time elapsed: 109.28362011909485 seconds\n",
      "Step 1700, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 115.68793392181396 seconds\n",
      "Step 1800, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 121.98786306381226 seconds\n",
      "Step 1900, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 128.36158514022827 seconds\n",
      "Step 2000, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 134.73977208137512 seconds\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.3913, grad_fn=<SubBackward0>), time elapsed: 0.05751752853393555 seconds\n",
      "Step 100, loss: tensor(0.3285, grad_fn=<SubBackward0>), time elapsed: 6.92146897315979 seconds\n",
      "Step 200, loss: tensor(0.2202, grad_fn=<SubBackward0>), time elapsed: 13.216132879257202 seconds\n",
      "Step 300, loss: tensor(0.1350, grad_fn=<SubBackward0>), time elapsed: 19.487657070159912 seconds\n",
      "Step 400, loss: tensor(0.0853, grad_fn=<SubBackward0>), time elapsed: 25.7782461643219 seconds\n",
      "Step 500, loss: tensor(0.0675, grad_fn=<SubBackward0>), time elapsed: 32.15359854698181 seconds\n",
      "Step 600, loss: tensor(0.0284, grad_fn=<SubBackward0>), time elapsed: 38.622456312179565 seconds\n",
      "Step 700, loss: tensor(0.0191, grad_fn=<SubBackward0>), time elapsed: 45.1003851890564 seconds\n",
      "Step 800, loss: tensor(0.0107, grad_fn=<SubBackward0>), time elapsed: 51.41587042808533 seconds\n",
      "Step 900, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 58.15681481361389 seconds\n",
      "Step 1000, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 64.40846014022827 seconds\n",
      "Step 1100, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 70.6578779220581 seconds\n",
      "Step 1200, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 76.98204636573792 seconds\n",
      "Step 1300, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 83.44320726394653 seconds\n",
      "Step 1400, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 89.86428332328796 seconds\n",
      "Step 1500, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 96.280433177948 seconds\n",
      "Step 1600, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 102.66729593276978 seconds\n",
      "Step 1700, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 109.05558848381042 seconds\n",
      "Step 1800, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 116.05527877807617 seconds\n",
      "Step 1900, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 122.65356159210205 seconds\n",
      "Step 2000, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 128.99856638908386 seconds\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.7300, grad_fn=<SubBackward0>), time elapsed: 0.048532962799072266 seconds\n",
      "Step 100, loss: tensor(0.5920, grad_fn=<SubBackward0>), time elapsed: 6.517063856124878 seconds\n",
      "Step 200, loss: tensor(0.4298, grad_fn=<SubBackward0>), time elapsed: 12.857007265090942 seconds\n",
      "Step 300, loss: tensor(0.2724, grad_fn=<SubBackward0>), time elapsed: 19.30143976211548 seconds\n",
      "Step 400, loss: tensor(0.1551, grad_fn=<SubBackward0>), time elapsed: 25.598268032073975 seconds\n",
      "Step 500, loss: tensor(0.0859, grad_fn=<SubBackward0>), time elapsed: 31.927530765533447 seconds\n",
      "Step 600, loss: tensor(0.0470, grad_fn=<SubBackward0>), time elapsed: 38.50314116477966 seconds\n",
      "Step 700, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 44.82259941101074 seconds\n",
      "Step 800, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 51.05441761016846 seconds\n",
      "Step 900, loss: tensor(0.0201, grad_fn=<SubBackward0>), time elapsed: 57.4651255607605 seconds\n",
      "Step 1000, loss: tensor(0.0133, grad_fn=<SubBackward0>), time elapsed: 63.82288217544556 seconds\n",
      "Step 1100, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 70.06009793281555 seconds\n",
      "Step 1200, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 76.35284423828125 seconds\n",
      "Step 1300, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 82.66496157646179 seconds\n",
      "Step 1400, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 89.44540524482727 seconds\n",
      "Step 1500, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 95.6710147857666 seconds\n",
      "Step 1600, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 102.04765248298645 seconds\n",
      "Step 1700, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 110.8785514831543 seconds\n",
      "Step 1800, loss: tensor(0.0100, grad_fn=<SubBackward0>), time elapsed: 119.41239023208618 seconds\n",
      "Step 1900, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 126.8300666809082 seconds\n",
      "Step 2000, loss: tensor(0.0077, grad_fn=<SubBackward0>), time elapsed: 133.1474621295929 seconds\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.7224, grad_fn=<SubBackward0>), time elapsed: 0.06852436065673828 seconds\n",
      "Step 100, loss: tensor(0.5616, grad_fn=<SubBackward0>), time elapsed: 6.338623523712158 seconds\n",
      "Step 200, loss: tensor(0.4143, grad_fn=<SubBackward0>), time elapsed: 13.209621906280518 seconds\n",
      "Step 300, loss: tensor(0.2084, grad_fn=<SubBackward0>), time elapsed: 19.505941152572632 seconds\n",
      "Step 400, loss: tensor(0.0973, grad_fn=<SubBackward0>), time elapsed: 25.892130851745605 seconds\n",
      "Step 500, loss: tensor(0.0390, grad_fn=<SubBackward0>), time elapsed: 32.23616814613342 seconds\n",
      "Step 600, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 38.51642465591431 seconds\n",
      "Step 700, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 44.899312019348145 seconds\n",
      "Step 800, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 51.23126769065857 seconds\n",
      "Step 900, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 57.59181332588196 seconds\n",
      "Step 1000, loss: tensor(0.0080, grad_fn=<SubBackward0>), time elapsed: 63.87759351730347 seconds\n",
      "Step 1100, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 70.553297996521 seconds\n",
      "Step 1200, loss: tensor(0.0098, grad_fn=<SubBackward0>), time elapsed: 76.85477447509766 seconds\n",
      "Step 1300, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 83.19750714302063 seconds\n",
      "Step 1400, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 89.62163162231445 seconds\n",
      "Step 1500, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 96.09070110321045 seconds\n",
      "Step 1600, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 102.62478375434875 seconds\n",
      "Step 1700, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 109.02799129486084 seconds\n",
      "Step 1800, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 115.42757320404053 seconds\n",
      "Step 1900, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 122.14926528930664 seconds\n",
      "Step 2000, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 128.44722890853882 seconds\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.7717, grad_fn=<SubBackward0>), time elapsed: 0.04964137077331543 seconds\n",
      "Step 100, loss: tensor(0.6544, grad_fn=<SubBackward0>), time elapsed: 6.303650379180908 seconds\n",
      "Step 200, loss: tensor(0.4205, grad_fn=<SubBackward0>), time elapsed: 12.778253316879272 seconds\n",
      "Step 300, loss: tensor(0.2723, grad_fn=<SubBackward0>), time elapsed: 19.199222803115845 seconds\n",
      "Step 400, loss: tensor(0.1220, grad_fn=<SubBackward0>), time elapsed: 25.720402717590332 seconds\n",
      "Step 500, loss: tensor(0.0542, grad_fn=<SubBackward0>), time elapsed: 32.011935234069824 seconds\n",
      "Step 600, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 38.5435791015625 seconds\n",
      "Step 700, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 45.266003370285034 seconds\n",
      "Step 800, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 51.501919746398926 seconds\n",
      "Step 900, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 57.75457310676575 seconds\n",
      "Step 1000, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 64.0093822479248 seconds\n",
      "Step 1100, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 71.16365814208984 seconds\n",
      "Step 1200, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 79.9082658290863 seconds\n",
      "Step 1300, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 88.03165245056152 seconds\n",
      "Step 1400, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 94.33404016494751 seconds\n",
      "Step 1500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 100.69294166564941 seconds\n",
      "Step 1600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 107.7647614479065 seconds\n",
      "Step 1700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 114.12584590911865 seconds\n",
      "Step 1800, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 120.5092248916626 seconds\n",
      "Step 1900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 129.03177881240845 seconds\n",
      "Step 2000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 137.5310924053192 seconds\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.7895, grad_fn=<SubBackward0>), time elapsed: 0.056816816329956055 seconds\n",
      "Step 100, loss: tensor(0.6538, grad_fn=<SubBackward0>), time elapsed: 6.9603822231292725 seconds\n",
      "Step 200, loss: tensor(0.5248, grad_fn=<SubBackward0>), time elapsed: 13.29295825958252 seconds\n",
      "Step 300, loss: tensor(0.3403, grad_fn=<SubBackward0>), time elapsed: 19.59690237045288 seconds\n",
      "Step 400, loss: tensor(0.2321, grad_fn=<SubBackward0>), time elapsed: 26.220747709274292 seconds\n",
      "Step 500, loss: tensor(0.1275, grad_fn=<SubBackward0>), time elapsed: 32.54996204376221 seconds\n",
      "Step 600, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 38.81703996658325 seconds\n",
      "Step 700, loss: tensor(0.0591, grad_fn=<SubBackward0>), time elapsed: 45.08880019187927 seconds\n",
      "Step 800, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 51.41976451873779 seconds\n",
      "Step 900, loss: tensor(0.0331, grad_fn=<SubBackward0>), time elapsed: 57.73118472099304 seconds\n",
      "Step 1000, loss: tensor(0.0249, grad_fn=<SubBackward0>), time elapsed: 64.23253655433655 seconds\n",
      "Step 1100, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 70.49413728713989 seconds\n",
      "Step 1200, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 77.16941380500793 seconds\n",
      "Step 1300, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 83.48780727386475 seconds\n",
      "Step 1400, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 89.85753631591797 seconds\n",
      "Step 1500, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 96.17570328712463 seconds\n",
      "Step 1600, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 102.56757426261902 seconds\n",
      "Step 1700, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 108.92022895812988 seconds\n",
      "Step 1800, loss: tensor(0.0104, grad_fn=<SubBackward0>), time elapsed: 115.2166211605072 seconds\n",
      "Step 1900, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 121.70818591117859 seconds\n",
      "Step 2000, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 128.46259260177612 seconds\n"
     ]
    }
   ],
   "source": [
    "#train on diffusion data\n",
    "n, na = 2, 1 # number of data and ancilla qubits\n",
    "T = 20 # number of diffusion steps\n",
    "L = 6 # layer of backward PQC\n",
    "Ndata = 400 # number of data in the training data set\n",
    "epochs = 2001 # number of training epochs\n",
    "\n",
    "#gen random samples at t = T\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_T = diffModel.HaarSampleGeneration(Ndata, seed = 22)\n",
    "\n",
    "#load diffusion process\n",
    "states_diff = Xout\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "model.set_diffusionSet(states_diff)\n",
    "\n",
    "params_total = []\n",
    "loss_hist_total = []\n",
    "\n",
    "for t in range(T - 1, -1, -1):\n",
    "    params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "    for tt in range(t+1, 20):\n",
    "        print(tt)\n",
    "        params_tot[tt] = np.load('data/cluster/n2/QDDPMcluster0params_n2na1T20L6_t%d_mmd.npy'%tt)\n",
    "    params, loss_hist = Training_t(model, t, inputs_T, params_tot, Ndata, epochs)\n",
    "    params = params.detach().numpy()\n",
    "    loss_hist = loss_hist.detach().numpy()\n",
    "    np.save('data/cluster/n2/QDDPMcluster0params_n2na1T20L6_t%d_mmd.npy'%t, params)\n",
    "    np.save('data/cluster/n2/QDDPMcluster0loss_n2na1T20L6_t%d_mmd.npy'%t, loss_hist)\n",
    "    \n",
    "    #params_total = np.append(params_total, params, axis = 1)\n",
    "    #loss_hist_total = np.append(loss_hist_total, loss_hist, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, na = 2, 1\n",
    "T = 20\n",
    "L = 6\n",
    "Ndata = 400\n",
    "epochs = 2001\n",
    "\n",
    "params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "loss_tot = np.zeros((T, epochs))\n",
    "f0_tot = np.zeros((T, epochs))\n",
    "\n",
    "for t in range(T):\n",
    "    params_tot[t] = np.load('data/cluster/n2/QDDPMcluster0params_n2na1T20L6_t%d_mmd.npy'%t)\n",
    "    loss_tot[t] = np.load('data/cluster/n2/QDDPMcluster0loss_n2na1T20L6_t%d_mmd.npy'%t)\n",
    "    \n",
    "\n",
    "np.save(\"params_total\", params_tot)\n",
    "np.save(\"loss_tot\", loss_tot)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"states_diff\", Xout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02723781+0.j 0.02737894+0.j 0.02737894+0.j 0.02752007+0.j]\n",
      " [0.0276612 +0.j 0.0276612 +0.j 0.02780233+0.j 0.02780233+0.j]\n",
      " [0.02780233+0.j 0.02780233+0.j 0.02794345+0.j 0.0276612 +0.j]\n",
      " ...\n",
      " [0.02328621+0.j 0.02526201+0.j 0.02737894+0.j 0.02808458+0.j]\n",
      " [0.02610878+0.j 0.02624991+0.j 0.02709668+0.j 0.02794345+0.j]\n",
      " [0.02822571+0.j 0.02822571+0.j 0.02780233+0.j 0.02794345+0.j]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11cf550ea20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "scale = 40\n",
    "img = Image.open('dog.jpg')\n",
    "img_greyscale = img.convert(\"L\")\n",
    "img_resized = img_greyscale.resize((scale, scale))\n",
    "\n",
    "img_array = np.array(img_resized, ndmin = 2)\n",
    "img_resized.save('dog_resized_greyscale.jpg')\n",
    " \n",
    "normalized_img_array = ie.normalize(img_array)\n",
    "normalized_img_array = normalized_img_array.ravel() + 1j*np.zeros(scale*scale)\n",
    "test_data = np.zeros((int(normalized_img_array.size / 4), 4)) + 1j * np.zeros((int(normalized_img_array.size / 4), 4))\n",
    "for i in range(0, normalized_img_array.size):\n",
    "    test_data[int(i / 4)][i % 4] = normalized_img_array[i]\n",
    "\n",
    "\n",
    "print(test_data)\n",
    "\n",
    "plt.imshow(img_array, cmap='grey',interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't assign a numpy.ndarray to a torch.ComplexFloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m params_tot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams_total.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m QDDPM(n\u001b[38;5;241m=\u001b[39mn, na\u001b[38;5;241m=\u001b[39mna, T\u001b[38;5;241m=\u001b[39mT, L\u001b[38;5;241m=\u001b[39mL)\n\u001b[1;32m---> 12\u001b[0m data_te \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackDataGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_tot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNdata\u001b[49m\u001b[43m)\u001b[49m[:, :, :\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mn]\n\u001b[0;32m     14\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_backwardsgen\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_te)\n",
      "File \u001b[1;32mc:\\Users\\georg\\OneDrive\\Desktop\\Workspace\\Research_Worskpace\\QuantGenMdl\\src\\QDDPM_torch.py:180\u001b[0m, in \u001b[0;36mQDDPM.backDataGeneration\u001b[1;34m(self, inputs_T, params_tot, Ndata)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03mgenerate the dataset in backward denoise process with training data set\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    179\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, Ndata, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tot))\u001b[38;5;241m.\u001b[39mcfloat()\n\u001b[1;32m--> 180\u001b[0m \u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m inputs_T\n\u001b[0;32m    181\u001b[0m params_tot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(params_tot)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mTypeError\u001b[0m: can't assign a numpy.ndarray to a torch.ComplexFloatTensor"
     ]
    }
   ],
   "source": [
    "n, na = 2, 1\n",
    "T = 20\n",
    "L = 6\n",
    "Ndata = 400\n",
    "\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "\n",
    "params_tot = np.load('params_total.npy')\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "\n",
    "data_te = model.backDataGeneration(test_data, params_tot, Ndata)[:, :, :2**n]\n",
    "\n",
    "np.save(\"test_backwardsgen\", data_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.33]\n",
      " [0.66 0.99]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        +0.j 0.26726124+0.j 0.53452248+0.j 0.80178373+0.j]\n"
     ]
    }
   ],
   "source": [
    "values = np.array([pixelValues[0:2], pixelValues[2:4]])\n",
    "print(values)\n",
    "normalized = ie.normalize(values)\n",
    "plt.imshow(values, cmap='grey', interpolation='nearest')\n",
    "plt.show()\n",
    "normalized = normalized.ravel() + 0j*np.zeros(4)\n",
    "print(normalized)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
