{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import unitary_group\n",
    "from skimage.measure import block_reduce\n",
    "from opt_einsum import contract\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from src.QDDPM_torch import DiffusionModel, QDDPM, naturalDistance\n",
    "import src.ImageEncode as ie\n",
    "rc('text', usetex=False)\n",
    "rc('axes', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Torch will use CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Torch will use CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training(values: np.array, n_train: int, scale: float, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    n = values.size\n",
    "    noise = abs(np.random.randn(n_train,n))+0j*np.random.randn(n_train,n) \n",
    "    print(noise.shape)\n",
    "    print(values.shape)\n",
    "    states = (noise*scale) + values\n",
    "    states/=np.tile(np.linalg.norm(states, axis=1).reshape(1,n_train), (n,1)).T\n",
    "    return states\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#will not work with mixed state probabilities\n",
    "from PIL import Image\n",
    "scale = 40\n",
    "img = Image.open('images.png')\n",
    "img_greyscale = img.convert(\"L\")\n",
    "img_resized = img_greyscale.resize((scale, scale))\n",
    "\n",
    "img_array = np.array(img_resized, ndmin = 2)\n",
    "img_resized.save('dog_resized_greyscale.jpg')\n",
    "\n",
    "collapsed_array = np.zeros(img_array.size)\n",
    "\n",
    "for i in range(0, img_array.size):\n",
    "    collapsed_array[i] = img_array[int(i/scale)][i%scale]\n",
    "\n",
    "test_data = np.zeros((int(img_array.size / 4), 4)) + 1j * np.zeros((int(img_array.size / 4), 4))\n",
    "temp_normalize = max(collapsed_array) * 4\n",
    "for i in range(0, int(img_array.size / 4)):\n",
    "    test_data[i] = collapsed_array[4*i : 4*i+4] / temp_normalize + 1j * np.zeros(4)\n",
    "\n",
    "plt.imshow(img_array, cmap='grey',interpolation = 'nearest')\n",
    "\n",
    "# Find where NaN values are\n",
    "print(np.isnan(test_data).any())\n",
    "\n",
    "# Get the indices of the NaN values\n",
    "#nan_indices = np.where(img_array)\n",
    "\n",
    "#print(nan_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_test = np.array([[1, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1], [1, 1, 1, 1]])\n",
    "plt.imshow(temp_test, cmap='grey',interpolation = 'nearest')\n",
    "temp_test = temp_test.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "#generate training data for diffusion\n",
    "n = 4\n",
    "T = 20\n",
    "Ndata = 2000\n",
    "\n",
    "diff_hs = torch.from_numpy(np.linspace(0.5, 4., T))\n",
    "\n",
    "model_diff = DiffusionModel(n, T, Ndata)\n",
    "\n",
    "X = torch.from_numpy(generate_training(temp_test, Ndata, 0.05))\n",
    "\n",
    "np.save('training_data', np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2800x1400 with 50 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize training data before they are forward diffused\n",
    "training_temp = np.abs(np.array(X))\n",
    "multiplier = np.max(training_temp)\n",
    "training_temp /= multiplier\n",
    "fig, axs = plt.subplots(5, 10, figsize = (28, 14))\n",
    "for index_1 in range(0, 50):\n",
    "    training_images = training_temp[index_1]\n",
    "\n",
    "    picture_training  = np.zeros((4, 4))\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            picture_training[i][j] = training_images[i*4 + j]\n",
    "\n",
    "    axs[int(index_1 / 10)][index_1 % 10].imshow(picture_training, cmap='grey',interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#diffuse training data\n",
    "Xout = np.zeros((T+1, Ndata, 2**n), dtype = np.complex64)\n",
    "Xout[0] = X.numpy()\n",
    "\n",
    "for t in range(1, T+1):\n",
    "    Xout[t] = model_diff.set_diffusionData_t(t, X, diff_hs[:t], seed = t).numpy()\n",
    "    print(t)\n",
    "\n",
    "np.save(\"states_diff\", Xout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         1.         ... 1.         0.99999988 1.        ]\n",
      " [0.95379411 0.94283223 0.95989051 ... 0.96230815 0.93162313 0.94562588]\n",
      " [0.93342679 0.94968429 0.80878709 ... 0.84761103 0.76824997 0.73139394]\n",
      " ...\n",
      " [0.03011609 0.10262213 0.04946043 ... 0.09197472 0.0871414  0.150233  ]\n",
      " [0.05082321 0.06157269 0.08411071 ... 0.16418842 0.0980667  0.00735945]\n",
      " [0.08787546 0.0059782  0.01721599 ... 0.04829054 0.00670319 0.0607175 ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "fidelity = np.zeros((T + 1, Ndata))\n",
    "for i in range(0, T + 1):\n",
    "    for j in range(0, Ndata):\n",
    "        #different calculations for mixed state fidelity\n",
    "        fidelity[i][j] = np.abs(np.vdot(states_diff[i][j], states_diff[0][j])) ** 2\n",
    "\n",
    "\n",
    "fidelity_mean = np.mean(fidelity, axis = 1)\n",
    "print(fidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the fidelity decay in the diffusion process\n",
    "n = 2\n",
    "T = 20\n",
    "Ndata = 600\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "indices = np.random.permutation(Ndata)\n",
    "\n",
    "ax.plot(range(T+1), fidelity_mean, 'o--', markersize=8, lw=2, c='r')\n",
    "ax.plot(range(T+1), 0.25*np.ones(T+1), '--', lw=2, c='gold')\n",
    "ax.set_ylabel(r'$F_0$', fontsize=30)\n",
    "ax.set_xlabel(r'$t$', fontsize=30)\n",
    "ax.set_ylim(0,1)\n",
    "ax.tick_params(direction='in', length=10, width=3, top='on', right='on', labelsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_t(model, t, inputs_T, params_tot, Ndata, epochs):\n",
    "    '''\n",
    "    the trianing for the backward PQC at step t\n",
    "    input_tplus1: the output from step t+1, as the role of input at step t\n",
    "    Args:\n",
    "    model: the QDDPM model\n",
    "    t: the diffusion step\n",
    "    inputs_T: the input data at step t=T\n",
    "    params_tot: collection of PQC parameters before step t\n",
    "    Ndata: number of samples in dataset\n",
    "    epochs: the number of iterations\n",
    "    '''\n",
    "    input_tplus1 = model.prepareInput_t(inputs_T, params_tot, t, Ndata) # prepare input\n",
    "    states_diff = model.states_diff\n",
    "    loss_hist = [] # record of training history\n",
    "\n",
    "    # initialize parameters\n",
    "    np.random.seed()\n",
    "    params_t = torch.tensor(np.random.normal(size=2 * model.n_tot * model.L), requires_grad=True)\n",
    "    # set optimizer and learning rate decay\n",
    "    optimizer = torch.optim.Adam([params_t], lr=0.0005)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for step in range(epochs):\n",
    "        indices = np.random.choice(states_diff.shape[1], size=Ndata, replace=False)\n",
    "        true_data = states_diff[t, indices]\n",
    "\n",
    "        output_t = model.backwardOutput_t(input_tplus1, params_t)\n",
    "        loss = naturalDistance(output_t, true_data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_hist.append(loss) # record the current loss\n",
    "        \n",
    "        if step%100 == 0:\n",
    "            loss_value = loss_hist[-1]\n",
    "            print(\"Step %s, loss: %s, time elapsed: %s seconds\"%(step, loss_value, time.time() - t0))\n",
    "\n",
    "    return params_t, torch.stack(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: tensor(0.0207, grad_fn=<SubBackward0>), time elapsed: 0.08776545524597168 seconds\n",
      "Step 100, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 5.974656581878662 seconds\n",
      "Step 200, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 11.656195878982544 seconds\n",
      "Step 300, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 17.42595076560974 seconds\n",
      "Step 400, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 23.094358682632446 seconds\n",
      "Step 500, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 28.875133991241455 seconds\n",
      "Step 600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 34.505762815475464 seconds\n",
      "Step 700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 40.16259956359863 seconds\n",
      "Step 800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 45.904818058013916 seconds\n",
      "Step 900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 51.573132038116455 seconds\n",
      "Step 1000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 57.461182832717896 seconds\n",
      "Step 1100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 63.14864706993103 seconds\n",
      "Step 1200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 73.8249843120575 seconds\n",
      "Step 1300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 84.26455616950989 seconds\n",
      "Step 1400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 90.52780675888062 seconds\n",
      "Step 1500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 96.37097215652466 seconds\n",
      "Step 1600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 102.04809379577637 seconds\n",
      "Step 1700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 107.82140445709229 seconds\n",
      "Step 1800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 113.50526857376099 seconds\n",
      "Step 1900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 119.28499579429626 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 124.91267013549805 seconds\n",
      "Step 2100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 130.5266933441162 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 136.22830533981323 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 141.84679102897644 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 147.64419674873352 seconds\n",
      "Step 2500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 153.37086033821106 seconds\n",
      "Step 2600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 159.22691679000854 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 164.91325569152832 seconds\n",
      "Step 2800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 170.6092870235443 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 176.402024269104 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 182.05828619003296 seconds\n",
      "Step 3100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 187.77012276649475 seconds\n",
      "Step 3200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 193.3716254234314 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 198.9502112865448 seconds\n",
      "Step 3400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 204.69045686721802 seconds\n",
      "Step 3500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 210.27309131622314 seconds\n",
      "Step 3600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 216.0008246898651 seconds\n",
      "Step 3700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 221.58672618865967 seconds\n",
      "Step 3800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 227.2997806072235 seconds\n",
      "Step 3900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 232.8690001964569 seconds\n",
      "Step 4000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 238.43914341926575 seconds\n",
      "Step 4100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 244.17954397201538 seconds\n",
      "Step 4200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 249.7433729171753 seconds\n",
      "Step 4300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 255.45767426490784 seconds\n",
      "Step 4400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 261.04651141166687 seconds\n",
      "Step 4500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 266.7733254432678 seconds\n",
      "Step 4600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 272.38298058509827 seconds\n",
      "Step 4700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 277.95547699928284 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 283.71546053886414 seconds\n",
      "Step 4900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 289.3171441555023 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 295.043607711792 seconds\n",
      "Step 5100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 300.6451301574707 seconds\n",
      "Step 5200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 306.2850778102875 seconds\n",
      "Step 5300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 312.0064699649811 seconds\n",
      "Step 5400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 317.58821272850037 seconds\n",
      "Step 5500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 323.2793040275574 seconds\n",
      "Step 5600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 328.86541628837585 seconds\n",
      "Step 5700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 334.60022234916687 seconds\n",
      "Step 5800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 340.198618888855 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 345.77422547340393 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 351.5049104690552 seconds\n",
      "Step 6100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 357.09384059906006 seconds\n",
      "Step 6200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 362.8831944465637 seconds\n",
      "Step 6300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 368.5614335536957 seconds\n",
      "Step 6400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 374.37548446655273 seconds\n",
      "Step 6500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 380.0462725162506 seconds\n",
      "Step 6600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 385.62692856788635 seconds\n",
      "Step 6700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 391.36196851730347 seconds\n",
      "Step 6800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 396.97758412361145 seconds\n",
      "Step 6900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 402.6984622478485 seconds\n",
      "Step 7000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 408.2749478816986 seconds\n",
      "Step 7100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 413.84303545951843 seconds\n",
      "Step 7200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 419.5717387199402 seconds\n",
      "Step 7300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 425.16720056533813 seconds\n",
      "Step 7400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 430.87031865119934 seconds\n",
      "Step 7500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 436.4744963645935 seconds\n",
      "Step 7600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 442.2051031589508 seconds\n",
      "Step 7700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 447.7859218120575 seconds\n",
      "Step 7800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 453.3707320690155 seconds\n",
      "Step 7900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 459.1055762767792 seconds\n",
      "Step 8000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 464.6851496696472 seconds\n",
      "Step 8100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 470.4130356311798 seconds\n",
      "Step 8200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 475.98518466949463 seconds\n",
      "Step 8300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 481.7326707839966 seconds\n",
      "Step 8400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 487.3258566856384 seconds\n",
      "Step 8500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 492.90542697906494 seconds\n",
      "Step 8600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 498.6289699077606 seconds\n",
      "Step 8700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 504.21269726753235 seconds\n",
      "Step 8800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 509.90289330482483 seconds\n",
      "Step 8900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 515.4835481643677 seconds\n",
      "Step 9000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 521.0700507164001 seconds\n",
      "Step 9100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 526.80335688591 seconds\n",
      "Step 9200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 532.3881828784943 seconds\n",
      "Step 9300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 538.1210653781891 seconds\n",
      "Step 9400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 543.716493844986 seconds\n",
      "Step 9500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 549.4710667133331 seconds\n",
      "Step 9600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 555.0423882007599 seconds\n",
      "Step 9700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 560.6240239143372 seconds\n",
      "Step 9800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 566.3434481620789 seconds\n",
      "Step 9900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 571.939670085907 seconds\n",
      "Step 10000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 577.7028732299805 seconds\n",
      "Step 10100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 583.2835233211517 seconds\n",
      "Step 10200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 588.8662810325623 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 594.5845608711243 seconds\n",
      "Step 10400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 600.1444575786591 seconds\n",
      "Step 10500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 605.8670387268066 seconds\n",
      "Step 10600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 611.4726901054382 seconds\n",
      "Step 10700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 617.2210702896118 seconds\n",
      "Step 10800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 622.8272202014923 seconds\n",
      "Step 10900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 628.4271876811981 seconds\n",
      "Step 11000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 634.1830296516418 seconds\n",
      "Step 11100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 639.8001091480255 seconds\n",
      "Step 11200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 645.5154976844788 seconds\n",
      "Step 11300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 651.0804150104523 seconds\n",
      "Step 11400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 656.6432869434357 seconds\n",
      "Step 11500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 662.3532481193542 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 667.9591240882874 seconds\n",
      "Step 11700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 673.7064137458801 seconds\n",
      "Step 11800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 679.3329601287842 seconds\n",
      "Step 11900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 685.0793371200562 seconds\n",
      "Step 12000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 690.6391694545746 seconds\n",
      "Step 12100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 696.1969494819641 seconds\n",
      "Step 12200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 701.9360990524292 seconds\n",
      "Step 12300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 707.4968898296356 seconds\n",
      "Step 12400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 713.204799413681 seconds\n",
      "Step 12500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 718.7854459285736 seconds\n",
      "Step 12600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 724.3671233654022 seconds\n",
      "Step 12700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 730.1178212165833 seconds\n",
      "Step 12800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 735.6859042644501 seconds\n",
      "Step 12900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 741.4041991233826 seconds\n",
      "Step 13000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 746.9848494529724 seconds\n",
      "Step 13100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 752.7046675682068 seconds\n",
      "Step 13200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 758.3015847206116 seconds\n",
      "Step 13300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 763.9196977615356 seconds\n",
      "Step 13400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 769.6747081279755 seconds\n",
      "Step 13500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 775.276083946228 seconds\n",
      "Step 13600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 781.0153167247772 seconds\n",
      "Step 13700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 786.6000807285309 seconds\n",
      "Step 13800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 792.1765446662903 seconds\n",
      "Step 13900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 797.9010469913483 seconds\n",
      "Step 14000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 803.4837744235992 seconds\n",
      "Step 14100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 809.2062485218048 seconds\n",
      "Step 14200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 814.8213429450989 seconds\n",
      "Step 14300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 820.5678265094757 seconds\n",
      "Step 14400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 826.1275656223297 seconds\n",
      "Step 14500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 831.7125403881073 seconds\n",
      "Step 14600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 837.4349126815796 seconds\n",
      "Step 14700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 843.0207612514496 seconds\n",
      "Step 14800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 848.7557277679443 seconds\n",
      "Step 14900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 854.3478269577026 seconds\n",
      "Step 15000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 859.9119589328766 seconds\n",
      "Step 15100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 865.6436989307404 seconds\n",
      "Step 15200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 871.2274353504181 seconds\n",
      "Step 15300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 876.9709184169769 seconds\n",
      "Step 15400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 882.5847780704498 seconds\n",
      "Step 15500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 888.3250193595886 seconds\n",
      "Step 15600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 893.9045903682709 seconds\n",
      "Step 15700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 899.4852366447449 seconds\n",
      "Step 15800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 905.1910164356232 seconds\n",
      "Step 15900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 910.7841885089874 seconds\n",
      "Step 16000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 916.5568814277649 seconds\n",
      "Step 16100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 922.1634564399719 seconds\n",
      "Step 16200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 927.7377715110779 seconds\n",
      "Step 16300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 933.4446671009064 seconds\n",
      "Step 16400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 939.0129773616791 seconds\n",
      "Step 16500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 944.7353339195251 seconds\n",
      "Step 16600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 950.3034207820892 seconds\n",
      "Step 16700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 956.0426437854767 seconds\n",
      "Step 16800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 961.6284630298615 seconds\n",
      "Step 16900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 967.2288956642151 seconds\n",
      "Step 17000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 972.9847331047058 seconds\n",
      "Step 17100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 978.5642747879028 seconds\n",
      "Step 17200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 984.3002684116364 seconds\n",
      "Step 17300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 989.8821139335632 seconds\n",
      "Step 17400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 995.4491639137268 seconds\n",
      "Step 17500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1001.2258477210999 seconds\n",
      "Step 17600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1006.8232746124268 seconds\n",
      "Step 17700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1012.545661687851 seconds\n",
      "Step 17800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1018.1429851055145 seconds\n",
      "Step 17900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1023.7028107643127 seconds\n",
      "Step 18000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1029.4398577213287 seconds\n",
      "Step 18100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1035.0393619537354 seconds\n",
      "Step 18200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1040.7409782409668 seconds\n",
      "Step 18300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1046.339313030243 seconds\n",
      "Step 18400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1052.083645105362 seconds\n",
      "Step 18500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1057.6707282066345 seconds\n",
      "Step 18600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1063.2555565834045 seconds\n",
      "Step 18700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1069.0028676986694 seconds\n",
      "Step 18800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1074.5782589912415 seconds\n",
      "Step 18900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1080.2851340770721 seconds\n",
      "Step 19000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1085.8449602127075 seconds\n",
      "Step 19100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1091.4256162643433 seconds\n",
      "Step 19200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1097.1845240592957 seconds\n",
      "Step 19300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1102.7705788612366 seconds\n",
      "Step 19400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1108.5231728553772 seconds\n",
      "Step 19500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1114.0860579013824 seconds\n",
      "Step 19600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1119.645866394043 seconds\n",
      "Step 19700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1125.3850240707397 seconds\n",
      "Step 19800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1130.9645643234253 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1136.6808953285217 seconds\n",
      "Step 20000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1142.2438867092133 seconds\n",
      "19\n",
      "Step 0, loss: tensor(0.0104, grad_fn=<SubBackward0>), time elapsed: 0.058374881744384766 seconds\n",
      "Step 100, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 5.801647424697876 seconds\n",
      "Step 200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 11.42495846748352 seconds\n",
      "Step 300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 17.071259021759033 seconds\n",
      "Step 400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 22.814573764801025 seconds\n",
      "Step 500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 28.391058444976807 seconds\n",
      "Step 600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 34.088496685028076 seconds\n",
      "Step 700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 39.69000840187073 seconds\n",
      "Step 800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 45.45019221305847 seconds\n",
      "Step 900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 51.03490948677063 seconds\n",
      "Step 1000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 56.62801694869995 seconds\n",
      "Step 1100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 62.36923670768738 seconds\n",
      "Step 1200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 67.95630884170532 seconds\n",
      "Step 1300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 73.66719365119934 seconds\n",
      "Step 1400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 79.24682450294495 seconds\n",
      "Step 1500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 84.78997111320496 seconds\n",
      "Step 1600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 90.51026940345764 seconds\n",
      "Step 1700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 96.08256483078003 seconds\n",
      "Step 1800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 101.79669046401978 seconds\n",
      "Step 1900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 107.39629673957825 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 113.10710000991821 seconds\n",
      "Step 2100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 118.68465971946716 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 124.2715630531311 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 129.9815800189972 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 135.55382990837097 seconds\n",
      "Step 2500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 141.2119631767273 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 146.7509047985077 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 152.41690349578857 seconds\n",
      "Step 2800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 157.93621492385864 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 163.46743059158325 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 169.148024559021 seconds\n",
      "Step 3100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 174.65367937088013 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 180.31016373634338 seconds\n",
      "Step 3300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 185.80308055877686 seconds\n",
      "Step 3400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 191.32202553749084 seconds\n",
      "Step 3500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 196.9888575077057 seconds\n",
      "Step 3600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 202.51524376869202 seconds\n",
      "Step 3700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 208.2020902633667 seconds\n",
      "Step 3800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 213.71729397773743 seconds\n",
      "Step 3900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 219.3866970539093 seconds\n",
      "Step 4000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 224.8956959247589 seconds\n",
      "Step 4100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 230.4185950756073 seconds\n",
      "Step 4200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 236.07876300811768 seconds\n",
      "Step 4300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 241.58919095993042 seconds\n",
      "Step 4400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 247.2524573802948 seconds\n",
      "Step 4500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 252.7995743751526 seconds\n",
      "Step 4600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 258.49875354766846 seconds\n",
      "Step 4700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 264.02956795692444 seconds\n",
      "Step 4800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 269.55335426330566 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 275.2741873264313 seconds\n",
      "Step 5000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 280.8207392692566 seconds\n",
      "Step 5100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 286.50381875038147 seconds\n",
      "Step 5200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 292.0773093700409 seconds\n",
      "Step 5300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 298.2805278301239 seconds\n",
      "Step 5400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 303.80914282798767 seconds\n",
      "Step 5500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 309.316472530365 seconds\n",
      "Step 5600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 314.9686539173126 seconds\n",
      "Step 5700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 320.5187096595764 seconds\n",
      "Step 5800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 326.18671011924744 seconds\n",
      "Step 5900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 331.72266006469727 seconds\n",
      "Step 6000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 337.23882031440735 seconds\n",
      "Step 6100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 342.9203288555145 seconds\n",
      "Step 6200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 348.4531433582306 seconds\n",
      "Step 6300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 354.10708355903625 seconds\n",
      "Step 6400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 359.6233592033386 seconds\n",
      "Step 6500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 365.3012685775757 seconds\n",
      "Step 6600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 370.8075361251831 seconds\n",
      "Step 6700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 376.3182158470154 seconds\n",
      "Step 6800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 382.0273394584656 seconds\n",
      "Step 6900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 387.5771391391754 seconds\n",
      "Step 7000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 393.21324944496155 seconds\n",
      "Step 7100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 398.75151658058167 seconds\n",
      "Step 7200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 404.4300117492676 seconds\n",
      "Step 7300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 409.9746880531311 seconds\n",
      "Step 7400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 415.51358819007874 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 421.6877932548523 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 427.23864555358887 seconds\n",
      "Step 7700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 432.9344983100891 seconds\n",
      "Step 7800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 438.4451639652252 seconds\n",
      "Step 7900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 443.9789729118347 seconds\n",
      "Step 8000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 449.64538526535034 seconds\n",
      "Step 8100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 455.1568660736084 seconds\n",
      "Step 8200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 460.8026900291443 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 466.3437066078186 seconds\n",
      "Step 8400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 472.04855465888977 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 477.5796058177948 seconds\n",
      "Step 8600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 483.1040585041046 seconds\n",
      "Step 8700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 488.7916717529297 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 494.3114278316498 seconds\n",
      "Step 8900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 499.97879338264465 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 505.52699065208435 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 511.0474889278412 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 517.0570693016052 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 522.5619921684265 seconds\n",
      "Step 9400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 528.223531961441 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 533.7782607078552 seconds\n",
      "Step 9600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 539.4412994384766 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 544.9314336776733 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 550.4685337543488 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 556.1628398895264 seconds\n",
      "Step 10000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 561.7453455924988 seconds\n",
      "Step 10100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 567.5853319168091 seconds\n",
      "Step 10200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 573.1457998752594 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 578.6866915225983 seconds\n",
      "Step 10400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 584.3572628498077 seconds\n",
      "Step 10500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 589.8765025138855 seconds\n",
      "Step 10600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 595.5953402519226 seconds\n",
      "Step 10700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 601.794201374054 seconds\n",
      "Step 10800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 607.7107000350952 seconds\n",
      "Step 10900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 613.3706028461456 seconds\n",
      "Step 11000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 619.0325446128845 seconds\n",
      "Step 11100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 624.8671395778656 seconds\n",
      "Step 11200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 630.4649994373322 seconds\n",
      "Step 11300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 636.1563482284546 seconds\n",
      "Step 11400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 641.7118434906006 seconds\n",
      "Step 11500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 647.2883071899414 seconds\n",
      "Step 11600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 653.010769367218 seconds\n",
      "Step 11700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 658.6101982593536 seconds\n",
      "Step 11800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 664.3516283035278 seconds\n",
      "Step 11900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 669.9404547214508 seconds\n",
      "Step 12000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 675.6589121818542 seconds\n",
      "Step 12100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 681.2310450077057 seconds\n",
      "Step 12200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 686.8409245014191 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 692.5696141719818 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 698.160760641098 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 703.9040834903717 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 709.5180451869965 seconds\n",
      "Step 12700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 715.1278913021088 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 720.8964087963104 seconds\n",
      "Step 12900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 726.4895005226135 seconds\n",
      "Step 13000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 732.2285516262054 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 737.8300592899323 seconds\n",
      "Step 13200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 743.6026475429535 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 749.2417891025543 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 754.851667881012 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 760.5833697319031 seconds\n",
      "Step 13600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 766.1713087558746 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 771.933349609375 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 777.5286271572113 seconds\n",
      "Step 13900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 783.1134610176086 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 788.8672134876251 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 794.4501094818115 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 800.189094543457 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 805.9508016109467 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 811.7497093677521 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 817.3398704528809 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 822.8912694454193 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 828.6137325763702 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 834.2079782485962 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 839.9962565898895 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 845.9523403644562 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 851.511935710907 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 857.2512607574463 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 862.8485758304596 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 868.6002421379089 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 874.185010433197 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 880.5038776397705 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 886.088648557663 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 891.6694524288177 seconds\n",
      "Step 15900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 897.4574053287506 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 903.0611670017242 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 908.9273829460144 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 914.5716750621796 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 920.1690411567688 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 925.9290316104889 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 931.5294466018677 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 937.2696888446808 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 942.8670382499695 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 948.6144218444824 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 954.1837546825409 seconds\n",
      "Step 17000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 959.7643678188324 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 965.4836752414703 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 971.0499148368835 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 976.7836513519287 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 982.3572070598602 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 987.9544520378113 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 993.6977257728577 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 999.290864944458 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1005.0133445262909 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1010.6106743812561 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1016.1998291015625 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1021.9513595104218 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1027.5320234298706 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1033.2857480049133 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1038.8893601894379 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1044.6462273597717 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1050.2570781707764 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1055.8419208526611 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1061.563381433487 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1067.130455493927 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1072.8986976146698 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1078.5075600147247 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1084.1487710475922 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1089.9087979793549 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1095.4883630275726 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1101.2108438014984 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1106.7914848327637 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1112.380485534668 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1118.1072130203247 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1123.6962678432465 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1129.4437868595123 seconds\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0089, grad_fn=<SubBackward0>), time elapsed: 0.07213568687438965 seconds\n",
      "Step 100, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 5.702676773071289 seconds\n",
      "Step 200, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 11.432360410690308 seconds\n",
      "Step 300, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 17.02261996269226 seconds\n",
      "Step 400, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 22.58644461631775 seconds\n",
      "Step 500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 28.31742525100708 seconds\n",
      "Step 600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 33.90096640586853 seconds\n",
      "Step 700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 39.603694915771484 seconds\n",
      "Step 800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 45.1989209651947 seconds\n",
      "Step 900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 50.94024586677551 seconds\n",
      "Step 1000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 56.504154205322266 seconds\n",
      "Step 1100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 62.07868552207947 seconds\n",
      "Step 1200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 67.742835521698 seconds\n",
      "Step 1300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 73.31718039512634 seconds\n",
      "Step 1400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 79.0425820350647 seconds\n",
      "Step 1500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 84.62322425842285 seconds\n",
      "Step 1600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 90.20921802520752 seconds\n",
      "Step 1700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 95.89614868164062 seconds\n",
      "Step 1800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 101.46222853660583 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 107.18639063835144 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 112.76535034179688 seconds\n",
      "Step 2100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 118.4712917804718 seconds\n",
      "Step 2200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 124.03617334365845 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 129.6032645702362 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 135.36324501037598 seconds\n",
      "Step 2500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 140.9647672176361 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 146.6705687046051 seconds\n",
      "Step 2700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 152.23658204078674 seconds\n",
      "Step 2800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 157.93215131759644 seconds\n",
      "Step 2900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 163.50429129600525 seconds\n",
      "Step 3000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 169.11848998069763 seconds\n",
      "Step 3100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 174.85750198364258 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 180.43701457977295 seconds\n",
      "Step 3300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 186.14391112327576 seconds\n",
      "Step 3400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 191.72472882270813 seconds\n",
      "Step 3500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 197.42504858970642 seconds\n",
      "Step 3600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 203.0027937889099 seconds\n",
      "Step 3700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 208.6000781059265 seconds\n",
      "Step 3800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 214.30727100372314 seconds\n",
      "Step 3900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 219.87411737442017 seconds\n",
      "Step 4000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 225.60625433921814 seconds\n",
      "Step 4100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 231.202054977417 seconds\n",
      "Step 4200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 236.78688645362854 seconds\n",
      "Step 4300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 242.50936102867126 seconds\n",
      "Step 4400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 248.09725069999695 seconds\n",
      "Step 4500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 253.78753018379211 seconds\n",
      "Step 4600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 259.3639278411865 seconds\n",
      "Step 4700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 265.0781292915344 seconds\n",
      "Step 4800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 270.6754114627838 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 276.2435607910156 seconds\n",
      "Step 5000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 281.9745125770569 seconds\n",
      "Step 5100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 287.5633451938629 seconds\n",
      "Step 5200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 293.29723501205444 seconds\n",
      "Step 5300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 298.87903237342834 seconds\n",
      "Step 5400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 304.6056230068207 seconds\n",
      "Step 5500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 310.17392015457153 seconds\n",
      "Step 5600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 315.7972083091736 seconds\n",
      "Step 5700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 321.9440426826477 seconds\n",
      "Step 5800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 327.62495732307434 seconds\n",
      "Step 5900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 333.83731031417847 seconds\n",
      "Step 6000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 339.48271799087524 seconds\n",
      "Step 6100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 345.09772968292236 seconds\n",
      "Step 6200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 350.8420581817627 seconds\n",
      "Step 6300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 356.4874620437622 seconds\n",
      "Step 6400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 362.2355601787567 seconds\n",
      "Step 6500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 368.91233801841736 seconds\n",
      "Step 6600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 374.61380410194397 seconds\n",
      "Step 6700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 380.13073110580444 seconds\n",
      "Step 6800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 385.6799032688141 seconds\n",
      "Step 6900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 391.30949878692627 seconds\n",
      "Step 7000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 396.8428235054016 seconds\n",
      "Step 7100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 402.50303506851196 seconds\n",
      "Step 7200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 408.03455114364624 seconds\n",
      "Step 7300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 413.7323684692383 seconds\n",
      "Step 7400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 419.2495596408844 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 424.77498483657837 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 430.412823677063 seconds\n",
      "Step 7700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 435.95179176330566 seconds\n",
      "Step 7800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 441.6513900756836 seconds\n",
      "Step 7900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 447.199355840683 seconds\n",
      "Step 8000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 452.73592877388 seconds\n",
      "Step 8100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 460.0047376155853 seconds\n",
      "Step 8200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 465.51171708106995 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 471.1926112174988 seconds\n",
      "Step 8400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 476.7417731285095 seconds\n",
      "Step 8500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 482.42599272727966 seconds\n",
      "Step 8600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 488.7171206474304 seconds\n",
      "Step 8700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 494.2690713405609 seconds\n",
      "Step 8800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 499.9383523464203 seconds\n",
      "Step 8900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 505.50703024864197 seconds\n",
      "Step 9000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 511.1655933856964 seconds\n",
      "Step 9100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 516.6693913936615 seconds\n",
      "Step 9200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 522.1936485767365 seconds\n",
      "Step 9300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 527.8661193847656 seconds\n",
      "Step 9400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 533.4165332317352 seconds\n",
      "Step 9500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 539.1052796840668 seconds\n",
      "Step 9600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 544.6478817462921 seconds\n",
      "Step 9700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 550.3583629131317 seconds\n",
      "Step 9800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 555.8918981552124 seconds\n",
      "Step 9900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 561.410961151123 seconds\n",
      "Step 10000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 567.1268889904022 seconds\n",
      "Step 10100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 572.6700189113617 seconds\n",
      "Step 10200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 578.3548314571381 seconds\n",
      "Step 10300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 584.6038529872894 seconds\n",
      "Step 10400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 590.1354458332062 seconds\n",
      "Step 10500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 595.8037419319153 seconds\n",
      "Step 10600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 601.3213684558868 seconds\n",
      "Step 10700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 606.9813573360443 seconds\n",
      "Step 10800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 612.5085990428925 seconds\n",
      "Step 10900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 618.170820236206 seconds\n",
      "Step 11000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 623.6957125663757 seconds\n",
      "Step 11100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 629.2658047676086 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 634.9730806350708 seconds\n",
      "Step 11300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 640.5438039302826 seconds\n",
      "Step 11400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 646.2682814598083 seconds\n",
      "Step 11500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 652.1674718856812 seconds\n",
      "Step 11600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 657.7212634086609 seconds\n",
      "Step 11700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 663.5137190818787 seconds\n",
      "Step 11800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 669.0915653705597 seconds\n",
      "Step 11900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 674.8000164031982 seconds\n",
      "Step 12000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 680.7899897098541 seconds\n",
      "Step 12100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 686.4711961746216 seconds\n",
      "Step 12200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 692.024816274643 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 697.5640442371368 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 703.277664899826 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 708.8495781421661 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 714.9754374027252 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 720.5669205188751 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 726.1101379394531 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 731.7787818908691 seconds\n",
      "Step 13000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 737.3340086936951 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 743.0262529850006 seconds\n",
      "Step 13200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 748.5793573856354 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 754.3052518367767 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 759.8499412536621 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 765.4170308113098 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 771.1133246421814 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 776.6520760059357 seconds\n",
      "Step 13800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 782.3717305660248 seconds\n",
      "Step 13900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 788.3748669624329 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 793.8731474876404 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 799.5488178730011 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 805.0909724235535 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 810.8368880748749 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 816.3797569274902 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 822.0870585441589 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 827.6078948974609 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 833.1421041488647 seconds\n",
      "Step 14800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 838.9936938285828 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 845.0170059204102 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 850.7043809890747 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 856.3578882217407 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 861.8811047077179 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 867.5721724033356 seconds\n",
      "Step 15400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 873.2960715293884 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 879.3264861106873 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 884.835547208786 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 890.447329044342 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 896.1392221450806 seconds\n",
      "Step 15900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 901.9465727806091 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 908.0802664756775 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 913.9507586956024 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 919.6411929130554 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 925.239275932312 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 930.7642331123352 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 936.8066973686218 seconds\n",
      "Step 16600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 942.3281955718994 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 948.0843377113342 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 953.6345121860504 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 959.1827626228333 seconds\n",
      "Step 17000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 965.2668035030365 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 970.8107781410217 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 977.1673505306244 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 982.7004380226135 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 988.3624258041382 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 993.8855166435242 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 999.4374523162842 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1005.2706332206726 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1010.7855477333069 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1017.4338436126709 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1022.9700982570648 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1028.497498512268 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1034.1771285533905 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1039.7039353847504 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1045.3899302482605 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1050.9129424095154 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1056.4646155834198 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1062.1542375087738 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1067.6971607208252 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1073.3832304477692 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1078.915206670761 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1084.6644222736359 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1090.225491285324 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1095.7653496265411 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1101.4763867855072 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1107.0297546386719 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1112.7675850391388 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1118.7667953968048 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1124.3330490589142 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1130.0458707809448 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1135.6064257621765 seconds\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0252, grad_fn=<SubBackward0>), time elapsed: 0.06104612350463867 seconds\n",
      "Step 100, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 5.734208106994629 seconds\n",
      "Step 200, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 11.243655681610107 seconds\n",
      "Step 300, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 16.90635848045349 seconds\n",
      "Step 400, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 22.405108213424683 seconds\n",
      "Step 500, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 27.919087886810303 seconds\n",
      "Step 600, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 33.56527519226074 seconds\n",
      "Step 700, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 39.0777690410614 seconds\n",
      "Step 800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 44.70958733558655 seconds\n",
      "Step 900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 50.22515296936035 seconds\n",
      "Step 1000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 55.87300491333008 seconds\n",
      "Step 1100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 61.37632870674133 seconds\n",
      "Step 1200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 66.89590001106262 seconds\n",
      "Step 1300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 72.55580139160156 seconds\n",
      "Step 1400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 78.10654592514038 seconds\n",
      "Step 1500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 83.76004600524902 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 89.26222348213196 seconds\n",
      "Step 1700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 94.7647716999054 seconds\n",
      "Step 1800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 100.39011716842651 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 105.9054548740387 seconds\n",
      "Step 2000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 111.5659122467041 seconds\n",
      "Step 2100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 117.08203721046448 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 122.74281072616577 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 128.22500729560852 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 133.71634936332703 seconds\n",
      "Step 2500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 139.36959624290466 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 144.87011861801147 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 150.5031237602234 seconds\n",
      "Step 2800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 156.02144527435303 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 161.67922067642212 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 167.1741099357605 seconds\n",
      "Step 3100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 172.69362664222717 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 178.32041931152344 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 183.8279025554657 seconds\n",
      "Step 3400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 189.45561265945435 seconds\n",
      "Step 3500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 194.95166087150574 seconds\n",
      "Step 3600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 200.4891641139984 seconds\n",
      "Step 3700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 206.13868761062622 seconds\n",
      "Step 3800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 211.67858266830444 seconds\n",
      "Step 3900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 217.33155989646912 seconds\n",
      "Step 4000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 222.84700298309326 seconds\n",
      "Step 4100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 228.48756742477417 seconds\n",
      "Step 4200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 234.02415776252747 seconds\n",
      "Step 4300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 239.53168082237244 seconds\n",
      "Step 4400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 245.175635099411 seconds\n",
      "Step 4500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 250.69162678718567 seconds\n",
      "Step 4600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 256.3186237812042 seconds\n",
      "Step 4700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 261.85623121261597 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 267.52527141571045 seconds\n",
      "Step 4900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 273.0266168117523 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 278.5254530906677 seconds\n",
      "Step 5100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 284.14933824539185 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.69140362739563 seconds\n",
      "Step 5300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 295.3877422809601 seconds\n",
      "Step 5400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 300.91175532341003 seconds\n",
      "Step 5500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 306.4403042793274 seconds\n",
      "Step 5600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 312.1159977912903 seconds\n",
      "Step 5700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 317.66580390930176 seconds\n",
      "Step 5800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 323.3741171360016 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 328.9233949184418 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 334.5922694206238 seconds\n",
      "Step 6100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 340.1170918941498 seconds\n",
      "Step 6200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 345.66538524627686 seconds\n",
      "Step 6300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 351.3634717464447 seconds\n",
      "Step 6400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 356.8904733657837 seconds\n",
      "Step 6500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 362.5532171726227 seconds\n",
      "Step 6600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 368.07183027267456 seconds\n",
      "Step 6700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 373.7512695789337 seconds\n",
      "Step 6800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 379.2815718650818 seconds\n",
      "Step 6900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 384.8277907371521 seconds\n",
      "Step 7000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 390.498074054718 seconds\n",
      "Step 7100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 396.02922320365906 seconds\n",
      "Step 7200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 401.69940423965454 seconds\n",
      "Step 7300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 407.2379856109619 seconds\n",
      "Step 7400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 412.76017713546753 seconds\n",
      "Step 7500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 418.4042887687683 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 423.9156403541565 seconds\n",
      "Step 7700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 429.56330704689026 seconds\n",
      "Step 7800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 435.06485056877136 seconds\n",
      "Step 7900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 440.760849237442 seconds\n",
      "Step 8000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 446.3157515525818 seconds\n",
      "Step 8100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 451.87577724456787 seconds\n",
      "Step 8200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 457.56143736839294 seconds\n",
      "Step 8300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 463.17198944091797 seconds\n",
      "Step 8400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 468.9332127571106 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 474.55150151252747 seconds\n",
      "Step 8600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 480.25520038604736 seconds\n",
      "Step 8700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 486.53687143325806 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 492.05949783325195 seconds\n",
      "Step 8900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 497.74524569511414 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 503.27477741241455 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 508.9784564971924 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 514.4897887706757 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 519.9938724040985 seconds\n",
      "Step 9400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 525.666699886322 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 531.2187542915344 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 536.911702632904 seconds\n",
      "Step 9700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 542.457923412323 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 550.9693074226379 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 556.53067445755 seconds\n",
      "Step 10000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 562.0457453727722 seconds\n",
      "Step 10100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 567.7209243774414 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 573.2366614341736 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 578.8986282348633 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 584.4017679691315 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 589.9175248146057 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 595.5809977054596 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 601.0804109573364 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 606.7408878803253 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 612.2701210975647 seconds\n",
      "Step 11000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 617.9665439128876 seconds\n",
      "Step 11100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 623.4912898540497 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 629.0605225563049 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 634.7732992172241 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 640.3244531154633 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 646.0181910991669 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 651.5625641345978 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 657.6509099006653 seconds\n",
      "Step 11800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 663.4806363582611 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 669.013991355896 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 674.6753304004669 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 680.1907546520233 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 685.8702132701874 seconds\n",
      "Step 12300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 691.4003713130951 seconds\n",
      "Step 12400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 697.212644815445 seconds\n",
      "Step 12500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 702.8921728134155 seconds\n",
      "Step 12600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 708.4204947948456 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 714.1057593822479 seconds\n",
      "Step 12800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 719.6401913166046 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 725.1491310596466 seconds\n",
      "Step 13000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 730.8135621547699 seconds\n",
      "Step 13100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 736.3296029567719 seconds\n",
      "Step 13200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 742.0219097137451 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 747.5938534736633 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 753.3035175800323 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 758.8491270542145 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 764.3891711235046 seconds\n",
      "Step 13700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 770.0951337814331 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 775.6590394973755 seconds\n",
      "Step 13900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 781.3513419628143 seconds\n",
      "Step 14000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 786.9010636806488 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 792.6151530742645 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 798.330073595047 seconds\n",
      "Step 14300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 803.9007883071899 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 809.6133894920349 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 815.7921347618103 seconds\n",
      "Step 14600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 821.302880525589 seconds\n",
      "Step 14700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 826.962860584259 seconds\n",
      "Step 14800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 832.4860274791718 seconds\n",
      "Step 14900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 838.1615381240845 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 843.7139041423798 seconds\n",
      "Step 15100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 849.4184215068817 seconds\n",
      "Step 15200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 854.9286379814148 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 860.4413890838623 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 866.0908088684082 seconds\n",
      "Step 15500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 871.6336581707001 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 877.2795188426971 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 882.786422252655 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 888.3053781986237 seconds\n",
      "Step 15900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 894.018976688385 seconds\n",
      "Step 16000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 899.546947479248 seconds\n",
      "Step 16100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 905.2550935745239 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 910.7914497852325 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 916.48561835289 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 921.9959180355072 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 927.5466601848602 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 933.2862815856934 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 938.833646774292 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 944.5360510349274 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 950.0856931209564 seconds\n",
      "Step 17000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 955.6079568862915 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 961.3405108451843 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 966.8856563568115 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 972.5947830677032 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 978.1390635967255 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 983.8699791431427 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 989.4102246761322 seconds\n",
      "Step 17700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 994.9983336925507 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1000.8181262016296 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1006.3431444168091 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1012.0386373996735 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1017.5774040222168 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1023.7795171737671 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1029.4762136936188 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1034.9983923435211 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1040.67418050766 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1046.195878982544 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1051.7172181606293 seconds\n",
      "Step 18800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1057.3927767276764 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1062.9259150028229 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1068.598335981369 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1074.1397347450256 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1079.8325889110565 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1085.3824725151062 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1090.9379818439484 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1096.6433193683624 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1102.1663262844086 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1108.1260755062103 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1113.6901490688324 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1119.2373585700989 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1124.994999885559 seconds\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 0.062408447265625 seconds\n",
      "Step 100, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 5.696375370025635 seconds\n",
      "Step 200, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 11.356139898300171 seconds\n",
      "Step 300, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 16.874448776245117 seconds\n",
      "Step 400, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 22.549752712249756 seconds\n",
      "Step 500, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 28.05535912513733 seconds\n",
      "Step 600, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 33.564220666885376 seconds\n",
      "Step 700, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 39.20470595359802 seconds\n",
      "Step 800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 44.69365906715393 seconds\n",
      "Step 900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 50.32076048851013 seconds\n",
      "Step 1000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 55.82115364074707 seconds\n",
      "Step 1100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 61.33740735054016 seconds\n",
      "Step 1200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 66.94639563560486 seconds\n",
      "Step 1300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 72.43548440933228 seconds\n",
      "Step 1400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 78.05605030059814 seconds\n",
      "Step 1500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 83.5745849609375 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 89.18767786026001 seconds\n",
      "Step 1700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 94.65889430046082 seconds\n",
      "Step 1800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 100.16338777542114 seconds\n",
      "Step 1900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 105.81823682785034 seconds\n",
      "Step 2000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 111.33467555046082 seconds\n",
      "Step 2100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 117.00308394432068 seconds\n",
      "Step 2200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 122.5227313041687 seconds\n",
      "Step 2300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 128.17369866371155 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 133.6724145412445 seconds\n",
      "Step 2500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 139.1772608757019 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 144.83270025253296 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 150.35105776786804 seconds\n",
      "Step 2800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 155.9951992034912 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 161.5031032562256 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 167.16948318481445 seconds\n",
      "Step 3100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 172.6698796749115 seconds\n",
      "Step 3200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 178.2107813358307 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 183.8490424156189 seconds\n",
      "Step 3400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 189.34838724136353 seconds\n",
      "Step 3500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 194.99189949035645 seconds\n",
      "Step 3600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 200.49588656425476 seconds\n",
      "Step 3700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 206.0148115158081 seconds\n",
      "Step 3800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 211.67961025238037 seconds\n",
      "Step 3900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 217.21962904930115 seconds\n",
      "Step 4000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 222.84956073760986 seconds\n",
      "Step 4100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 228.3407280445099 seconds\n",
      "Step 4200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 233.98933792114258 seconds\n",
      "Step 4300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 239.49882793426514 seconds\n",
      "Step 4400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 244.9912793636322 seconds\n",
      "Step 4500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 250.60625171661377 seconds\n",
      "Step 4600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 256.1503973007202 seconds\n",
      "Step 4700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 261.8335111141205 seconds\n",
      "Step 4800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 267.3791344165802 seconds\n",
      "Step 4900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 273.0315291881561 seconds\n",
      "Step 5000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 278.54478788375854 seconds\n",
      "Step 5100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 284.051869392395 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.6912434101105 seconds\n",
      "Step 5300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 295.2274441719055 seconds\n",
      "Step 5400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 301.0412838459015 seconds\n",
      "Step 5500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 306.63375067710876 seconds\n",
      "Step 5600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 312.36538100242615 seconds\n",
      "Step 5700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 318.01928544044495 seconds\n",
      "Step 5800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 323.6002857685089 seconds\n",
      "Step 5900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 329.2669520378113 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 335.10569739341736 seconds\n",
      "Step 6100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 340.7737216949463 seconds\n",
      "Step 6200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 346.3189344406128 seconds\n",
      "Step 6300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 351.8970708847046 seconds\n",
      "Step 6400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 357.5411503314972 seconds\n",
      "Step 6500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 363.60115242004395 seconds\n",
      "Step 6600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 369.2760157585144 seconds\n",
      "Step 6700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 374.77955651283264 seconds\n",
      "Step 6800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 380.4307096004486 seconds\n",
      "Step 6900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 385.94662284851074 seconds\n",
      "Step 7000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 391.5105664730072 seconds\n",
      "Step 7100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 397.2053031921387 seconds\n",
      "Step 7200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 402.73500084877014 seconds\n",
      "Step 7300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 409.0199065208435 seconds\n",
      "Step 7400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 414.5445201396942 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 420.08415818214417 seconds\n",
      "Step 7600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 425.7440912723541 seconds\n",
      "Step 7700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 431.2938895225525 seconds\n",
      "Step 7800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 437.077285528183 seconds\n",
      "Step 7900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 442.6989121437073 seconds\n",
      "Step 8000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 448.4052336215973 seconds\n",
      "Step 8100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 454.1779203414917 seconds\n",
      "Step 8200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 459.6638250350952 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 465.3725280761719 seconds\n",
      "Step 8400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 470.89377331733704 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 476.59131383895874 seconds\n",
      "Step 8600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 482.1314516067505 seconds\n",
      "Step 8700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 488.12305974960327 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 493.63715386390686 seconds\n",
      "Step 8900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 499.23192381858826 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 504.88069701194763 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 510.6939241886139 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 516.3896825313568 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 522.0544168949127 seconds\n",
      "Step 9400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 527.6032629013062 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 533.5042760372162 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 539.0410859584808 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 544.8525738716125 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 550.3919503688812 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 556.363445520401 seconds\n",
      "Step 10000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 561.968624830246 seconds\n",
      "Step 10100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 567.5850212574005 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 573.4762668609619 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 579.0196993350983 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 585.4795236587524 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 591.0104584693909 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 596.5257363319397 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 602.2011423110962 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 607.7054314613342 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 613.398533821106 seconds\n",
      "Step 11000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 618.9451117515564 seconds\n",
      "Step 11100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 624.6660213470459 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 630.2028577327728 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 635.7669126987457 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 642.2804756164551 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 647.8170714378357 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 653.476095199585 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 659.0052664279938 seconds\n",
      "Step 11800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 664.7066004276276 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 670.512247800827 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 676.0521655082703 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 682.4996745586395 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 688.0339295864105 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 693.757485628128 seconds\n",
      "Step 12400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 699.279896736145 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 704.8117897510529 seconds\n",
      "Step 12600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 710.4937098026276 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 716.0311918258667 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 721.7476274967194 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 727.3132901191711 seconds\n",
      "Step 13000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 732.8407299518585 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 738.5507116317749 seconds\n",
      "Step 13200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 744.0761578083038 seconds\n",
      "Step 13300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 749.7764909267426 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 755.3499977588654 seconds\n",
      "Step 13500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 761.059056520462 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 766.5749368667603 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 772.1401298046112 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 777.8281714916229 seconds\n",
      "Step 13900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 783.3666870594025 seconds\n",
      "Step 14000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 789.0803029537201 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 794.6418609619141 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 800.1977670192719 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 805.9134199619293 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 812.5471160411835 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 818.2593183517456 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 823.7959759235382 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 829.5334892272949 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 835.0652041435242 seconds\n",
      "Step 14900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 840.6293125152588 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 846.3370172977448 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 852.6689028739929 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 858.3804221153259 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 863.8879086971283 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 869.3940896987915 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 875.0835852622986 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 880.6328010559082 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 886.3191637992859 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 891.8448326587677 seconds\n",
      "Step 15900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 897.5148179531097 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 903.0317509174347 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 908.5612268447876 seconds\n",
      "Step 16200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 914.202761888504 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 919.7247595787048 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 925.4048388004303 seconds\n",
      "Step 16500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 930.9378428459167 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 936.4750442504883 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 942.184654712677 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 947.7223062515259 seconds\n",
      "Step 16900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 953.4235413074493 seconds\n",
      "Step 17000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 958.9469783306122 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 964.495992898941 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 970.2228078842163 seconds\n",
      "Step 17300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 975.9410753250122 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 981.6579887866974 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 987.3075630664825 seconds\n",
      "Step 17600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 993.0174686908722 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 998.5818662643433 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1004.1122608184814 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1009.9207649230957 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1015.4350085258484 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1021.2610034942627 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1026.776380777359 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1032.660828113556 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1038.3512954711914 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1043.8595705032349 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1049.548374414444 seconds\n",
      "Step 18700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1055.0907137393951 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1060.6728491783142 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1066.396125793457 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1071.949861049652 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1077.701786994934 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1083.9018244743347 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1089.6335468292236 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1095.1713914871216 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1100.731683731079 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1106.5035297870636 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1112.1160607337952 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1117.8304777145386 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1123.6561300754547 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1129.2146844863892 seconds\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 0.05976462364196777 seconds\n",
      "Step 100, loss: tensor(0.0083, grad_fn=<SubBackward0>), time elapsed: 5.809586524963379 seconds\n",
      "Step 200, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 11.32264757156372 seconds\n",
      "Step 300, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 16.962754487991333 seconds\n",
      "Step 400, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 22.47853684425354 seconds\n",
      "Step 500, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 28.122157096862793 seconds\n",
      "Step 600, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 33.64934706687927 seconds\n",
      "Step 700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 39.155890226364136 seconds\n",
      "Step 800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 44.795947551727295 seconds\n",
      "Step 900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 50.298255443573 seconds\n",
      "Step 1000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 55.91584014892578 seconds\n",
      "Step 1100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 61.42700242996216 seconds\n",
      "Step 1200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 67.08045244216919 seconds\n",
      "Step 1300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 72.58123207092285 seconds\n",
      "Step 1400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 78.10072040557861 seconds\n",
      "Step 1500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 83.7224509716034 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 89.22169899940491 seconds\n",
      "Step 1700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 94.8899896144867 seconds\n",
      "Step 1800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 100.3969612121582 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 105.90123867988586 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 111.55184674263 seconds\n",
      "Step 2100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 117.06472325325012 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 122.74810338020325 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 128.26601195335388 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 133.9359667301178 seconds\n",
      "Step 2500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 139.42031359672546 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 144.92477250099182 seconds\n",
      "Step 2700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 150.55643343925476 seconds\n",
      "Step 2800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 156.08165979385376 seconds\n",
      "Step 2900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 161.72348046302795 seconds\n",
      "Step 3000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 167.2283239364624 seconds\n",
      "Step 3100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 172.8768548965454 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 178.37858819961548 seconds\n",
      "Step 3300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 183.90256023406982 seconds\n",
      "Step 3400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 189.52807712554932 seconds\n",
      "Step 3500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 195.03188014030457 seconds\n",
      "Step 3600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 200.6740539073944 seconds\n",
      "Step 3700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 206.1911153793335 seconds\n",
      "Step 3800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 211.84776282310486 seconds\n",
      "Step 3900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 217.3700668811798 seconds\n",
      "Step 4000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 222.87488913536072 seconds\n",
      "Step 4100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 228.51852774620056 seconds\n",
      "Step 4200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 234.01812863349915 seconds\n",
      "Step 4300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 239.66209411621094 seconds\n",
      "Step 4400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 245.19171023368835 seconds\n",
      "Step 4500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 250.71590948104858 seconds\n",
      "Step 4600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 256.37010407447815 seconds\n",
      "Step 4700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 261.9004592895508 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 267.5456736087799 seconds\n",
      "Step 4900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 273.0667414665222 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 278.7213418483734 seconds\n",
      "Step 5100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 284.2301585674286 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.74536323547363 seconds\n",
      "Step 5300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 295.3888351917267 seconds\n",
      "Step 5400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 300.8954334259033 seconds\n",
      "Step 5500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 306.5755937099457 seconds\n",
      "Step 5600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 312.1050035953522 seconds\n",
      "Step 5700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 317.7699990272522 seconds\n",
      "Step 5800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 323.25768280029297 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 328.78535199165344 seconds\n",
      "Step 6000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 334.45333766937256 seconds\n",
      "Step 6100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 339.950156211853 seconds\n",
      "Step 6200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 345.5843434333801 seconds\n",
      "Step 6300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 351.0767834186554 seconds\n",
      "Step 6400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 356.57354402542114 seconds\n",
      "Step 6500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 362.22276043891907 seconds\n",
      "Step 6600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 367.73783230781555 seconds\n",
      "Step 6700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 373.3661983013153 seconds\n",
      "Step 6800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 378.874351978302 seconds\n",
      "Step 6900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 384.508811712265 seconds\n",
      "Step 7000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 390.0481927394867 seconds\n",
      "Step 7100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 395.5883390903473 seconds\n",
      "Step 7200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 401.2607045173645 seconds\n",
      "Step 7300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 406.7698905467987 seconds\n",
      "Step 7400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 412.4196426868439 seconds\n",
      "Step 7500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 417.949515581131 seconds\n",
      "Step 7600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 423.6239261627197 seconds\n",
      "Step 7700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 429.1613030433655 seconds\n",
      "Step 7800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 434.6909170150757 seconds\n",
      "Step 7900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 440.34901785850525 seconds\n",
      "Step 8000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 445.8953266143799 seconds\n",
      "Step 8100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 451.5611057281494 seconds\n",
      "Step 8200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 457.10882568359375 seconds\n",
      "Step 8300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 462.6309506893158 seconds\n",
      "Step 8400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 468.28206610679626 seconds\n",
      "Step 8500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 473.7941725254059 seconds\n",
      "Step 8600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 479.46640038490295 seconds\n",
      "Step 8700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 485.00726199150085 seconds\n",
      "Step 8800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 490.70447301864624 seconds\n",
      "Step 8900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 496.2120637893677 seconds\n",
      "Step 9000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 501.74134635925293 seconds\n",
      "Step 9100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 507.4003572463989 seconds\n",
      "Step 9200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 512.91335105896 seconds\n",
      "Step 9300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 518.5798783302307 seconds\n",
      "Step 9400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 524.1065108776093 seconds\n",
      "Step 9500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 529.6429951190948 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 535.3118045330048 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 540.8547384738922 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 546.5418450832367 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 552.1296229362488 seconds\n",
      "Step 10000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 557.814975976944 seconds\n",
      "Step 10100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 563.3574223518372 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 568.9001142978668 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 574.6053383350372 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 580.1724507808685 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 585.8739233016968 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 591.6204581260681 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 597.1847190856934 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 603.0862953662872 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 608.6527094841003 seconds\n",
      "Step 11000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 614.3305974006653 seconds\n",
      "Step 11100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 619.8812961578369 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 625.5981063842773 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 631.4706566333771 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 636.9842014312744 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 642.6877584457397 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 648.1973164081573 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 653.8606839179993 seconds\n",
      "Step 11800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 659.3813669681549 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 664.9133498668671 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 670.6357507705688 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 676.1817166805267 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 681.8598594665527 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 687.753500699997 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 693.42937541008 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 698.944155216217 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 704.4872894287109 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 710.147319316864 seconds\n",
      "Step 12800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 715.6894958019257 seconds\n",
      "Step 12900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 721.37824177742 seconds\n",
      "Step 13000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 726.9223260879517 seconds\n",
      "Step 13100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 732.5756614208221 seconds\n",
      "Step 13200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 738.3206334114075 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 743.8849663734436 seconds\n",
      "Step 13400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 749.5849361419678 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 755.2100439071655 seconds\n",
      "Step 13600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 761.2333796024323 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 766.8138699531555 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 772.3450062274933 seconds\n",
      "Step 13900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 778.0236189365387 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 783.5857994556427 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 789.3931787014008 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 794.9230906963348 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 800.4816346168518 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 806.4294850826263 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 811.9610040187836 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 817.6270923614502 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 823.1551222801208 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 828.8125762939453 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 834.3238263130188 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 839.8569347858429 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 845.7151257991791 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 851.2468118667603 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 856.9983921051025 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 862.9393923282623 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 868.4626414775848 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 874.1329071521759 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 879.639327287674 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 885.3315443992615 seconds\n",
      "Step 15900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 890.8633298873901 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 896.5782256126404 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 902.1314287185669 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 907.681314945221 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 913.6124680042267 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 919.1493232250214 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 924.8423783779144 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 930.37637591362 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 935.9592838287354 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 942.3118922710419 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 947.9203164577484 seconds\n",
      "Step 17000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 953.6250727176666 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 959.1420073509216 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 964.6573147773743 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 970.3173906803131 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 975.8465120792389 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 981.5108513832092 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 987.0350692272186 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 992.7310254573822 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 998.3001310825348 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1003.8164455890656 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1009.5417423248291 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1015.0845959186554 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1020.7926044464111 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1026.3616268634796 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1031.934430360794 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1037.676941871643 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1043.2564463615417 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1048.97469496727 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1054.5201840400696 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1060.1435840129852 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1065.8962247371674 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1071.4344387054443 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1077.132113456726 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1083.4435346126556 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1089.176728963852 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1094.6822063922882 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1100.236656665802 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1105.9095611572266 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1111.442922115326 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1117.1323268413544 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1122.678377866745 seconds\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0250, grad_fn=<SubBackward0>), time elapsed: 0.061799049377441406 seconds\n",
      "Step 100, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 5.660330295562744 seconds\n",
      "Step 200, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 11.341418266296387 seconds\n",
      "Step 300, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 16.84400200843811 seconds\n",
      "Step 400, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 22.479936599731445 seconds\n",
      "Step 500, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 27.972734928131104 seconds\n",
      "Step 600, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 33.606866121292114 seconds\n",
      "Step 700, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 39.133134841918945 seconds\n",
      "Step 800, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 44.65788245201111 seconds\n",
      "Step 900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 50.2734580039978 seconds\n",
      "Step 1000, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 55.77788162231445 seconds\n",
      "Step 1100, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 61.40591788291931 seconds\n",
      "Step 1200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 66.91237020492554 seconds\n",
      "Step 1300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 72.57054924964905 seconds\n",
      "Step 1400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 78.0735821723938 seconds\n",
      "Step 1500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 83.5859203338623 seconds\n",
      "Step 1600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 89.21130442619324 seconds\n",
      "Step 1700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 94.71198868751526 seconds\n",
      "Step 1800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 100.3637261390686 seconds\n",
      "Step 1900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 105.8610270023346 seconds\n",
      "Step 2000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 111.35819053649902 seconds\n",
      "Step 2100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 116.98390054702759 seconds\n",
      "Step 2200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 122.49946522712708 seconds\n",
      "Step 2300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 128.14981269836426 seconds\n",
      "Step 2400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 133.6891098022461 seconds\n",
      "Step 2500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 139.33144164085388 seconds\n",
      "Step 2600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 144.8365375995636 seconds\n",
      "Step 2700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 150.34799909591675 seconds\n",
      "Step 2800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 155.9775846004486 seconds\n",
      "Step 2900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 161.5023069381714 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 167.14320492744446 seconds\n",
      "Step 3100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 172.64663672447205 seconds\n",
      "Step 3200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 178.31255960464478 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 183.83059668540955 seconds\n",
      "Step 3400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 189.34862685203552 seconds\n",
      "Step 3500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 195.01672267913818 seconds\n",
      "Step 3600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 200.52543878555298 seconds\n",
      "Step 3700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 206.1606080532074 seconds\n",
      "Step 3800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 211.6711084842682 seconds\n",
      "Step 3900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 217.31331181526184 seconds\n",
      "Step 4000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 222.8430380821228 seconds\n",
      "Step 4100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 228.35319757461548 seconds\n",
      "Step 4200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 234.0042462348938 seconds\n",
      "Step 4300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 239.52733278274536 seconds\n",
      "Step 4400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 245.1570703983307 seconds\n",
      "Step 4500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 250.675213098526 seconds\n",
      "Step 4600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 256.18557834625244 seconds\n",
      "Step 4700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 261.8235104084015 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 267.3484137058258 seconds\n",
      "Step 4900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 272.9973909854889 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 278.51500701904297 seconds\n",
      "Step 5100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 284.19960737228394 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.7401623725891 seconds\n",
      "Step 5300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 295.460077047348 seconds\n",
      "Step 5400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 301.21001982688904 seconds\n",
      "Step 5500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 307.832879781723 seconds\n",
      "Step 5600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 313.52512669563293 seconds\n",
      "Step 5700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 319.0414996147156 seconds\n",
      "Step 5800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 324.6967785358429 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 330.19734382629395 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 335.7126712799072 seconds\n",
      "Step 6100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 341.3656198978424 seconds\n",
      "Step 6200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 346.90833568573 seconds\n",
      "Step 6300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 352.56276631355286 seconds\n",
      "Step 6400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 358.0993494987488 seconds\n",
      "Step 6500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 363.63742232322693 seconds\n",
      "Step 6600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 370.0353240966797 seconds\n",
      "Step 6700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 375.5560111999512 seconds\n",
      "Step 6800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 381.19084191322327 seconds\n",
      "Step 6900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 386.7077338695526 seconds\n",
      "Step 7000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 392.3817820549011 seconds\n",
      "Step 7100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 397.89482402801514 seconds\n",
      "Step 7200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 403.4397830963135 seconds\n",
      "Step 7300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 409.11099553108215 seconds\n",
      "Step 7400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 414.64569306373596 seconds\n",
      "Step 7500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 420.32745337486267 seconds\n",
      "Step 7600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 425.8335521221161 seconds\n",
      "Step 7700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 431.5099992752075 seconds\n",
      "Step 7800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 437.07602071762085 seconds\n",
      "Step 7900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 442.6216678619385 seconds\n",
      "Step 8000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 448.31846475601196 seconds\n",
      "Step 8100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 453.88017892837524 seconds\n",
      "Step 8200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 459.55634784698486 seconds\n",
      "Step 8300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 465.12262654304504 seconds\n",
      "Step 8400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 470.7962911128998 seconds\n",
      "Step 8500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 477.00541591644287 seconds\n",
      "Step 8600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 482.57770109176636 seconds\n",
      "Step 8700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 488.2362823486328 seconds\n",
      "Step 8800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 493.74257946014404 seconds\n",
      "Step 8900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 499.4379527568817 seconds\n",
      "Step 9000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 504.9460859298706 seconds\n",
      "Step 9100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 510.4827947616577 seconds\n",
      "Step 9200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 516.1157612800598 seconds\n",
      "Step 9300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 521.6283257007599 seconds\n",
      "Step 9400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 528.0898678302765 seconds\n",
      "Step 9500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 533.608996629715 seconds\n",
      "Step 9600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 539.1386592388153 seconds\n",
      "Step 9700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 544.810733795166 seconds\n",
      "Step 9800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 550.3305387496948 seconds\n",
      "Step 9900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 556.0183525085449 seconds\n",
      "Step 10000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 561.539644241333 seconds\n",
      "Step 10100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 567.1914527416229 seconds\n",
      "Step 10200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 572.7143774032593 seconds\n",
      "Step 10300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 578.275105714798 seconds\n",
      "Step 10400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 583.989669084549 seconds\n",
      "Step 10500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 589.5851624011993 seconds\n",
      "Step 10600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 595.2860250473022 seconds\n",
      "Step 10700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 600.824627161026 seconds\n",
      "Step 10800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 606.3589744567871 seconds\n",
      "Step 10900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 612.0505800247192 seconds\n",
      "Step 11000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 617.6011657714844 seconds\n",
      "Step 11100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 623.7899308204651 seconds\n",
      "Step 11200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 629.3196806907654 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 634.9811079502106 seconds\n",
      "Step 11400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 640.5906221866608 seconds\n",
      "Step 11500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 646.1348209381104 seconds\n",
      "Step 11600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 651.8354620933533 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 657.3812236785889 seconds\n",
      "Step 11800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 663.0984754562378 seconds\n",
      "Step 11900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 668.714750289917 seconds\n",
      "Step 12000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 674.2591607570648 seconds\n",
      "Step 12100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 680.37104845047 seconds\n",
      "Step 12200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 685.8985962867737 seconds\n",
      "Step 12300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 691.5675346851349 seconds\n",
      "Step 12400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 697.1311497688293 seconds\n",
      "Step 12500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 702.8532121181488 seconds\n",
      "Step 12600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 708.4333341121674 seconds\n",
      "Step 12700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 713.9993612766266 seconds\n",
      "Step 12800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 719.7555305957794 seconds\n",
      "Step 12900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 725.34046626091 seconds\n",
      "Step 13000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 731.0693123340607 seconds\n",
      "Step 13100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 737.2306094169617 seconds\n",
      "Step 13200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 742.7955956459045 seconds\n",
      "Step 13300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 748.4694221019745 seconds\n",
      "Step 13400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 754.0130026340485 seconds\n",
      "Step 13500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 759.6879122257233 seconds\n",
      "Step 13600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 765.2382304668427 seconds\n",
      "Step 13700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 770.9484333992004 seconds\n",
      "Step 13800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 776.5252594947815 seconds\n",
      "Step 13900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 782.0712270736694 seconds\n",
      "Step 14000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 787.7569055557251 seconds\n",
      "Step 14100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 793.280636548996 seconds\n",
      "Step 14200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 798.9899642467499 seconds\n",
      "Step 14300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 804.5455746650696 seconds\n",
      "Step 14400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 810.1033136844635 seconds\n",
      "Step 14500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 815.7945032119751 seconds\n",
      "Step 14600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 821.3442947864532 seconds\n",
      "Step 14700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 827.0399839878082 seconds\n",
      "Step 14800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 832.6054513454437 seconds\n",
      "Step 14900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 838.3030550479889 seconds\n",
      "Step 15000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 844.1448571681976 seconds\n",
      "Step 15100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 849.7440888881683 seconds\n",
      "Step 15200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 855.4814622402191 seconds\n",
      "Step 15300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 861.0702221393585 seconds\n",
      "Step 15400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 867.111118555069 seconds\n",
      "Step 15500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 872.6824307441711 seconds\n",
      "Step 15600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 878.2149593830109 seconds\n",
      "Step 15700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 883.9048316478729 seconds\n",
      "Step 15800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 890.0393249988556 seconds\n",
      "Step 15900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 895.7453844547272 seconds\n",
      "Step 16000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 901.2815432548523 seconds\n",
      "Step 16100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 906.959956407547 seconds\n",
      "Step 16200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 912.6565179824829 seconds\n",
      "Step 16300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 918.59921002388 seconds\n",
      "Step 16400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 924.3131613731384 seconds\n",
      "Step 16500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 929.8558692932129 seconds\n",
      "Step 16600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 935.5197038650513 seconds\n",
      "Step 16700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 941.0682702064514 seconds\n",
      "Step 16800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 946.6302216053009 seconds\n",
      "Step 16900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 952.3437156677246 seconds\n",
      "Step 17000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 958.3360795974731 seconds\n",
      "Step 17100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 964.0122828483582 seconds\n",
      "Step 17200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 969.6032409667969 seconds\n",
      "Step 17300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 975.2918267250061 seconds\n",
      "Step 17400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 980.8757512569427 seconds\n",
      "Step 17500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 986.6980590820312 seconds\n",
      "Step 17600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 992.4060673713684 seconds\n",
      "Step 17700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 998.0267913341522 seconds\n",
      "Step 17800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1003.7246916294098 seconds\n",
      "Step 17900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1009.3897104263306 seconds\n",
      "Step 18000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1014.928740978241 seconds\n",
      "Step 18100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1021.4289302825928 seconds\n",
      "Step 18200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1026.961228132248 seconds\n",
      "Step 18300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1032.6658635139465 seconds\n",
      "Step 18400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1038.1978125572205 seconds\n",
      "Step 18500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1043.7438399791718 seconds\n",
      "Step 18600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1049.419662952423 seconds\n",
      "Step 18700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1054.962968826294 seconds\n",
      "Step 18800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1060.6817548274994 seconds\n",
      "Step 18900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1066.2147316932678 seconds\n",
      "Step 19000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1071.9323184490204 seconds\n",
      "Step 19100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1077.5120632648468 seconds\n",
      "Step 19200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1083.0840439796448 seconds\n",
      "Step 19300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1088.809496164322 seconds\n",
      "Step 19400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1095.9188432693481 seconds\n",
      "Step 19500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1101.6735444068909 seconds\n",
      "Step 19600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1107.2107396125793 seconds\n",
      "Step 19700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1112.755295753479 seconds\n",
      "Step 19800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1118.449541568756 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1124.0046863555908 seconds\n",
      "Step 20000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1129.7078301906586 seconds\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 0.06112265586853027 seconds\n",
      "Step 100, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 5.6908814907073975 seconds\n",
      "Step 200, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 11.253300905227661 seconds\n",
      "Step 300, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 16.877676963806152 seconds\n",
      "Step 400, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 22.41338610649109 seconds\n",
      "Step 500, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 28.03304672241211 seconds\n",
      "Step 600, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 33.51479983329773 seconds\n",
      "Step 700, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 39.17566895484924 seconds\n",
      "Step 800, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 44.6731059551239 seconds\n",
      "Step 900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 50.18402409553528 seconds\n",
      "Step 1000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 55.817137241363525 seconds\n",
      "Step 1100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 61.31541442871094 seconds\n",
      "Step 1200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 66.97697710990906 seconds\n",
      "Step 1300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 72.50171113014221 seconds\n",
      "Step 1400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 78.15946841239929 seconds\n",
      "Step 1500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 83.6525981426239 seconds\n",
      "Step 1600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 89.15808701515198 seconds\n",
      "Step 1700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 94.79078030586243 seconds\n",
      "Step 1800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 100.31637978553772 seconds\n",
      "Step 1900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 105.96645379066467 seconds\n",
      "Step 2000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 111.49436521530151 seconds\n",
      "Step 2100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 117.1558837890625 seconds\n",
      "Step 2200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 122.63043403625488 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 128.1326949596405 seconds\n",
      "Step 2400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 133.76214003562927 seconds\n",
      "Step 2500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 139.2493588924408 seconds\n",
      "Step 2600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 144.87441444396973 seconds\n",
      "Step 2700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 150.392094373703 seconds\n",
      "Step 2800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 155.90868210792542 seconds\n",
      "Step 2900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 161.58039355278015 seconds\n",
      "Step 3000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 167.081556558609 seconds\n",
      "Step 3100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 172.73565459251404 seconds\n",
      "Step 3200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 178.22690439224243 seconds\n",
      "Step 3300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 183.86473536491394 seconds\n",
      "Step 3400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 189.35118961334229 seconds\n",
      "Step 3500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 194.90471696853638 seconds\n",
      "Step 3600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 200.542720079422 seconds\n",
      "Step 3700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 206.06767988204956 seconds\n",
      "Step 3800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 211.70816946029663 seconds\n",
      "Step 3900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 217.23409056663513 seconds\n",
      "Step 4000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 222.9183418750763 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 228.41643166542053 seconds\n",
      "Step 4200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 233.9362223148346 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 239.57948565483093 seconds\n",
      "Step 4400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 245.09160661697388 seconds\n",
      "Step 4500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 250.73205494880676 seconds\n",
      "Step 4600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 256.28328251838684 seconds\n",
      "Step 4700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 261.94808173179626 seconds\n",
      "Step 4800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 267.4451491832733 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 272.942346572876 seconds\n",
      "Step 5000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 278.7833602428436 seconds\n",
      "Step 5100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 284.31241631507874 seconds\n",
      "Step 5200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 289.96419072151184 seconds\n",
      "Step 5300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 295.4799373149872 seconds\n",
      "Step 5400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 301.03256821632385 seconds\n",
      "Step 5500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 306.71249198913574 seconds\n",
      "Step 5600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 313.09924149513245 seconds\n",
      "Step 5700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 318.74930572509766 seconds\n",
      "Step 5800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 324.25660014152527 seconds\n",
      "Step 5900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 329.9054524898529 seconds\n",
      "Step 6000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 335.40904927253723 seconds\n",
      "Step 6100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 340.9369385242462 seconds\n",
      "Step 6200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 346.62188267707825 seconds\n",
      "Step 6300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 352.1423535346985 seconds\n",
      "Step 6400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 357.7920687198639 seconds\n",
      "Step 6500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 363.3174641132355 seconds\n",
      "Step 6600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 368.9721426963806 seconds\n",
      "Step 6700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 374.4760067462921 seconds\n",
      "Step 6800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 379.99012088775635 seconds\n",
      "Step 6900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 385.643741607666 seconds\n",
      "Step 7000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 391.1918406486511 seconds\n",
      "Step 7100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 396.8559534549713 seconds\n",
      "Step 7200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 402.3805627822876 seconds\n",
      "Step 7300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 407.9272060394287 seconds\n",
      "Step 7400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 413.5831370353699 seconds\n",
      "Step 7500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 419.0840301513672 seconds\n",
      "Step 7600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 424.73356533050537 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 430.26845383644104 seconds\n",
      "Step 7800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 435.96921730041504 seconds\n",
      "Step 7900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 442.13990020751953 seconds\n",
      "Step 8000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 447.80983304977417 seconds\n",
      "Step 8100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 453.5088768005371 seconds\n",
      "Step 8200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 459.06612610816956 seconds\n",
      "Step 8300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 464.7229058742523 seconds\n",
      "Step 8400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 470.28172612190247 seconds\n",
      "Step 8500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 475.9722673892975 seconds\n",
      "Step 8600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 481.5122084617615 seconds\n",
      "Step 8700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 487.07589197158813 seconds\n",
      "Step 8800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 492.75380539894104 seconds\n",
      "Step 8900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 498.312997341156 seconds\n",
      "Step 9000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 504.0104477405548 seconds\n",
      "Step 9100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 509.52489590644836 seconds\n",
      "Step 9200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 515.1092052459717 seconds\n",
      "Step 9300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 520.8030064105988 seconds\n",
      "Step 9400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 526.3554470539093 seconds\n",
      "Step 9500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 532.043062210083 seconds\n",
      "Step 9600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 537.5941445827484 seconds\n",
      "Step 9700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 543.3008749485016 seconds\n",
      "Step 9800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 548.8534915447235 seconds\n",
      "Step 9900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 554.3870849609375 seconds\n",
      "Step 10000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 560.0944769382477 seconds\n",
      "Step 10100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 565.6457297801971 seconds\n",
      "Step 10200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 571.3359162807465 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 576.8693356513977 seconds\n",
      "Step 10400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 582.4192280769348 seconds\n",
      "Step 10500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 588.1041910648346 seconds\n",
      "Step 10600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 593.6185824871063 seconds\n",
      "Step 10700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 599.3015546798706 seconds\n",
      "Step 10800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 604.8150551319122 seconds\n",
      "Step 10900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 610.4908089637756 seconds\n",
      "Step 11000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 616.0177667140961 seconds\n",
      "Step 11100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 621.5889620780945 seconds\n",
      "Step 11200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 627.2767026424408 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 632.8309011459351 seconds\n",
      "Step 11400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 638.529034614563 seconds\n",
      "Step 11500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 644.5270462036133 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 650.2873976230621 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 655.9683110713959 seconds\n",
      "Step 11800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 661.5073118209839 seconds\n",
      "Step 11900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 667.2088575363159 seconds\n",
      "Step 12000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 672.777704000473 seconds\n",
      "Step 12100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 678.4796140193939 seconds\n",
      "Step 12200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 684.0217189788818 seconds\n",
      "Step 12300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 689.5660378932953 seconds\n",
      "Step 12400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 695.2492597103119 seconds\n",
      "Step 12500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 700.7891080379486 seconds\n",
      "Step 12600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 706.4721493721008 seconds\n",
      "Step 12700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 712.0518372058868 seconds\n",
      "Step 12800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 717.6230647563934 seconds\n",
      "Step 12900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 724.5221168994904 seconds\n",
      "Step 13000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 730.2406938076019 seconds\n",
      "Step 13100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 735.9319496154785 seconds\n",
      "Step 13200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 741.4710423946381 seconds\n",
      "Step 13300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 747.1395437717438 seconds\n",
      "Step 13400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 752.6574084758759 seconds\n",
      "Step 13500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 758.1944925785065 seconds\n",
      "Step 13600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 763.8698241710663 seconds\n",
      "Step 13700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 769.3798594474792 seconds\n",
      "Step 13800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 775.0618126392365 seconds\n",
      "Step 13900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 780.5935757160187 seconds\n",
      "Step 14000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 786.1173238754272 seconds\n",
      "Step 14100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 791.8119449615479 seconds\n",
      "Step 14200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 797.345109462738 seconds\n",
      "Step 14300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 803.06614112854 seconds\n",
      "Step 14400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 808.6055293083191 seconds\n",
      "Step 14500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 814.2987298965454 seconds\n",
      "Step 14600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 819.8265981674194 seconds\n",
      "Step 14700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 825.3613367080688 seconds\n",
      "Step 14800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 831.0511152744293 seconds\n",
      "Step 14900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 836.5973334312439 seconds\n",
      "Step 15000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 842.3046855926514 seconds\n",
      "Step 15100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 847.8324384689331 seconds\n",
      "Step 15200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 853.3770852088928 seconds\n",
      "Step 15300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 859.0734372138977 seconds\n",
      "Step 15400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 864.630752325058 seconds\n",
      "Step 15500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 870.3100626468658 seconds\n",
      "Step 15600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 875.8544249534607 seconds\n",
      "Step 15700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 881.5466902256012 seconds\n",
      "Step 15800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 887.1100120544434 seconds\n",
      "Step 15900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 892.7027177810669 seconds\n",
      "Step 16000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 898.4006206989288 seconds\n",
      "Step 16100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 903.9397990703583 seconds\n",
      "Step 16200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 909.6600046157837 seconds\n",
      "Step 16300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 915.2060735225677 seconds\n",
      "Step 16400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 920.7318696975708 seconds\n",
      "Step 16500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 926.4629645347595 seconds\n",
      "Step 16600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 932.350263595581 seconds\n",
      "Step 16700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 938.0861051082611 seconds\n",
      "Step 16800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 943.6199135780334 seconds\n",
      "Step 16900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 949.1571185588837 seconds\n",
      "Step 17000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 954.8675410747528 seconds\n",
      "Step 17100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 960.387957572937 seconds\n",
      "Step 17200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 966.0365252494812 seconds\n",
      "Step 17300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 971.5534858703613 seconds\n",
      "Step 17400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 977.6738111972809 seconds\n",
      "Step 17500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 983.2184689044952 seconds\n",
      "Step 17600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 988.7656257152557 seconds\n",
      "Step 17700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 994.4567379951477 seconds\n",
      "Step 17800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 999.9815113544464 seconds\n",
      "Step 17900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1005.6468670368195 seconds\n",
      "Step 18000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1011.1681079864502 seconds\n",
      "Step 18100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1016.7290353775024 seconds\n",
      "Step 18200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1022.4385027885437 seconds\n",
      "Step 18300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1027.9890275001526 seconds\n",
      "Step 18400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1033.7388770580292 seconds\n",
      "Step 18500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1039.3075449466705 seconds\n",
      "Step 18600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1045.0683472156525 seconds\n",
      "Step 18700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1051.0265078544617 seconds\n",
      "Step 18800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1056.5481066703796 seconds\n",
      "Step 18900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1062.230807542801 seconds\n",
      "Step 19000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1067.7657644748688 seconds\n",
      "Step 19100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1073.4520344734192 seconds\n",
      "Step 19200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1079.0016479492188 seconds\n",
      "Step 19300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1084.5529115200043 seconds\n",
      "Step 19400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1090.2863986492157 seconds\n",
      "Step 19500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1095.8261258602142 seconds\n",
      "Step 19600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1101.5595638751984 seconds\n",
      "Step 19700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1107.5469899177551 seconds\n",
      "Step 19800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1113.3721668720245 seconds\n",
      "Step 19900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1119.0896763801575 seconds\n",
      "Step 20000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1124.6610758304596 seconds\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0268, grad_fn=<SubBackward0>), time elapsed: 0.06131720542907715 seconds\n",
      "Step 100, loss: tensor(0.0200, grad_fn=<SubBackward0>), time elapsed: 5.839563608169556 seconds\n",
      "Step 200, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 11.38443398475647 seconds\n",
      "Step 300, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 17.046024322509766 seconds\n",
      "Step 400, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 22.55169177055359 seconds\n",
      "Step 500, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 28.0757577419281 seconds\n",
      "Step 600, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 33.7706573009491 seconds\n",
      "Step 700, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 39.29278326034546 seconds\n",
      "Step 800, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 44.92242741584778 seconds\n",
      "Step 900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 50.447142124176025 seconds\n",
      "Step 1000, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 56.091723680496216 seconds\n",
      "Step 1100, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 61.616432905197144 seconds\n",
      "Step 1200, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 67.13268160820007 seconds\n",
      "Step 1300, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 72.78461956977844 seconds\n",
      "Step 1400, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 78.31994485855103 seconds\n",
      "Step 1500, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 83.9662516117096 seconds\n",
      "Step 1600, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 89.45564222335815 seconds\n",
      "Step 1700, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 94.95372128486633 seconds\n",
      "Step 1800, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 100.57231950759888 seconds\n",
      "Step 1900, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 106.08648037910461 seconds\n",
      "Step 2000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 111.7385573387146 seconds\n",
      "Step 2100, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 117.25725102424622 seconds\n",
      "Step 2200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 122.90696167945862 seconds\n",
      "Step 2300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 128.42523550987244 seconds\n",
      "Step 2400, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 133.92213582992554 seconds\n",
      "Step 2500, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 139.5522484779358 seconds\n",
      "Step 2600, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 145.06816172599792 seconds\n",
      "Step 2700, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 150.7189700603485 seconds\n",
      "Step 2800, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 156.23301005363464 seconds\n",
      "Step 2900, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 161.88010048866272 seconds\n",
      "Step 3000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 167.40494179725647 seconds\n",
      "Step 3100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 172.93349647521973 seconds\n",
      "Step 3200, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 178.55584120750427 seconds\n",
      "Step 3300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 184.07965397834778 seconds\n",
      "Step 3400, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 189.7369782924652 seconds\n",
      "Step 3500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 195.2349066734314 seconds\n",
      "Step 3600, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 200.91847276687622 seconds\n",
      "Step 3700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 206.42460346221924 seconds\n",
      "Step 3800, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 211.95678758621216 seconds\n",
      "Step 3900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 217.5994291305542 seconds\n",
      "Step 4000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 223.1082468032837 seconds\n",
      "Step 4100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 228.78735089302063 seconds\n",
      "Step 4200, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 234.29518055915833 seconds\n",
      "Step 4300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 239.77857637405396 seconds\n",
      "Step 4400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 245.40501761436462 seconds\n",
      "Step 4500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 250.90009760856628 seconds\n",
      "Step 4600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 256.5462644100189 seconds\n",
      "Step 4700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 262.0764195919037 seconds\n",
      "Step 4800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 267.86612939834595 seconds\n",
      "Step 4900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 273.42171907424927 seconds\n",
      "Step 5000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 279.6571328639984 seconds\n",
      "Step 5100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 285.3035960197449 seconds\n",
      "Step 5200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 290.8480770587921 seconds\n",
      "Step 5300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 296.5155580043793 seconds\n",
      "Step 5400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 302.0284686088562 seconds\n",
      "Step 5500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 307.6857075691223 seconds\n",
      "Step 5600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 313.2034375667572 seconds\n",
      "Step 5700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 318.7359399795532 seconds\n",
      "Step 5800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 325.1374452114105 seconds\n",
      "Step 5900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 330.65584897994995 seconds\n",
      "Step 6000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 336.310396194458 seconds\n",
      "Step 6100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 341.8141996860504 seconds\n",
      "Step 6200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 347.33398246765137 seconds\n",
      "Step 6300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 353.04497957229614 seconds\n",
      "Step 6400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 358.5775668621063 seconds\n",
      "Step 6500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 364.2717969417572 seconds\n",
      "Step 6600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 369.8028004169464 seconds\n",
      "Step 6700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 375.46446776390076 seconds\n",
      "Step 6800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 381.0394403934479 seconds\n",
      "Step 6900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 386.57348370552063 seconds\n",
      "Step 7000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 392.2561309337616 seconds\n",
      "Step 7100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 397.8068974018097 seconds\n",
      "Step 7200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 403.48610067367554 seconds\n",
      "Step 7300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 409.18378925323486 seconds\n",
      "Step 7400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 414.88511538505554 seconds\n",
      "Step 7500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 421.13614797592163 seconds\n",
      "Step 7600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 426.6577410697937 seconds\n",
      "Step 7700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 432.33440923690796 seconds\n",
      "Step 7800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 437.86584639549255 seconds\n",
      "Step 7900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 443.57696747779846 seconds\n",
      "Step 8000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 449.1400074958801 seconds\n",
      "Step 8100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 454.8627619743347 seconds\n",
      "Step 8200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 460.3916103839874 seconds\n",
      "Step 8300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 465.9793174266815 seconds\n",
      "Step 8400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 471.6895558834076 seconds\n",
      "Step 8500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 477.9176287651062 seconds\n",
      "Step 8600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 483.5991325378418 seconds\n",
      "Step 8700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 489.1567442417145 seconds\n",
      "Step 8800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 494.6742241382599 seconds\n",
      "Step 8900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 500.3187026977539 seconds\n",
      "Step 9000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 505.9400351047516 seconds\n",
      "Step 9100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 511.67027020454407 seconds\n",
      "Step 9200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 517.1941056251526 seconds\n",
      "Step 9300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 522.9372363090515 seconds\n",
      "Step 9400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 528.4955642223358 seconds\n",
      "Step 9500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 535.0454368591309 seconds\n",
      "Step 9600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 540.7260000705719 seconds\n",
      "Step 9700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 546.2485568523407 seconds\n",
      "Step 9800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 551.8997542858124 seconds\n",
      "Step 9900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 557.413946390152 seconds\n",
      "Step 10000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 562.9285840988159 seconds\n",
      "Step 10100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 568.6069912910461 seconds\n",
      "Step 10200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 574.1355328559875 seconds\n",
      "Step 10300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 579.8190762996674 seconds\n",
      "Step 10400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 585.3363392353058 seconds\n",
      "Step 10500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 591.0576856136322 seconds\n",
      "Step 10600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 596.5686287879944 seconds\n",
      "Step 10700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 602.0881972312927 seconds\n",
      "Step 10800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 608.197968006134 seconds\n",
      "Step 10900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 613.7399406433105 seconds\n",
      "Step 11000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 619.4419963359833 seconds\n",
      "Step 11100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 624.9993886947632 seconds\n",
      "Step 11200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 630.5330069065094 seconds\n",
      "Step 11300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 636.1982622146606 seconds\n",
      "Step 11400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 641.7926287651062 seconds\n",
      "Step 11500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 647.4810819625854 seconds\n",
      "Step 11600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 653.00790143013 seconds\n",
      "Step 11700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 658.753500699997 seconds\n",
      "Step 11800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 664.3609337806702 seconds\n",
      "Step 11900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 670.6820402145386 seconds\n",
      "Step 12000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 676.4153277873993 seconds\n",
      "Step 12100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 681.9458644390106 seconds\n",
      "Step 12200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 687.6461405754089 seconds\n",
      "Step 12300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 693.1773157119751 seconds\n",
      "Step 12400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 698.7187836170197 seconds\n",
      "Step 12500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 704.4116010665894 seconds\n",
      "Step 12600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 709.9348213672638 seconds\n",
      "Step 12700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 715.6187319755554 seconds\n",
      "Step 12800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 721.2064707279205 seconds\n",
      "Step 12900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 726.8809566497803 seconds\n",
      "Step 13000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 732.410964012146 seconds\n",
      "Step 13100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 737.9129366874695 seconds\n",
      "Step 13200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 743.6152212619781 seconds\n",
      "Step 13300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 749.1899085044861 seconds\n",
      "Step 13400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 754.9095878601074 seconds\n",
      "Step 13500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 760.4685769081116 seconds\n",
      "Step 13600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 766.0448834896088 seconds\n",
      "Step 13700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 771.7226023674011 seconds\n",
      "Step 13800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 777.2999401092529 seconds\n",
      "Step 13900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 783.0130889415741 seconds\n",
      "Step 14000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 788.5712673664093 seconds\n",
      "Step 14100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 794.7644157409668 seconds\n",
      "Step 14200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 800.462112903595 seconds\n",
      "Step 14300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 806.0109021663666 seconds\n",
      "Step 14400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 811.7483701705933 seconds\n",
      "Step 14500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 817.27734541893 seconds\n",
      "Step 14600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 822.9725635051727 seconds\n",
      "Step 14700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 828.5277671813965 seconds\n",
      "Step 14800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 834.1033406257629 seconds\n",
      "Step 14900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 839.8630497455597 seconds\n",
      "Step 15000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 845.4546074867249 seconds\n",
      "Step 15100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 851.2190372943878 seconds\n",
      "Step 15200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 856.861421585083 seconds\n",
      "Step 15300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 862.5458006858826 seconds\n",
      "Step 15400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 868.2785699367523 seconds\n",
      "Step 15500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 873.9349203109741 seconds\n",
      "Step 15600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 880.2749540805817 seconds\n",
      "Step 15700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 885.7829241752625 seconds\n",
      "Step 15800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 891.4853868484497 seconds\n",
      "Step 15900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 896.9966342449188 seconds\n",
      "Step 16000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 902.5558412075043 seconds\n",
      "Step 16100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 908.2156422138214 seconds\n",
      "Step 16200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 913.7549374103546 seconds\n",
      "Step 16300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 919.4506907463074 seconds\n",
      "Step 16400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 924.9835822582245 seconds\n",
      "Step 16500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 931.0790424346924 seconds\n",
      "Step 16600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 936.8122601509094 seconds\n",
      "Step 16700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 942.3285694122314 seconds\n",
      "Step 16800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 948.0137917995453 seconds\n",
      "Step 16900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 953.6754026412964 seconds\n",
      "Step 17000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 959.4469728469849 seconds\n",
      "Step 17100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 965.0073657035828 seconds\n",
      "Step 17200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 971.2529299259186 seconds\n",
      "Step 17300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 976.9622611999512 seconds\n",
      "Step 17400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 982.5097014904022 seconds\n",
      "Step 17500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 988.1981782913208 seconds\n",
      "Step 17600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 993.75603556633 seconds\n",
      "Step 17700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 999.3110389709473 seconds\n",
      "Step 17800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1005.0701427459717 seconds\n",
      "Step 17900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1010.6450095176697 seconds\n",
      "Step 18000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1016.3832488059998 seconds\n",
      "Step 18100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1021.9711904525757 seconds\n",
      "Step 18200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1028.5229308605194 seconds\n",
      "Step 18300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1034.0670578479767 seconds\n",
      "Step 18400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1039.5846590995789 seconds\n",
      "Step 18500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1045.258157491684 seconds\n",
      "Step 18600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1050.7683365345001 seconds\n",
      "Step 18700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1056.4794862270355 seconds\n",
      "Step 18800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1062.010468006134 seconds\n",
      "Step 18900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1067.5431339740753 seconds\n",
      "Step 19000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1073.2394423484802 seconds\n",
      "Step 19100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1078.7638280391693 seconds\n",
      "Step 19200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1084.469209909439 seconds\n",
      "Step 19300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1089.9995205402374 seconds\n",
      "Step 19400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1095.524052619934 seconds\n",
      "Step 19500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1101.2212352752686 seconds\n",
      "Step 19600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1106.7711369991302 seconds\n",
      "Step 19700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1112.5334780216217 seconds\n",
      "Step 19800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1118.0958774089813 seconds\n",
      "Step 19900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1123.8363943099976 seconds\n",
      "Step 20000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1129.4083988666534 seconds\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0203, grad_fn=<SubBackward0>), time elapsed: 0.06705641746520996 seconds\n",
      "Step 100, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 5.6780173778533936 seconds\n",
      "Step 200, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 11.353258848190308 seconds\n",
      "Step 300, loss: tensor(0.0090, grad_fn=<SubBackward0>), time elapsed: 16.883031845092773 seconds\n",
      "Step 400, loss: tensor(0.0077, grad_fn=<SubBackward0>), time elapsed: 22.55505633354187 seconds\n",
      "Step 500, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 28.061743021011353 seconds\n",
      "Step 600, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 33.571738481521606 seconds\n",
      "Step 700, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 39.20498561859131 seconds\n",
      "Step 800, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 44.7062246799469 seconds\n",
      "Step 900, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 50.34315514564514 seconds\n",
      "Step 1000, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 55.83336138725281 seconds\n",
      "Step 1100, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 61.48166298866272 seconds\n",
      "Step 1200, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 66.98461055755615 seconds\n",
      "Step 1300, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 72.49153590202332 seconds\n",
      "Step 1400, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 78.11459541320801 seconds\n",
      "Step 1500, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 83.6406421661377 seconds\n",
      "Step 1600, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 89.26758170127869 seconds\n",
      "Step 1700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 94.77395558357239 seconds\n",
      "Step 1800, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 100.4152364730835 seconds\n",
      "Step 1900, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 105.91846442222595 seconds\n",
      "Step 2000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 111.46080112457275 seconds\n",
      "Step 2100, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 117.10937023162842 seconds\n",
      "Step 2200, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 122.6124975681305 seconds\n",
      "Step 2300, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 128.24838399887085 seconds\n",
      "Step 2400, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 133.73201513290405 seconds\n",
      "Step 2500, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 139.3998851776123 seconds\n",
      "Step 2600, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 144.90452218055725 seconds\n",
      "Step 2700, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 150.4198956489563 seconds\n",
      "Step 2800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 156.06393885612488 seconds\n",
      "Step 2900, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 161.58394026756287 seconds\n",
      "Step 3000, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 167.22536993026733 seconds\n",
      "Step 3100, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 172.76625752449036 seconds\n",
      "Step 3200, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 178.25373315811157 seconds\n",
      "Step 3300, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 183.88597893714905 seconds\n",
      "Step 3400, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 189.40049266815186 seconds\n",
      "Step 3500, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 195.047607421875 seconds\n",
      "Step 3600, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 200.57535767555237 seconds\n",
      "Step 3700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 206.21688771247864 seconds\n",
      "Step 3800, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 211.70524215698242 seconds\n",
      "Step 3900, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 217.22563338279724 seconds\n",
      "Step 4000, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 222.84887552261353 seconds\n",
      "Step 4100, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 228.35923981666565 seconds\n",
      "Step 4200, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 234.01105642318726 seconds\n",
      "Step 4300, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 239.51981902122498 seconds\n",
      "Step 4400, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 245.1728298664093 seconds\n",
      "Step 4500, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 250.69545769691467 seconds\n",
      "Step 4600, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 256.22567868232727 seconds\n",
      "Step 4700, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 261.890841960907 seconds\n",
      "Step 4800, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 267.3979504108429 seconds\n",
      "Step 4900, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 273.0474591255188 seconds\n",
      "Step 5000, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 278.55936193466187 seconds\n",
      "Step 5100, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 284.07015323638916 seconds\n",
      "Step 5200, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 289.743070602417 seconds\n",
      "Step 5300, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 295.28937005996704 seconds\n",
      "Step 5400, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 300.950875043869 seconds\n",
      "Step 5500, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 306.463796377182 seconds\n",
      "Step 5600, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 312.0977261066437 seconds\n",
      "Step 5700, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 317.6065058708191 seconds\n",
      "Step 5800, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 323.13708114624023 seconds\n",
      "Step 5900, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 328.7541253566742 seconds\n",
      "Step 6000, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 334.26903009414673 seconds\n",
      "Step 6100, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 339.903391122818 seconds\n",
      "Step 6200, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 345.4150776863098 seconds\n",
      "Step 6300, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 351.07388186454773 seconds\n",
      "Step 6400, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 356.5990719795227 seconds\n",
      "Step 6500, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 362.09383034706116 seconds\n",
      "Step 6600, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 367.71875190734863 seconds\n",
      "Step 6700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 373.25284218788147 seconds\n",
      "Step 6800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 379.30522775650024 seconds\n",
      "Step 6900, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 384.84716725349426 seconds\n",
      "Step 7000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 390.35397505760193 seconds\n",
      "Step 7100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 396.04254055023193 seconds\n",
      "Step 7200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 401.5557379722595 seconds\n",
      "Step 7300, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 407.2283375263214 seconds\n",
      "Step 7400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 412.7572479248047 seconds\n",
      "Step 7500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 418.4595868587494 seconds\n",
      "Step 7600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 424.3928482532501 seconds\n",
      "Step 7700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 429.92211079597473 seconds\n",
      "Step 7800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 435.61729621887207 seconds\n",
      "Step 7900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 441.161062002182 seconds\n",
      "Step 8000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 446.83539724349976 seconds\n",
      "Step 8100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 452.39935779571533 seconds\n",
      "Step 8200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 458.0862741470337 seconds\n",
      "Step 8300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 463.56697130203247 seconds\n",
      "Step 8400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 469.1097493171692 seconds\n",
      "Step 8500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 474.7859275341034 seconds\n",
      "Step 8600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 480.47344160079956 seconds\n",
      "Step 8700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 486.1473078727722 seconds\n",
      "Step 8800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 491.66640400886536 seconds\n",
      "Step 8900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 497.2025189399719 seconds\n",
      "Step 9000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 502.8779878616333 seconds\n",
      "Step 9100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 508.42483139038086 seconds\n",
      "Step 9200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 514.3481080532074 seconds\n",
      "Step 9300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 519.8548171520233 seconds\n",
      "Step 9400, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 525.5374224185944 seconds\n",
      "Step 9500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 531.0579614639282 seconds\n",
      "Step 9600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 536.6481621265411 seconds\n",
      "Step 9700, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 542.3035864830017 seconds\n",
      "Step 9800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 548.3883180618286 seconds\n",
      "Step 9900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 554.1057894229889 seconds\n",
      "Step 10000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 559.7074346542358 seconds\n",
      "Step 10100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 565.3239862918854 seconds\n",
      "Step 10200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 571.1528384685516 seconds\n",
      "Step 10300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 576.7831814289093 seconds\n",
      "Step 10400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 582.6955370903015 seconds\n",
      "Step 10500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 588.3415598869324 seconds\n",
      "Step 10600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 594.0365252494812 seconds\n",
      "Step 10700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 599.6039750576019 seconds\n",
      "Step 10800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 605.2446012496948 seconds\n",
      "Step 10900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 611.0337474346161 seconds\n",
      "Step 11000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 617.4653191566467 seconds\n",
      "Step 11100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 623.1607673168182 seconds\n",
      "Step 11200, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 628.7004923820496 seconds\n",
      "Step 11300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 634.2120769023895 seconds\n",
      "Step 11400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 639.8757696151733 seconds\n",
      "Step 11500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 645.423020362854 seconds\n",
      "Step 11600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 651.0747210979462 seconds\n",
      "Step 11700, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 656.6125495433807 seconds\n",
      "Step 11800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 662.3138673305511 seconds\n",
      "Step 11900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 667.8371443748474 seconds\n",
      "Step 12000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 673.3654294013977 seconds\n",
      "Step 12100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 679.0387227535248 seconds\n",
      "Step 12200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 684.5854635238647 seconds\n",
      "Step 12300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 690.3098349571228 seconds\n",
      "Step 12400, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 695.8421823978424 seconds\n",
      "Step 12500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 701.3863387107849 seconds\n",
      "Step 12600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 707.0748591423035 seconds\n",
      "Step 12700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 712.6162066459656 seconds\n",
      "Step 12800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 719.0308451652527 seconds\n",
      "Step 12900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 724.555389881134 seconds\n",
      "Step 13000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 730.2425138950348 seconds\n",
      "Step 13100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 735.7752661705017 seconds\n",
      "Step 13200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 741.3007352352142 seconds\n",
      "Step 13300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 746.9991593360901 seconds\n",
      "Step 13400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 752.5670766830444 seconds\n",
      "Step 13500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 758.2496519088745 seconds\n",
      "Step 13600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 763.7871608734131 seconds\n",
      "Step 13700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 769.3311684131622 seconds\n",
      "Step 13800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 775.030237197876 seconds\n",
      "Step 13900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 780.6046378612518 seconds\n",
      "Step 14000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 786.2844824790955 seconds\n",
      "Step 14100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 791.813827753067 seconds\n",
      "Step 14200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 797.5106618404388 seconds\n",
      "Step 14300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 803.0260837078094 seconds\n",
      "Step 14400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 808.5793871879578 seconds\n",
      "Step 14500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 814.3032820224762 seconds\n",
      "Step 14600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 819.8707013130188 seconds\n",
      "Step 14700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 825.5951533317566 seconds\n",
      "Step 14800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 831.191930770874 seconds\n",
      "Step 14900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 836.7441592216492 seconds\n",
      "Step 15000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 842.506498336792 seconds\n",
      "Step 15100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 848.0676157474518 seconds\n",
      "Step 15200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 853.8258175849915 seconds\n",
      "Step 15300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 859.3778991699219 seconds\n",
      "Step 15400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 865.0970797538757 seconds\n",
      "Step 15500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 870.6274604797363 seconds\n",
      "Step 15600, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 876.2205767631531 seconds\n",
      "Step 15700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 881.9020705223083 seconds\n",
      "Step 15800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 887.4580714702606 seconds\n",
      "Step 15900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 893.1509921550751 seconds\n",
      "Step 16000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 898.6840958595276 seconds\n",
      "Step 16100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 904.2485272884369 seconds\n",
      "Step 16200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 909.9509944915771 seconds\n",
      "Step 16300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 916.0656545162201 seconds\n",
      "Step 16400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 921.7693846225739 seconds\n",
      "Step 16500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 927.2957646846771 seconds\n",
      "Step 16600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 932.8441982269287 seconds\n",
      "Step 16700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 938.5368502140045 seconds\n",
      "Step 16800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 944.0903992652893 seconds\n",
      "Step 16900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 949.7737166881561 seconds\n",
      "Step 17000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 955.3089420795441 seconds\n",
      "Step 17100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 961.0048727989197 seconds\n",
      "Step 17200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 966.5636699199677 seconds\n",
      "Step 17300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 972.1028883457184 seconds\n",
      "Step 17400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 977.860426902771 seconds\n",
      "Step 17500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 983.4076285362244 seconds\n",
      "Step 17600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 989.098210811615 seconds\n",
      "Step 17700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 994.7981579303741 seconds\n",
      "Step 17800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1000.3149869441986 seconds\n",
      "Step 17900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1006.0373125076294 seconds\n",
      "Step 18000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1011.5985879898071 seconds\n",
      "Step 18100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1017.3086831569672 seconds\n",
      "Step 18200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1022.8541932106018 seconds\n",
      "Step 18300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1028.6345341205597 seconds\n",
      "Step 18400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1034.1845893859863 seconds\n",
      "Step 18500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1040.3074204921722 seconds\n",
      "Step 18600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1045.995600938797 seconds\n",
      "Step 18700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1051.5416555404663 seconds\n",
      "Step 18800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1057.2655091285706 seconds\n",
      "Step 18900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1062.766709804535 seconds\n",
      "Step 19000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1068.2993099689484 seconds\n",
      "Step 19100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1073.9825901985168 seconds\n",
      "Step 19200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1079.527071237564 seconds\n",
      "Step 19300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1085.2407500743866 seconds\n",
      "Step 19400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1090.7718136310577 seconds\n",
      "Step 19500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1096.7864243984222 seconds\n",
      "Step 19600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1102.5511274337769 seconds\n",
      "Step 19700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1108.0683076381683 seconds\n",
      "Step 19800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1113.7643067836761 seconds\n",
      "Step 19900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1119.3246443271637 seconds\n",
      "Step 20000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1125.023999929428 seconds\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 0.06233978271484375 seconds\n",
      "Step 100, loss: tensor(0.0232, grad_fn=<SubBackward0>), time elapsed: 5.723978042602539 seconds\n",
      "Step 200, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 11.291936874389648 seconds\n",
      "Step 300, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 16.9344699382782 seconds\n",
      "Step 400, loss: tensor(0.0145, grad_fn=<SubBackward0>), time elapsed: 22.43358039855957 seconds\n",
      "Step 500, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 28.103668689727783 seconds\n",
      "Step 600, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 33.59884023666382 seconds\n",
      "Step 700, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 39.10199451446533 seconds\n",
      "Step 800, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 44.75360727310181 seconds\n",
      "Step 900, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 50.25390625 seconds\n",
      "Step 1000, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 55.90637683868408 seconds\n",
      "Step 1100, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 61.40460538864136 seconds\n",
      "Step 1200, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 67.05175185203552 seconds\n",
      "Step 1300, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 72.55066657066345 seconds\n",
      "Step 1400, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 78.06082701683044 seconds\n",
      "Step 1500, loss: tensor(0.0110, grad_fn=<SubBackward0>), time elapsed: 83.68422245979309 seconds\n",
      "Step 1600, loss: tensor(0.0105, grad_fn=<SubBackward0>), time elapsed: 89.22890996932983 seconds\n",
      "Step 1700, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 94.86425876617432 seconds\n",
      "Step 1800, loss: tensor(0.0108, grad_fn=<SubBackward0>), time elapsed: 100.3726851940155 seconds\n",
      "Step 1900, loss: tensor(0.0104, grad_fn=<SubBackward0>), time elapsed: 106.01401472091675 seconds\n",
      "Step 2000, loss: tensor(0.0101, grad_fn=<SubBackward0>), time elapsed: 111.52348184585571 seconds\n",
      "Step 2100, loss: tensor(0.0101, grad_fn=<SubBackward0>), time elapsed: 117.05768299102783 seconds\n",
      "Step 2200, loss: tensor(0.0102, grad_fn=<SubBackward0>), time elapsed: 122.70073056221008 seconds\n",
      "Step 2300, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 128.21632599830627 seconds\n",
      "Step 2400, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 133.8643536567688 seconds\n",
      "Step 2500, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 139.36913681030273 seconds\n",
      "Step 2600, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 145.02396202087402 seconds\n",
      "Step 2700, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 150.5463466644287 seconds\n",
      "Step 2800, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 156.06081438064575 seconds\n",
      "Step 2900, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 161.70347929000854 seconds\n",
      "Step 3000, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 167.19700574874878 seconds\n",
      "Step 3100, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 172.84527134895325 seconds\n",
      "Step 3200, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 178.37531423568726 seconds\n",
      "Step 3300, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 183.89143061637878 seconds\n",
      "Step 3400, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 189.52969527244568 seconds\n",
      "Step 3500, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 195.0445737838745 seconds\n",
      "Step 3600, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 200.69220209121704 seconds\n",
      "Step 3700, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 206.20357704162598 seconds\n",
      "Step 3800, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 211.86895108222961 seconds\n",
      "Step 3900, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 217.37724208831787 seconds\n",
      "Step 4000, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 222.8924605846405 seconds\n",
      "Step 4100, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 228.54268336296082 seconds\n",
      "Step 4200, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 234.05487871170044 seconds\n",
      "Step 4300, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 239.73361682891846 seconds\n",
      "Step 4400, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 245.25606298446655 seconds\n",
      "Step 4500, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 250.9119234085083 seconds\n",
      "Step 4600, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 256.4081394672394 seconds\n",
      "Step 4700, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 262.0629394054413 seconds\n",
      "Step 4800, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 267.72090196609497 seconds\n",
      "Step 4900, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 273.7256109714508 seconds\n",
      "Step 5000, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 279.3996465206146 seconds\n",
      "Step 5100, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 285.54602694511414 seconds\n",
      "Step 5200, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 291.06154465675354 seconds\n",
      "Step 5300, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 296.70413970947266 seconds\n",
      "Step 5400, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 302.2535455226898 seconds\n",
      "Step 5500, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 307.92690348625183 seconds\n",
      "Step 5600, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 313.4642143249512 seconds\n",
      "Step 5700, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 319.197212934494 seconds\n",
      "Step 5800, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 324.8245303630829 seconds\n",
      "Step 5900, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 330.38206243515015 seconds\n",
      "Step 6000, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 336.04999828338623 seconds\n",
      "Step 6100, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 341.58787631988525 seconds\n",
      "Step 6200, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 347.25838255882263 seconds\n",
      "Step 6300, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 352.7960422039032 seconds\n",
      "Step 6400, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 358.6399314403534 seconds\n",
      "Step 6500, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 364.25202083587646 seconds\n",
      "Step 6600, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 369.7840898036957 seconds\n",
      "Step 6700, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 375.89074420928955 seconds\n",
      "Step 6800, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 381.51100182533264 seconds\n",
      "Step 6900, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 387.27424573898315 seconds\n",
      "Step 7000, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 392.81840777397156 seconds\n",
      "Step 7100, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 398.325875043869 seconds\n",
      "Step 7200, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 403.99767661094666 seconds\n",
      "Step 7300, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 409.5253632068634 seconds\n",
      "Step 7400, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 415.4555072784424 seconds\n",
      "Step 7500, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 421.66302371025085 seconds\n",
      "Step 7600, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 427.3311712741852 seconds\n",
      "Step 7700, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 432.8495693206787 seconds\n",
      "Step 7800, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 438.3645496368408 seconds\n",
      "Step 7900, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 444.00831604003906 seconds\n",
      "Step 8000, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 449.578262090683 seconds\n",
      "Step 8100, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 455.27104330062866 seconds\n",
      "Step 8200, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 460.7971031665802 seconds\n",
      "Step 8300, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 466.7464988231659 seconds\n",
      "Step 8400, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 472.25097918510437 seconds\n",
      "Step 8500, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 477.88806891441345 seconds\n",
      "Step 8600, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 483.57876920700073 seconds\n",
      "Step 8700, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 489.1113452911377 seconds\n",
      "Step 8800, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 494.8112382888794 seconds\n",
      "Step 8900, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 500.3266201019287 seconds\n",
      "Step 9000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 506.597225189209 seconds\n",
      "Step 9100, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 512.2588629722595 seconds\n",
      "Step 9200, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 517.7998020648956 seconds\n",
      "Step 9300, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 523.4406147003174 seconds\n",
      "Step 9400, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 528.9471147060394 seconds\n",
      "Step 9500, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 534.7023177146912 seconds\n",
      "Step 9600, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 540.3270432949066 seconds\n",
      "Step 9700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 545.8698856830597 seconds\n",
      "Step 9800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 551.81081199646 seconds\n",
      "Step 9900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 557.3365721702576 seconds\n",
      "Step 10000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 563.3430712223053 seconds\n",
      "Step 10100, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 569.7208981513977 seconds\n",
      "Step 10200, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 575.2739131450653 seconds\n",
      "Step 10300, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 580.9762489795685 seconds\n",
      "Step 10400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 586.5085785388947 seconds\n",
      "Step 10500, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 592.2192385196686 seconds\n",
      "Step 10600, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 597.767112493515 seconds\n",
      "Step 10700, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 604.2446157932281 seconds\n",
      "Step 10800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 609.7682318687439 seconds\n",
      "Step 10900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 615.2788240909576 seconds\n",
      "Step 11000, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 620.9516227245331 seconds\n",
      "Step 11100, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 626.4922964572906 seconds\n",
      "Step 11200, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 632.1776237487793 seconds\n",
      "Step 11300, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 637.742990732193 seconds\n",
      "Step 11400, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 643.2694666385651 seconds\n",
      "Step 11500, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 649.0515460968018 seconds\n",
      "Step 11600, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 654.5677449703217 seconds\n",
      "Step 11700, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 660.2502286434174 seconds\n",
      "Step 11800, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 666.0377202033997 seconds\n",
      "Step 11900, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 671.7269186973572 seconds\n",
      "Step 12000, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 677.2341940402985 seconds\n",
      "Step 12100, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 682.7891128063202 seconds\n",
      "Step 12200, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 688.6123144626617 seconds\n",
      "Step 12300, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 694.1348204612732 seconds\n",
      "Step 12400, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 699.8256435394287 seconds\n",
      "Step 12500, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 705.3614857196808 seconds\n",
      "Step 12600, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 711.7527163028717 seconds\n",
      "Step 12700, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 717.4527184963226 seconds\n",
      "Step 12800, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 722.9872674942017 seconds\n",
      "Step 12900, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 728.6871538162231 seconds\n",
      "Step 13000, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 734.2187469005585 seconds\n",
      "Step 13100, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 739.9043757915497 seconds\n",
      "Step 13200, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 745.4230892658234 seconds\n",
      "Step 13300, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 750.9631769657135 seconds\n",
      "Step 13400, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 756.6762316226959 seconds\n",
      "Step 13500, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 762.2563395500183 seconds\n",
      "Step 13600, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 767.9647417068481 seconds\n",
      "Step 13700, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 773.5234129428864 seconds\n",
      "Step 13800, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 779.0766017436981 seconds\n",
      "Step 13900, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 784.7675385475159 seconds\n",
      "Step 14000, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 790.3265745639801 seconds\n",
      "Step 14100, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 796.0145297050476 seconds\n",
      "Step 14200, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 801.5558896064758 seconds\n",
      "Step 14300, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 807.2471420764923 seconds\n",
      "Step 14400, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 813.3567261695862 seconds\n",
      "Step 14500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 818.9959998130798 seconds\n",
      "Step 14600, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 824.6800301074982 seconds\n",
      "Step 14700, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 830.2172901630402 seconds\n",
      "Step 14800, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 835.9200096130371 seconds\n",
      "Step 14900, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 841.5322279930115 seconds\n",
      "Step 15000, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 847.1124696731567 seconds\n",
      "Step 15100, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 852.8409266471863 seconds\n",
      "Step 15200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 858.4093344211578 seconds\n",
      "Step 15300, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 864.1285221576691 seconds\n",
      "Step 15400, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 869.869048833847 seconds\n",
      "Step 15500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 875.5591149330139 seconds\n",
      "Step 15600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 881.1263227462769 seconds\n",
      "Step 15700, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 886.6857192516327 seconds\n",
      "Step 15800, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 893.1289749145508 seconds\n",
      "Step 15900, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 898.6922428607941 seconds\n",
      "Step 16000, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 904.4506340026855 seconds\n",
      "Step 16100, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 910.1264073848724 seconds\n",
      "Step 16200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 915.7011206150055 seconds\n",
      "Step 16300, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 921.388402223587 seconds\n",
      "Step 16400, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 927.007621049881 seconds\n",
      "Step 16500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 932.8700768947601 seconds\n",
      "Step 16600, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 938.4455947875977 seconds\n",
      "Step 16700, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 944.2621092796326 seconds\n",
      "Step 16800, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 949.8038580417633 seconds\n",
      "Step 16900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 955.5364000797272 seconds\n",
      "Step 17000, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 961.2221078872681 seconds\n",
      "Step 17100, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 966.7851836681366 seconds\n",
      "Step 17200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 972.5156166553497 seconds\n",
      "Step 17300, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 978.05206823349 seconds\n",
      "Step 17400, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 984.0827808380127 seconds\n",
      "Step 17500, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 989.7854316234589 seconds\n",
      "Step 17600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 995.3320541381836 seconds\n",
      "Step 17700, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1001.035924911499 seconds\n",
      "Step 17800, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1006.5751619338989 seconds\n",
      "Step 17900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1012.3049705028534 seconds\n",
      "Step 18000, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1018.004804611206 seconds\n",
      "Step 18100, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1023.5471153259277 seconds\n",
      "Step 18200, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1029.2805697917938 seconds\n",
      "Step 18300, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1034.8929278850555 seconds\n",
      "Step 18400, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1041.231393814087 seconds\n",
      "Step 18500, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1046.7825932502747 seconds\n",
      "Step 18600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1052.319095134735 seconds\n",
      "Step 18700, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1058.0139565467834 seconds\n",
      "Step 18800, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1063.5568821430206 seconds\n",
      "Step 18900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1069.2605321407318 seconds\n",
      "Step 19000, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1074.7930104732513 seconds\n",
      "Step 19100, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1080.3536577224731 seconds\n",
      "Step 19200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1086.085501909256 seconds\n",
      "Step 19300, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1091.6239614486694 seconds\n",
      "Step 19400, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1098.9929072856903 seconds\n",
      "Step 19500, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1104.5197582244873 seconds\n",
      "Step 19600, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1110.0379328727722 seconds\n",
      "Step 19700, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1115.7503135204315 seconds\n",
      "Step 19800, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 1121.318619966507 seconds\n",
      "Step 19900, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1127.0641932487488 seconds\n",
      "Step 20000, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1132.618617773056 seconds\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 0.06322193145751953 seconds\n",
      "Step 100, loss: tensor(0.0689, grad_fn=<SubBackward0>), time elapsed: 5.83461856842041 seconds\n",
      "Step 200, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 11.412747859954834 seconds\n",
      "Step 300, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 16.91568922996521 seconds\n",
      "Step 400, loss: tensor(0.0408, grad_fn=<SubBackward0>), time elapsed: 22.545907735824585 seconds\n",
      "Step 500, loss: tensor(0.0369, grad_fn=<SubBackward0>), time elapsed: 28.069189310073853 seconds\n",
      "Step 600, loss: tensor(0.0338, grad_fn=<SubBackward0>), time elapsed: 33.705358028411865 seconds\n",
      "Step 700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 39.2135329246521 seconds\n",
      "Step 800, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 44.86344289779663 seconds\n",
      "Step 900, loss: tensor(0.0285, grad_fn=<SubBackward0>), time elapsed: 50.377275466918945 seconds\n",
      "Step 1000, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 55.92368268966675 seconds\n",
      "Step 1100, loss: tensor(0.0253, grad_fn=<SubBackward0>), time elapsed: 61.576396226882935 seconds\n",
      "Step 1200, loss: tensor(0.0252, grad_fn=<SubBackward0>), time elapsed: 67.10011005401611 seconds\n",
      "Step 1300, loss: tensor(0.0247, grad_fn=<SubBackward0>), time elapsed: 72.74263453483582 seconds\n",
      "Step 1400, loss: tensor(0.0240, grad_fn=<SubBackward0>), time elapsed: 78.24697232246399 seconds\n",
      "Step 1500, loss: tensor(0.0236, grad_fn=<SubBackward0>), time elapsed: 83.75945353507996 seconds\n",
      "Step 1600, loss: tensor(0.0237, grad_fn=<SubBackward0>), time elapsed: 89.40973711013794 seconds\n",
      "Step 1700, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 94.92094326019287 seconds\n",
      "Step 1800, loss: tensor(0.0225, grad_fn=<SubBackward0>), time elapsed: 100.57371044158936 seconds\n",
      "Step 1900, loss: tensor(0.0227, grad_fn=<SubBackward0>), time elapsed: 106.09973549842834 seconds\n",
      "Step 2000, loss: tensor(0.0228, grad_fn=<SubBackward0>), time elapsed: 111.7511990070343 seconds\n",
      "Step 2100, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 117.27934122085571 seconds\n",
      "Step 2200, loss: tensor(0.0217, grad_fn=<SubBackward0>), time elapsed: 122.79649782180786 seconds\n",
      "Step 2300, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 128.4202916622162 seconds\n",
      "Step 2400, loss: tensor(0.0217, grad_fn=<SubBackward0>), time elapsed: 133.92150163650513 seconds\n",
      "Step 2500, loss: tensor(0.0210, grad_fn=<SubBackward0>), time elapsed: 139.56496572494507 seconds\n",
      "Step 2600, loss: tensor(0.0207, grad_fn=<SubBackward0>), time elapsed: 145.07069373130798 seconds\n",
      "Step 2700, loss: tensor(0.0209, grad_fn=<SubBackward0>), time elapsed: 150.7434606552124 seconds\n",
      "Step 2800, loss: tensor(0.0206, grad_fn=<SubBackward0>), time elapsed: 156.2567753791809 seconds\n",
      "Step 2900, loss: tensor(0.0199, grad_fn=<SubBackward0>), time elapsed: 161.75443148612976 seconds\n",
      "Step 3000, loss: tensor(0.0200, grad_fn=<SubBackward0>), time elapsed: 167.39836502075195 seconds\n",
      "Step 3100, loss: tensor(0.0199, grad_fn=<SubBackward0>), time elapsed: 172.89619493484497 seconds\n",
      "Step 3200, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 178.5434534549713 seconds\n",
      "Step 3300, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 184.04618620872498 seconds\n",
      "Step 3400, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 189.705482006073 seconds\n",
      "Step 3500, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 195.21847891807556 seconds\n",
      "Step 3600, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 200.75083231925964 seconds\n",
      "Step 3700, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 206.37914180755615 seconds\n",
      "Step 3800, loss: tensor(0.0178, grad_fn=<SubBackward0>), time elapsed: 211.9063422679901 seconds\n",
      "Step 3900, loss: tensor(0.0176, grad_fn=<SubBackward0>), time elapsed: 217.54955053329468 seconds\n",
      "Step 4000, loss: tensor(0.0172, grad_fn=<SubBackward0>), time elapsed: 223.05497694015503 seconds\n",
      "Step 4100, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 228.54956603050232 seconds\n",
      "Step 4200, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 234.18584823608398 seconds\n",
      "Step 4300, loss: tensor(0.0174, grad_fn=<SubBackward0>), time elapsed: 239.73816084861755 seconds\n",
      "Step 4400, loss: tensor(0.0167, grad_fn=<SubBackward0>), time elapsed: 245.392560005188 seconds\n",
      "Step 4500, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 250.9028398990631 seconds\n",
      "Step 4600, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 256.55813479423523 seconds\n",
      "Step 4700, loss: tensor(0.0170, grad_fn=<SubBackward0>), time elapsed: 262.06742453575134 seconds\n",
      "Step 4800, loss: tensor(0.0170, grad_fn=<SubBackward0>), time elapsed: 267.5618414878845 seconds\n",
      "Step 4900, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 273.2138249874115 seconds\n",
      "Step 5000, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 278.72084856033325 seconds\n",
      "Step 5100, loss: tensor(0.0170, grad_fn=<SubBackward0>), time elapsed: 284.38018321990967 seconds\n",
      "Step 5200, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 289.9037330150604 seconds\n",
      "Step 5300, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 295.56076669692993 seconds\n",
      "Step 5400, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 301.0850987434387 seconds\n",
      "Step 5500, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 306.60190510749817 seconds\n",
      "Step 5600, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 312.2172601222992 seconds\n",
      "Step 5700, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 317.72573947906494 seconds\n",
      "Step 5800, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 323.3908944129944 seconds\n",
      "Step 5900, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 328.91293597221375 seconds\n",
      "Step 6000, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 334.4490351676941 seconds\n",
      "Step 6100, loss: tensor(0.0159, grad_fn=<SubBackward0>), time elapsed: 340.11591267585754 seconds\n",
      "Step 6200, loss: tensor(0.0159, grad_fn=<SubBackward0>), time elapsed: 345.6530306339264 seconds\n",
      "Step 6300, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 351.3206310272217 seconds\n",
      "Step 6400, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 356.8043203353882 seconds\n",
      "Step 6500, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 362.48076820373535 seconds\n",
      "Step 6600, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 367.9849989414215 seconds\n",
      "Step 6700, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 373.5193500518799 seconds\n",
      "Step 6800, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 379.1690740585327 seconds\n",
      "Step 6900, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 384.6955668926239 seconds\n",
      "Step 7000, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 390.39300060272217 seconds\n",
      "Step 7100, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 395.91093015670776 seconds\n",
      "Step 7200, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 401.56850957870483 seconds\n",
      "Step 7300, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 407.06886315345764 seconds\n",
      "Step 7400, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 412.5798943042755 seconds\n",
      "Step 7500, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 418.2148959636688 seconds\n",
      "Step 7600, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 423.76298546791077 seconds\n",
      "Step 7700, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 429.4297094345093 seconds\n",
      "Step 7800, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 434.955441236496 seconds\n",
      "Step 7900, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 440.4715487957001 seconds\n",
      "Step 8000, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 446.14420795440674 seconds\n",
      "Step 8100, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 451.6760342121124 seconds\n",
      "Step 8200, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 457.3647093772888 seconds\n",
      "Step 8300, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 462.85499572753906 seconds\n",
      "Step 8400, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 468.5393283367157 seconds\n",
      "Step 8500, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 474.06827878952026 seconds\n",
      "Step 8600, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 479.65496802330017 seconds\n",
      "Step 8700, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 485.52892804145813 seconds\n",
      "Step 8800, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 491.0392019748688 seconds\n",
      "Step 8900, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 496.78982186317444 seconds\n",
      "Step 9000, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 502.3131630420685 seconds\n",
      "Step 9100, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 507.8075785636902 seconds\n",
      "Step 9200, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 514.1696376800537 seconds\n",
      "Step 9300, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 519.6949594020844 seconds\n",
      "Step 9400, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 525.3654274940491 seconds\n",
      "Step 9500, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 530.8940539360046 seconds\n",
      "Step 9600, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 536.5531287193298 seconds\n",
      "Step 9700, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 542.0600581169128 seconds\n",
      "Step 9800, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 547.5935516357422 seconds\n",
      "Step 9900, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 553.2464897632599 seconds\n",
      "Step 10000, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 558.7731363773346 seconds\n",
      "Step 10100, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 564.4267761707306 seconds\n",
      "Step 10200, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 569.9404149055481 seconds\n",
      "Step 10300, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 575.4875888824463 seconds\n",
      "Step 10400, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 581.1637029647827 seconds\n",
      "Step 10500, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 586.7012557983398 seconds\n",
      "Step 10600, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 592.3531670570374 seconds\n",
      "Step 10700, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 597.8717601299286 seconds\n",
      "Step 10800, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 603.5743863582611 seconds\n",
      "Step 10900, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 609.6531407833099 seconds\n",
      "Step 11000, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 615.1632578372955 seconds\n",
      "Step 11100, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 620.819319486618 seconds\n",
      "Step 11200, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 626.3386220932007 seconds\n",
      "Step 11300, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 631.9864356517792 seconds\n",
      "Step 11400, loss: tensor(0.0146, grad_fn=<SubBackward0>), time elapsed: 637.55304646492 seconds\n",
      "Step 11500, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 643.0971143245697 seconds\n",
      "Step 11600, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 648.7832217216492 seconds\n",
      "Step 11700, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 654.366052865982 seconds\n",
      "Step 11800, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 660.1416389942169 seconds\n",
      "Step 11900, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 665.7342166900635 seconds\n",
      "Step 12000, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 671.5316574573517 seconds\n",
      "Step 12100, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 677.1423602104187 seconds\n",
      "Step 12200, loss: tensor(0.0145, grad_fn=<SubBackward0>), time elapsed: 682.6961607933044 seconds\n",
      "Step 12300, loss: tensor(0.0146, grad_fn=<SubBackward0>), time elapsed: 688.6000134944916 seconds\n",
      "Step 12400, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 694.1332025527954 seconds\n",
      "Step 12500, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 699.8304252624512 seconds\n",
      "Step 12600, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 705.3661167621613 seconds\n",
      "Step 12700, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 712.2589218616486 seconds\n",
      "Step 12800, loss: tensor(0.0145, grad_fn=<SubBackward0>), time elapsed: 717.9504818916321 seconds\n",
      "Step 12900, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 723.4808104038239 seconds\n",
      "Step 13000, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 729.1604189872742 seconds\n",
      "Step 13100, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 734.681806564331 seconds\n",
      "Step 13200, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 740.3595407009125 seconds\n",
      "Step 13300, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 745.8751895427704 seconds\n",
      "Step 13400, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 751.4045221805573 seconds\n",
      "Step 13500, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 757.1050667762756 seconds\n",
      "Step 13600, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 762.6351418495178 seconds\n",
      "Step 13700, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 768.2979371547699 seconds\n",
      "Step 13800, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 773.8227667808533 seconds\n",
      "Step 13900, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 779.3503980636597 seconds\n",
      "Step 14000, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 785.0508601665497 seconds\n",
      "Step 14100, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 790.609979391098 seconds\n",
      "Step 14200, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 796.3252012729645 seconds\n",
      "Step 14300, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 801.8861904144287 seconds\n",
      "Step 14400, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 807.6142647266388 seconds\n",
      "Step 14500, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 813.1602501869202 seconds\n",
      "Step 14600, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 818.734500169754 seconds\n",
      "Step 14700, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 824.4499049186707 seconds\n",
      "Step 14800, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 830.0058379173279 seconds\n",
      "Step 14900, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 835.7266659736633 seconds\n",
      "Step 15000, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 841.2882122993469 seconds\n",
      "Step 15100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 846.8453500270844 seconds\n",
      "Step 15200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 852.5889163017273 seconds\n",
      "Step 15300, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 858.1241493225098 seconds\n",
      "Step 15400, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 863.8294382095337 seconds\n",
      "Step 15500, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 869.3628761768341 seconds\n",
      "Step 15600, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 875.0767500400543 seconds\n",
      "Step 15700, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 880.6597876548767 seconds\n",
      "Step 15800, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 886.266224861145 seconds\n",
      "Step 15900, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 892.097069978714 seconds\n",
      "Step 16000, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 897.6483228206635 seconds\n",
      "Step 16100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 903.9805343151093 seconds\n",
      "Step 16200, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 909.5568645000458 seconds\n",
      "Step 16300, loss: tensor(0.0133, grad_fn=<SubBackward0>), time elapsed: 915.1116745471954 seconds\n",
      "Step 16400, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 920.8176999092102 seconds\n",
      "Step 16500, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 926.4150812625885 seconds\n",
      "Step 16600, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 932.183146238327 seconds\n",
      "Step 16700, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 937.7109007835388 seconds\n",
      "Step 16800, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 943.2594132423401 seconds\n",
      "Step 16900, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 948.9404628276825 seconds\n",
      "Step 17000, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 954.4753234386444 seconds\n",
      "Step 17100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 960.1712663173676 seconds\n",
      "Step 17200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 965.7073264122009 seconds\n",
      "Step 17300, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 971.4736466407776 seconds\n",
      "Step 17400, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 977.0224945545197 seconds\n",
      "Step 17500, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 982.5719709396362 seconds\n",
      "Step 17600, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 988.3179149627686 seconds\n",
      "Step 17700, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 996.4139888286591 seconds\n",
      "Step 17800, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1002.131195306778 seconds\n",
      "Step 17900, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1007.690274477005 seconds\n",
      "Step 18000, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 1013.2291300296783 seconds\n",
      "Step 18100, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1018.9218950271606 seconds\n",
      "Step 18200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1024.470463514328 seconds\n",
      "Step 18300, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1030.168726682663 seconds\n",
      "Step 18400, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1035.703349351883 seconds\n",
      "Step 18500, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 1041.3901181221008 seconds\n",
      "Step 18600, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 1046.8937797546387 seconds\n",
      "Step 18700, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 1052.4473781585693 seconds\n",
      "Step 18800, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1058.1331470012665 seconds\n",
      "Step 18900, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1063.6907050609589 seconds\n",
      "Step 19000, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1069.4047546386719 seconds\n",
      "Step 19100, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 1074.9407842159271 seconds\n",
      "Step 19200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1080.771758556366 seconds\n",
      "Step 19300, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 1086.502452135086 seconds\n",
      "Step 19400, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1092.1167259216309 seconds\n",
      "Step 19500, loss: tensor(0.0128, grad_fn=<SubBackward0>), time elapsed: 1097.8725972175598 seconds\n",
      "Step 19600, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 1103.4740679264069 seconds\n",
      "Step 19700, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 1109.0879645347595 seconds\n",
      "Step 19800, loss: tensor(0.0133, grad_fn=<SubBackward0>), time elapsed: 1114.8645322322845 seconds\n",
      "Step 19900, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1120.5005927085876 seconds\n",
      "Step 20000, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1126.3305914402008 seconds\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1059, grad_fn=<SubBackward0>), time elapsed: 0.06616783142089844 seconds\n",
      "Step 100, loss: tensor(0.0958, grad_fn=<SubBackward0>), time elapsed: 5.824069261550903 seconds\n",
      "Step 200, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 11.636232852935791 seconds\n",
      "Step 300, loss: tensor(0.0730, grad_fn=<SubBackward0>), time elapsed: 17.208541870117188 seconds\n",
      "Step 400, loss: tensor(0.0632, grad_fn=<SubBackward0>), time elapsed: 22.782327890396118 seconds\n",
      "Step 500, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 28.49082112312317 seconds\n",
      "Step 600, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 34.10900259017944 seconds\n",
      "Step 700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 39.83146905899048 seconds\n",
      "Step 800, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 45.426713943481445 seconds\n",
      "Step 900, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 51.175193786621094 seconds\n",
      "Step 1000, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 56.74862003326416 seconds\n",
      "Step 1100, loss: tensor(0.0456, grad_fn=<SubBackward0>), time elapsed: 62.387656688690186 seconds\n",
      "Step 1200, loss: tensor(0.0440, grad_fn=<SubBackward0>), time elapsed: 68.11637139320374 seconds\n",
      "Step 1300, loss: tensor(0.0458, grad_fn=<SubBackward0>), time elapsed: 73.72607755661011 seconds\n",
      "Step 1400, loss: tensor(0.0420, grad_fn=<SubBackward0>), time elapsed: 79.46331071853638 seconds\n",
      "Step 1500, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 85.06915163993835 seconds\n",
      "Step 1600, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 90.72067260742188 seconds\n",
      "Step 1700, loss: tensor(0.0432, grad_fn=<SubBackward0>), time elapsed: 96.44718384742737 seconds\n",
      "Step 1800, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 102.06744599342346 seconds\n",
      "Step 1900, loss: tensor(0.0425, grad_fn=<SubBackward0>), time elapsed: 107.82430076599121 seconds\n",
      "Step 2000, loss: tensor(0.0415, grad_fn=<SubBackward0>), time elapsed: 113.40604209899902 seconds\n",
      "Step 2100, loss: tensor(0.0415, grad_fn=<SubBackward0>), time elapsed: 119.14943146705627 seconds\n",
      "Step 2200, loss: tensor(0.0419, grad_fn=<SubBackward0>), time elapsed: 124.76355218887329 seconds\n",
      "Step 2300, loss: tensor(0.0416, grad_fn=<SubBackward0>), time elapsed: 130.34821605682373 seconds\n",
      "Step 2400, loss: tensor(0.0397, grad_fn=<SubBackward0>), time elapsed: 136.0873568058014 seconds\n",
      "Step 2500, loss: tensor(0.0410, grad_fn=<SubBackward0>), time elapsed: 141.6681718826294 seconds\n",
      "Step 2600, loss: tensor(0.0401, grad_fn=<SubBackward0>), time elapsed: 147.3904881477356 seconds\n",
      "Step 2700, loss: tensor(0.0395, grad_fn=<SubBackward0>), time elapsed: 153.02842211723328 seconds\n",
      "Step 2800, loss: tensor(0.0413, grad_fn=<SubBackward0>), time elapsed: 158.764568567276 seconds\n",
      "Step 2900, loss: tensor(0.0398, grad_fn=<SubBackward0>), time elapsed: 164.35350489616394 seconds\n",
      "Step 3000, loss: tensor(0.0398, grad_fn=<SubBackward0>), time elapsed: 169.94674110412598 seconds\n",
      "Step 3100, loss: tensor(0.0388, grad_fn=<SubBackward0>), time elapsed: 175.68269157409668 seconds\n",
      "Step 3200, loss: tensor(0.0408, grad_fn=<SubBackward0>), time elapsed: 181.29150938987732 seconds\n",
      "Step 3300, loss: tensor(0.0396, grad_fn=<SubBackward0>), time elapsed: 187.0442521572113 seconds\n",
      "Step 3400, loss: tensor(0.0389, grad_fn=<SubBackward0>), time elapsed: 192.66985297203064 seconds\n",
      "Step 3500, loss: tensor(0.0388, grad_fn=<SubBackward0>), time elapsed: 198.42760467529297 seconds\n",
      "Step 3600, loss: tensor(0.0385, grad_fn=<SubBackward0>), time elapsed: 204.025945186615 seconds\n",
      "Step 3700, loss: tensor(0.0385, grad_fn=<SubBackward0>), time elapsed: 209.6463177204132 seconds\n",
      "Step 3800, loss: tensor(0.0383, grad_fn=<SubBackward0>), time elapsed: 215.38855743408203 seconds\n",
      "Step 3900, loss: tensor(0.0387, grad_fn=<SubBackward0>), time elapsed: 220.9858911037445 seconds\n",
      "Step 4000, loss: tensor(0.0380, grad_fn=<SubBackward0>), time elapsed: 226.7168788909912 seconds\n",
      "Step 4100, loss: tensor(0.0378, grad_fn=<SubBackward0>), time elapsed: 232.41851997375488 seconds\n",
      "Step 4200, loss: tensor(0.0389, grad_fn=<SubBackward0>), time elapsed: 238.44736790657043 seconds\n",
      "Step 4300, loss: tensor(0.0379, grad_fn=<SubBackward0>), time elapsed: 244.4294035434723 seconds\n",
      "Step 4400, loss: tensor(0.0375, grad_fn=<SubBackward0>), time elapsed: 250.17819380760193 seconds\n",
      "Step 4500, loss: tensor(0.0378, grad_fn=<SubBackward0>), time elapsed: 255.92971086502075 seconds\n",
      "Step 4600, loss: tensor(0.0367, grad_fn=<SubBackward0>), time elapsed: 261.51554703712463 seconds\n",
      "Step 4700, loss: tensor(0.0381, grad_fn=<SubBackward0>), time elapsed: 267.24836468696594 seconds\n",
      "Step 4800, loss: tensor(0.0366, grad_fn=<SubBackward0>), time elapsed: 272.86780047416687 seconds\n",
      "Step 4900, loss: tensor(0.0365, grad_fn=<SubBackward0>), time elapsed: 278.46096420288086 seconds\n",
      "Step 5000, loss: tensor(0.0367, grad_fn=<SubBackward0>), time elapsed: 284.1875593662262 seconds\n",
      "Step 5100, loss: tensor(0.0359, grad_fn=<SubBackward0>), time elapsed: 289.78953981399536 seconds\n",
      "Step 5200, loss: tensor(0.0365, grad_fn=<SubBackward0>), time elapsed: 295.50725746154785 seconds\n",
      "Step 5300, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 301.0879113674164 seconds\n",
      "Step 5400, loss: tensor(0.0366, grad_fn=<SubBackward0>), time elapsed: 306.86059641838074 seconds\n",
      "Step 5500, loss: tensor(0.0359, grad_fn=<SubBackward0>), time elapsed: 312.4495904445648 seconds\n",
      "Step 5600, loss: tensor(0.0376, grad_fn=<SubBackward0>), time elapsed: 318.0509395599365 seconds\n",
      "Step 5700, loss: tensor(0.0357, grad_fn=<SubBackward0>), time elapsed: 323.7900860309601 seconds\n",
      "Step 5800, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 329.4051899909973 seconds\n",
      "Step 5900, loss: tensor(0.0356, grad_fn=<SubBackward0>), time elapsed: 335.1671905517578 seconds\n",
      "Step 6000, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 340.77831864356995 seconds\n",
      "Step 6100, loss: tensor(0.0372, grad_fn=<SubBackward0>), time elapsed: 346.3879904747009 seconds\n",
      "Step 6200, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 352.1354808807373 seconds\n",
      "Step 6300, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 357.7510552406311 seconds\n",
      "Step 6400, loss: tensor(0.0361, grad_fn=<SubBackward0>), time elapsed: 363.4886496067047 seconds\n",
      "Step 6500, loss: tensor(0.0347, grad_fn=<SubBackward0>), time elapsed: 369.13188123703003 seconds\n",
      "Step 6600, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 374.8877184391022 seconds\n",
      "Step 6700, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 380.48923349380493 seconds\n",
      "Step 6800, loss: tensor(0.0346, grad_fn=<SubBackward0>), time elapsed: 386.10632848739624 seconds\n",
      "Step 6900, loss: tensor(0.0360, grad_fn=<SubBackward0>), time elapsed: 391.87999534606934 seconds\n",
      "Step 7000, loss: tensor(0.0342, grad_fn=<SubBackward0>), time elapsed: 397.5149748325348 seconds\n",
      "Step 7100, loss: tensor(0.0360, grad_fn=<SubBackward0>), time elapsed: 403.2708339691162 seconds\n",
      "Step 7200, loss: tensor(0.0346, grad_fn=<SubBackward0>), time elapsed: 408.8512876033783 seconds\n",
      "Step 7300, loss: tensor(0.0347, grad_fn=<SubBackward0>), time elapsed: 414.5831563472748 seconds\n",
      "Step 7400, loss: tensor(0.0346, grad_fn=<SubBackward0>), time elapsed: 420.16295337677 seconds\n",
      "Step 7500, loss: tensor(0.0344, grad_fn=<SubBackward0>), time elapsed: 425.7872052192688 seconds\n",
      "Step 7600, loss: tensor(0.0352, grad_fn=<SubBackward0>), time elapsed: 431.49931502342224 seconds\n",
      "Step 7700, loss: tensor(0.0341, grad_fn=<SubBackward0>), time elapsed: 437.08839750289917 seconds\n",
      "Step 7800, loss: tensor(0.0349, grad_fn=<SubBackward0>), time elapsed: 442.83047103881836 seconds\n",
      "Step 7900, loss: tensor(0.0342, grad_fn=<SubBackward0>), time elapsed: 448.43738055229187 seconds\n",
      "Step 8000, loss: tensor(0.0354, grad_fn=<SubBackward0>), time elapsed: 454.030410528183 seconds\n",
      "Step 8100, loss: tensor(0.0345, grad_fn=<SubBackward0>), time elapsed: 459.80301094055176 seconds\n",
      "Step 8200, loss: tensor(0.0338, grad_fn=<SubBackward0>), time elapsed: 465.40860772132874 seconds\n",
      "Step 8300, loss: tensor(0.0336, grad_fn=<SubBackward0>), time elapsed: 471.1604859828949 seconds\n",
      "Step 8400, loss: tensor(0.0343, grad_fn=<SubBackward0>), time elapsed: 476.7702889442444 seconds\n",
      "Step 8500, loss: tensor(0.0341, grad_fn=<SubBackward0>), time elapsed: 482.5177619457245 seconds\n",
      "Step 8600, loss: tensor(0.0333, grad_fn=<SubBackward0>), time elapsed: 488.125364780426 seconds\n",
      "Step 8700, loss: tensor(0.0337, grad_fn=<SubBackward0>), time elapsed: 493.7207808494568 seconds\n",
      "Step 8800, loss: tensor(0.0335, grad_fn=<SubBackward0>), time elapsed: 499.44727063179016 seconds\n",
      "Step 8900, loss: tensor(0.0334, grad_fn=<SubBackward0>), time elapsed: 505.0487976074219 seconds\n",
      "Step 9000, loss: tensor(0.0326, grad_fn=<SubBackward0>), time elapsed: 510.8087956905365 seconds\n",
      "Step 9100, loss: tensor(0.0325, grad_fn=<SubBackward0>), time elapsed: 516.4437403678894 seconds\n",
      "Step 9200, loss: tensor(0.0326, grad_fn=<SubBackward0>), time elapsed: 522.0744252204895 seconds\n",
      "Step 9300, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 527.8428001403809 seconds\n",
      "Step 9400, loss: tensor(0.0323, grad_fn=<SubBackward0>), time elapsed: 533.448403596878 seconds\n",
      "Step 9500, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 539.2084066867828 seconds\n",
      "Step 9600, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 544.8234820365906 seconds\n",
      "Step 9700, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 550.6012051105499 seconds\n",
      "Step 9800, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 556.2175161838531 seconds\n",
      "Step 9900, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 561.8105053901672 seconds\n",
      "Step 10000, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 567.5789973735809 seconds\n",
      "Step 10100, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 573.1820876598358 seconds\n",
      "Step 10200, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 578.9475665092468 seconds\n",
      "Step 10300, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 584.5335021018982 seconds\n",
      "Step 10400, loss: tensor(0.0285, grad_fn=<SubBackward0>), time elapsed: 590.1267306804657 seconds\n",
      "Step 10500, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 595.8492512702942 seconds\n",
      "Step 10600, loss: tensor(0.0288, grad_fn=<SubBackward0>), time elapsed: 601.4506447315216 seconds\n",
      "Step 10700, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 607.1751706600189 seconds\n",
      "Step 10800, loss: tensor(0.0275, grad_fn=<SubBackward0>), time elapsed: 612.8165068626404 seconds\n",
      "Step 10900, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 618.5638966560364 seconds\n",
      "Step 11000, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 624.148665189743 seconds\n",
      "Step 11100, loss: tensor(0.0287, grad_fn=<SubBackward0>), time elapsed: 629.7835993766785 seconds\n",
      "Step 11200, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 635.5604162216187 seconds\n",
      "Step 11300, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 641.1909282207489 seconds\n",
      "Step 11400, loss: tensor(0.0288, grad_fn=<SubBackward0>), time elapsed: 646.965115070343 seconds\n",
      "Step 11500, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 652.5692844390869 seconds\n",
      "Step 11600, loss: tensor(0.0266, grad_fn=<SubBackward0>), time elapsed: 658.2081608772278 seconds\n",
      "Step 11700, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 663.9683468341827 seconds\n",
      "Step 11800, loss: tensor(0.0292, grad_fn=<SubBackward0>), time elapsed: 669.6072957515717 seconds\n",
      "Step 11900, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 675.3505568504333 seconds\n",
      "Step 12000, loss: tensor(0.0270, grad_fn=<SubBackward0>), time elapsed: 680.9812934398651 seconds\n",
      "Step 12100, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 686.7163825035095 seconds\n",
      "Step 12200, loss: tensor(0.0279, grad_fn=<SubBackward0>), time elapsed: 692.3448166847229 seconds\n",
      "Step 12300, loss: tensor(0.0287, grad_fn=<SubBackward0>), time elapsed: 697.9359226226807 seconds\n",
      "Step 12400, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 703.7169232368469 seconds\n",
      "Step 12500, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 709.3245310783386 seconds\n",
      "Step 12600, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 715.1262345314026 seconds\n",
      "Step 12700, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 720.7340157032013 seconds\n",
      "Step 12800, loss: tensor(0.0284, grad_fn=<SubBackward0>), time elapsed: 726.3482277393341 seconds\n",
      "Step 12900, loss: tensor(0.0269, grad_fn=<SubBackward0>), time elapsed: 732.1165990829468 seconds\n",
      "Step 13000, loss: tensor(0.0279, grad_fn=<SubBackward0>), time elapsed: 737.7251334190369 seconds\n",
      "Step 13100, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 743.4768006801605 seconds\n",
      "Step 13200, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 749.10036444664 seconds\n",
      "Step 13300, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 754.8687863349915 seconds\n",
      "Step 13400, loss: tensor(0.0278, grad_fn=<SubBackward0>), time elapsed: 760.4909715652466 seconds\n",
      "Step 13500, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 766.0884618759155 seconds\n",
      "Step 13600, loss: tensor(0.0278, grad_fn=<SubBackward0>), time elapsed: 771.865076303482 seconds\n",
      "Step 13700, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 777.4498534202576 seconds\n",
      "Step 13800, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 783.2098331451416 seconds\n",
      "Step 13900, loss: tensor(0.0273, grad_fn=<SubBackward0>), time elapsed: 788.8198375701904 seconds\n",
      "Step 14000, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 794.4630641937256 seconds\n",
      "Step 14100, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 800.2312560081482 seconds\n",
      "Step 14200, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 805.8621368408203 seconds\n",
      "Step 14300, loss: tensor(0.0280, grad_fn=<SubBackward0>), time elapsed: 811.6220273971558 seconds\n",
      "Step 14400, loss: tensor(0.0278, grad_fn=<SubBackward0>), time elapsed: 817.2432281970978 seconds\n",
      "Step 14500, loss: tensor(0.0272, grad_fn=<SubBackward0>), time elapsed: 822.9845175743103 seconds\n",
      "Step 14600, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 828.5151083469391 seconds\n",
      "Step 14700, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 834.0600528717041 seconds\n",
      "Step 14800, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 839.7112228870392 seconds\n",
      "Step 14900, loss: tensor(0.0259, grad_fn=<SubBackward0>), time elapsed: 845.247392654419 seconds\n",
      "Step 15000, loss: tensor(0.0259, grad_fn=<SubBackward0>), time elapsed: 850.9296207427979 seconds\n",
      "Step 15100, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 856.4918527603149 seconds\n",
      "Step 15200, loss: tensor(0.0260, grad_fn=<SubBackward0>), time elapsed: 862.0103023052216 seconds\n",
      "Step 15300, loss: tensor(0.0268, grad_fn=<SubBackward0>), time elapsed: 867.6603178977966 seconds\n",
      "Step 15400, loss: tensor(0.0265, grad_fn=<SubBackward0>), time elapsed: 873.1879260540009 seconds\n",
      "Step 15500, loss: tensor(0.0249, grad_fn=<SubBackward0>), time elapsed: 878.8714692592621 seconds\n",
      "Step 15600, loss: tensor(0.0247, grad_fn=<SubBackward0>), time elapsed: 884.4213447570801 seconds\n",
      "Step 15700, loss: tensor(0.0243, grad_fn=<SubBackward0>), time elapsed: 890.0947844982147 seconds\n",
      "Step 15800, loss: tensor(0.0234, grad_fn=<SubBackward0>), time elapsed: 895.6176810264587 seconds\n",
      "Step 15900, loss: tensor(0.0241, grad_fn=<SubBackward0>), time elapsed: 901.1578793525696 seconds\n",
      "Step 16000, loss: tensor(0.0236, grad_fn=<SubBackward0>), time elapsed: 906.8347179889679 seconds\n",
      "Step 16100, loss: tensor(0.0238, grad_fn=<SubBackward0>), time elapsed: 912.3777613639832 seconds\n",
      "Step 16200, loss: tensor(0.0245, grad_fn=<SubBackward0>), time elapsed: 918.0590560436249 seconds\n",
      "Step 16300, loss: tensor(0.0233, grad_fn=<SubBackward0>), time elapsed: 923.5902149677277 seconds\n",
      "Step 16400, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 929.1205244064331 seconds\n",
      "Step 16500, loss: tensor(0.0231, grad_fn=<SubBackward0>), time elapsed: 934.8303906917572 seconds\n",
      "Step 16600, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 940.3862192630768 seconds\n",
      "Step 16700, loss: tensor(0.0224, grad_fn=<SubBackward0>), time elapsed: 946.1100995540619 seconds\n",
      "Step 16800, loss: tensor(0.0234, grad_fn=<SubBackward0>), time elapsed: 951.6924631595612 seconds\n",
      "Step 16900, loss: tensor(0.0233, grad_fn=<SubBackward0>), time elapsed: 957.2688591480255 seconds\n",
      "Step 17000, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 962.9609189033508 seconds\n",
      "Step 17100, loss: tensor(0.0215, grad_fn=<SubBackward0>), time elapsed: 968.5173273086548 seconds\n",
      "Step 17200, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 974.2397768497467 seconds\n",
      "Step 17300, loss: tensor(0.0225, grad_fn=<SubBackward0>), time elapsed: 979.8187167644501 seconds\n",
      "Step 17400, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 985.5411322116852 seconds\n",
      "Step 17500, loss: tensor(0.0234, grad_fn=<SubBackward0>), time elapsed: 991.1137363910675 seconds\n",
      "Step 17600, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 996.6851723194122 seconds\n",
      "Step 17700, loss: tensor(0.0224, grad_fn=<SubBackward0>), time elapsed: 1002.4325168132782 seconds\n",
      "Step 17800, loss: tensor(0.0222, grad_fn=<SubBackward0>), time elapsed: 1007.994250535965 seconds\n",
      "Step 17900, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 1013.676043510437 seconds\n",
      "Step 18000, loss: tensor(0.0222, grad_fn=<SubBackward0>), time elapsed: 1019.1957190036774 seconds\n",
      "Step 18100, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 1024.7476246356964 seconds\n",
      "Step 18200, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 1030.4712381362915 seconds\n",
      "Step 18300, loss: tensor(0.0224, grad_fn=<SubBackward0>), time elapsed: 1036.0457317829132 seconds\n",
      "Step 18400, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 1041.7859308719635 seconds\n",
      "Step 18500, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 1047.3135526180267 seconds\n",
      "Step 18600, loss: tensor(0.0208, grad_fn=<SubBackward0>), time elapsed: 1053.0455577373505 seconds\n",
      "Step 18700, loss: tensor(0.0213, grad_fn=<SubBackward0>), time elapsed: 1058.5802955627441 seconds\n",
      "Step 18800, loss: tensor(0.0221, grad_fn=<SubBackward0>), time elapsed: 1064.1203138828278 seconds\n",
      "Step 18900, loss: tensor(0.0225, grad_fn=<SubBackward0>), time elapsed: 1069.8612401485443 seconds\n",
      "Step 19000, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 1075.4463152885437 seconds\n",
      "Step 19100, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 1081.168471813202 seconds\n",
      "Step 19200, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 1086.7281725406647 seconds\n",
      "Step 19300, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 1092.2716686725616 seconds\n",
      "Step 19400, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 1098.0013637542725 seconds\n",
      "Step 19500, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 1103.5487480163574 seconds\n",
      "Step 19600, loss: tensor(0.0229, grad_fn=<SubBackward0>), time elapsed: 1109.264904975891 seconds\n",
      "Step 19700, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 1114.833734512329 seconds\n",
      "Step 19800, loss: tensor(0.0207, grad_fn=<SubBackward0>), time elapsed: 1120.4274525642395 seconds\n",
      "Step 19900, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 1126.199602842331 seconds\n",
      "Step 20000, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 1131.801593542099 seconds\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.2090, grad_fn=<SubBackward0>), time elapsed: 0.062062740325927734 seconds\n",
      "Step 100, loss: tensor(0.1864, grad_fn=<SubBackward0>), time elapsed: 5.914261102676392 seconds\n",
      "Step 200, loss: tensor(0.1532, grad_fn=<SubBackward0>), time elapsed: 11.539004802703857 seconds\n",
      "Step 300, loss: tensor(0.1291, grad_fn=<SubBackward0>), time elapsed: 17.18545126914978 seconds\n",
      "Step 400, loss: tensor(0.1158, grad_fn=<SubBackward0>), time elapsed: 22.699230670928955 seconds\n",
      "Step 500, loss: tensor(0.1083, grad_fn=<SubBackward0>), time elapsed: 28.213378429412842 seconds\n",
      "Step 600, loss: tensor(0.1061, grad_fn=<SubBackward0>), time elapsed: 33.86864686012268 seconds\n",
      "Step 700, loss: tensor(0.0982, grad_fn=<SubBackward0>), time elapsed: 39.36850070953369 seconds\n",
      "Step 800, loss: tensor(0.0967, grad_fn=<SubBackward0>), time elapsed: 45.010740995407104 seconds\n",
      "Step 900, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 50.51896405220032 seconds\n",
      "Step 1000, loss: tensor(0.0895, grad_fn=<SubBackward0>), time elapsed: 56.18190121650696 seconds\n",
      "Step 1100, loss: tensor(0.0857, grad_fn=<SubBackward0>), time elapsed: 61.68891644477844 seconds\n",
      "Step 1200, loss: tensor(0.0828, grad_fn=<SubBackward0>), time elapsed: 67.23157668113708 seconds\n",
      "Step 1300, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 72.8789496421814 seconds\n",
      "Step 1400, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 78.40281128883362 seconds\n",
      "Step 1500, loss: tensor(0.0802, grad_fn=<SubBackward0>), time elapsed: 84.04602575302124 seconds\n",
      "Step 1600, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 89.56595087051392 seconds\n",
      "Step 1700, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 95.25049614906311 seconds\n",
      "Step 1800, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 100.75752830505371 seconds\n",
      "Step 1900, loss: tensor(0.0696, grad_fn=<SubBackward0>), time elapsed: 106.28764605522156 seconds\n",
      "Step 2000, loss: tensor(0.0709, grad_fn=<SubBackward0>), time elapsed: 111.93590235710144 seconds\n",
      "Step 2100, loss: tensor(0.0679, grad_fn=<SubBackward0>), time elapsed: 117.4632499217987 seconds\n",
      "Step 2200, loss: tensor(0.0669, grad_fn=<SubBackward0>), time elapsed: 123.10934591293335 seconds\n",
      "Step 2300, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 128.63653898239136 seconds\n",
      "Step 2400, loss: tensor(0.0664, grad_fn=<SubBackward0>), time elapsed: 134.15347528457642 seconds\n",
      "Step 2500, loss: tensor(0.0659, grad_fn=<SubBackward0>), time elapsed: 139.7987334728241 seconds\n",
      "Step 2600, loss: tensor(0.0657, grad_fn=<SubBackward0>), time elapsed: 145.32171392440796 seconds\n",
      "Step 2700, loss: tensor(0.0654, grad_fn=<SubBackward0>), time elapsed: 150.96953892707825 seconds\n",
      "Step 2800, loss: tensor(0.0665, grad_fn=<SubBackward0>), time elapsed: 156.52113819122314 seconds\n",
      "Step 2900, loss: tensor(0.0646, grad_fn=<SubBackward0>), time elapsed: 162.16967916488647 seconds\n",
      "Step 3000, loss: tensor(0.0647, grad_fn=<SubBackward0>), time elapsed: 167.6913652420044 seconds\n",
      "Step 3100, loss: tensor(0.0633, grad_fn=<SubBackward0>), time elapsed: 173.22821307182312 seconds\n",
      "Step 3200, loss: tensor(0.0637, grad_fn=<SubBackward0>), time elapsed: 178.8937427997589 seconds\n",
      "Step 3300, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 184.40462470054626 seconds\n",
      "Step 3400, loss: tensor(0.0609, grad_fn=<SubBackward0>), time elapsed: 190.06577563285828 seconds\n",
      "Step 3500, loss: tensor(0.0629, grad_fn=<SubBackward0>), time elapsed: 195.57733416557312 seconds\n",
      "Step 3600, loss: tensor(0.0620, grad_fn=<SubBackward0>), time elapsed: 201.23616933822632 seconds\n",
      "Step 3700, loss: tensor(0.0621, grad_fn=<SubBackward0>), time elapsed: 206.74548983573914 seconds\n",
      "Step 3800, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 212.27950763702393 seconds\n",
      "Step 3900, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 217.9434633255005 seconds\n",
      "Step 4000, loss: tensor(0.0576, grad_fn=<SubBackward0>), time elapsed: 223.47261452674866 seconds\n",
      "Step 4100, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 229.13537549972534 seconds\n",
      "Step 4200, loss: tensor(0.0557, grad_fn=<SubBackward0>), time elapsed: 234.64012360572815 seconds\n",
      "Step 4300, loss: tensor(0.0549, grad_fn=<SubBackward0>), time elapsed: 240.31853723526 seconds\n",
      "Step 4400, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 245.85525012016296 seconds\n",
      "Step 4500, loss: tensor(0.0535, grad_fn=<SubBackward0>), time elapsed: 251.4261782169342 seconds\n",
      "Step 4600, loss: tensor(0.0552, grad_fn=<SubBackward0>), time elapsed: 257.1024417877197 seconds\n",
      "Step 4700, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 262.64177656173706 seconds\n",
      "Step 4800, loss: tensor(0.0547, grad_fn=<SubBackward0>), time elapsed: 268.3113284111023 seconds\n",
      "Step 4900, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 273.82122588157654 seconds\n",
      "Step 5000, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 279.385142326355 seconds\n",
      "Step 5100, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 285.16333174705505 seconds\n",
      "Step 5200, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 290.68963384628296 seconds\n",
      "Step 5300, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 296.35582208633423 seconds\n",
      "Step 5400, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 301.911159992218 seconds\n",
      "Step 5500, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 307.6025302410126 seconds\n",
      "Step 5600, loss: tensor(0.0465, grad_fn=<SubBackward0>), time elapsed: 313.1151611804962 seconds\n",
      "Step 5700, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 319.11694526672363 seconds\n",
      "Step 5800, loss: tensor(0.0465, grad_fn=<SubBackward0>), time elapsed: 324.76825070381165 seconds\n",
      "Step 5900, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 330.26574420928955 seconds\n",
      "Step 6000, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 335.9507210254669 seconds\n",
      "Step 6100, loss: tensor(0.0449, grad_fn=<SubBackward0>), time elapsed: 341.48739290237427 seconds\n",
      "Step 6200, loss: tensor(0.0450, grad_fn=<SubBackward0>), time elapsed: 347.16762590408325 seconds\n",
      "Step 6300, loss: tensor(0.0439, grad_fn=<SubBackward0>), time elapsed: 352.67083287239075 seconds\n",
      "Step 6400, loss: tensor(0.0462, grad_fn=<SubBackward0>), time elapsed: 358.1870036125183 seconds\n",
      "Step 6500, loss: tensor(0.0450, grad_fn=<SubBackward0>), time elapsed: 363.8343918323517 seconds\n",
      "Step 6600, loss: tensor(0.0446, grad_fn=<SubBackward0>), time elapsed: 369.3621950149536 seconds\n",
      "Step 6700, loss: tensor(0.0448, grad_fn=<SubBackward0>), time elapsed: 374.99418210983276 seconds\n",
      "Step 6800, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 380.5062017440796 seconds\n",
      "Step 6900, loss: tensor(0.0439, grad_fn=<SubBackward0>), time elapsed: 386.0210180282593 seconds\n",
      "Step 7000, loss: tensor(0.0433, grad_fn=<SubBackward0>), time elapsed: 391.66776943206787 seconds\n",
      "Step 7100, loss: tensor(0.0434, grad_fn=<SubBackward0>), time elapsed: 397.18723583221436 seconds\n",
      "Step 7200, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 402.85593938827515 seconds\n",
      "Step 7300, loss: tensor(0.0409, grad_fn=<SubBackward0>), time elapsed: 408.3912341594696 seconds\n",
      "Step 7400, loss: tensor(0.0421, grad_fn=<SubBackward0>), time elapsed: 414.07877564430237 seconds\n",
      "Step 7500, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 419.5971019268036 seconds\n",
      "Step 7600, loss: tensor(0.0420, grad_fn=<SubBackward0>), time elapsed: 425.14655685424805 seconds\n",
      "Step 7700, loss: tensor(0.0417, grad_fn=<SubBackward0>), time elapsed: 430.87453508377075 seconds\n",
      "Step 7800, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 436.41236639022827 seconds\n",
      "Step 7900, loss: tensor(0.0422, grad_fn=<SubBackward0>), time elapsed: 442.12187337875366 seconds\n",
      "Step 8000, loss: tensor(0.0410, grad_fn=<SubBackward0>), time elapsed: 448.12255334854126 seconds\n",
      "Step 8100, loss: tensor(0.0414, grad_fn=<SubBackward0>), time elapsed: 453.64374113082886 seconds\n",
      "Step 8200, loss: tensor(0.0421, grad_fn=<SubBackward0>), time elapsed: 459.2941517829895 seconds\n",
      "Step 8300, loss: tensor(0.0417, grad_fn=<SubBackward0>), time elapsed: 464.8198552131653 seconds\n",
      "Step 8400, loss: tensor(0.0408, grad_fn=<SubBackward0>), time elapsed: 470.47235012054443 seconds\n",
      "Step 8500, loss: tensor(0.0397, grad_fn=<SubBackward0>), time elapsed: 475.9995937347412 seconds\n",
      "Step 8600, loss: tensor(0.0399, grad_fn=<SubBackward0>), time elapsed: 481.67139315605164 seconds\n",
      "Step 8700, loss: tensor(0.0377, grad_fn=<SubBackward0>), time elapsed: 487.20130038261414 seconds\n",
      "Step 8800, loss: tensor(0.0386, grad_fn=<SubBackward0>), time elapsed: 492.74923872947693 seconds\n",
      "Step 8900, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 498.4276397228241 seconds\n",
      "Step 9000, loss: tensor(0.0373, grad_fn=<SubBackward0>), time elapsed: 503.94417238235474 seconds\n",
      "Step 9100, loss: tensor(0.0372, grad_fn=<SubBackward0>), time elapsed: 509.6053774356842 seconds\n",
      "Step 9200, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 515.119019985199 seconds\n",
      "Step 9300, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 520.6624953746796 seconds\n",
      "Step 9400, loss: tensor(0.0354, grad_fn=<SubBackward0>), time elapsed: 526.8654954433441 seconds\n",
      "Step 9500, loss: tensor(0.0348, grad_fn=<SubBackward0>), time elapsed: 532.3984415531158 seconds\n",
      "Step 9600, loss: tensor(0.0350, grad_fn=<SubBackward0>), time elapsed: 538.051675081253 seconds\n",
      "Step 9700, loss: tensor(0.0340, grad_fn=<SubBackward0>), time elapsed: 543.5662083625793 seconds\n",
      "Step 9800, loss: tensor(0.0341, grad_fn=<SubBackward0>), time elapsed: 549.2285931110382 seconds\n",
      "Step 9900, loss: tensor(0.0335, grad_fn=<SubBackward0>), time elapsed: 554.750789642334 seconds\n",
      "Step 10000, loss: tensor(0.0321, grad_fn=<SubBackward0>), time elapsed: 560.2829451560974 seconds\n",
      "Step 10100, loss: tensor(0.0333, grad_fn=<SubBackward0>), time elapsed: 565.9349687099457 seconds\n",
      "Step 10200, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 571.4602138996124 seconds\n",
      "Step 10300, loss: tensor(0.0322, grad_fn=<SubBackward0>), time elapsed: 577.1338860988617 seconds\n",
      "Step 10400, loss: tensor(0.0323, grad_fn=<SubBackward0>), time elapsed: 582.669810295105 seconds\n",
      "Step 10500, loss: tensor(0.0327, grad_fn=<SubBackward0>), time elapsed: 588.1829915046692 seconds\n",
      "Step 10600, loss: tensor(0.0320, grad_fn=<SubBackward0>), time elapsed: 593.828771352768 seconds\n",
      "Step 10700, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 599.3431322574615 seconds\n",
      "Step 10800, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 605.0036957263947 seconds\n",
      "Step 10900, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 610.5558376312256 seconds\n",
      "Step 11000, loss: tensor(0.0327, grad_fn=<SubBackward0>), time elapsed: 616.2699506282806 seconds\n",
      "Step 11100, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 621.8092494010925 seconds\n",
      "Step 11200, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 627.3503448963165 seconds\n",
      "Step 11300, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 633.0298895835876 seconds\n",
      "Step 11400, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 638.5622799396515 seconds\n",
      "Step 11500, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 644.2561225891113 seconds\n",
      "Step 11600, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 649.7775931358337 seconds\n",
      "Step 11700, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 655.2888798713684 seconds\n",
      "Step 11800, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 660.997768163681 seconds\n",
      "Step 11900, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 666.5424988269806 seconds\n",
      "Step 12000, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 672.2548711299896 seconds\n",
      "Step 12100, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 677.842808008194 seconds\n",
      "Step 12200, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 683.568124294281 seconds\n",
      "Step 12300, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 689.0793750286102 seconds\n",
      "Step 12400, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 694.6087803840637 seconds\n",
      "Step 12500, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 700.3285026550293 seconds\n",
      "Step 12600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 705.9144427776337 seconds\n",
      "Step 12700, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 711.7207527160645 seconds\n",
      "Step 12800, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 717.495046377182 seconds\n",
      "Step 12900, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 723.0211520195007 seconds\n",
      "Step 13000, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 728.715469121933 seconds\n",
      "Step 13100, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 734.2805716991425 seconds\n",
      "Step 13200, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 740.1306819915771 seconds\n",
      "Step 13300, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 745.6607446670532 seconds\n",
      "Step 13400, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 751.3369817733765 seconds\n",
      "Step 13500, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 756.8519911766052 seconds\n",
      "Step 13600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 762.3826441764832 seconds\n",
      "Step 13700, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 768.0949206352234 seconds\n",
      "Step 13800, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 773.8802154064178 seconds\n",
      "Step 13900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 779.546767950058 seconds\n",
      "Step 14000, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 785.1143264770508 seconds\n",
      "Step 14100, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 790.6495192050934 seconds\n",
      "Step 14200, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 796.3689484596252 seconds\n",
      "Step 14300, loss: tensor(0.0322, grad_fn=<SubBackward0>), time elapsed: 801.9386873245239 seconds\n",
      "Step 14400, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 807.628674030304 seconds\n",
      "Step 14500, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 813.1860072612762 seconds\n",
      "Step 14600, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 818.7615053653717 seconds\n",
      "Step 14700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 824.6137399673462 seconds\n",
      "Step 14800, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 830.1577208042145 seconds\n",
      "Step 14900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 835.9692018032074 seconds\n",
      "Step 15000, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 841.5116305351257 seconds\n",
      "Step 15100, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 847.2027840614319 seconds\n",
      "Step 15200, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 852.8893115520477 seconds\n",
      "Step 15300, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 858.4654037952423 seconds\n",
      "Step 15400, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 864.1783652305603 seconds\n",
      "Step 15500, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 869.7130138874054 seconds\n",
      "Step 15600, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 875.4324028491974 seconds\n",
      "Step 15700, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 881.1670429706573 seconds\n",
      "Step 15800, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 886.7130236625671 seconds\n",
      "Step 15900, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 892.4312806129456 seconds\n",
      "Step 16000, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 897.9793190956116 seconds\n",
      "Step 16100, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 903.6672358512878 seconds\n",
      "Step 16200, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 909.2066395282745 seconds\n",
      "Step 16300, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 915.2378239631653 seconds\n",
      "Step 16400, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 920.7973339557648 seconds\n",
      "Step 16500, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 926.3321685791016 seconds\n",
      "Step 16600, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 932.0225365161896 seconds\n",
      "Step 16700, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 937.5666637420654 seconds\n",
      "Step 16800, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 943.2707257270813 seconds\n",
      "Step 16900, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 948.8427860736847 seconds\n",
      "Step 17000, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 954.3976554870605 seconds\n",
      "Step 17100, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 960.1059110164642 seconds\n",
      "Step 17200, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 965.7735812664032 seconds\n",
      "Step 17300, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 971.4908924102783 seconds\n",
      "Step 17400, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 977.1084272861481 seconds\n",
      "Step 17500, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 982.9147565364838 seconds\n",
      "Step 17600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 988.5093867778778 seconds\n",
      "Step 17700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 994.6740581989288 seconds\n",
      "Step 17800, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1000.3606269359589 seconds\n",
      "Step 17900, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 1005.8871023654938 seconds\n",
      "Step 18000, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 1011.6653780937195 seconds\n",
      "Step 18100, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 1017.2448973655701 seconds\n",
      "Step 18200, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 1022.8391525745392 seconds\n",
      "Step 18300, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 1028.5856583118439 seconds\n",
      "Step 18400, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 1034.2089929580688 seconds\n",
      "Step 18500, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 1039.9846034049988 seconds\n",
      "Step 18600, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 1045.57359457016 seconds\n",
      "Step 18700, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1051.1667740345001 seconds\n",
      "Step 18800, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 1056.9257068634033 seconds\n",
      "Step 18900, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 1062.5178349018097 seconds\n",
      "Step 19000, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 1068.3019132614136 seconds\n",
      "Step 19100, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1073.9440214633942 seconds\n",
      "Step 19200, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1079.7394556999207 seconds\n",
      "Step 19300, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 1085.360764503479 seconds\n",
      "Step 19400, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 1090.9862899780273 seconds\n",
      "Step 19500, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 1096.7807505130768 seconds\n",
      "Step 19600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1102.4147124290466 seconds\n",
      "Step 19700, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 1108.2215247154236 seconds\n",
      "Step 19800, loss: tensor(0.0321, grad_fn=<SubBackward0>), time elapsed: 1113.8512780666351 seconds\n",
      "Step 19900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 1119.5235431194305 seconds\n",
      "Step 20000, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 1125.352337360382 seconds\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.3176, grad_fn=<SubBackward0>), time elapsed: 0.05955243110656738 seconds\n",
      "Step 100, loss: tensor(0.2851, grad_fn=<SubBackward0>), time elapsed: 5.748473882675171 seconds\n",
      "Step 200, loss: tensor(0.2483, grad_fn=<SubBackward0>), time elapsed: 11.529330015182495 seconds\n",
      "Step 300, loss: tensor(0.2187, grad_fn=<SubBackward0>), time elapsed: 17.10581946372986 seconds\n",
      "Step 400, loss: tensor(0.2048, grad_fn=<SubBackward0>), time elapsed: 22.82937526702881 seconds\n",
      "Step 500, loss: tensor(0.1950, grad_fn=<SubBackward0>), time elapsed: 28.3880832195282 seconds\n",
      "Step 600, loss: tensor(0.1810, grad_fn=<SubBackward0>), time elapsed: 33.97398138046265 seconds\n",
      "Step 700, loss: tensor(0.1685, grad_fn=<SubBackward0>), time elapsed: 39.658910512924194 seconds\n",
      "Step 800, loss: tensor(0.1570, grad_fn=<SubBackward0>), time elapsed: 45.217639684677124 seconds\n",
      "Step 900, loss: tensor(0.1476, grad_fn=<SubBackward0>), time elapsed: 50.869389057159424 seconds\n",
      "Step 1000, loss: tensor(0.1388, grad_fn=<SubBackward0>), time elapsed: 56.43752861022949 seconds\n",
      "Step 1100, loss: tensor(0.1299, grad_fn=<SubBackward0>), time elapsed: 62.13061165809631 seconds\n",
      "Step 1200, loss: tensor(0.1263, grad_fn=<SubBackward0>), time elapsed: 67.71975088119507 seconds\n",
      "Step 1300, loss: tensor(0.1211, grad_fn=<SubBackward0>), time elapsed: 73.27947115898132 seconds\n",
      "Step 1400, loss: tensor(0.1207, grad_fn=<SubBackward0>), time elapsed: 78.97702932357788 seconds\n",
      "Step 1500, loss: tensor(0.1123, grad_fn=<SubBackward0>), time elapsed: 84.54190921783447 seconds\n",
      "Step 1600, loss: tensor(0.1115, grad_fn=<SubBackward0>), time elapsed: 90.22992014884949 seconds\n",
      "Step 1700, loss: tensor(0.1096, grad_fn=<SubBackward0>), time elapsed: 95.80231833457947 seconds\n",
      "Step 1800, loss: tensor(0.1050, grad_fn=<SubBackward0>), time elapsed: 101.53613567352295 seconds\n",
      "Step 1900, loss: tensor(0.1055, grad_fn=<SubBackward0>), time elapsed: 107.10846018791199 seconds\n",
      "Step 2000, loss: tensor(0.0995, grad_fn=<SubBackward0>), time elapsed: 112.7068784236908 seconds\n",
      "Step 2100, loss: tensor(0.0972, grad_fn=<SubBackward0>), time elapsed: 118.42626333236694 seconds\n",
      "Step 2200, loss: tensor(0.0975, grad_fn=<SubBackward0>), time elapsed: 124.01848006248474 seconds\n",
      "Step 2300, loss: tensor(0.0962, grad_fn=<SubBackward0>), time elapsed: 129.7324583530426 seconds\n",
      "Step 2400, loss: tensor(0.0922, grad_fn=<SubBackward0>), time elapsed: 135.30587649345398 seconds\n",
      "Step 2500, loss: tensor(0.0938, grad_fn=<SubBackward0>), time elapsed: 140.88127517700195 seconds\n",
      "Step 2600, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 146.58695912361145 seconds\n",
      "Step 2700, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 152.18027138710022 seconds\n",
      "Step 2800, loss: tensor(0.0886, grad_fn=<SubBackward0>), time elapsed: 157.92360711097717 seconds\n",
      "Step 2900, loss: tensor(0.0888, grad_fn=<SubBackward0>), time elapsed: 163.5072774887085 seconds\n",
      "Step 3000, loss: tensor(0.0882, grad_fn=<SubBackward0>), time elapsed: 169.2235562801361 seconds\n",
      "Step 3100, loss: tensor(0.0860, grad_fn=<SubBackward0>), time elapsed: 174.78645300865173 seconds\n",
      "Step 3200, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 180.34625053405762 seconds\n",
      "Step 3300, loss: tensor(0.0847, grad_fn=<SubBackward0>), time elapsed: 186.04778909683228 seconds\n",
      "Step 3400, loss: tensor(0.0844, grad_fn=<SubBackward0>), time elapsed: 191.60766863822937 seconds\n",
      "Step 3500, loss: tensor(0.0844, grad_fn=<SubBackward0>), time elapsed: 197.3248791694641 seconds\n",
      "Step 3600, loss: tensor(0.0846, grad_fn=<SubBackward0>), time elapsed: 202.87760734558105 seconds\n",
      "Step 3700, loss: tensor(0.0835, grad_fn=<SubBackward0>), time elapsed: 208.5832815170288 seconds\n",
      "Step 3800, loss: tensor(0.0837, grad_fn=<SubBackward0>), time elapsed: 214.13459300994873 seconds\n",
      "Step 3900, loss: tensor(0.0824, grad_fn=<SubBackward0>), time elapsed: 219.7100899219513 seconds\n",
      "Step 4000, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 225.413831949234 seconds\n",
      "Step 4100, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 230.96858596801758 seconds\n",
      "Step 4200, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 236.6866934299469 seconds\n",
      "Step 4300, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 242.24649930000305 seconds\n",
      "Step 4400, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 247.84800148010254 seconds\n",
      "Step 4500, loss: tensor(0.0784, grad_fn=<SubBackward0>), time elapsed: 253.56836581230164 seconds\n",
      "Step 4600, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 259.1647198200226 seconds\n",
      "Step 4700, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 264.88627195358276 seconds\n",
      "Step 4800, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 270.5042061805725 seconds\n",
      "Step 4900, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 277.21961522102356 seconds\n",
      "Step 5000, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 282.85013365745544 seconds\n",
      "Step 5100, loss: tensor(0.0695, grad_fn=<SubBackward0>), time elapsed: 288.43079805374146 seconds\n",
      "Step 5200, loss: tensor(0.0663, grad_fn=<SubBackward0>), time elapsed: 294.14910769462585 seconds\n",
      "Step 5300, loss: tensor(0.0638, grad_fn=<SubBackward0>), time elapsed: 299.7464406490326 seconds\n",
      "Step 5400, loss: tensor(0.0623, grad_fn=<SubBackward0>), time elapsed: 305.47313833236694 seconds\n",
      "Step 5500, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 311.0808160305023 seconds\n",
      "Step 5600, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 316.9085958003998 seconds\n",
      "Step 5700, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 322.5841727256775 seconds\n",
      "Step 5800, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 328.24615025520325 seconds\n",
      "Step 5900, loss: tensor(0.0583, grad_fn=<SubBackward0>), time elapsed: 333.9604423046112 seconds\n",
      "Step 6000, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 339.5507962703705 seconds\n",
      "Step 6100, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 345.2863390445709 seconds\n",
      "Step 6200, loss: tensor(0.0553, grad_fn=<SubBackward0>), time elapsed: 350.86925745010376 seconds\n",
      "Step 6300, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 356.44347739219666 seconds\n",
      "Step 6400, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 362.14718532562256 seconds\n",
      "Step 6500, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 367.7257409095764 seconds\n",
      "Step 6600, loss: tensor(0.0549, grad_fn=<SubBackward0>), time elapsed: 373.42944526672363 seconds\n",
      "Step 6700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 378.99349999427795 seconds\n",
      "Step 6800, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 384.7160475254059 seconds\n",
      "Step 6900, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 390.32161808013916 seconds\n",
      "Step 7000, loss: tensor(0.0554, grad_fn=<SubBackward0>), time elapsed: 395.931414604187 seconds\n",
      "Step 7100, loss: tensor(0.0536, grad_fn=<SubBackward0>), time elapsed: 401.68725204467773 seconds\n",
      "Step 7200, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 407.3096160888672 seconds\n",
      "Step 7300, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 413.08639192581177 seconds\n",
      "Step 7400, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 418.98705101013184 seconds\n",
      "Step 7500, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 424.7481164932251 seconds\n",
      "Step 7600, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 430.3663799762726 seconds\n",
      "Step 7700, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 436.347375869751 seconds\n",
      "Step 7800, loss: tensor(0.0547, grad_fn=<SubBackward0>), time elapsed: 442.07501316070557 seconds\n",
      "Step 7900, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 447.64105772972107 seconds\n",
      "Step 8000, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 453.37301683425903 seconds\n",
      "Step 8100, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 459.05995512008667 seconds\n",
      "Step 8200, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 464.7887306213379 seconds\n",
      "Step 8300, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 470.3860397338867 seconds\n",
      "Step 8400, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 477.26818799972534 seconds\n",
      "Step 8500, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 483.0259165763855 seconds\n",
      "Step 8600, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 488.60759115219116 seconds\n",
      "Step 8700, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 494.3688757419586 seconds\n",
      "Step 8800, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 499.93288135528564 seconds\n",
      "Step 8900, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 505.5008466243744 seconds\n",
      "Step 9000, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 511.20658707618713 seconds\n",
      "Step 9100, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 516.7726154327393 seconds\n",
      "Step 9200, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 522.5096955299377 seconds\n",
      "Step 9300, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 528.1070251464844 seconds\n",
      "Step 9400, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 533.8837847709656 seconds\n",
      "Step 9500, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 539.4603731632233 seconds\n",
      "Step 9600, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 545.0252559185028 seconds\n",
      "Step 9700, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 550.7676687240601 seconds\n",
      "Step 9800, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 556.3483068943024 seconds\n",
      "Step 9900, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 562.0873031616211 seconds\n",
      "Step 10000, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 567.6681394577026 seconds\n",
      "Step 10100, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 573.248613357544 seconds\n",
      "Step 10200, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 578.9878339767456 seconds\n",
      "Step 10300, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 584.6423873901367 seconds\n",
      "Step 10400, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 590.3868091106415 seconds\n",
      "Step 10500, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 596.0164141654968 seconds\n",
      "Step 10600, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 601.7693402767181 seconds\n",
      "Step 10700, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 607.3832886219025 seconds\n",
      "Step 10800, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 613.0182299613953 seconds\n",
      "Step 10900, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 618.8042025566101 seconds\n",
      "Step 11000, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 624.4296867847443 seconds\n",
      "Step 11100, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 630.2479376792908 seconds\n",
      "Step 11200, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 635.8705625534058 seconds\n",
      "Step 11300, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 641.5083866119385 seconds\n",
      "Step 11400, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 647.3099083900452 seconds\n",
      "Step 11500, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 652.9460439682007 seconds\n",
      "Step 11600, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 658.7352364063263 seconds\n",
      "Step 11700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 664.3619492053986 seconds\n",
      "Step 11800, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 670.7601130008698 seconds\n",
      "Step 11900, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 676.3865756988525 seconds\n",
      "Step 12000, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 682.0075442790985 seconds\n",
      "Step 12100, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 687.7606558799744 seconds\n",
      "Step 12200, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 693.3817830085754 seconds\n",
      "Step 12300, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 699.1449429988861 seconds\n",
      "Step 12400, loss: tensor(0.0533, grad_fn=<SubBackward0>), time elapsed: 704.8046991825104 seconds\n",
      "Step 12500, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 710.4418804645538 seconds\n",
      "Step 12600, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 716.2603561878204 seconds\n",
      "Step 12700, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 721.9607198238373 seconds\n",
      "Step 12800, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 727.7968726158142 seconds\n",
      "Step 12900, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 733.7320804595947 seconds\n",
      "Step 13000, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 739.630802154541 seconds\n",
      "Step 13100, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 745.3313539028168 seconds\n",
      "Step 13200, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 751.6402590274811 seconds\n",
      "Step 13300, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 757.4940614700317 seconds\n",
      "Step 13400, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 763.1889896392822 seconds\n",
      "Step 13500, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 769.0601093769073 seconds\n",
      "Step 13600, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 774.7170853614807 seconds\n",
      "Step 13700, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 780.3799502849579 seconds\n",
      "Step 13800, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 786.2182383537292 seconds\n",
      "Step 13900, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 791.9325187206268 seconds\n",
      "Step 14000, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 798.0648708343506 seconds\n",
      "Step 14100, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 803.8085134029388 seconds\n",
      "Step 14200, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 809.5701491832733 seconds\n",
      "Step 14300, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 815.2806940078735 seconds\n",
      "Step 14400, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 820.8605124950409 seconds\n",
      "Step 14500, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 826.596892118454 seconds\n",
      "Step 14600, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 832.5196936130524 seconds\n",
      "Step 14700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 838.2242531776428 seconds\n",
      "Step 14800, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 843.88321352005 seconds\n",
      "Step 14900, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 849.4241764545441 seconds\n",
      "Step 15000, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 855.1978225708008 seconds\n",
      "Step 15100, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 860.8924238681793 seconds\n",
      "Step 15200, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 866.610265493393 seconds\n",
      "Step 15300, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 872.3215978145599 seconds\n",
      "Step 15400, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 877.9235453605652 seconds\n",
      "Step 15500, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 883.6518213748932 seconds\n",
      "Step 15600, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 889.365291595459 seconds\n",
      "Step 15700, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 895.1727449893951 seconds\n",
      "Step 15800, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 900.726633310318 seconds\n",
      "Step 15900, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 906.5364263057709 seconds\n",
      "Step 16000, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 912.113046169281 seconds\n",
      "Step 16100, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 918.3234853744507 seconds\n",
      "Step 16200, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 924.0252883434296 seconds\n",
      "Step 16300, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 929.5860455036163 seconds\n",
      "Step 16400, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 935.2930870056152 seconds\n",
      "Step 16500, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 940.861466884613 seconds\n",
      "Step 16600, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 946.5521433353424 seconds\n",
      "Step 16700, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 952.3761126995087 seconds\n",
      "Step 16800, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 957.9234426021576 seconds\n",
      "Step 16900, loss: tensor(0.0538, grad_fn=<SubBackward0>), time elapsed: 963.9816210269928 seconds\n",
      "Step 17000, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 969.5239350795746 seconds\n",
      "Step 17100, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 975.2326884269714 seconds\n",
      "Step 17200, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 980.8031227588654 seconds\n",
      "Step 17300, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 986.3434529304504 seconds\n",
      "Step 17400, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 992.0465984344482 seconds\n",
      "Step 17500, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 997.6012053489685 seconds\n",
      "Step 17600, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 1003.3417098522186 seconds\n",
      "Step 17700, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 1008.9661452770233 seconds\n",
      "Step 17800, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 1015.2822494506836 seconds\n",
      "Step 17900, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 1021.0028312206268 seconds\n",
      "Step 18000, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1026.5667371749878 seconds\n",
      "Step 18100, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 1032.2854351997375 seconds\n",
      "Step 18200, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 1037.8567569255829 seconds\n",
      "Step 18300, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 1043.421864748001 seconds\n",
      "Step 18400, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 1049.1612088680267 seconds\n",
      "Step 18500, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 1054.7826571464539 seconds\n",
      "Step 18600, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 1060.4891011714935 seconds\n",
      "Step 18700, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 1066.0472767353058 seconds\n",
      "Step 18800, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 1071.7778539657593 seconds\n",
      "Step 18900, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 1077.310611963272 seconds\n",
      "Step 19000, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 1082.8800299167633 seconds\n",
      "Step 19100, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 1088.5931570529938 seconds\n",
      "Step 19200, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 1094.1426129341125 seconds\n",
      "Step 19300, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1099.8860943317413 seconds\n",
      "Step 19400, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 1105.4398539066315 seconds\n",
      "Step 19500, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 1111.002469778061 seconds\n",
      "Step 19600, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1116.7968497276306 seconds\n",
      "Step 19700, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 1123.875884771347 seconds\n",
      "Step 19800, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 1129.6128253936768 seconds\n",
      "Step 19900, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 1135.1553122997284 seconds\n",
      "Step 20000, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 1140.7229056358337 seconds\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.4988, grad_fn=<SubBackward0>), time elapsed: 0.0599675178527832 seconds\n",
      "Step 100, loss: tensor(0.4451, grad_fn=<SubBackward0>), time elapsed: 5.673086404800415 seconds\n",
      "Step 200, loss: tensor(0.3738, grad_fn=<SubBackward0>), time elapsed: 11.24161696434021 seconds\n",
      "Step 300, loss: tensor(0.2956, grad_fn=<SubBackward0>), time elapsed: 16.864001750946045 seconds\n",
      "Step 400, loss: tensor(0.2498, grad_fn=<SubBackward0>), time elapsed: 22.365018844604492 seconds\n",
      "Step 500, loss: tensor(0.2220, grad_fn=<SubBackward0>), time elapsed: 28.001725912094116 seconds\n",
      "Step 600, loss: tensor(0.2042, grad_fn=<SubBackward0>), time elapsed: 33.48745512962341 seconds\n",
      "Step 700, loss: tensor(0.1913, grad_fn=<SubBackward0>), time elapsed: 39.01809763908386 seconds\n",
      "Step 800, loss: tensor(0.1802, grad_fn=<SubBackward0>), time elapsed: 44.65255284309387 seconds\n",
      "Step 900, loss: tensor(0.1721, grad_fn=<SubBackward0>), time elapsed: 50.17551612854004 seconds\n",
      "Step 1000, loss: tensor(0.1664, grad_fn=<SubBackward0>), time elapsed: 55.83338451385498 seconds\n",
      "Step 1100, loss: tensor(0.1608, grad_fn=<SubBackward0>), time elapsed: 61.375001192092896 seconds\n",
      "Step 1200, loss: tensor(0.1562, grad_fn=<SubBackward0>), time elapsed: 67.01808547973633 seconds\n",
      "Step 1300, loss: tensor(0.1519, grad_fn=<SubBackward0>), time elapsed: 72.506676197052 seconds\n",
      "Step 1400, loss: tensor(0.1496, grad_fn=<SubBackward0>), time elapsed: 78.00006413459778 seconds\n",
      "Step 1500, loss: tensor(0.1481, grad_fn=<SubBackward0>), time elapsed: 83.63940644264221 seconds\n",
      "Step 1600, loss: tensor(0.1474, grad_fn=<SubBackward0>), time elapsed: 89.16807675361633 seconds\n",
      "Step 1700, loss: tensor(0.1444, grad_fn=<SubBackward0>), time elapsed: 94.81337904930115 seconds\n",
      "Step 1800, loss: tensor(0.1396, grad_fn=<SubBackward0>), time elapsed: 100.33804321289062 seconds\n",
      "Step 1900, loss: tensor(0.1367, grad_fn=<SubBackward0>), time elapsed: 105.97847986221313 seconds\n",
      "Step 2000, loss: tensor(0.1340, grad_fn=<SubBackward0>), time elapsed: 111.47847127914429 seconds\n",
      "Step 2100, loss: tensor(0.1355, grad_fn=<SubBackward0>), time elapsed: 116.9933431148529 seconds\n",
      "Step 2200, loss: tensor(0.1292, grad_fn=<SubBackward0>), time elapsed: 122.63233184814453 seconds\n",
      "Step 2300, loss: tensor(0.1278, grad_fn=<SubBackward0>), time elapsed: 128.14141654968262 seconds\n",
      "Step 2400, loss: tensor(0.1271, grad_fn=<SubBackward0>), time elapsed: 133.7721812725067 seconds\n",
      "Step 2500, loss: tensor(0.1267, grad_fn=<SubBackward0>), time elapsed: 139.28489136695862 seconds\n",
      "Step 2600, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 144.9409384727478 seconds\n",
      "Step 2700, loss: tensor(0.1192, grad_fn=<SubBackward0>), time elapsed: 150.46457171440125 seconds\n",
      "Step 2800, loss: tensor(0.1165, grad_fn=<SubBackward0>), time elapsed: 155.99446320533752 seconds\n",
      "Step 2900, loss: tensor(0.1190, grad_fn=<SubBackward0>), time elapsed: 161.61800456047058 seconds\n",
      "Step 3000, loss: tensor(0.1171, grad_fn=<SubBackward0>), time elapsed: 167.12133979797363 seconds\n",
      "Step 3100, loss: tensor(0.1149, grad_fn=<SubBackward0>), time elapsed: 172.74762296676636 seconds\n",
      "Step 3200, loss: tensor(0.1155, grad_fn=<SubBackward0>), time elapsed: 178.2496473789215 seconds\n",
      "Step 3300, loss: tensor(0.1132, grad_fn=<SubBackward0>), time elapsed: 183.78439331054688 seconds\n",
      "Step 3400, loss: tensor(0.1144, grad_fn=<SubBackward0>), time elapsed: 189.4445445537567 seconds\n",
      "Step 3500, loss: tensor(0.1141, grad_fn=<SubBackward0>), time elapsed: 194.9724669456482 seconds\n",
      "Step 3600, loss: tensor(0.1120, grad_fn=<SubBackward0>), time elapsed: 200.60442185401917 seconds\n",
      "Step 3700, loss: tensor(0.1114, grad_fn=<SubBackward0>), time elapsed: 206.11015701293945 seconds\n",
      "Step 3800, loss: tensor(0.1094, grad_fn=<SubBackward0>), time elapsed: 211.7900938987732 seconds\n",
      "Step 3900, loss: tensor(0.1118, grad_fn=<SubBackward0>), time elapsed: 217.28892970085144 seconds\n",
      "Step 4000, loss: tensor(0.1089, grad_fn=<SubBackward0>), time elapsed: 222.7831666469574 seconds\n",
      "Step 4100, loss: tensor(0.1087, grad_fn=<SubBackward0>), time elapsed: 228.40738797187805 seconds\n",
      "Step 4200, loss: tensor(0.1052, grad_fn=<SubBackward0>), time elapsed: 233.92565393447876 seconds\n",
      "Step 4300, loss: tensor(0.1025, grad_fn=<SubBackward0>), time elapsed: 239.6027045249939 seconds\n",
      "Step 4400, loss: tensor(0.1042, grad_fn=<SubBackward0>), time elapsed: 245.10628294944763 seconds\n",
      "Step 4500, loss: tensor(0.1023, grad_fn=<SubBackward0>), time elapsed: 250.7570915222168 seconds\n",
      "Step 4600, loss: tensor(0.1019, grad_fn=<SubBackward0>), time elapsed: 256.25228929519653 seconds\n",
      "Step 4700, loss: tensor(0.0986, grad_fn=<SubBackward0>), time elapsed: 261.7512278556824 seconds\n",
      "Step 4800, loss: tensor(0.0974, grad_fn=<SubBackward0>), time elapsed: 267.3765971660614 seconds\n",
      "Step 4900, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 272.9295725822449 seconds\n",
      "Step 5000, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 278.5631742477417 seconds\n",
      "Step 5100, loss: tensor(0.0956, grad_fn=<SubBackward0>), time elapsed: 284.0665936470032 seconds\n",
      "Step 5200, loss: tensor(0.0911, grad_fn=<SubBackward0>), time elapsed: 289.5810661315918 seconds\n",
      "Step 5300, loss: tensor(0.0850, grad_fn=<SubBackward0>), time elapsed: 295.242205619812 seconds\n",
      "Step 5400, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 300.7599060535431 seconds\n",
      "Step 5500, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 306.3815643787384 seconds\n",
      "Step 5600, loss: tensor(0.0856, grad_fn=<SubBackward0>), time elapsed: 311.87810349464417 seconds\n",
      "Step 5700, loss: tensor(0.0859, grad_fn=<SubBackward0>), time elapsed: 317.54471015930176 seconds\n",
      "Step 5800, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 323.0485098361969 seconds\n",
      "Step 5900, loss: tensor(0.0834, grad_fn=<SubBackward0>), time elapsed: 328.5654036998749 seconds\n",
      "Step 6000, loss: tensor(0.0827, grad_fn=<SubBackward0>), time elapsed: 334.244802236557 seconds\n",
      "Step 6100, loss: tensor(0.0815, grad_fn=<SubBackward0>), time elapsed: 339.77567434310913 seconds\n",
      "Step 6200, loss: tensor(0.0796, grad_fn=<SubBackward0>), time elapsed: 345.4236104488373 seconds\n",
      "Step 6300, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 350.9368839263916 seconds\n",
      "Step 6400, loss: tensor(0.0786, grad_fn=<SubBackward0>), time elapsed: 356.57995772361755 seconds\n",
      "Step 6500, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 362.0937993526459 seconds\n",
      "Step 6600, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 367.5864565372467 seconds\n",
      "Step 6700, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 373.2098114490509 seconds\n",
      "Step 6800, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 378.7232904434204 seconds\n",
      "Step 6900, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 384.3595414161682 seconds\n",
      "Step 7000, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 389.8657877445221 seconds\n",
      "Step 7100, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 395.40313720703125 seconds\n",
      "Step 7200, loss: tensor(0.0692, grad_fn=<SubBackward0>), time elapsed: 401.03588032722473 seconds\n",
      "Step 7300, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 406.54494428634644 seconds\n",
      "Step 7400, loss: tensor(0.0683, grad_fn=<SubBackward0>), time elapsed: 412.19260907173157 seconds\n",
      "Step 7500, loss: tensor(0.0698, grad_fn=<SubBackward0>), time elapsed: 417.67816519737244 seconds\n",
      "Step 7600, loss: tensor(0.0708, grad_fn=<SubBackward0>), time elapsed: 423.3643014431 seconds\n",
      "Step 7700, loss: tensor(0.0667, grad_fn=<SubBackward0>), time elapsed: 428.8981611728668 seconds\n",
      "Step 7800, loss: tensor(0.0668, grad_fn=<SubBackward0>), time elapsed: 434.4219686985016 seconds\n",
      "Step 7900, loss: tensor(0.0664, grad_fn=<SubBackward0>), time elapsed: 440.07550382614136 seconds\n",
      "Step 8000, loss: tensor(0.0687, grad_fn=<SubBackward0>), time elapsed: 445.5874009132385 seconds\n",
      "Step 8100, loss: tensor(0.0641, grad_fn=<SubBackward0>), time elapsed: 451.2412705421448 seconds\n",
      "Step 8200, loss: tensor(0.0663, grad_fn=<SubBackward0>), time elapsed: 456.7759494781494 seconds\n",
      "Step 8300, loss: tensor(0.0611, grad_fn=<SubBackward0>), time elapsed: 462.4183371067047 seconds\n",
      "Step 8400, loss: tensor(0.0625, grad_fn=<SubBackward0>), time elapsed: 467.9539477825165 seconds\n",
      "Step 8500, loss: tensor(0.0630, grad_fn=<SubBackward0>), time elapsed: 473.5014202594757 seconds\n",
      "Step 8600, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 479.1730053424835 seconds\n",
      "Step 8700, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 484.7386963367462 seconds\n",
      "Step 8800, loss: tensor(0.0602, grad_fn=<SubBackward0>), time elapsed: 490.38742303848267 seconds\n",
      "Step 8900, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 495.890825510025 seconds\n",
      "Step 9000, loss: tensor(0.0603, grad_fn=<SubBackward0>), time elapsed: 501.3968241214752 seconds\n",
      "Step 9100, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 507.034375667572 seconds\n",
      "Step 9200, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 512.5718734264374 seconds\n",
      "Step 9300, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 518.2810888290405 seconds\n",
      "Step 9400, loss: tensor(0.0550, grad_fn=<SubBackward0>), time elapsed: 523.8460538387299 seconds\n",
      "Step 9500, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 529.6781101226807 seconds\n",
      "Step 9600, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 535.1822938919067 seconds\n",
      "Step 9700, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 540.8322486877441 seconds\n",
      "Step 9800, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 546.6477773189545 seconds\n",
      "Step 9900, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 552.2231426239014 seconds\n",
      "Step 10000, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 557.9065403938293 seconds\n",
      "Step 10100, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 563.439530134201 seconds\n",
      "Step 10200, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 568.9841668605804 seconds\n",
      "Step 10300, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 574.7065677642822 seconds\n",
      "Step 10400, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 580.243301153183 seconds\n",
      "Step 10500, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 586.1811199188232 seconds\n",
      "Step 10600, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 591.7654511928558 seconds\n",
      "Step 10700, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 597.4948587417603 seconds\n",
      "Step 10800, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 603.0189030170441 seconds\n",
      "Step 10900, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 608.5819706916809 seconds\n",
      "Step 11000, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 614.2862317562103 seconds\n",
      "Step 11100, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 619.8410928249359 seconds\n",
      "Step 11200, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 625.5774357318878 seconds\n",
      "Step 11300, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 631.1902642250061 seconds\n",
      "Step 11400, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 637.24422955513 seconds\n",
      "Step 11500, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 642.9186019897461 seconds\n",
      "Step 11600, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 648.453046798706 seconds\n",
      "Step 11700, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 654.1252748966217 seconds\n",
      "Step 11800, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 659.661972284317 seconds\n",
      "Step 11900, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 665.3366413116455 seconds\n",
      "Step 12000, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 670.8865487575531 seconds\n",
      "Step 12100, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 676.3966162204742 seconds\n",
      "Step 12200, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 683.5611908435822 seconds\n",
      "Step 12300, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 689.0823228359222 seconds\n",
      "Step 12400, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 694.7885458469391 seconds\n",
      "Step 12500, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 700.3312077522278 seconds\n",
      "Step 12600, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 705.8883099555969 seconds\n",
      "Step 12700, loss: tensor(0.0490, grad_fn=<SubBackward0>), time elapsed: 711.5646901130676 seconds\n",
      "Step 12800, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 717.0920596122742 seconds\n",
      "Step 12900, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 723.0514805316925 seconds\n",
      "Step 13000, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 728.6466352939606 seconds\n",
      "Step 13100, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 734.3439521789551 seconds\n",
      "Step 13200, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 739.8784718513489 seconds\n",
      "Step 13300, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 745.4277634620667 seconds\n",
      "Step 13400, loss: tensor(0.0490, grad_fn=<SubBackward0>), time elapsed: 751.129613161087 seconds\n",
      "Step 13500, loss: tensor(0.0470, grad_fn=<SubBackward0>), time elapsed: 756.6651866436005 seconds\n",
      "Step 13600, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 762.350359916687 seconds\n",
      "Step 13700, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 768.2872657775879 seconds\n",
      "Step 13800, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 773.8382461071014 seconds\n",
      "Step 13900, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 779.5263504981995 seconds\n",
      "Step 14000, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 785.0602910518646 seconds\n",
      "Step 14100, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 790.748804807663 seconds\n",
      "Step 14200, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 796.2898495197296 seconds\n",
      "Step 14300, loss: tensor(0.0465, grad_fn=<SubBackward0>), time elapsed: 801.9644291400909 seconds\n",
      "Step 14400, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 807.4875628948212 seconds\n",
      "Step 14500, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 813.0062470436096 seconds\n",
      "Step 14600, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 818.7102348804474 seconds\n",
      "Step 14700, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 824.2460877895355 seconds\n",
      "Step 14800, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 829.9279897212982 seconds\n",
      "Step 14900, loss: tensor(0.0476, grad_fn=<SubBackward0>), time elapsed: 835.482593536377 seconds\n",
      "Step 15000, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 841.028082370758 seconds\n",
      "Step 15100, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 846.7138230800629 seconds\n",
      "Step 15200, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 852.2825059890747 seconds\n",
      "Step 15300, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 857.9814918041229 seconds\n",
      "Step 15400, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 863.5412068367004 seconds\n",
      "Step 15500, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 869.2450213432312 seconds\n",
      "Step 15600, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 874.8102378845215 seconds\n",
      "Step 15700, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 880.3904340267181 seconds\n",
      "Step 15800, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 886.1103668212891 seconds\n",
      "Step 15900, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 891.640810251236 seconds\n",
      "Step 16000, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 897.3554458618164 seconds\n",
      "Step 16100, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 902.890917301178 seconds\n",
      "Step 16200, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 908.4556124210358 seconds\n",
      "Step 16300, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 914.1698575019836 seconds\n",
      "Step 16400, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 919.7221763134003 seconds\n",
      "Step 16500, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 925.4267356395721 seconds\n",
      "Step 16600, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 930.9922246932983 seconds\n",
      "Step 16700, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 936.6995394229889 seconds\n",
      "Step 16800, loss: tensor(0.0474, grad_fn=<SubBackward0>), time elapsed: 942.26478099823 seconds\n",
      "Step 16900, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 948.070100069046 seconds\n",
      "Step 17000, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 953.7785315513611 seconds\n",
      "Step 17100, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 959.3301241397858 seconds\n",
      "Step 17200, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 965.0182387828827 seconds\n",
      "Step 17300, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 970.585462808609 seconds\n",
      "Step 17400, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 976.1398568153381 seconds\n",
      "Step 17500, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 981.8573267459869 seconds\n",
      "Step 17600, loss: tensor(0.0463, grad_fn=<SubBackward0>), time elapsed: 987.4308831691742 seconds\n",
      "Step 17700, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 993.1442363262177 seconds\n",
      "Step 17800, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 998.7782990932465 seconds\n",
      "Step 17900, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 1004.3922004699707 seconds\n",
      "Step 18000, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 1010.1226665973663 seconds\n",
      "Step 18100, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1015.7214493751526 seconds\n",
      "Step 18200, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1021.5352745056152 seconds\n",
      "Step 18300, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 1027.6863079071045 seconds\n",
      "Step 18400, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 1033.4458253383636 seconds\n",
      "Step 18500, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 1038.9813511371613 seconds\n",
      "Step 18600, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 1044.5038740634918 seconds\n",
      "Step 18700, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 1050.1986043453217 seconds\n",
      "Step 18800, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 1055.7481174468994 seconds\n",
      "Step 18900, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 1061.4542796611786 seconds\n",
      "Step 19000, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 1067.0076925754547 seconds\n",
      "Step 19100, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 1072.5519406795502 seconds\n",
      "Step 19200, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1078.256949186325 seconds\n",
      "Step 19300, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 1083.808467388153 seconds\n",
      "Step 19400, loss: tensor(0.0468, grad_fn=<SubBackward0>), time elapsed: 1089.5544850826263 seconds\n",
      "Step 19500, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 1095.1537899971008 seconds\n",
      "Step 19600, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 1101.854486465454 seconds\n",
      "Step 19700, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1107.5655677318573 seconds\n",
      "Step 19800, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1113.1126036643982 seconds\n",
      "Step 19900, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 1118.8268251419067 seconds\n",
      "Step 20000, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 1124.401051044464 seconds\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.7608, grad_fn=<SubBackward0>), time elapsed: 0.0603337287902832 seconds\n",
      "Step 100, loss: tensor(0.7097, grad_fn=<SubBackward0>), time elapsed: 5.879121780395508 seconds\n",
      "Step 200, loss: tensor(0.6380, grad_fn=<SubBackward0>), time elapsed: 11.447932243347168 seconds\n",
      "Step 300, loss: tensor(0.5466, grad_fn=<SubBackward0>), time elapsed: 16.93709635734558 seconds\n",
      "Step 400, loss: tensor(0.4489, grad_fn=<SubBackward0>), time elapsed: 22.574985027313232 seconds\n",
      "Step 500, loss: tensor(0.3625, grad_fn=<SubBackward0>), time elapsed: 28.082396745681763 seconds\n",
      "Step 600, loss: tensor(0.3036, grad_fn=<SubBackward0>), time elapsed: 33.736506938934326 seconds\n",
      "Step 700, loss: tensor(0.2621, grad_fn=<SubBackward0>), time elapsed: 39.241549491882324 seconds\n",
      "Step 800, loss: tensor(0.2312, grad_fn=<SubBackward0>), time elapsed: 44.88120126724243 seconds\n",
      "Step 900, loss: tensor(0.2048, grad_fn=<SubBackward0>), time elapsed: 50.38529968261719 seconds\n",
      "Step 1000, loss: tensor(0.1855, grad_fn=<SubBackward0>), time elapsed: 55.91467046737671 seconds\n",
      "Step 1100, loss: tensor(0.1687, grad_fn=<SubBackward0>), time elapsed: 61.550291776657104 seconds\n",
      "Step 1200, loss: tensor(0.1561, grad_fn=<SubBackward0>), time elapsed: 67.0875494480133 seconds\n",
      "Step 1300, loss: tensor(0.1446, grad_fn=<SubBackward0>), time elapsed: 72.74131965637207 seconds\n",
      "Step 1400, loss: tensor(0.1363, grad_fn=<SubBackward0>), time elapsed: 78.23391461372375 seconds\n",
      "Step 1500, loss: tensor(0.1338, grad_fn=<SubBackward0>), time elapsed: 83.74945855140686 seconds\n",
      "Step 1600, loss: tensor(0.1265, grad_fn=<SubBackward0>), time elapsed: 89.38594770431519 seconds\n",
      "Step 1700, loss: tensor(0.1239, grad_fn=<SubBackward0>), time elapsed: 94.94654273986816 seconds\n",
      "Step 1800, loss: tensor(0.1237, grad_fn=<SubBackward0>), time elapsed: 100.59437608718872 seconds\n",
      "Step 1900, loss: tensor(0.1207, grad_fn=<SubBackward0>), time elapsed: 106.10434412956238 seconds\n",
      "Step 2000, loss: tensor(0.1196, grad_fn=<SubBackward0>), time elapsed: 111.72513341903687 seconds\n",
      "Step 2100, loss: tensor(0.1191, grad_fn=<SubBackward0>), time elapsed: 117.22833752632141 seconds\n",
      "Step 2200, loss: tensor(0.1160, grad_fn=<SubBackward0>), time elapsed: 122.70956897735596 seconds\n",
      "Step 2300, loss: tensor(0.1175, grad_fn=<SubBackward0>), time elapsed: 128.3465437889099 seconds\n",
      "Step 2400, loss: tensor(0.1154, grad_fn=<SubBackward0>), time elapsed: 133.8225281238556 seconds\n",
      "Step 2500, loss: tensor(0.1156, grad_fn=<SubBackward0>), time elapsed: 139.45855569839478 seconds\n",
      "Step 2600, loss: tensor(0.1144, grad_fn=<SubBackward0>), time elapsed: 144.97835874557495 seconds\n",
      "Step 2700, loss: tensor(0.1159, grad_fn=<SubBackward0>), time elapsed: 150.63480591773987 seconds\n",
      "Step 2800, loss: tensor(0.1162, grad_fn=<SubBackward0>), time elapsed: 156.15611219406128 seconds\n",
      "Step 2900, loss: tensor(0.1149, grad_fn=<SubBackward0>), time elapsed: 161.65440964698792 seconds\n",
      "Step 3000, loss: tensor(0.1150, grad_fn=<SubBackward0>), time elapsed: 167.26637768745422 seconds\n",
      "Step 3100, loss: tensor(0.1139, grad_fn=<SubBackward0>), time elapsed: 172.77274107933044 seconds\n",
      "Step 3200, loss: tensor(0.1120, grad_fn=<SubBackward0>), time elapsed: 178.39787793159485 seconds\n",
      "Step 3300, loss: tensor(0.1135, grad_fn=<SubBackward0>), time elapsed: 183.91302037239075 seconds\n",
      "Step 3400, loss: tensor(0.1121, grad_fn=<SubBackward0>), time elapsed: 189.58109402656555 seconds\n",
      "Step 3500, loss: tensor(0.1138, grad_fn=<SubBackward0>), time elapsed: 195.10535264015198 seconds\n",
      "Step 3600, loss: tensor(0.1119, grad_fn=<SubBackward0>), time elapsed: 200.63884592056274 seconds\n",
      "Step 3700, loss: tensor(0.1123, grad_fn=<SubBackward0>), time elapsed: 206.3046178817749 seconds\n",
      "Step 3800, loss: tensor(0.1122, grad_fn=<SubBackward0>), time elapsed: 211.81570529937744 seconds\n",
      "Step 3900, loss: tensor(0.1120, grad_fn=<SubBackward0>), time elapsed: 217.4867389202118 seconds\n",
      "Step 4000, loss: tensor(0.1082, grad_fn=<SubBackward0>), time elapsed: 222.99141097068787 seconds\n",
      "Step 4100, loss: tensor(0.1100, grad_fn=<SubBackward0>), time elapsed: 228.51919984817505 seconds\n",
      "Step 4200, loss: tensor(0.1081, grad_fn=<SubBackward0>), time elapsed: 234.17058038711548 seconds\n",
      "Step 4300, loss: tensor(0.1074, grad_fn=<SubBackward0>), time elapsed: 239.69841647148132 seconds\n",
      "Step 4400, loss: tensor(0.1075, grad_fn=<SubBackward0>), time elapsed: 245.3645932674408 seconds\n",
      "Step 4500, loss: tensor(0.1080, grad_fn=<SubBackward0>), time elapsed: 250.9122474193573 seconds\n",
      "Step 4600, loss: tensor(0.1052, grad_fn=<SubBackward0>), time elapsed: 256.6622009277344 seconds\n",
      "Step 4700, loss: tensor(0.1060, grad_fn=<SubBackward0>), time elapsed: 262.16317343711853 seconds\n",
      "Step 4800, loss: tensor(0.1060, grad_fn=<SubBackward0>), time elapsed: 267.67206740379333 seconds\n",
      "Step 4900, loss: tensor(0.1040, grad_fn=<SubBackward0>), time elapsed: 273.32578134536743 seconds\n",
      "Step 5000, loss: tensor(0.1050, grad_fn=<SubBackward0>), time elapsed: 278.98210096359253 seconds\n",
      "Step 5100, loss: tensor(0.1009, grad_fn=<SubBackward0>), time elapsed: 284.689058303833 seconds\n",
      "Step 5200, loss: tensor(0.1028, grad_fn=<SubBackward0>), time elapsed: 290.24828815460205 seconds\n",
      "Step 5300, loss: tensor(0.1020, grad_fn=<SubBackward0>), time elapsed: 296.00524830818176 seconds\n",
      "Step 5400, loss: tensor(0.1004, grad_fn=<SubBackward0>), time elapsed: 301.8977732658386 seconds\n",
      "Step 5500, loss: tensor(0.1010, grad_fn=<SubBackward0>), time elapsed: 307.4168395996094 seconds\n",
      "Step 5600, loss: tensor(0.0986, grad_fn=<SubBackward0>), time elapsed: 313.0764813423157 seconds\n",
      "Step 5700, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 318.5865454673767 seconds\n",
      "Step 5800, loss: tensor(0.0965, grad_fn=<SubBackward0>), time elapsed: 324.3381977081299 seconds\n",
      "Step 5900, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 329.92324805259705 seconds\n",
      "Step 6000, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 335.46077966690063 seconds\n",
      "Step 6100, loss: tensor(0.0953, grad_fn=<SubBackward0>), time elapsed: 341.1923863887787 seconds\n",
      "Step 6200, loss: tensor(0.0956, grad_fn=<SubBackward0>), time elapsed: 346.74488592147827 seconds\n",
      "Step 6300, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 352.4346971511841 seconds\n",
      "Step 6400, loss: tensor(0.0947, grad_fn=<SubBackward0>), time elapsed: 358.008757352829 seconds\n",
      "Step 6500, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 363.67988109588623 seconds\n",
      "Step 6600, loss: tensor(0.0926, grad_fn=<SubBackward0>), time elapsed: 369.6839122772217 seconds\n",
      "Step 6700, loss: tensor(0.0881, grad_fn=<SubBackward0>), time elapsed: 375.2170674800873 seconds\n",
      "Step 6800, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 380.8889539241791 seconds\n",
      "Step 6900, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 386.43874168395996 seconds\n",
      "Step 7000, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 392.1860282421112 seconds\n",
      "Step 7100, loss: tensor(0.0903, grad_fn=<SubBackward0>), time elapsed: 397.7644066810608 seconds\n",
      "Step 7200, loss: tensor(0.0897, grad_fn=<SubBackward0>), time elapsed: 403.7826421260834 seconds\n",
      "Step 7300, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 409.29382252693176 seconds\n",
      "Step 7400, loss: tensor(0.0887, grad_fn=<SubBackward0>), time elapsed: 414.8438663482666 seconds\n",
      "Step 7500, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 420.53237438201904 seconds\n",
      "Step 7600, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 426.04705142974854 seconds\n",
      "Step 7700, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 432.03768968582153 seconds\n",
      "Step 7800, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 437.54259490966797 seconds\n",
      "Step 7900, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 443.1467490196228 seconds\n",
      "Step 8000, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 448.84343671798706 seconds\n",
      "Step 8100, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 454.3517143726349 seconds\n",
      "Step 8200, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 461.65694856643677 seconds\n",
      "Step 8300, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 467.16131830215454 seconds\n",
      "Step 8400, loss: tensor(0.0858, grad_fn=<SubBackward0>), time elapsed: 472.804869890213 seconds\n",
      "Step 8500, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 478.3243441581726 seconds\n",
      "Step 8600, loss: tensor(0.0856, grad_fn=<SubBackward0>), time elapsed: 483.83592915534973 seconds\n",
      "Step 8700, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 489.47475481033325 seconds\n",
      "Step 8800, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 495.0138421058655 seconds\n",
      "Step 8900, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 500.67922163009644 seconds\n",
      "Step 9000, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 506.224782705307 seconds\n",
      "Step 9100, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 511.7584102153778 seconds\n",
      "Step 9200, loss: tensor(0.0858, grad_fn=<SubBackward0>), time elapsed: 517.4158658981323 seconds\n",
      "Step 9300, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 522.9623482227325 seconds\n",
      "Step 9400, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 528.6061420440674 seconds\n",
      "Step 9500, loss: tensor(0.0856, grad_fn=<SubBackward0>), time elapsed: 534.1157503128052 seconds\n",
      "Step 9600, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 539.7808523178101 seconds\n",
      "Step 9700, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 545.2888009548187 seconds\n",
      "Step 9800, loss: tensor(0.0834, grad_fn=<SubBackward0>), time elapsed: 550.8068788051605 seconds\n",
      "Step 9900, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 556.490473985672 seconds\n",
      "Step 10000, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 562.0432133674622 seconds\n",
      "Step 10100, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 567.7386200428009 seconds\n",
      "Step 10200, loss: tensor(0.0829, grad_fn=<SubBackward0>), time elapsed: 573.2413337230682 seconds\n",
      "Step 10300, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 578.735963344574 seconds\n",
      "Step 10400, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 584.4220261573792 seconds\n",
      "Step 10500, loss: tensor(0.0838, grad_fn=<SubBackward0>), time elapsed: 589.9419734477997 seconds\n",
      "Step 10600, loss: tensor(0.0840, grad_fn=<SubBackward0>), time elapsed: 595.6065537929535 seconds\n",
      "Step 10700, loss: tensor(0.0854, grad_fn=<SubBackward0>), time elapsed: 601.1274037361145 seconds\n",
      "Step 10800, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 606.8149135112762 seconds\n",
      "Step 10900, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 612.3506281375885 seconds\n",
      "Step 11000, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 617.8863000869751 seconds\n",
      "Step 11100, loss: tensor(0.0852, grad_fn=<SubBackward0>), time elapsed: 623.5642166137695 seconds\n",
      "Step 11200, loss: tensor(0.0833, grad_fn=<SubBackward0>), time elapsed: 629.0907504558563 seconds\n",
      "Step 11300, loss: tensor(0.0839, grad_fn=<SubBackward0>), time elapsed: 634.7890930175781 seconds\n",
      "Step 11400, loss: tensor(0.0809, grad_fn=<SubBackward0>), time elapsed: 640.3459007740021 seconds\n",
      "Step 11500, loss: tensor(0.0845, grad_fn=<SubBackward0>), time elapsed: 646.2340710163116 seconds\n",
      "Step 11600, loss: tensor(0.0848, grad_fn=<SubBackward0>), time elapsed: 651.9247403144836 seconds\n",
      "Step 11700, loss: tensor(0.0837, grad_fn=<SubBackward0>), time elapsed: 657.4716284275055 seconds\n",
      "Step 11800, loss: tensor(0.0846, grad_fn=<SubBackward0>), time elapsed: 663.1652472019196 seconds\n",
      "Step 11900, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 668.6839876174927 seconds\n",
      "Step 12000, loss: tensor(0.0813, grad_fn=<SubBackward0>), time elapsed: 674.4261243343353 seconds\n",
      "Step 12100, loss: tensor(0.0826, grad_fn=<SubBackward0>), time elapsed: 679.942088842392 seconds\n",
      "Step 12200, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 686.2031238079071 seconds\n",
      "Step 12300, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 691.892386674881 seconds\n",
      "Step 12400, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 697.4202644824982 seconds\n",
      "Step 12500, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 703.091105222702 seconds\n",
      "Step 12600, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 708.6271376609802 seconds\n",
      "Step 12700, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 714.2588460445404 seconds\n",
      "Step 12800, loss: tensor(0.0819, grad_fn=<SubBackward0>), time elapsed: 720.7527046203613 seconds\n",
      "Step 12900, loss: tensor(0.0802, grad_fn=<SubBackward0>), time elapsed: 726.264258146286 seconds\n",
      "Step 13000, loss: tensor(0.0805, grad_fn=<SubBackward0>), time elapsed: 731.9313812255859 seconds\n",
      "Step 13100, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 737.490659236908 seconds\n",
      "Step 13200, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 743.1541695594788 seconds\n",
      "Step 13300, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 748.6570460796356 seconds\n",
      "Step 13400, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 754.1950271129608 seconds\n",
      "Step 13500, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 759.9047477245331 seconds\n",
      "Step 13600, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 765.4645259380341 seconds\n",
      "Step 13700, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 771.1920506954193 seconds\n",
      "Step 13800, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 776.7399156093597 seconds\n",
      "Step 13900, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 782.2929570674896 seconds\n",
      "Step 14000, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 788.0089118480682 seconds\n",
      "Step 14100, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 793.5523526668549 seconds\n",
      "Step 14200, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 799.4713568687439 seconds\n",
      "Step 14300, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 805.0211291313171 seconds\n",
      "Step 14400, loss: tensor(0.0720, grad_fn=<SubBackward0>), time elapsed: 810.8129241466522 seconds\n",
      "Step 14500, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 816.4064371585846 seconds\n",
      "Step 14600, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 822.0110766887665 seconds\n",
      "Step 14700, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 829.4356262683868 seconds\n",
      "Step 14800, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 834.9770126342773 seconds\n",
      "Step 14900, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 840.6798319816589 seconds\n",
      "Step 15000, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 846.2099134922028 seconds\n",
      "Step 15100, loss: tensor(0.0728, grad_fn=<SubBackward0>), time elapsed: 851.7436838150024 seconds\n",
      "Step 15200, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 857.438560962677 seconds\n",
      "Step 15300, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 862.9740941524506 seconds\n",
      "Step 15400, loss: tensor(0.0701, grad_fn=<SubBackward0>), time elapsed: 868.6587691307068 seconds\n",
      "Step 15500, loss: tensor(0.0721, grad_fn=<SubBackward0>), time elapsed: 874.1878924369812 seconds\n",
      "Step 15600, loss: tensor(0.0728, grad_fn=<SubBackward0>), time elapsed: 879.8690364360809 seconds\n",
      "Step 15700, loss: tensor(0.0700, grad_fn=<SubBackward0>), time elapsed: 885.392781496048 seconds\n",
      "Step 15800, loss: tensor(0.0719, grad_fn=<SubBackward0>), time elapsed: 890.940096616745 seconds\n",
      "Step 15900, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 896.6258842945099 seconds\n",
      "Step 16000, loss: tensor(0.0715, grad_fn=<SubBackward0>), time elapsed: 902.146240234375 seconds\n",
      "Step 16100, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 907.9746460914612 seconds\n",
      "Step 16200, loss: tensor(0.0719, grad_fn=<SubBackward0>), time elapsed: 913.5090169906616 seconds\n",
      "Step 16300, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 919.0538387298584 seconds\n",
      "Step 16400, loss: tensor(0.0701, grad_fn=<SubBackward0>), time elapsed: 924.7681548595428 seconds\n",
      "Step 16500, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 930.2952003479004 seconds\n",
      "Step 16600, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 936.3065390586853 seconds\n",
      "Step 16700, loss: tensor(0.0703, grad_fn=<SubBackward0>), time elapsed: 941.843498468399 seconds\n",
      "Step 16800, loss: tensor(0.0708, grad_fn=<SubBackward0>), time elapsed: 947.3911111354828 seconds\n",
      "Step 16900, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 953.111980676651 seconds\n",
      "Step 17000, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 958.6562175750732 seconds\n",
      "Step 17100, loss: tensor(0.0682, grad_fn=<SubBackward0>), time elapsed: 964.35626745224 seconds\n",
      "Step 17200, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 969.9033029079437 seconds\n",
      "Step 17300, loss: tensor(0.0701, grad_fn=<SubBackward0>), time elapsed: 975.5986108779907 seconds\n",
      "Step 17400, loss: tensor(0.0712, grad_fn=<SubBackward0>), time elapsed: 981.1729204654694 seconds\n",
      "Step 17500, loss: tensor(0.0676, grad_fn=<SubBackward0>), time elapsed: 986.7095324993134 seconds\n",
      "Step 17600, loss: tensor(0.0673, grad_fn=<SubBackward0>), time elapsed: 992.4306647777557 seconds\n",
      "Step 17700, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 997.9745721817017 seconds\n",
      "Step 17800, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 1004.3386538028717 seconds\n",
      "Step 17900, loss: tensor(0.0678, grad_fn=<SubBackward0>), time elapsed: 1009.9074931144714 seconds\n",
      "Step 18000, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 1015.4733119010925 seconds\n",
      "Step 18100, loss: tensor(0.0671, grad_fn=<SubBackward0>), time elapsed: 1021.1605265140533 seconds\n",
      "Step 18200, loss: tensor(0.0721, grad_fn=<SubBackward0>), time elapsed: 1026.6853835582733 seconds\n",
      "Step 18300, loss: tensor(0.0676, grad_fn=<SubBackward0>), time elapsed: 1032.3889744281769 seconds\n",
      "Step 18400, loss: tensor(0.0677, grad_fn=<SubBackward0>), time elapsed: 1037.9504313468933 seconds\n",
      "Step 18500, loss: tensor(0.0689, grad_fn=<SubBackward0>), time elapsed: 1043.6925644874573 seconds\n",
      "Step 18600, loss: tensor(0.0678, grad_fn=<SubBackward0>), time elapsed: 1049.242736339569 seconds\n",
      "Step 18700, loss: tensor(0.0673, grad_fn=<SubBackward0>), time elapsed: 1054.8013832569122 seconds\n",
      "Step 18800, loss: tensor(0.0666, grad_fn=<SubBackward0>), time elapsed: 1060.5497028827667 seconds\n",
      "Step 18900, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 1066.1200411319733 seconds\n",
      "Step 19000, loss: tensor(0.0651, grad_fn=<SubBackward0>), time elapsed: 1071.9036588668823 seconds\n",
      "Step 19100, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 1077.4908678531647 seconds\n",
      "Step 19200, loss: tensor(0.0624, grad_fn=<SubBackward0>), time elapsed: 1083.0265316963196 seconds\n",
      "Step 19300, loss: tensor(0.0677, grad_fn=<SubBackward0>), time elapsed: 1088.764208316803 seconds\n",
      "Step 19400, loss: tensor(0.0648, grad_fn=<SubBackward0>), time elapsed: 1094.319179058075 seconds\n",
      "Step 19500, loss: tensor(0.0657, grad_fn=<SubBackward0>), time elapsed: 1100.0799469947815 seconds\n",
      "Step 19600, loss: tensor(0.0656, grad_fn=<SubBackward0>), time elapsed: 1105.6524164676666 seconds\n",
      "Step 19700, loss: tensor(0.0643, grad_fn=<SubBackward0>), time elapsed: 1111.1856527328491 seconds\n",
      "Step 19800, loss: tensor(0.0649, grad_fn=<SubBackward0>), time elapsed: 1116.9231343269348 seconds\n",
      "Step 19900, loss: tensor(0.0681, grad_fn=<SubBackward0>), time elapsed: 1122.4857308864594 seconds\n",
      "Step 20000, loss: tensor(0.0674, grad_fn=<SubBackward0>), time elapsed: 1128.1913888454437 seconds\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.9006, grad_fn=<SubBackward0>), time elapsed: 0.0589601993560791 seconds\n",
      "Step 100, loss: tensor(0.8347, grad_fn=<SubBackward0>), time elapsed: 5.685437202453613 seconds\n",
      "Step 200, loss: tensor(0.7449, grad_fn=<SubBackward0>), time elapsed: 11.38449478149414 seconds\n",
      "Step 300, loss: tensor(0.6671, grad_fn=<SubBackward0>), time elapsed: 16.866544723510742 seconds\n",
      "Step 400, loss: tensor(0.6072, grad_fn=<SubBackward0>), time elapsed: 22.361700296401978 seconds\n",
      "Step 500, loss: tensor(0.5596, grad_fn=<SubBackward0>), time elapsed: 27.980072259902954 seconds\n",
      "Step 600, loss: tensor(0.5040, grad_fn=<SubBackward0>), time elapsed: 33.497756242752075 seconds\n",
      "Step 700, loss: tensor(0.4467, grad_fn=<SubBackward0>), time elapsed: 39.14647674560547 seconds\n",
      "Step 800, loss: tensor(0.3918, grad_fn=<SubBackward0>), time elapsed: 44.674232006073 seconds\n",
      "Step 900, loss: tensor(0.3541, grad_fn=<SubBackward0>), time elapsed: 50.32330775260925 seconds\n",
      "Step 1000, loss: tensor(0.3122, grad_fn=<SubBackward0>), time elapsed: 55.838579177856445 seconds\n",
      "Step 1100, loss: tensor(0.2707, grad_fn=<SubBackward0>), time elapsed: 61.36057424545288 seconds\n",
      "Step 1200, loss: tensor(0.2358, grad_fn=<SubBackward0>), time elapsed: 66.9978895187378 seconds\n",
      "Step 1300, loss: tensor(0.1987, grad_fn=<SubBackward0>), time elapsed: 72.52506995201111 seconds\n",
      "Step 1400, loss: tensor(0.1686, grad_fn=<SubBackward0>), time elapsed: 78.16832375526428 seconds\n",
      "Step 1500, loss: tensor(0.1536, grad_fn=<SubBackward0>), time elapsed: 83.67292618751526 seconds\n",
      "Step 1600, loss: tensor(0.1420, grad_fn=<SubBackward0>), time elapsed: 89.31725788116455 seconds\n",
      "Step 1700, loss: tensor(0.1375, grad_fn=<SubBackward0>), time elapsed: 94.81732845306396 seconds\n",
      "Step 1800, loss: tensor(0.1332, grad_fn=<SubBackward0>), time elapsed: 100.335458278656 seconds\n",
      "Step 1900, loss: tensor(0.1317, grad_fn=<SubBackward0>), time elapsed: 105.996906042099 seconds\n",
      "Step 2000, loss: tensor(0.1309, grad_fn=<SubBackward0>), time elapsed: 111.50765299797058 seconds\n",
      "Step 2100, loss: tensor(0.1293, grad_fn=<SubBackward0>), time elapsed: 117.1560845375061 seconds\n",
      "Step 2200, loss: tensor(0.1289, grad_fn=<SubBackward0>), time elapsed: 122.65601682662964 seconds\n",
      "Step 2300, loss: tensor(0.1282, grad_fn=<SubBackward0>), time elapsed: 128.15339517593384 seconds\n",
      "Step 2400, loss: tensor(0.1289, grad_fn=<SubBackward0>), time elapsed: 133.81958031654358 seconds\n",
      "Step 2500, loss: tensor(0.1280, grad_fn=<SubBackward0>), time elapsed: 139.32093024253845 seconds\n",
      "Step 2600, loss: tensor(0.1274, grad_fn=<SubBackward0>), time elapsed: 144.96576595306396 seconds\n",
      "Step 2700, loss: tensor(0.1275, grad_fn=<SubBackward0>), time elapsed: 150.4640429019928 seconds\n",
      "Step 2800, loss: tensor(0.1273, grad_fn=<SubBackward0>), time elapsed: 156.1130726337433 seconds\n",
      "Step 2900, loss: tensor(0.1274, grad_fn=<SubBackward0>), time elapsed: 161.59652972221375 seconds\n",
      "Step 3000, loss: tensor(0.1271, grad_fn=<SubBackward0>), time elapsed: 167.133873462677 seconds\n",
      "Step 3100, loss: tensor(0.1265, grad_fn=<SubBackward0>), time elapsed: 172.77470302581787 seconds\n",
      "Step 3200, loss: tensor(0.1265, grad_fn=<SubBackward0>), time elapsed: 178.2645947933197 seconds\n",
      "Step 3300, loss: tensor(0.1267, grad_fn=<SubBackward0>), time elapsed: 183.89729237556458 seconds\n",
      "Step 3400, loss: tensor(0.1261, grad_fn=<SubBackward0>), time elapsed: 189.41304278373718 seconds\n",
      "Step 3500, loss: tensor(0.1261, grad_fn=<SubBackward0>), time elapsed: 195.11370730400085 seconds\n",
      "Step 3600, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 200.6266894340515 seconds\n",
      "Step 3700, loss: tensor(0.1263, grad_fn=<SubBackward0>), time elapsed: 206.1461329460144 seconds\n",
      "Step 3800, loss: tensor(0.1263, grad_fn=<SubBackward0>), time elapsed: 211.77829670906067 seconds\n",
      "Step 3900, loss: tensor(0.1261, grad_fn=<SubBackward0>), time elapsed: 217.29269361495972 seconds\n",
      "Step 4000, loss: tensor(0.1260, grad_fn=<SubBackward0>), time elapsed: 222.93043160438538 seconds\n",
      "Step 4100, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 228.47283720970154 seconds\n",
      "Step 4200, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 233.98174214363098 seconds\n",
      "Step 4300, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 239.63052678108215 seconds\n",
      "Step 4400, loss: tensor(0.1255, grad_fn=<SubBackward0>), time elapsed: 245.14523363113403 seconds\n",
      "Step 4500, loss: tensor(0.1255, grad_fn=<SubBackward0>), time elapsed: 250.79987907409668 seconds\n",
      "Step 4600, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 256.33619022369385 seconds\n",
      "Step 4700, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 261.9691514968872 seconds\n",
      "Step 4800, loss: tensor(0.1254, grad_fn=<SubBackward0>), time elapsed: 267.46748304367065 seconds\n",
      "Step 4900, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 272.9828522205353 seconds\n",
      "Step 5000, loss: tensor(0.1255, grad_fn=<SubBackward0>), time elapsed: 278.62629985809326 seconds\n",
      "Step 5100, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 284.1362340450287 seconds\n",
      "Step 5200, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 289.79220962524414 seconds\n",
      "Step 5300, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 295.30193758010864 seconds\n",
      "Step 5400, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 300.9367835521698 seconds\n",
      "Step 5500, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 306.42805552482605 seconds\n",
      "Step 5600, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 311.9183497428894 seconds\n",
      "Step 5700, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 317.6050179004669 seconds\n",
      "Step 5800, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 323.11590027809143 seconds\n",
      "Step 5900, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 328.7794120311737 seconds\n",
      "Step 6000, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 334.2915029525757 seconds\n",
      "Step 6100, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 339.8208341598511 seconds\n",
      "Step 6200, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 345.4687931537628 seconds\n",
      "Step 6300, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 351.0031487941742 seconds\n",
      "Step 6400, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 356.61629819869995 seconds\n",
      "Step 6500, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 362.1283931732178 seconds\n",
      "Step 6600, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 367.79641008377075 seconds\n",
      "Step 6700, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 373.30381083488464 seconds\n",
      "Step 6800, loss: tensor(0.1246, grad_fn=<SubBackward0>), time elapsed: 378.8469820022583 seconds\n",
      "Step 6900, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 384.4952335357666 seconds\n",
      "Step 7000, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 390.01288890838623 seconds\n",
      "Step 7100, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 395.65164589881897 seconds\n",
      "Step 7200, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 401.136745929718 seconds\n",
      "Step 7300, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 406.7719361782074 seconds\n",
      "Step 7400, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 412.2852351665497 seconds\n",
      "Step 7500, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 417.79533195495605 seconds\n",
      "Step 7600, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 423.46334505081177 seconds\n",
      "Step 7700, loss: tensor(0.1242, grad_fn=<SubBackward0>), time elapsed: 428.9919717311859 seconds\n",
      "Step 7800, loss: tensor(0.1240, grad_fn=<SubBackward0>), time elapsed: 434.6622791290283 seconds\n",
      "Step 7900, loss: tensor(0.1240, grad_fn=<SubBackward0>), time elapsed: 440.21053767204285 seconds\n",
      "Step 8000, loss: tensor(0.1237, grad_fn=<SubBackward0>), time elapsed: 445.7121877670288 seconds\n",
      "Step 8100, loss: tensor(0.1236, grad_fn=<SubBackward0>), time elapsed: 451.38631534576416 seconds\n",
      "Step 8200, loss: tensor(0.1234, grad_fn=<SubBackward0>), time elapsed: 457.09366035461426 seconds\n",
      "Step 8300, loss: tensor(0.1233, grad_fn=<SubBackward0>), time elapsed: 462.72738432884216 seconds\n",
      "Step 8400, loss: tensor(0.1232, grad_fn=<SubBackward0>), time elapsed: 468.70948791503906 seconds\n",
      "Step 8500, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 474.36397981643677 seconds\n",
      "Step 8600, loss: tensor(0.1230, grad_fn=<SubBackward0>), time elapsed: 479.8702247142792 seconds\n",
      "Step 8700, loss: tensor(0.1228, grad_fn=<SubBackward0>), time elapsed: 485.38734459877014 seconds\n",
      "Step 8800, loss: tensor(0.1228, grad_fn=<SubBackward0>), time elapsed: 491.03122305870056 seconds\n",
      "Step 8900, loss: tensor(0.1226, grad_fn=<SubBackward0>), time elapsed: 496.57352924346924 seconds\n",
      "Step 9000, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 502.2739214897156 seconds\n",
      "Step 9100, loss: tensor(0.1224, grad_fn=<SubBackward0>), time elapsed: 507.83039140701294 seconds\n",
      "Step 9200, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 513.4098234176636 seconds\n",
      "Step 9300, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 519.2541375160217 seconds\n",
      "Step 9400, loss: tensor(0.1224, grad_fn=<SubBackward0>), time elapsed: 524.8406705856323 seconds\n",
      "Step 9500, loss: tensor(0.1223, grad_fn=<SubBackward0>), time elapsed: 530.5392289161682 seconds\n",
      "Step 9600, loss: tensor(0.1222, grad_fn=<SubBackward0>), time elapsed: 536.095682144165 seconds\n",
      "Step 9700, loss: tensor(0.1221, grad_fn=<SubBackward0>), time elapsed: 541.8161988258362 seconds\n",
      "Step 9800, loss: tensor(0.1220, grad_fn=<SubBackward0>), time elapsed: 548.6547391414642 seconds\n",
      "Step 9900, loss: tensor(0.1216, grad_fn=<SubBackward0>), time elapsed: 554.169615983963 seconds\n",
      "Step 10000, loss: tensor(0.1215, grad_fn=<SubBackward0>), time elapsed: 559.8807461261749 seconds\n",
      "Step 10100, loss: tensor(0.1214, grad_fn=<SubBackward0>), time elapsed: 565.400110244751 seconds\n",
      "Step 10200, loss: tensor(0.1210, grad_fn=<SubBackward0>), time elapsed: 571.0764985084534 seconds\n",
      "Step 10300, loss: tensor(0.1210, grad_fn=<SubBackward0>), time elapsed: 576.6062614917755 seconds\n",
      "Step 10400, loss: tensor(0.1200, grad_fn=<SubBackward0>), time elapsed: 582.1375465393066 seconds\n",
      "Step 10500, loss: tensor(0.1184, grad_fn=<SubBackward0>), time elapsed: 588.3636810779572 seconds\n",
      "Step 10600, loss: tensor(0.1122, grad_fn=<SubBackward0>), time elapsed: 593.9109020233154 seconds\n",
      "Step 10700, loss: tensor(0.1035, grad_fn=<SubBackward0>), time elapsed: 599.5703639984131 seconds\n",
      "Step 10800, loss: tensor(0.0983, grad_fn=<SubBackward0>), time elapsed: 605.0879509449005 seconds\n",
      "Step 10900, loss: tensor(0.0971, grad_fn=<SubBackward0>), time elapsed: 610.7476110458374 seconds\n",
      "Step 11000, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 616.2652924060822 seconds\n",
      "Step 11100, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 621.8000500202179 seconds\n",
      "Step 11200, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 627.4642395973206 seconds\n",
      "Step 11300, loss: tensor(0.0864, grad_fn=<SubBackward0>), time elapsed: 632.9707005023956 seconds\n",
      "Step 11400, loss: tensor(0.0834, grad_fn=<SubBackward0>), time elapsed: 638.6702828407288 seconds\n",
      "Step 11500, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 644.1937870979309 seconds\n",
      "Step 11600, loss: tensor(0.0815, grad_fn=<SubBackward0>), time elapsed: 649.7442600727081 seconds\n",
      "Step 11700, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 655.4151754379272 seconds\n",
      "Step 11800, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 660.9543333053589 seconds\n",
      "Step 11900, loss: tensor(0.0793, grad_fn=<SubBackward0>), time elapsed: 666.6457672119141 seconds\n",
      "Step 12000, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 672.1971890926361 seconds\n",
      "Step 12100, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 677.8685023784637 seconds\n",
      "Step 12200, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 683.4238965511322 seconds\n",
      "Step 12300, loss: tensor(0.0785, grad_fn=<SubBackward0>), time elapsed: 688.9389963150024 seconds\n",
      "Step 12400, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 694.6184952259064 seconds\n",
      "Step 12500, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 700.1529939174652 seconds\n",
      "Step 12600, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 705.8291256427765 seconds\n",
      "Step 12700, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 711.3538262844086 seconds\n",
      "Step 12800, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 716.8952696323395 seconds\n",
      "Step 12900, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 723.9145405292511 seconds\n",
      "Step 13000, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 729.4602553844452 seconds\n",
      "Step 13100, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 735.1461985111237 seconds\n",
      "Step 13200, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 740.658447265625 seconds\n",
      "Step 13300, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 746.3378534317017 seconds\n",
      "Step 13400, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 751.9048082828522 seconds\n",
      "Step 13500, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 757.4377663135529 seconds\n",
      "Step 13600, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 763.1254546642303 seconds\n",
      "Step 13700, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 768.6440889835358 seconds\n",
      "Step 13800, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 774.3513581752777 seconds\n",
      "Step 13900, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 779.8813421726227 seconds\n",
      "Step 14000, loss: tensor(0.0730, grad_fn=<SubBackward0>), time elapsed: 785.4156968593597 seconds\n",
      "Step 14100, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 791.0927317142487 seconds\n",
      "Step 14200, loss: tensor(0.0725, grad_fn=<SubBackward0>), time elapsed: 796.6431844234467 seconds\n",
      "Step 14300, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 802.3137023448944 seconds\n",
      "Step 14400, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 807.8585379123688 seconds\n",
      "Step 14500, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 813.5299725532532 seconds\n",
      "Step 14600, loss: tensor(0.0724, grad_fn=<SubBackward0>), time elapsed: 819.0455651283264 seconds\n",
      "Step 14700, loss: tensor(0.0724, grad_fn=<SubBackward0>), time elapsed: 824.5815489292145 seconds\n",
      "Step 14800, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 830.2527358531952 seconds\n",
      "Step 14900, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 835.831887960434 seconds\n",
      "Step 15000, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 841.5136342048645 seconds\n",
      "Step 15100, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 847.0333671569824 seconds\n",
      "Step 15200, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 852.5594940185547 seconds\n",
      "Step 15300, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 858.2485692501068 seconds\n",
      "Step 15400, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 863.7578725814819 seconds\n",
      "Step 15500, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 869.442877292633 seconds\n",
      "Step 15600, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 874.9418082237244 seconds\n",
      "Step 15700, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 880.6845374107361 seconds\n",
      "Step 15800, loss: tensor(0.0714, grad_fn=<SubBackward0>), time elapsed: 886.2071025371552 seconds\n",
      "Step 15900, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 891.7472791671753 seconds\n",
      "Step 16000, loss: tensor(0.0726, grad_fn=<SubBackward0>), time elapsed: 897.4452726840973 seconds\n",
      "Step 16100, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 902.9996609687805 seconds\n",
      "Step 16200, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 908.6851408481598 seconds\n",
      "Step 16300, loss: tensor(0.0717, grad_fn=<SubBackward0>), time elapsed: 914.2064793109894 seconds\n",
      "Step 16400, loss: tensor(0.0709, grad_fn=<SubBackward0>), time elapsed: 919.7341768741608 seconds\n",
      "Step 16500, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 925.4613420963287 seconds\n",
      "Step 16600, loss: tensor(0.0717, grad_fn=<SubBackward0>), time elapsed: 931.0260698795319 seconds\n",
      "Step 16700, loss: tensor(0.0723, grad_fn=<SubBackward0>), time elapsed: 936.7593252658844 seconds\n",
      "Step 16800, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 942.3120663166046 seconds\n",
      "Step 16900, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 947.8682975769043 seconds\n",
      "Step 17000, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 953.5881226062775 seconds\n",
      "Step 17100, loss: tensor(0.0674, grad_fn=<SubBackward0>), time elapsed: 959.3082129955292 seconds\n",
      "Step 17200, loss: tensor(0.0696, grad_fn=<SubBackward0>), time elapsed: 965.0532808303833 seconds\n",
      "Step 17300, loss: tensor(0.0690, grad_fn=<SubBackward0>), time elapsed: 970.6271409988403 seconds\n",
      "Step 17400, loss: tensor(0.0703, grad_fn=<SubBackward0>), time elapsed: 977.4132900238037 seconds\n",
      "Step 17500, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 982.9929776191711 seconds\n",
      "Step 17600, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 988.5647337436676 seconds\n",
      "Step 17700, loss: tensor(0.0710, grad_fn=<SubBackward0>), time elapsed: 994.2817220687866 seconds\n",
      "Step 17800, loss: tensor(0.0702, grad_fn=<SubBackward0>), time elapsed: 999.8199117183685 seconds\n",
      "Step 17900, loss: tensor(0.0711, grad_fn=<SubBackward0>), time elapsed: 1005.5386884212494 seconds\n",
      "Step 18000, loss: tensor(0.0705, grad_fn=<SubBackward0>), time elapsed: 1011.085953950882 seconds\n",
      "Step 18100, loss: tensor(0.0704, grad_fn=<SubBackward0>), time elapsed: 1016.6277616024017 seconds\n",
      "Step 18200, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 1022.3486104011536 seconds\n",
      "Step 18300, loss: tensor(0.0702, grad_fn=<SubBackward0>), time elapsed: 1027.9026205539703 seconds\n",
      "Step 18400, loss: tensor(0.0679, grad_fn=<SubBackward0>), time elapsed: 1033.6269624233246 seconds\n",
      "Step 18500, loss: tensor(0.0694, grad_fn=<SubBackward0>), time elapsed: 1039.1865587234497 seconds\n",
      "Step 18600, loss: tensor(0.0698, grad_fn=<SubBackward0>), time elapsed: 1044.8931646347046 seconds\n",
      "Step 18700, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 1050.4910371303558 seconds\n",
      "Step 18800, loss: tensor(0.0704, grad_fn=<SubBackward0>), time elapsed: 1056.0720779895782 seconds\n",
      "Step 18900, loss: tensor(0.0703, grad_fn=<SubBackward0>), time elapsed: 1061.8197619915009 seconds\n",
      "Step 19000, loss: tensor(0.0698, grad_fn=<SubBackward0>), time elapsed: 1067.42076253891 seconds\n",
      "Step 19100, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 1073.1322212219238 seconds\n",
      "Step 19200, loss: tensor(0.0685, grad_fn=<SubBackward0>), time elapsed: 1078.6815793514252 seconds\n",
      "Step 19300, loss: tensor(0.0684, grad_fn=<SubBackward0>), time elapsed: 1084.2178654670715 seconds\n",
      "Step 19400, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 1089.947408914566 seconds\n",
      "Step 19500, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 1095.5418405532837 seconds\n",
      "Step 19600, loss: tensor(0.0702, grad_fn=<SubBackward0>), time elapsed: 1101.639924287796 seconds\n",
      "Step 19700, loss: tensor(0.0711, grad_fn=<SubBackward0>), time elapsed: 1107.1912875175476 seconds\n",
      "Step 19800, loss: tensor(0.0693, grad_fn=<SubBackward0>), time elapsed: 1112.7883338928223 seconds\n",
      "Step 19900, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 1118.4788446426392 seconds\n",
      "Step 20000, loss: tensor(0.0684, grad_fn=<SubBackward0>), time elapsed: 1124.0350768566132 seconds\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(1.0626, grad_fn=<SubBackward0>), time elapsed: 0.06366205215454102 seconds\n",
      "Step 100, loss: tensor(0.9619, grad_fn=<SubBackward0>), time elapsed: 5.856937646865845 seconds\n",
      "Step 200, loss: tensor(0.8342, grad_fn=<SubBackward0>), time elapsed: 11.411599397659302 seconds\n",
      "Step 300, loss: tensor(0.7113, grad_fn=<SubBackward0>), time elapsed: 17.04464101791382 seconds\n",
      "Step 400, loss: tensor(0.6038, grad_fn=<SubBackward0>), time elapsed: 22.575085639953613 seconds\n",
      "Step 500, loss: tensor(0.5203, grad_fn=<SubBackward0>), time elapsed: 28.08791184425354 seconds\n",
      "Step 600, loss: tensor(0.4396, grad_fn=<SubBackward0>), time elapsed: 33.69665718078613 seconds\n",
      "Step 700, loss: tensor(0.3817, grad_fn=<SubBackward0>), time elapsed: 39.20098900794983 seconds\n",
      "Step 800, loss: tensor(0.3364, grad_fn=<SubBackward0>), time elapsed: 44.82703495025635 seconds\n",
      "Step 900, loss: tensor(0.2882, grad_fn=<SubBackward0>), time elapsed: 50.357731103897095 seconds\n",
      "Step 1000, loss: tensor(0.2341, grad_fn=<SubBackward0>), time elapsed: 55.98743271827698 seconds\n",
      "Step 1100, loss: tensor(0.2076, grad_fn=<SubBackward0>), time elapsed: 61.47940421104431 seconds\n",
      "Step 1200, loss: tensor(0.1865, grad_fn=<SubBackward0>), time elapsed: 66.97989964485168 seconds\n",
      "Step 1300, loss: tensor(0.1693, grad_fn=<SubBackward0>), time elapsed: 72.60607314109802 seconds\n",
      "Step 1400, loss: tensor(0.1535, grad_fn=<SubBackward0>), time elapsed: 78.10436916351318 seconds\n",
      "Step 1500, loss: tensor(0.1438, grad_fn=<SubBackward0>), time elapsed: 83.75465273857117 seconds\n",
      "Step 1600, loss: tensor(0.1310, grad_fn=<SubBackward0>), time elapsed: 89.24928283691406 seconds\n",
      "Step 1700, loss: tensor(0.1279, grad_fn=<SubBackward0>), time elapsed: 94.87662768363953 seconds\n",
      "Step 1800, loss: tensor(0.1153, grad_fn=<SubBackward0>), time elapsed: 100.38036894798279 seconds\n",
      "Step 1900, loss: tensor(0.1126, grad_fn=<SubBackward0>), time elapsed: 105.88838601112366 seconds\n",
      "Step 2000, loss: tensor(0.1119, grad_fn=<SubBackward0>), time elapsed: 111.56647634506226 seconds\n",
      "Step 2100, loss: tensor(0.1085, grad_fn=<SubBackward0>), time elapsed: 117.06910133361816 seconds\n",
      "Step 2200, loss: tensor(0.1026, grad_fn=<SubBackward0>), time elapsed: 122.71047902107239 seconds\n",
      "Step 2300, loss: tensor(0.1028, grad_fn=<SubBackward0>), time elapsed: 128.21481943130493 seconds\n",
      "Step 2400, loss: tensor(0.0948, grad_fn=<SubBackward0>), time elapsed: 133.8704056739807 seconds\n",
      "Step 2500, loss: tensor(0.0955, grad_fn=<SubBackward0>), time elapsed: 139.3709762096405 seconds\n",
      "Step 2600, loss: tensor(0.0928, grad_fn=<SubBackward0>), time elapsed: 144.91666746139526 seconds\n",
      "Step 2700, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 150.60518884658813 seconds\n",
      "Step 2800, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 156.13659811019897 seconds\n",
      "Step 2900, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 161.77836179733276 seconds\n",
      "Step 3000, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 167.29023385047913 seconds\n",
      "Step 3100, loss: tensor(0.0854, grad_fn=<SubBackward0>), time elapsed: 172.81529355049133 seconds\n",
      "Step 3200, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 178.4465720653534 seconds\n",
      "Step 3300, loss: tensor(0.0853, grad_fn=<SubBackward0>), time elapsed: 183.94220399856567 seconds\n",
      "Step 3400, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 189.59465646743774 seconds\n",
      "Step 3500, loss: tensor(0.0828, grad_fn=<SubBackward0>), time elapsed: 195.1121735572815 seconds\n",
      "Step 3600, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 200.76309466362 seconds\n",
      "Step 3700, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 206.274995803833 seconds\n",
      "Step 3800, loss: tensor(0.0796, grad_fn=<SubBackward0>), time elapsed: 211.77875804901123 seconds\n",
      "Step 3900, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 217.4238748550415 seconds\n",
      "Step 4000, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 222.9165494441986 seconds\n",
      "Step 4100, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 228.56310725212097 seconds\n",
      "Step 4200, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 234.08530378341675 seconds\n",
      "Step 4300, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 239.74097442626953 seconds\n",
      "Step 4400, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 245.24835348129272 seconds\n",
      "Step 4500, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 250.7830512523651 seconds\n",
      "Step 4600, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 256.4469828605652 seconds\n",
      "Step 4700, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 261.9753084182739 seconds\n",
      "Step 4800, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 267.62809896469116 seconds\n",
      "Step 4900, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 274.2188847064972 seconds\n",
      "Step 5000, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 280.2834663391113 seconds\n",
      "Step 5100, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 285.96069383621216 seconds\n",
      "Step 5200, loss: tensor(0.0716, grad_fn=<SubBackward0>), time elapsed: 291.5083785057068 seconds\n",
      "Step 5300, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 297.1924498081207 seconds\n",
      "Step 5400, loss: tensor(0.0705, grad_fn=<SubBackward0>), time elapsed: 302.708922624588 seconds\n",
      "Step 5500, loss: tensor(0.0686, grad_fn=<SubBackward0>), time elapsed: 308.37460803985596 seconds\n",
      "Step 5600, loss: tensor(0.0691, grad_fn=<SubBackward0>), time elapsed: 314.07137966156006 seconds\n",
      "Step 5700, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 319.5717968940735 seconds\n",
      "Step 5800, loss: tensor(0.0674, grad_fn=<SubBackward0>), time elapsed: 325.2573926448822 seconds\n",
      "Step 5900, loss: tensor(0.0694, grad_fn=<SubBackward0>), time elapsed: 330.7734658718109 seconds\n",
      "Step 6000, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 336.4769811630249 seconds\n",
      "Step 6100, loss: tensor(0.0656, grad_fn=<SubBackward0>), time elapsed: 342.21183943748474 seconds\n",
      "Step 6200, loss: tensor(0.0646, grad_fn=<SubBackward0>), time elapsed: 347.86743330955505 seconds\n",
      "Step 6300, loss: tensor(0.0646, grad_fn=<SubBackward0>), time elapsed: 353.395299911499 seconds\n",
      "Step 6400, loss: tensor(0.0657, grad_fn=<SubBackward0>), time elapsed: 358.9256327152252 seconds\n",
      "Step 6500, loss: tensor(0.0642, grad_fn=<SubBackward0>), time elapsed: 364.60861325263977 seconds\n",
      "Step 6600, loss: tensor(0.0645, grad_fn=<SubBackward0>), time elapsed: 370.14488339424133 seconds\n",
      "Step 6700, loss: tensor(0.0651, grad_fn=<SubBackward0>), time elapsed: 375.83851528167725 seconds\n",
      "Step 6800, loss: tensor(0.0622, grad_fn=<SubBackward0>), time elapsed: 381.4244980812073 seconds\n",
      "Step 6900, loss: tensor(0.0619, grad_fn=<SubBackward0>), time elapsed: 387.1333546638489 seconds\n",
      "Step 7000, loss: tensor(0.0641, grad_fn=<SubBackward0>), time elapsed: 393.29481649398804 seconds\n",
      "Step 7100, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 398.809024810791 seconds\n",
      "Step 7200, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 404.46404457092285 seconds\n",
      "Step 7300, loss: tensor(0.0610, grad_fn=<SubBackward0>), time elapsed: 409.9781596660614 seconds\n",
      "Step 7400, loss: tensor(0.0622, grad_fn=<SubBackward0>), time elapsed: 415.69148349761963 seconds\n",
      "Step 7500, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 421.2323360443115 seconds\n",
      "Step 7600, loss: tensor(0.0627, grad_fn=<SubBackward0>), time elapsed: 426.77100563049316 seconds\n",
      "Step 7700, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 432.56025195121765 seconds\n",
      "Step 7800, loss: tensor(0.0637, grad_fn=<SubBackward0>), time elapsed: 438.29197359085083 seconds\n",
      "Step 7900, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 443.95631980895996 seconds\n",
      "Step 8000, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 449.4931983947754 seconds\n",
      "Step 8100, loss: tensor(0.0627, grad_fn=<SubBackward0>), time elapsed: 455.19588804244995 seconds\n",
      "Step 8200, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 460.74261808395386 seconds\n",
      "Step 8300, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 466.25044417381287 seconds\n",
      "Step 8400, loss: tensor(0.0621, grad_fn=<SubBackward0>), time elapsed: 471.9205038547516 seconds\n",
      "Step 8500, loss: tensor(0.0615, grad_fn=<SubBackward0>), time elapsed: 477.75867533683777 seconds\n",
      "Step 8600, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 483.4395709037781 seconds\n",
      "Step 8700, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 489.0100202560425 seconds\n",
      "Step 8800, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 494.53993225097656 seconds\n",
      "Step 8900, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 500.6031105518341 seconds\n",
      "Step 9000, loss: tensor(0.0602, grad_fn=<SubBackward0>), time elapsed: 506.1539583206177 seconds\n",
      "Step 9100, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 511.8027799129486 seconds\n",
      "Step 9200, loss: tensor(0.0592, grad_fn=<SubBackward0>), time elapsed: 517.3128833770752 seconds\n",
      "Step 9300, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 523.0170311927795 seconds\n",
      "Step 9400, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 528.5270364284515 seconds\n",
      "Step 9500, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 534.0412495136261 seconds\n",
      "Step 9600, loss: tensor(0.0600, grad_fn=<SubBackward0>), time elapsed: 539.7478048801422 seconds\n",
      "Step 9700, loss: tensor(0.0606, grad_fn=<SubBackward0>), time elapsed: 546.0374310016632 seconds\n",
      "Step 9800, loss: tensor(0.0612, grad_fn=<SubBackward0>), time elapsed: 551.7143807411194 seconds\n",
      "Step 9900, loss: tensor(0.0609, grad_fn=<SubBackward0>), time elapsed: 557.2383823394775 seconds\n",
      "Step 10000, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 562.759290933609 seconds\n",
      "Step 10100, loss: tensor(0.0576, grad_fn=<SubBackward0>), time elapsed: 568.4417607784271 seconds\n",
      "Step 10200, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 573.9622147083282 seconds\n",
      "Step 10300, loss: tensor(0.0620, grad_fn=<SubBackward0>), time elapsed: 579.6428940296173 seconds\n",
      "Step 10400, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 585.1805417537689 seconds\n",
      "Step 10500, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 590.870744228363 seconds\n",
      "Step 10600, loss: tensor(0.0600, grad_fn=<SubBackward0>), time elapsed: 596.4013757705688 seconds\n",
      "Step 10700, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 601.9948146343231 seconds\n",
      "Step 10800, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 607.7336113452911 seconds\n",
      "Step 10900, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 613.6002259254456 seconds\n",
      "Step 11000, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 619.340677022934 seconds\n",
      "Step 11100, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 624.9154813289642 seconds\n",
      "Step 11200, loss: tensor(0.0603, grad_fn=<SubBackward0>), time elapsed: 630.4546291828156 seconds\n",
      "Step 11300, loss: tensor(0.0604, grad_fn=<SubBackward0>), time elapsed: 636.1885275840759 seconds\n",
      "Step 11400, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 642.4418745040894 seconds\n",
      "Step 11500, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 648.1489112377167 seconds\n",
      "Step 11600, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 653.654951095581 seconds\n",
      "Step 11700, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 659.3500738143921 seconds\n",
      "Step 11800, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 664.8673875331879 seconds\n",
      "Step 11900, loss: tensor(0.0591, grad_fn=<SubBackward0>), time elapsed: 670.3685851097107 seconds\n",
      "Step 12000, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 676.0357239246368 seconds\n",
      "Step 12100, loss: tensor(0.0583, grad_fn=<SubBackward0>), time elapsed: 681.5463383197784 seconds\n",
      "Step 12200, loss: tensor(0.0575, grad_fn=<SubBackward0>), time elapsed: 687.2151391506195 seconds\n",
      "Step 12300, loss: tensor(0.0572, grad_fn=<SubBackward0>), time elapsed: 692.783043384552 seconds\n",
      "Step 12400, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 698.3612751960754 seconds\n",
      "Step 12500, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 704.74485206604 seconds\n",
      "Step 12600, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 710.275310754776 seconds\n",
      "Step 12700, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 715.9433703422546 seconds\n",
      "Step 12800, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 721.4931735992432 seconds\n",
      "Step 12900, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 727.1709191799164 seconds\n",
      "Step 13000, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 732.6950812339783 seconds\n",
      "Step 13100, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 738.2257797718048 seconds\n",
      "Step 13200, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 743.9028792381287 seconds\n",
      "Step 13300, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 749.4643921852112 seconds\n",
      "Step 13400, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 755.2758159637451 seconds\n",
      "Step 13500, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 760.7962436676025 seconds\n",
      "Step 13600, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 766.9473214149475 seconds\n",
      "Step 13700, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 772.6497004032135 seconds\n",
      "Step 13800, loss: tensor(0.0582, grad_fn=<SubBackward0>), time elapsed: 778.1859471797943 seconds\n",
      "Step 13900, loss: tensor(0.0593, grad_fn=<SubBackward0>), time elapsed: 783.8866443634033 seconds\n",
      "Step 14000, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 789.4202265739441 seconds\n",
      "Step 14100, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 795.0998258590698 seconds\n",
      "Step 14200, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 800.6420660018921 seconds\n",
      "Step 14300, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 806.1751415729523 seconds\n",
      "Step 14400, loss: tensor(0.0568, grad_fn=<SubBackward0>), time elapsed: 811.8887858390808 seconds\n",
      "Step 14500, loss: tensor(0.0606, grad_fn=<SubBackward0>), time elapsed: 817.433794260025 seconds\n",
      "Step 14600, loss: tensor(0.0593, grad_fn=<SubBackward0>), time elapsed: 823.135577917099 seconds\n",
      "Step 14700, loss: tensor(0.0592, grad_fn=<SubBackward0>), time elapsed: 828.6737494468689 seconds\n",
      "Step 14800, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 834.1995713710785 seconds\n",
      "Step 14900, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 839.882609128952 seconds\n",
      "Step 15000, loss: tensor(0.0572, grad_fn=<SubBackward0>), time elapsed: 845.4597582817078 seconds\n",
      "Step 15100, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 851.1474885940552 seconds\n",
      "Step 15200, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 856.6972243785858 seconds\n",
      "Step 15300, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 862.4240198135376 seconds\n",
      "Step 15400, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 867.9753406047821 seconds\n",
      "Step 15500, loss: tensor(0.0581, grad_fn=<SubBackward0>), time elapsed: 873.5395841598511 seconds\n",
      "Step 15600, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 879.2479798793793 seconds\n",
      "Step 15700, loss: tensor(0.0592, grad_fn=<SubBackward0>), time elapsed: 885.6220850944519 seconds\n",
      "Step 15800, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 891.329377412796 seconds\n",
      "Step 15900, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 896.8506269454956 seconds\n",
      "Step 16000, loss: tensor(0.0593, grad_fn=<SubBackward0>), time elapsed: 902.4009482860565 seconds\n",
      "Step 16100, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 908.0831904411316 seconds\n",
      "Step 16200, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 913.6168220043182 seconds\n",
      "Step 16300, loss: tensor(0.0590, grad_fn=<SubBackward0>), time elapsed: 919.3100171089172 seconds\n",
      "Step 16400, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 924.8614883422852 seconds\n",
      "Step 16500, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 931.3418016433716 seconds\n",
      "Step 16600, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 936.8931241035461 seconds\n",
      "Step 16700, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 942.4242446422577 seconds\n",
      "Step 16800, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 948.0972962379456 seconds\n",
      "Step 16900, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 953.6799824237823 seconds\n",
      "Step 17000, loss: tensor(0.0563, grad_fn=<SubBackward0>), time elapsed: 959.3921272754669 seconds\n",
      "Step 17100, loss: tensor(0.0557, grad_fn=<SubBackward0>), time elapsed: 964.9774451255798 seconds\n",
      "Step 17200, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 970.5394623279572 seconds\n",
      "Step 17300, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 976.2372596263885 seconds\n",
      "Step 17400, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 981.7772860527039 seconds\n",
      "Step 17500, loss: tensor(0.0573, grad_fn=<SubBackward0>), time elapsed: 987.517288684845 seconds\n",
      "Step 17600, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 993.8112921714783 seconds\n",
      "Step 17700, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 999.3656537532806 seconds\n",
      "Step 17800, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 1005.0421707630157 seconds\n",
      "Step 17900, loss: tensor(0.0582, grad_fn=<SubBackward0>), time elapsed: 1010.5778560638428 seconds\n",
      "Step 18000, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 1016.2797529697418 seconds\n",
      "Step 18100, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 1021.8152439594269 seconds\n",
      "Step 18200, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 1027.5337193012238 seconds\n",
      "Step 18300, loss: tensor(0.0583, grad_fn=<SubBackward0>), time elapsed: 1033.0899305343628 seconds\n",
      "Step 18400, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 1038.6342799663544 seconds\n",
      "Step 18500, loss: tensor(0.0591, grad_fn=<SubBackward0>), time elapsed: 1044.3804759979248 seconds\n",
      "Step 18600, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 1049.9277963638306 seconds\n",
      "Step 18700, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 1055.8949303627014 seconds\n",
      "Step 18800, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 1061.4664688110352 seconds\n",
      "Step 18900, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 1067.0357646942139 seconds\n",
      "Step 19000, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 1072.9355251789093 seconds\n",
      "Step 19100, loss: tensor(0.0565, grad_fn=<SubBackward0>), time elapsed: 1079.3397300243378 seconds\n",
      "Step 19200, loss: tensor(0.0559, grad_fn=<SubBackward0>), time elapsed: 1085.1037938594818 seconds\n",
      "Step 19300, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 1090.6914727687836 seconds\n",
      "Step 19400, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 1096.2862575054169 seconds\n",
      "Step 19500, loss: tensor(0.0573, grad_fn=<SubBackward0>), time elapsed: 1101.9996011257172 seconds\n",
      "Step 19600, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 1107.5420277118683 seconds\n",
      "Step 19700, loss: tensor(0.0576, grad_fn=<SubBackward0>), time elapsed: 1113.2342340946198 seconds\n",
      "Step 19800, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 1118.8107323646545 seconds\n",
      "Step 19900, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 1124.5297067165375 seconds\n",
      "Step 20000, loss: tensor(0.0585, grad_fn=<SubBackward0>), time elapsed: 1130.118370771408 seconds\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(1.1024, grad_fn=<SubBackward0>), time elapsed: 0.06127309799194336 seconds\n",
      "Step 100, loss: tensor(0.9126, grad_fn=<SubBackward0>), time elapsed: 5.6715171337127686 seconds\n",
      "Step 200, loss: tensor(0.7467, grad_fn=<SubBackward0>), time elapsed: 11.347835779190063 seconds\n",
      "Step 300, loss: tensor(0.6280, grad_fn=<SubBackward0>), time elapsed: 16.84639048576355 seconds\n",
      "Step 400, loss: tensor(0.5410, grad_fn=<SubBackward0>), time elapsed: 22.477572679519653 seconds\n",
      "Step 500, loss: tensor(0.4670, grad_fn=<SubBackward0>), time elapsed: 27.987683057785034 seconds\n",
      "Step 600, loss: tensor(0.4107, grad_fn=<SubBackward0>), time elapsed: 33.626829385757446 seconds\n",
      "Step 700, loss: tensor(0.3521, grad_fn=<SubBackward0>), time elapsed: 39.13586783409119 seconds\n",
      "Step 800, loss: tensor(0.3067, grad_fn=<SubBackward0>), time elapsed: 44.66277599334717 seconds\n",
      "Step 900, loss: tensor(0.2615, grad_fn=<SubBackward0>), time elapsed: 50.305768728256226 seconds\n",
      "Step 1000, loss: tensor(0.2271, grad_fn=<SubBackward0>), time elapsed: 55.83565640449524 seconds\n",
      "Step 1100, loss: tensor(0.1972, grad_fn=<SubBackward0>), time elapsed: 61.46279859542847 seconds\n",
      "Step 1200, loss: tensor(0.1750, grad_fn=<SubBackward0>), time elapsed: 66.94369196891785 seconds\n",
      "Step 1300, loss: tensor(0.1598, grad_fn=<SubBackward0>), time elapsed: 72.5898232460022 seconds\n",
      "Step 1400, loss: tensor(0.1461, grad_fn=<SubBackward0>), time elapsed: 78.0912024974823 seconds\n",
      "Step 1500, loss: tensor(0.1366, grad_fn=<SubBackward0>), time elapsed: 83.63745903968811 seconds\n",
      "Step 1600, loss: tensor(0.1291, grad_fn=<SubBackward0>), time elapsed: 89.30206680297852 seconds\n",
      "Step 1700, loss: tensor(0.1229, grad_fn=<SubBackward0>), time elapsed: 94.81974005699158 seconds\n",
      "Step 1800, loss: tensor(0.1180, grad_fn=<SubBackward0>), time elapsed: 100.48179030418396 seconds\n",
      "Step 1900, loss: tensor(0.1143, grad_fn=<SubBackward0>), time elapsed: 105.9915189743042 seconds\n",
      "Step 2000, loss: tensor(0.1073, grad_fn=<SubBackward0>), time elapsed: 111.54001212120056 seconds\n",
      "Step 2100, loss: tensor(0.1048, grad_fn=<SubBackward0>), time elapsed: 117.1845166683197 seconds\n",
      "Step 2200, loss: tensor(0.1032, grad_fn=<SubBackward0>), time elapsed: 122.70169949531555 seconds\n",
      "Step 2300, loss: tensor(0.0996, grad_fn=<SubBackward0>), time elapsed: 128.340900182724 seconds\n",
      "Step 2400, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 133.87253427505493 seconds\n",
      "Step 2500, loss: tensor(0.0942, grad_fn=<SubBackward0>), time elapsed: 139.52904963493347 seconds\n",
      "Step 2600, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 145.0523386001587 seconds\n",
      "Step 2700, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 150.55467414855957 seconds\n",
      "Step 2800, loss: tensor(0.0886, grad_fn=<SubBackward0>), time elapsed: 156.1949806213379 seconds\n",
      "Step 2900, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 161.69279026985168 seconds\n",
      "Step 3000, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 167.33424973487854 seconds\n",
      "Step 3100, loss: tensor(0.0854, grad_fn=<SubBackward0>), time elapsed: 172.84259819984436 seconds\n",
      "Step 3200, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 178.50521302223206 seconds\n",
      "Step 3300, loss: tensor(0.0829, grad_fn=<SubBackward0>), time elapsed: 184.0066740512848 seconds\n",
      "Step 3400, loss: tensor(0.0816, grad_fn=<SubBackward0>), time elapsed: 189.51614022254944 seconds\n",
      "Step 3500, loss: tensor(0.0813, grad_fn=<SubBackward0>), time elapsed: 195.14990997314453 seconds\n",
      "Step 3600, loss: tensor(0.0817, grad_fn=<SubBackward0>), time elapsed: 200.6685700416565 seconds\n",
      "Step 3700, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 206.32434606552124 seconds\n",
      "Step 3800, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 211.8214201927185 seconds\n",
      "Step 3900, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 217.45604133605957 seconds\n",
      "Step 4000, loss: tensor(0.0777, grad_fn=<SubBackward0>), time elapsed: 222.9657747745514 seconds\n",
      "Step 4100, loss: tensor(0.0788, grad_fn=<SubBackward0>), time elapsed: 228.48394179344177 seconds\n",
      "Step 4200, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 234.14177942276 seconds\n",
      "Step 4300, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 239.64850735664368 seconds\n",
      "Step 4400, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 245.29300618171692 seconds\n",
      "Step 4500, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 250.8073811531067 seconds\n",
      "Step 4600, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 256.3065378665924 seconds\n",
      "Step 4700, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 262.434285402298 seconds\n",
      "Step 4800, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 268.062415599823 seconds\n",
      "Step 4900, loss: tensor(0.0726, grad_fn=<SubBackward0>), time elapsed: 274.22010350227356 seconds\n",
      "Step 5000, loss: tensor(0.0717, grad_fn=<SubBackward0>), time elapsed: 279.7360954284668 seconds\n",
      "Step 5100, loss: tensor(0.0714, grad_fn=<SubBackward0>), time elapsed: 285.4201512336731 seconds\n",
      "Step 5200, loss: tensor(0.0716, grad_fn=<SubBackward0>), time elapsed: 290.9356656074524 seconds\n",
      "Step 5300, loss: tensor(0.0707, grad_fn=<SubBackward0>), time elapsed: 296.4886209964752 seconds\n",
      "Step 5400, loss: tensor(0.0686, grad_fn=<SubBackward0>), time elapsed: 303.0358622074127 seconds\n",
      "Step 5500, loss: tensor(0.0689, grad_fn=<SubBackward0>), time elapsed: 308.5718150138855 seconds\n",
      "Step 5600, loss: tensor(0.0671, grad_fn=<SubBackward0>), time elapsed: 314.27351546287537 seconds\n",
      "Step 5700, loss: tensor(0.0676, grad_fn=<SubBackward0>), time elapsed: 319.78784441947937 seconds\n",
      "Step 5800, loss: tensor(0.0658, grad_fn=<SubBackward0>), time elapsed: 325.44705033302307 seconds\n",
      "Step 5900, loss: tensor(0.0655, grad_fn=<SubBackward0>), time elapsed: 330.9605493545532 seconds\n",
      "Step 6000, loss: tensor(0.0668, grad_fn=<SubBackward0>), time elapsed: 336.4807574748993 seconds\n",
      "Step 6100, loss: tensor(0.0656, grad_fn=<SubBackward0>), time elapsed: 342.1211955547333 seconds\n",
      "Step 6200, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 347.6421332359314 seconds\n",
      "Step 6300, loss: tensor(0.0634, grad_fn=<SubBackward0>), time elapsed: 353.3016428947449 seconds\n",
      "Step 6400, loss: tensor(0.0636, grad_fn=<SubBackward0>), time elapsed: 358.83563590049744 seconds\n",
      "Step 6500, loss: tensor(0.0617, grad_fn=<SubBackward0>), time elapsed: 364.34699964523315 seconds\n",
      "Step 6600, loss: tensor(0.0620, grad_fn=<SubBackward0>), time elapsed: 369.99903297424316 seconds\n",
      "Step 6700, loss: tensor(0.0624, grad_fn=<SubBackward0>), time elapsed: 375.51307559013367 seconds\n",
      "Step 6800, loss: tensor(0.0625, grad_fn=<SubBackward0>), time elapsed: 381.15696573257446 seconds\n",
      "Step 6900, loss: tensor(0.0622, grad_fn=<SubBackward0>), time elapsed: 386.69809913635254 seconds\n",
      "Step 7000, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 392.34743213653564 seconds\n",
      "Step 7100, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 397.8651988506317 seconds\n",
      "Step 7200, loss: tensor(0.0609, grad_fn=<SubBackward0>), time elapsed: 403.3893039226532 seconds\n",
      "Step 7300, loss: tensor(0.0599, grad_fn=<SubBackward0>), time elapsed: 409.057213306427 seconds\n",
      "Step 7400, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 414.5716829299927 seconds\n",
      "Step 7500, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 420.2739887237549 seconds\n",
      "Step 7600, loss: tensor(0.0581, grad_fn=<SubBackward0>), time elapsed: 425.7739963531494 seconds\n",
      "Step 7700, loss: tensor(0.0585, grad_fn=<SubBackward0>), time elapsed: 431.4463906288147 seconds\n",
      "Step 7800, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 436.9662232398987 seconds\n",
      "Step 7900, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 442.50547456741333 seconds\n",
      "Step 8000, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 448.2317056655884 seconds\n",
      "Step 8100, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 453.76620507240295 seconds\n",
      "Step 8200, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 459.47711181640625 seconds\n",
      "Step 8300, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 465.02250123023987 seconds\n",
      "Step 8400, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 470.55354714393616 seconds\n",
      "Step 8500, loss: tensor(0.0575, grad_fn=<SubBackward0>), time elapsed: 476.2201509475708 seconds\n",
      "Step 8600, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 481.766459941864 seconds\n",
      "Step 8700, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 487.4225935935974 seconds\n",
      "Step 8800, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 492.94405603408813 seconds\n",
      "Step 8900, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 498.74072337150574 seconds\n",
      "Step 9000, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 504.276424407959 seconds\n",
      "Step 9100, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 509.8367202281952 seconds\n",
      "Step 9200, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 515.4841959476471 seconds\n",
      "Step 9300, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 521.5908606052399 seconds\n",
      "Step 9400, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 527.2744948863983 seconds\n",
      "Step 9500, loss: tensor(0.0572, grad_fn=<SubBackward0>), time elapsed: 532.7897870540619 seconds\n",
      "Step 9600, loss: tensor(0.0575, grad_fn=<SubBackward0>), time elapsed: 538.3292503356934 seconds\n",
      "Step 9700, loss: tensor(0.0551, grad_fn=<SubBackward0>), time elapsed: 544.0246722698212 seconds\n",
      "Step 9800, loss: tensor(0.0564, grad_fn=<SubBackward0>), time elapsed: 549.6206266880035 seconds\n",
      "Step 9900, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 555.288565158844 seconds\n",
      "Step 10000, loss: tensor(0.0564, grad_fn=<SubBackward0>), time elapsed: 560.8042471408844 seconds\n",
      "Step 10100, loss: tensor(0.0549, grad_fn=<SubBackward0>), time elapsed: 566.4587273597717 seconds\n",
      "Step 10200, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 571.9870393276215 seconds\n",
      "Step 10300, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 577.504052400589 seconds\n",
      "Step 10400, loss: tensor(0.0560, grad_fn=<SubBackward0>), time elapsed: 583.1558156013489 seconds\n",
      "Step 10500, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 588.6835012435913 seconds\n",
      "Step 10600, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 594.3674554824829 seconds\n",
      "Step 10700, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 599.9322688579559 seconds\n",
      "Step 10800, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 605.4943957328796 seconds\n",
      "Step 10900, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 611.1824054718018 seconds\n",
      "Step 11000, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 616.7371206283569 seconds\n",
      "Step 11100, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 622.4104290008545 seconds\n",
      "Step 11200, loss: tensor(0.0540, grad_fn=<SubBackward0>), time elapsed: 627.9580652713776 seconds\n",
      "Step 11300, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 634.0456011295319 seconds\n",
      "Step 11400, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 639.5622549057007 seconds\n",
      "Step 11500, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 645.0807678699493 seconds\n",
      "Step 11600, loss: tensor(0.0550, grad_fn=<SubBackward0>), time elapsed: 650.7491993904114 seconds\n",
      "Step 11700, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 656.2686066627502 seconds\n",
      "Step 11800, loss: tensor(0.0543, grad_fn=<SubBackward0>), time elapsed: 662.051059961319 seconds\n",
      "Step 11900, loss: tensor(0.0553, grad_fn=<SubBackward0>), time elapsed: 667.6526017189026 seconds\n",
      "Step 12000, loss: tensor(0.0542, grad_fn=<SubBackward0>), time elapsed: 673.2235827445984 seconds\n",
      "Step 12100, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 678.8814959526062 seconds\n",
      "Step 12200, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 685.0388987064362 seconds\n",
      "Step 12300, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 690.731103181839 seconds\n",
      "Step 12400, loss: tensor(0.0535, grad_fn=<SubBackward0>), time elapsed: 696.2600948810577 seconds\n",
      "Step 12500, loss: tensor(0.0533, grad_fn=<SubBackward0>), time elapsed: 701.9402132034302 seconds\n",
      "Step 12600, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 707.472647190094 seconds\n",
      "Step 12700, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 712.9872524738312 seconds\n",
      "Step 12800, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 718.6666285991669 seconds\n",
      "Step 12900, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 724.2233757972717 seconds\n",
      "Step 13000, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 729.8973054885864 seconds\n",
      "Step 13100, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 735.4387547969818 seconds\n",
      "Step 13200, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 740.9814183712006 seconds\n",
      "Step 13300, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 747.0603196620941 seconds\n",
      "Step 13400, loss: tensor(0.0540, grad_fn=<SubBackward0>), time elapsed: 752.6677746772766 seconds\n",
      "Step 13500, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 758.4015424251556 seconds\n",
      "Step 13600, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 763.960177898407 seconds\n",
      "Step 13700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 770.199179649353 seconds\n",
      "Step 13800, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 775.753497838974 seconds\n",
      "Step 13900, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 781.8729887008667 seconds\n",
      "Step 14000, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 787.5797619819641 seconds\n",
      "Step 14100, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 793.0921356678009 seconds\n",
      "Step 14200, loss: tensor(0.0536, grad_fn=<SubBackward0>), time elapsed: 798.7618281841278 seconds\n",
      "Step 14300, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 804.266491651535 seconds\n",
      "Step 14400, loss: tensor(0.0535, grad_fn=<SubBackward0>), time elapsed: 809.7927303314209 seconds\n",
      "Step 14500, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 815.4841501712799 seconds\n",
      "Step 14600, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 821.0158307552338 seconds\n",
      "Step 14700, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 826.6965725421906 seconds\n",
      "Step 14800, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 832.2199156284332 seconds\n",
      "Step 14900, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 837.8842566013336 seconds\n",
      "Step 15000, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 843.4446849822998 seconds\n",
      "Step 15100, loss: tensor(0.0538, grad_fn=<SubBackward0>), time elapsed: 848.989381313324 seconds\n",
      "Step 15200, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 854.6723608970642 seconds\n",
      "Step 15300, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 860.1939985752106 seconds\n",
      "Step 15400, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 865.9038319587708 seconds\n",
      "Step 15500, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 871.4453122615814 seconds\n",
      "Step 15600, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 877.0386221408844 seconds\n",
      "Step 15700, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 882.7336685657501 seconds\n",
      "Step 15800, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 888.2780206203461 seconds\n",
      "Step 15900, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 894.0019116401672 seconds\n",
      "Step 16000, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 899.5530455112457 seconds\n",
      "Step 16100, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 905.2950778007507 seconds\n",
      "Step 16200, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 910.8432850837708 seconds\n",
      "Step 16300, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 916.7256052494049 seconds\n",
      "Step 16400, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 922.4151880741119 seconds\n",
      "Step 16500, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 927.952260017395 seconds\n",
      "Step 16600, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 933.6293203830719 seconds\n",
      "Step 16700, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 939.1675901412964 seconds\n",
      "Step 16800, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 944.6946909427643 seconds\n",
      "Step 16900, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 950.3833408355713 seconds\n",
      "Step 17000, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 955.9246695041656 seconds\n",
      "Step 17100, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 961.6282420158386 seconds\n",
      "Step 17200, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 967.2000916004181 seconds\n",
      "Step 17300, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 972.8307156562805 seconds\n",
      "Step 17400, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 978.6204776763916 seconds\n",
      "Step 17500, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 984.1652226448059 seconds\n",
      "Step 17600, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 990.2793476581573 seconds\n",
      "Step 17700, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 995.8477613925934 seconds\n",
      "Step 17800, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1001.5437209606171 seconds\n",
      "Step 17900, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 1007.0915729999542 seconds\n",
      "Step 18000, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 1012.6376776695251 seconds\n",
      "Step 18100, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 1018.3344006538391 seconds\n",
      "Step 18200, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1023.8804130554199 seconds\n",
      "Step 18300, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 1029.6142644882202 seconds\n",
      "Step 18400, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 1035.16548037529 seconds\n",
      "Step 18500, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 1040.836682319641 seconds\n",
      "Step 18600, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 1046.6226258277893 seconds\n",
      "Step 18700, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1052.1882600784302 seconds\n",
      "Step 18800, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 1058.910239458084 seconds\n",
      "Step 18900, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 1064.4784977436066 seconds\n",
      "Step 19000, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 1070.0485138893127 seconds\n",
      "Step 19100, loss: tensor(0.0490, grad_fn=<SubBackward0>), time elapsed: 1075.7678608894348 seconds\n",
      "Step 19200, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 1081.3343198299408 seconds\n",
      "Step 19300, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 1087.0792722702026 seconds\n",
      "Step 19400, loss: tensor(0.0468, grad_fn=<SubBackward0>), time elapsed: 1092.6563303470612 seconds\n",
      "Step 19500, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 1098.3458564281464 seconds\n",
      "Step 19600, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1103.8634746074677 seconds\n",
      "Step 19700, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1109.3799471855164 seconds\n",
      "Step 19800, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 1115.063797712326 seconds\n",
      "Step 19900, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 1120.667457818985 seconds\n",
      "Step 20000, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 1126.3942999839783 seconds\n"
     ]
    }
   ],
   "source": [
    "#train on diffusion data\n",
    "n, na = 4, 1 # number of data and ancilla qubits\n",
    "T = 20 # number of diffusion steps\n",
    "L = 6 # layer of backward PQC\n",
    "Ndata = 2000 # number of data in the training data set\n",
    "epochs = 20001\n",
    "\n",
    " # number of training epochs\n",
    "\n",
    "#gen random samples at t = T\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_T = diffModel.HaarSampleGeneration(Ndata, seed = 22)\n",
    "\n",
    "#load diffusion process\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "model.set_diffusionSet(states_diff)\n",
    "\n",
    "params_total = []\n",
    "loss_hist_total = []\n",
    "\n",
    "for t in range(T - 1, -1, -1):\n",
    "    params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "    for tt in range(t+1, T):\n",
    "        print(tt)\n",
    "        params_tot[tt] = np.load('paramsandloss_squaremodel/params_t%d.npy'%tt)\n",
    "    \n",
    "    params, loss_hist = Training_t(model, t, inputs_T, params_tot, Ndata, epochs)\n",
    "\n",
    "    np.save('paramsandloss_squaremodel/params_t%d'%t, params.detach().numpy())\n",
    "    np.save('paramsandloss_squaremodel/loss_t%d'%t, loss_hist.detach().numpy())\n",
    "    \n",
    "    #params_total = np.append(params_total, params, axis = 1)\n",
    "    #loss_hist_total = np.append(loss_hist_total, loss_hist, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: tensor(0.0262, grad_fn=<SubBackward0>), time elapsed: 0.03679513931274414 seconds\n",
      "Step 100, loss: tensor(0.0215, grad_fn=<SubBackward0>), time elapsed: 3.812459945678711 seconds\n",
      "Step 200, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 7.346958875656128 seconds\n",
      "Step 300, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 10.86698293685913 seconds\n",
      "Step 400, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 14.589691400527954 seconds\n",
      "Step 500, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 18.12534761428833 seconds\n",
      "Step 600, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 21.64742136001587 seconds\n",
      "Step 700, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 25.405336618423462 seconds\n",
      "Step 800, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 28.938697338104248 seconds\n",
      "Step 900, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 32.47132849693298 seconds\n",
      "Step 1000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 35.988669872283936 seconds\n",
      "Step 1100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 39.686453104019165 seconds\n",
      "Step 1200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 43.203102588653564 seconds\n",
      "Step 1300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 46.73646020889282 seconds\n",
      "Step 1400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 50.47656583786011 seconds\n",
      "Step 1500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 54.00294852256775 seconds\n",
      "Step 1600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 57.51102352142334 seconds\n",
      "Step 1700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 61.21854066848755 seconds\n",
      "Step 1800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 64.76162242889404 seconds\n",
      "Step 1900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 68.28668737411499 seconds\n",
      "Step 2000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 72.01335859298706 seconds\n",
      "Step 2100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 75.51773715019226 seconds\n",
      "Step 2200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 79.0456862449646 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 82.79448890686035 seconds\n",
      "Step 2400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 86.31014776229858 seconds\n",
      "Step 2500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 89.8660409450531 seconds\n",
      "Step 2600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 93.61159992218018 seconds\n",
      "Step 2700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 97.14932513237 seconds\n",
      "Step 2800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 100.71432590484619 seconds\n",
      "Step 2900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 104.46139621734619 seconds\n",
      "Step 3000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 107.97855138778687 seconds\n",
      "Step 3100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 111.50855016708374 seconds\n",
      "Step 3200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 115.21787619590759 seconds\n",
      "Step 3300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 118.70307350158691 seconds\n",
      "Step 3400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 122.2102267742157 seconds\n",
      "Step 3500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 125.70753312110901 seconds\n",
      "Step 3600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 129.43459296226501 seconds\n",
      "Step 3700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 133.02361845970154 seconds\n",
      "Step 3800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 136.51178407669067 seconds\n",
      "Step 3900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 140.20446968078613 seconds\n",
      "Step 4000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 143.67996048927307 seconds\n",
      "Step 4100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 147.148770570755 seconds\n",
      "Step 4200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 150.86054611206055 seconds\n",
      "Step 4300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 154.38039898872375 seconds\n",
      "Step 4400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 157.87466549873352 seconds\n",
      "Step 4500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 161.551096200943 seconds\n",
      "Step 4600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 165.0590181350708 seconds\n",
      "Step 4700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 168.52648782730103 seconds\n",
      "Step 4800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 172.20336294174194 seconds\n",
      "Step 4900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 175.70459961891174 seconds\n",
      "Step 5000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 179.17498922348022 seconds\n",
      "Step 5100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 182.88284993171692 seconds\n",
      "Step 5200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 186.38074707984924 seconds\n",
      "Step 5300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 189.89519262313843 seconds\n",
      "Step 5400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 193.63137197494507 seconds\n",
      "Step 5500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 197.1236035823822 seconds\n",
      "Step 5600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 200.61403346061707 seconds\n",
      "Step 5700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 204.30229449272156 seconds\n",
      "Step 5800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 207.80475497245789 seconds\n",
      "Step 5900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 211.289142370224 seconds\n",
      "Step 6000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 214.78245854377747 seconds\n",
      "Step 6100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 218.47379970550537 seconds\n",
      "Step 6200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 221.97430300712585 seconds\n",
      "Step 6300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 225.51269912719727 seconds\n",
      "Step 6400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 229.20926642417908 seconds\n",
      "Step 6500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 232.7987859249115 seconds\n",
      "Step 6600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 236.38131189346313 seconds\n",
      "Step 6700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 240.15682339668274 seconds\n",
      "Step 6800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 243.6726098060608 seconds\n",
      "Step 6900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 247.15685415267944 seconds\n",
      "Step 7000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 250.87683582305908 seconds\n",
      "Step 7100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 254.38344478607178 seconds\n",
      "Step 7200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 257.86183738708496 seconds\n",
      "Step 7300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 261.5839285850525 seconds\n",
      "Step 7400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 265.10242319107056 seconds\n",
      "Step 7500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 268.60680079460144 seconds\n",
      "Step 7600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 272.3317949771881 seconds\n",
      "Step 7700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 275.88242983818054 seconds\n",
      "Step 7800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 279.38866114616394 seconds\n",
      "Step 7900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 283.10926723480225 seconds\n",
      "Step 8000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 286.6014792919159 seconds\n",
      "Step 8100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 290.0814414024353 seconds\n",
      "Step 8200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 293.86214876174927 seconds\n",
      "Step 8300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 297.3714089393616 seconds\n",
      "Step 8400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 300.8989017009735 seconds\n",
      "Step 8500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 304.63336157798767 seconds\n",
      "Step 8600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 308.1456115245819 seconds\n",
      "Step 8700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 311.63061141967773 seconds\n",
      "Step 8800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 315.10849237442017 seconds\n",
      "Step 8900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 318.8288514614105 seconds\n",
      "Step 9000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 322.31965231895447 seconds\n",
      "Step 9100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 325.85157537460327 seconds\n",
      "Step 9200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 329.5807297229767 seconds\n",
      "Step 9300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 333.08884477615356 seconds\n",
      "Step 9400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 336.5797040462494 seconds\n",
      "Step 9500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 340.305233001709 seconds\n",
      "Step 9600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 343.7918574810028 seconds\n",
      "Step 9700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 347.2755615711212 seconds\n",
      "Step 9800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 350.9917049407959 seconds\n",
      "Step 9900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 354.4831929206848 seconds\n",
      "Step 10000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 357.9678883552551 seconds\n",
      "Step 10100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 361.6658296585083 seconds\n",
      "Step 10200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 365.16768884658813 seconds\n",
      "Step 10300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 368.66404390335083 seconds\n",
      "Step 10400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 372.3762700557709 seconds\n",
      "Step 10500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 375.87364506721497 seconds\n",
      "Step 10600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 379.3839011192322 seconds\n",
      "Step 10700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 383.08806347846985 seconds\n",
      "Step 10800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 386.59162044525146 seconds\n",
      "Step 10900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 390.088809967041 seconds\n",
      "Step 11000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 393.8002691268921 seconds\n",
      "Step 11100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 397.30385541915894 seconds\n",
      "Step 11200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 400.823942899704 seconds\n",
      "Step 11300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 404.33376455307007 seconds\n",
      "Step 11400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 408.0637905597687 seconds\n",
      "Step 11500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 411.57568359375 seconds\n",
      "Step 11600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 415.1388282775879 seconds\n",
      "Step 11700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 418.88720440864563 seconds\n",
      "Step 11800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 422.4255106449127 seconds\n",
      "Step 11900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 425.9762899875641 seconds\n",
      "Step 12000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 429.68371868133545 seconds\n",
      "19\n",
      "Step 0, loss: tensor(0.0245, grad_fn=<SubBackward0>), time elapsed: 0.03488302230834961 seconds\n",
      "Step 100, loss: tensor(0.0172, grad_fn=<SubBackward0>), time elapsed: 3.5310065746307373 seconds\n",
      "Step 200, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 7.0189549922943115 seconds\n",
      "Step 300, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 10.673619985580444 seconds\n",
      "Step 400, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 14.12725281715393 seconds\n",
      "Step 500, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 17.59183955192566 seconds\n",
      "Step 600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 21.25606918334961 seconds\n",
      "Step 700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 24.720569610595703 seconds\n",
      "Step 800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 28.222763538360596 seconds\n",
      "Step 900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 31.890825986862183 seconds\n",
      "Step 1000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 35.392661809921265 seconds\n",
      "Step 1100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 38.8666410446167 seconds\n",
      "Step 1200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 42.531219482421875 seconds\n",
      "Step 1300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 46.02445077896118 seconds\n",
      "Step 1400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 49.555790424346924 seconds\n",
      "Step 1500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 53.238577127456665 seconds\n",
      "Step 1600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 56.75557041168213 seconds\n",
      "Step 1700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 60.24803376197815 seconds\n",
      "Step 1800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 63.745383501052856 seconds\n",
      "Step 1900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 67.45482349395752 seconds\n",
      "Step 2000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 70.98024654388428 seconds\n",
      "Step 2100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 74.51008605957031 seconds\n",
      "Step 2200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 78.22211408615112 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 81.75372219085693 seconds\n",
      "Step 2400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 85.27566123008728 seconds\n",
      "Step 2500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 88.9729745388031 seconds\n",
      "Step 2600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 92.46184015274048 seconds\n",
      "Step 2700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 95.94553422927856 seconds\n",
      "Step 2800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 99.65808486938477 seconds\n",
      "Step 2900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 103.13169026374817 seconds\n",
      "Step 3000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 106.61108064651489 seconds\n",
      "Step 3100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 110.31293535232544 seconds\n",
      "Step 3200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 113.81450128555298 seconds\n",
      "Step 3300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 117.31038308143616 seconds\n",
      "Step 3400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 121.01347351074219 seconds\n",
      "Step 3500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 124.52533054351807 seconds\n",
      "Step 3600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 128.0009410381317 seconds\n",
      "Step 3700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 131.686527967453 seconds\n",
      "Step 3800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 135.1774206161499 seconds\n",
      "Step 3900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 138.69658374786377 seconds\n",
      "Step 4000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 142.39603066444397 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 145.8731653690338 seconds\n",
      "Step 4200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 149.36203813552856 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 153.0473062992096 seconds\n",
      "Step 4400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 156.54013800621033 seconds\n",
      "Step 4500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 160.0452561378479 seconds\n",
      "Step 4600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 163.52741527557373 seconds\n",
      "Step 4700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 167.23118686676025 seconds\n",
      "Step 4800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 170.7407693862915 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 174.24020099639893 seconds\n",
      "Step 5000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 177.90924263000488 seconds\n",
      "Step 5100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 181.3956310749054 seconds\n",
      "Step 5200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 184.8762149810791 seconds\n",
      "Step 5300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 188.56989908218384 seconds\n",
      "Step 5400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 192.0699908733368 seconds\n",
      "Step 5500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 195.54671025276184 seconds\n",
      "Step 5600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 199.23208808898926 seconds\n",
      "Step 5700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 202.71798586845398 seconds\n",
      "Step 5800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 206.19906640052795 seconds\n",
      "Step 5900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 209.9211814403534 seconds\n",
      "Step 6000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 213.39332628250122 seconds\n",
      "Step 6100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 216.86800694465637 seconds\n",
      "Step 6200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 220.5455677509308 seconds\n",
      "Step 6300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 224.02194666862488 seconds\n",
      "Step 6400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 227.53821802139282 seconds\n",
      "Step 6500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 231.2397496700287 seconds\n",
      "Step 6600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 234.7215497493744 seconds\n",
      "Step 6700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 238.20257258415222 seconds\n",
      "Step 6800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 241.89184093475342 seconds\n",
      "Step 6900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 245.40555143356323 seconds\n",
      "Step 7000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 248.89368295669556 seconds\n",
      "Step 7100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 252.3927240371704 seconds\n",
      "Step 7200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 256.1203553676605 seconds\n",
      "Step 7300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 259.6163420677185 seconds\n",
      "Step 7400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 263.1326858997345 seconds\n",
      "Step 7500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 266.8261625766754 seconds\n",
      "Step 7600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 270.30289101600647 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 273.79168581962585 seconds\n",
      "Step 7800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 277.49391412734985 seconds\n",
      "Step 7900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 281.0081820487976 seconds\n",
      "Step 8000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 284.4779734611511 seconds\n",
      "Step 8100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 288.1997983455658 seconds\n",
      "Step 8200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 291.72645139694214 seconds\n",
      "Step 8300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 295.2130217552185 seconds\n",
      "Step 8400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 298.9165873527527 seconds\n",
      "Step 8500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 302.4240288734436 seconds\n",
      "Step 8600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 305.9258990287781 seconds\n",
      "Step 8700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 309.6145353317261 seconds\n",
      "Step 8800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 313.13879776000977 seconds\n",
      "Step 8900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 316.6535427570343 seconds\n",
      "Step 9000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 320.34922885894775 seconds\n",
      "Step 9100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 323.8556833267212 seconds\n",
      "Step 9200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 327.37302684783936 seconds\n",
      "Step 9300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 331.0858449935913 seconds\n",
      "Step 9400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 334.61444091796875 seconds\n",
      "Step 9500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 338.1163604259491 seconds\n",
      "Step 9600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 341.61471009254456 seconds\n",
      "Step 9700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 345.3584599494934 seconds\n",
      "Step 9800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 348.93306612968445 seconds\n",
      "Step 9900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 352.4431154727936 seconds\n",
      "Step 10000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 356.1926066875458 seconds\n",
      "Step 10100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 359.7437963485718 seconds\n",
      "Step 10200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 363.276850938797 seconds\n",
      "Step 10300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 367.03989124298096 seconds\n",
      "Step 10400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 370.6094899177551 seconds\n",
      "Step 10500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 374.1506378650665 seconds\n",
      "Step 10600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 377.9711515903473 seconds\n",
      "Step 10700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 381.530873298645 seconds\n",
      "Step 10800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 385.1050834655762 seconds\n",
      "Step 10900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 388.890864610672 seconds\n",
      "Step 11000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 392.45503306388855 seconds\n",
      "Step 11100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 396.0293505191803 seconds\n",
      "Step 11200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 399.80914187431335 seconds\n",
      "Step 11300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 403.3934428691864 seconds\n",
      "Step 11400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 406.9586253166199 seconds\n",
      "Step 11500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 410.7473108768463 seconds\n",
      "Step 11600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 414.3049387931824 seconds\n",
      "Step 11700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 417.86874437332153 seconds\n",
      "Step 11800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 421.6602339744568 seconds\n",
      "Step 11900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 425.23009991645813 seconds\n",
      "Step 12000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 428.8350439071655 seconds\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0126, grad_fn=<SubBackward0>), time elapsed: 0.03887009620666504 seconds\n",
      "Step 100, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 3.5966341495513916 seconds\n",
      "Step 200, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 7.336001634597778 seconds\n",
      "Step 300, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 10.872233629226685 seconds\n",
      "Step 400, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 14.347691535949707 seconds\n",
      "Step 500, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 18.01476812362671 seconds\n",
      "Step 600, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 21.55010437965393 seconds\n",
      "Step 700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 25.063281536102295 seconds\n",
      "Step 800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 28.772546768188477 seconds\n",
      "Step 900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 32.282888889312744 seconds\n",
      "Step 1000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 35.76998043060303 seconds\n",
      "Step 1100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 39.48460674285889 seconds\n",
      "Step 1200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 42.985870361328125 seconds\n",
      "Step 1300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 46.476977586746216 seconds\n",
      "Step 1400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 50.17363476753235 seconds\n",
      "Step 1500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 53.68339133262634 seconds\n",
      "Step 1600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 57.18711853027344 seconds\n",
      "Step 1700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 60.85516095161438 seconds\n",
      "Step 1800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 64.3321943283081 seconds\n",
      "Step 1900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 67.81748533248901 seconds\n",
      "Step 2000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 71.54082155227661 seconds\n",
      "Step 2100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 75.02481341362 seconds\n",
      "Step 2200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 78.49166107177734 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 82.19707083702087 seconds\n",
      "Step 2400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 85.69721245765686 seconds\n",
      "Step 2500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 89.21373391151428 seconds\n",
      "Step 2600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 92.88231253623962 seconds\n",
      "Step 2700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 96.38782405853271 seconds\n",
      "Step 2800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 99.88821744918823 seconds\n",
      "Step 2900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 103.5798556804657 seconds\n",
      "Step 3000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 107.05190372467041 seconds\n",
      "Step 3100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 110.54090309143066 seconds\n",
      "Step 3200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 114.26495218276978 seconds\n",
      "Step 3300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 117.74004673957825 seconds\n",
      "Step 3400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 121.20593595504761 seconds\n",
      "Step 3500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 124.73811173439026 seconds\n",
      "Step 3600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 128.43732833862305 seconds\n",
      "Step 3700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 131.96342754364014 seconds\n",
      "Step 3800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 135.44082689285278 seconds\n",
      "Step 3900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 139.10559487342834 seconds\n",
      "Step 4000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 142.6070363521576 seconds\n",
      "Step 4100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 146.09333491325378 seconds\n",
      "Step 4200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 149.76048469543457 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 153.26914238929749 seconds\n",
      "Step 4400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 156.79507541656494 seconds\n",
      "Step 4500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 160.5224747657776 seconds\n",
      "Step 4600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 164.00454187393188 seconds\n",
      "Step 4700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 167.5293834209442 seconds\n",
      "Step 4800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 171.21391105651855 seconds\n",
      "Step 4900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 174.69645643234253 seconds\n",
      "Step 5000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 178.23523569107056 seconds\n",
      "Step 5100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 181.94371485710144 seconds\n",
      "Step 5200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 185.43360257148743 seconds\n",
      "Step 5300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 188.92535662651062 seconds\n",
      "Step 5400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 192.62492775917053 seconds\n",
      "Step 5500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 196.12158465385437 seconds\n",
      "Step 5600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 199.6380898952484 seconds\n",
      "Step 5700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 203.35961294174194 seconds\n",
      "Step 5800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 206.83819484710693 seconds\n",
      "Step 5900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 210.33377480506897 seconds\n",
      "Step 6000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 214.0683159828186 seconds\n",
      "Step 6100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 217.5854320526123 seconds\n",
      "Step 6200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 221.06867241859436 seconds\n",
      "Step 6300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 224.57340288162231 seconds\n",
      "Step 6400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 228.29679250717163 seconds\n",
      "Step 6500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 231.79785346984863 seconds\n",
      "Step 6600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 235.27392387390137 seconds\n",
      "Step 6700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 238.95345997810364 seconds\n",
      "Step 6800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 242.44801783561707 seconds\n",
      "Step 6900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 245.92605137825012 seconds\n",
      "Step 7000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 249.64599752426147 seconds\n",
      "Step 7100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 253.179851770401 seconds\n",
      "Step 7200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 256.66158962249756 seconds\n",
      "Step 7300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 260.37419509887695 seconds\n",
      "Step 7400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 263.8676187992096 seconds\n",
      "Step 7500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 267.34859323501587 seconds\n",
      "Step 7600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 271.07102704048157 seconds\n",
      "Step 7700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 274.5712559223175 seconds\n",
      "Step 7800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 278.09176325798035 seconds\n",
      "Step 7900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 281.7769401073456 seconds\n",
      "Step 8000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 285.28987288475037 seconds\n",
      "Step 8100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 288.783242225647 seconds\n",
      "Step 8200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 292.48922896385193 seconds\n",
      "Step 8300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 295.976487159729 seconds\n",
      "Step 8400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 299.49860191345215 seconds\n",
      "Step 8500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 303.2049677371979 seconds\n",
      "Step 8600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 306.7165651321411 seconds\n",
      "Step 8700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 310.31606554985046 seconds\n",
      "Step 8800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 313.85578775405884 seconds\n",
      "Step 8900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 317.6322400569916 seconds\n",
      "Step 9000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 321.2184307575226 seconds\n",
      "Step 9100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 324.7091588973999 seconds\n",
      "Step 9200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 328.39755177497864 seconds\n",
      "Step 9300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 331.9066801071167 seconds\n",
      "Step 9400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 335.4171988964081 seconds\n",
      "Step 9500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 339.1541500091553 seconds\n",
      "Step 9600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 342.6752943992615 seconds\n",
      "Step 9700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 346.18666553497314 seconds\n",
      "Step 9800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 349.89393877983093 seconds\n",
      "Step 9900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 353.38569736480713 seconds\n",
      "Step 10000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 356.9080719947815 seconds\n",
      "Step 10100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 360.602276802063 seconds\n",
      "Step 10200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 364.1006097793579 seconds\n",
      "Step 10300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 367.6101973056793 seconds\n",
      "Step 10400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 371.3389790058136 seconds\n",
      "Step 10500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 374.86303329467773 seconds\n",
      "Step 10600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 378.3756408691406 seconds\n",
      "Step 10700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 382.1010482311249 seconds\n",
      "Step 10800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 385.62064480781555 seconds\n",
      "Step 10900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 389.1707968711853 seconds\n",
      "Step 11000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 392.9074184894562 seconds\n",
      "Step 11100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 396.4308681488037 seconds\n",
      "Step 11200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 399.967577457428 seconds\n",
      "Step 11300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 403.47141003608704 seconds\n",
      "Step 11400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 407.2342941761017 seconds\n",
      "Step 11500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 410.77860379219055 seconds\n",
      "Step 11600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 414.3002769947052 seconds\n",
      "Step 11700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 418.0458221435547 seconds\n",
      "Step 11800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 421.59889340400696 seconds\n",
      "Step 11900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 425.1600890159607 seconds\n",
      "Step 12000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 428.9286870956421 seconds\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 3.5828912258148193 seconds\n",
      "Step 200, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 7.101088047027588 seconds\n",
      "Step 300, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 10.806631326675415 seconds\n",
      "Step 400, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 14.280253887176514 seconds\n",
      "Step 500, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 17.76872706413269 seconds\n",
      "Step 600, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 21.449604988098145 seconds\n",
      "Step 700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 24.94672656059265 seconds\n",
      "Step 800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 28.42120385169983 seconds\n",
      "Step 900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 32.13471531867981 seconds\n",
      "Step 1000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 35.63890719413757 seconds\n",
      "Step 1100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 39.12725472450256 seconds\n",
      "Step 1200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 42.854259967803955 seconds\n",
      "Step 1300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 46.38786292076111 seconds\n",
      "Step 1400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 49.86634397506714 seconds\n",
      "Step 1500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 53.53940415382385 seconds\n",
      "Step 1600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 57.01451230049133 seconds\n",
      "Step 1700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 60.53337383270264 seconds\n",
      "Step 1800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 64.23229813575745 seconds\n",
      "Step 1900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 67.75779247283936 seconds\n",
      "Step 2000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 71.23668384552002 seconds\n",
      "Step 2100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 74.9349377155304 seconds\n",
      "Step 2200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 78.44613766670227 seconds\n",
      "Step 2300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 81.91955661773682 seconds\n",
      "Step 2400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 85.37648391723633 seconds\n",
      "Step 2500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 89.04923033714294 seconds\n",
      "Step 2600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 92.52856731414795 seconds\n",
      "Step 2700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 96.0150077342987 seconds\n",
      "Step 2800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 99.70433211326599 seconds\n",
      "Step 2900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 103.18247270584106 seconds\n",
      "Step 3000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 106.68990969657898 seconds\n",
      "Step 3100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 110.4082977771759 seconds\n",
      "Step 3200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 113.88683581352234 seconds\n",
      "Step 3300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 117.35542607307434 seconds\n",
      "Step 3400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 121.05355453491211 seconds\n",
      "Step 3500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 124.55997681617737 seconds\n",
      "Step 3600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 128.03498792648315 seconds\n",
      "Step 3700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 131.7379777431488 seconds\n",
      "Step 3800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 135.21812343597412 seconds\n",
      "Step 3900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 138.69520354270935 seconds\n",
      "Step 4000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 142.39796209335327 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 145.88252806663513 seconds\n",
      "Step 4200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 149.35961651802063 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 153.02886581420898 seconds\n",
      "Step 4400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 156.5124020576477 seconds\n",
      "Step 4500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 159.99069738388062 seconds\n",
      "Step 4600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 163.6717655658722 seconds\n",
      "Step 4700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 167.1960415840149 seconds\n",
      "Step 4800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 170.68892002105713 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 174.18794441223145 seconds\n",
      "Step 5000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 177.91159391403198 seconds\n",
      "Step 5100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 181.40313172340393 seconds\n",
      "Step 5200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 184.8865213394165 seconds\n",
      "Step 5300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 188.56273484230042 seconds\n",
      "Step 5400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 192.05538725852966 seconds\n",
      "Step 5500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 195.57790613174438 seconds\n",
      "Step 5600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 199.2465386390686 seconds\n",
      "Step 5700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 202.72906064987183 seconds\n",
      "Step 5800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 206.20551538467407 seconds\n",
      "Step 5900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 209.89231181144714 seconds\n",
      "Step 6000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 213.39301419258118 seconds\n",
      "Step 6100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 216.88196635246277 seconds\n",
      "Step 6200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 220.58012700080872 seconds\n",
      "Step 6300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 224.0480535030365 seconds\n",
      "Step 6400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 227.51917433738708 seconds\n",
      "Step 6500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 231.20948696136475 seconds\n",
      "Step 6600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 234.68044686317444 seconds\n",
      "Step 6700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 238.1844072341919 seconds\n",
      "Step 6800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 241.8693289756775 seconds\n",
      "Step 6900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 245.3403902053833 seconds\n",
      "Step 7000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 248.84957075119019 seconds\n",
      "Step 7100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 252.51744723320007 seconds\n",
      "Step 7200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 255.98451495170593 seconds\n",
      "Step 7300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 259.47327065467834 seconds\n",
      "Step 7400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 263.1630914211273 seconds\n",
      "Step 7500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 266.6622405052185 seconds\n",
      "Step 7600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 270.14360332489014 seconds\n",
      "Step 7700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 273.61048412323 seconds\n",
      "Step 7800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 277.3190665245056 seconds\n",
      "Step 7900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 280.8510971069336 seconds\n",
      "Step 8000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 284.3398861885071 seconds\n",
      "Step 8100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 288.0162444114685 seconds\n",
      "Step 8200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 291.5513770580292 seconds\n",
      "Step 8300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 295.0347390174866 seconds\n",
      "Step 8400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 298.7858657836914 seconds\n",
      "Step 8500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 302.34705781936646 seconds\n",
      "Step 8600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 305.8864920139313 seconds\n",
      "Step 8700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 309.602326631546 seconds\n",
      "Step 8800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 313.0930063724518 seconds\n",
      "Step 8900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 316.5737462043762 seconds\n",
      "Step 9000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 320.288010597229 seconds\n",
      "Step 9100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 323.77028012275696 seconds\n",
      "Step 9200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 327.28544521331787 seconds\n",
      "Step 9300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 330.9736523628235 seconds\n",
      "Step 9400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 334.46143412590027 seconds\n",
      "Step 9500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 337.97680616378784 seconds\n",
      "Step 9600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 341.6651644706726 seconds\n",
      "Step 9700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 345.1372480392456 seconds\n",
      "Step 9800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 348.62248945236206 seconds\n",
      "Step 9900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 352.1002826690674 seconds\n",
      "Step 10000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 355.84514260292053 seconds\n",
      "Step 10100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 359.34317278862 seconds\n",
      "Step 10200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 362.83207631111145 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 366.5292868614197 seconds\n",
      "Step 10400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 370.0389220714569 seconds\n",
      "Step 10500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 373.5825560092926 seconds\n",
      "Step 10600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 377.30735445022583 seconds\n",
      "Step 10700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 380.8136022090912 seconds\n",
      "Step 10800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 384.30314803123474 seconds\n",
      "Step 10900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 388.04067039489746 seconds\n",
      "Step 11000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 391.63128876686096 seconds\n",
      "Step 11100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 395.2643105983734 seconds\n",
      "Step 11200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 399.0286431312561 seconds\n",
      "Step 11300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 402.6330609321594 seconds\n",
      "Step 11400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 406.19357919692993 seconds\n",
      "Step 11500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 409.99416851997375 seconds\n",
      "Step 11600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 413.6083753108978 seconds\n",
      "Step 11700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 417.2461392879486 seconds\n",
      "Step 11800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 421.04839849472046 seconds\n",
      "Step 11900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 424.66152715682983 seconds\n",
      "Step 12000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 428.29623341560364 seconds\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0227, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 3.768280029296875 seconds\n",
      "Step 200, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 7.261916637420654 seconds\n",
      "Step 300, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 10.797386646270752 seconds\n",
      "Step 400, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 14.506479501724243 seconds\n",
      "Step 500, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 18.061668634414673 seconds\n",
      "Step 600, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 21.604176998138428 seconds\n",
      "Step 700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 25.141831159591675 seconds\n",
      "Step 800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 28.829481601715088 seconds\n",
      "Step 900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 32.29324960708618 seconds\n",
      "Step 1000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 35.849563121795654 seconds\n",
      "Step 1100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 39.60171818733215 seconds\n",
      "Step 1200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 43.10779571533203 seconds\n",
      "Step 1300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 46.602736711502075 seconds\n",
      "Step 1400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 50.28721046447754 seconds\n",
      "Step 1500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 53.88081622123718 seconds\n",
      "Step 1600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 57.43643856048584 seconds\n",
      "Step 1700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 61.1841561794281 seconds\n",
      "Step 1800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 64.75919675827026 seconds\n",
      "Step 1900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 68.31012463569641 seconds\n",
      "Step 2000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 72.06109499931335 seconds\n",
      "Step 2100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 75.59466981887817 seconds\n",
      "Step 2200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 79.14653849601746 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 82.87652039527893 seconds\n",
      "Step 2400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 86.4088830947876 seconds\n",
      "Step 2500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 89.98018550872803 seconds\n",
      "Step 2600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 93.71250939369202 seconds\n",
      "Step 2700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 97.21357560157776 seconds\n",
      "Step 2800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 100.73585629463196 seconds\n",
      "Step 2900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 104.424556016922 seconds\n",
      "Step 3000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 107.90023159980774 seconds\n",
      "Step 3100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 111.42199349403381 seconds\n",
      "Step 3200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 114.91553354263306 seconds\n",
      "Step 3300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 118.59072494506836 seconds\n",
      "Step 3400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 122.08503746986389 seconds\n",
      "Step 3500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 125.55822920799255 seconds\n",
      "Step 3600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 129.2750949859619 seconds\n",
      "Step 3700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 132.75958490371704 seconds\n",
      "Step 3800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 136.23875164985657 seconds\n",
      "Step 3900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 139.899427652359 seconds\n",
      "Step 4000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 143.3748321533203 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 146.88464188575745 seconds\n",
      "Step 4200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 150.57867074012756 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 154.05976557731628 seconds\n",
      "Step 4400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 157.53787851333618 seconds\n",
      "Step 4500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 161.22277808189392 seconds\n",
      "Step 4600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 164.75369596481323 seconds\n",
      "Step 4700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 168.24239897727966 seconds\n",
      "Step 4800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 171.92261338233948 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 175.41477155685425 seconds\n",
      "Step 5000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 178.90637469291687 seconds\n",
      "Step 5100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 182.6069791316986 seconds\n",
      "Step 5200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 186.13480424880981 seconds\n",
      "Step 5300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 189.62737369537354 seconds\n",
      "Step 5400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 193.30372405052185 seconds\n",
      "Step 5500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 196.7900152206421 seconds\n",
      "Step 5600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 200.28793954849243 seconds\n",
      "Step 5700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 203.7586555480957 seconds\n",
      "Step 5800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 207.47601771354675 seconds\n",
      "Step 5900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 210.97498536109924 seconds\n",
      "Step 6000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 214.44708609580994 seconds\n",
      "Step 6100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 218.12318420410156 seconds\n",
      "Step 6200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 221.6098222732544 seconds\n",
      "Step 6300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 225.08321380615234 seconds\n",
      "Step 6400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 228.7642593383789 seconds\n",
      "Step 6500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 232.24109864234924 seconds\n",
      "Step 6600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 235.7089822292328 seconds\n",
      "Step 6700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 239.4320375919342 seconds\n",
      "Step 6800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 242.9465777873993 seconds\n",
      "Step 6900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 246.42489743232727 seconds\n",
      "Step 7000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 250.11041235923767 seconds\n",
      "Step 7100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 253.599102973938 seconds\n",
      "Step 7200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 257.07333159446716 seconds\n",
      "Step 7300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 260.76748037338257 seconds\n",
      "Step 7400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 264.24909257888794 seconds\n",
      "Step 7500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 267.72628688812256 seconds\n",
      "Step 7600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 271.42585802078247 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 274.91040110588074 seconds\n",
      "Step 7800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 278.39082646369934 seconds\n",
      "Step 7900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 282.0730028152466 seconds\n",
      "Step 8000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 285.5537133216858 seconds\n",
      "Step 8100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 289.0769021511078 seconds\n",
      "Step 8200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 292.79039764404297 seconds\n",
      "Step 8300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 296.28205609321594 seconds\n",
      "Step 8400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 299.79942655563354 seconds\n",
      "Step 8500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 303.27728748321533 seconds\n",
      "Step 8600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 306.9927623271942 seconds\n",
      "Step 8700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 310.53595447540283 seconds\n",
      "Step 8800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 314.0765883922577 seconds\n",
      "Step 8900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 317.7537417411804 seconds\n",
      "Step 9000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 321.24251437187195 seconds\n",
      "Step 9100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 324.7840688228607 seconds\n",
      "Step 9200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 328.51466178894043 seconds\n",
      "Step 9300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 332.0647966861725 seconds\n",
      "Step 9400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 335.5691361427307 seconds\n",
      "Step 9500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 339.2514748573303 seconds\n",
      "Step 9600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 342.8040096759796 seconds\n",
      "Step 9700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 346.33389687538147 seconds\n",
      "Step 9800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 350.03610038757324 seconds\n",
      "Step 9900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 353.51544880867004 seconds\n",
      "Step 10000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 357.01829838752747 seconds\n",
      "Step 10100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 360.7385959625244 seconds\n",
      "Step 10200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 364.22424054145813 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 367.7113516330719 seconds\n",
      "Step 10400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 371.41684794425964 seconds\n",
      "Step 10500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 374.94196105003357 seconds\n",
      "Step 10600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 378.46108841896057 seconds\n",
      "Step 10700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 381.94921255111694 seconds\n",
      "Step 10800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 385.69376945495605 seconds\n",
      "Step 10900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 389.256028175354 seconds\n",
      "Step 11000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 392.77650809288025 seconds\n",
      "Step 11100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 396.52445244789124 seconds\n",
      "Step 11200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 400.0359625816345 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 403.54897475242615 seconds\n",
      "Step 11400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 407.2780156135559 seconds\n",
      "Step 11500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 410.7981584072113 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 414.34382796287537 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 418.1316258907318 seconds\n",
      "Step 11800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 421.72098898887634 seconds\n",
      "Step 11900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 425.2804973125458 seconds\n",
      "Step 12000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 429.1292338371277 seconds\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(0.0102, grad_fn=<SubBackward0>), time elapsed: 3.570087194442749 seconds\n",
      "Step 200, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 7.105337619781494 seconds\n",
      "Step 300, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 10.833814144134521 seconds\n",
      "Step 400, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 14.366061687469482 seconds\n",
      "Step 500, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 17.884968280792236 seconds\n",
      "Step 600, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 21.565211057662964 seconds\n",
      "Step 700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 25.03697109222412 seconds\n",
      "Step 800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 28.517565488815308 seconds\n",
      "Step 900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 32.201135873794556 seconds\n",
      "Step 1000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 35.73167300224304 seconds\n",
      "Step 1100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 39.214903354644775 seconds\n",
      "Step 1200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 42.94651198387146 seconds\n",
      "Step 1300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 46.41737365722656 seconds\n",
      "Step 1400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 49.90142250061035 seconds\n",
      "Step 1500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 53.61031675338745 seconds\n",
      "Step 1600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 57.12578296661377 seconds\n",
      "Step 1700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 60.63590621948242 seconds\n",
      "Step 1800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 64.12310886383057 seconds\n",
      "Step 1900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 67.80297446250916 seconds\n",
      "Step 2000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 71.29753732681274 seconds\n",
      "Step 2100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 74.77433633804321 seconds\n",
      "Step 2200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 78.48276877403259 seconds\n",
      "Step 2300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 81.97031426429749 seconds\n",
      "Step 2400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 85.44509673118591 seconds\n",
      "Step 2500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 89.15024161338806 seconds\n",
      "Step 2600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 92.64381623268127 seconds\n",
      "Step 2700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 96.12448263168335 seconds\n",
      "Step 2800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 99.78758907318115 seconds\n",
      "Step 2900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 103.27006316184998 seconds\n",
      "Step 3000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 106.78135967254639 seconds\n",
      "Step 3100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 110.49269795417786 seconds\n",
      "Step 3200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 114.02465271949768 seconds\n",
      "Step 3300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 117.54749989509583 seconds\n",
      "Step 3400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 121.24989914894104 seconds\n",
      "Step 3500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 124.7374861240387 seconds\n",
      "Step 3600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 128.21822929382324 seconds\n",
      "Step 3700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 131.9138216972351 seconds\n",
      "Step 3800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 135.42324948310852 seconds\n",
      "Step 3900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 138.94194388389587 seconds\n",
      "Step 4000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 142.64611387252808 seconds\n",
      "Step 4100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 146.1769495010376 seconds\n",
      "Step 4200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 149.6957335472107 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 153.40411901474 seconds\n",
      "Step 4400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 156.8784646987915 seconds\n",
      "Step 4500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 160.3520860671997 seconds\n",
      "Step 4600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 163.82387518882751 seconds\n",
      "Step 4700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 167.48607540130615 seconds\n",
      "Step 4800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 170.99113178253174 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 174.50746822357178 seconds\n",
      "Step 5000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 178.1965627670288 seconds\n",
      "Step 5100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 181.6811592578888 seconds\n",
      "Step 5200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 185.1535885334015 seconds\n",
      "Step 5300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 188.87121963500977 seconds\n",
      "Step 5400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 192.37535667419434 seconds\n",
      "Step 5500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 195.8493914604187 seconds\n",
      "Step 5600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 199.55961108207703 seconds\n",
      "Step 5700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 203.0447359085083 seconds\n",
      "Step 5800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 206.52229690551758 seconds\n",
      "Step 5900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 210.21659564971924 seconds\n",
      "Step 6000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 213.69851875305176 seconds\n",
      "Step 6100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 217.17484974861145 seconds\n",
      "Step 6200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 220.86200833320618 seconds\n",
      "Step 6300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 224.33918404579163 seconds\n",
      "Step 6400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 227.8549449443817 seconds\n",
      "Step 6500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 231.53483510017395 seconds\n",
      "Step 6600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 235.0034658908844 seconds\n",
      "Step 6700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 238.47701334953308 seconds\n",
      "Step 6800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 242.1501100063324 seconds\n",
      "Step 6900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 245.64878273010254 seconds\n",
      "Step 7000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 249.13256239891052 seconds\n",
      "Step 7100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 252.80561876296997 seconds\n",
      "Step 7200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 256.33499932289124 seconds\n",
      "Step 7300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 259.8562924861908 seconds\n",
      "Step 7400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 263.34254264831543 seconds\n",
      "Step 7500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 267.03253078460693 seconds\n",
      "Step 7600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 270.52477264404297 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 274.07050371170044 seconds\n",
      "Step 7800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 277.75606632232666 seconds\n",
      "Step 7900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 281.28066992759705 seconds\n",
      "Step 8000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 284.80420088768005 seconds\n",
      "Step 8100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 288.51011753082275 seconds\n",
      "Step 8200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 292.01785349845886 seconds\n",
      "Step 8300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 295.516964673996 seconds\n",
      "Step 8400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 299.2258059978485 seconds\n",
      "Step 8500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 302.74250316619873 seconds\n",
      "Step 8600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 306.2290382385254 seconds\n",
      "Step 8700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 309.9097170829773 seconds\n",
      "Step 8800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 313.3971598148346 seconds\n",
      "Step 8900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 316.92419052124023 seconds\n",
      "Step 9000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 320.6219639778137 seconds\n",
      "Step 9100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 324.12095284461975 seconds\n",
      "Step 9200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 327.6162974834442 seconds\n",
      "Step 9300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 331.3355300426483 seconds\n",
      "Step 9400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 334.832852602005 seconds\n",
      "Step 9500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 338.32998061180115 seconds\n",
      "Step 9600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 342.0512297153473 seconds\n",
      "Step 9700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 345.5424304008484 seconds\n",
      "Step 9800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 349.0370225906372 seconds\n",
      "Step 9900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 352.58215737342834 seconds\n",
      "Step 10000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 356.2838432788849 seconds\n",
      "Step 10100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 359.78408908843994 seconds\n",
      "Step 10200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 363.33452892303467 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 367.07805371284485 seconds\n",
      "Step 10400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 370.65494751930237 seconds\n",
      "Step 10500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 374.1502721309662 seconds\n",
      "Step 10600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 377.89498472213745 seconds\n",
      "Step 10700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 381.3972523212433 seconds\n",
      "Step 10800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 384.9035632610321 seconds\n",
      "Step 10900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 388.6467926502228 seconds\n",
      "Step 11000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 392.1636781692505 seconds\n",
      "Step 11100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 395.72261667251587 seconds\n",
      "Step 11200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 399.49206137657166 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 403.05749106407166 seconds\n",
      "Step 11400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 406.61570167541504 seconds\n",
      "Step 11500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 410.3454191684723 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 413.8804864883423 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 417.41991686820984 seconds\n",
      "Step 11800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 421.18396830558777 seconds\n",
      "Step 11900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 424.72741055488586 seconds\n",
      "Step 12000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 428.3392126560211 seconds\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0174, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(0.0108, grad_fn=<SubBackward0>), time elapsed: 3.7658262252807617 seconds\n",
      "Step 200, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 7.26974081993103 seconds\n",
      "Step 300, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 10.758131265640259 seconds\n",
      "Step 400, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 14.223711252212524 seconds\n",
      "Step 500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 17.88692855834961 seconds\n",
      "Step 600, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 21.37123441696167 seconds\n",
      "Step 700, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 24.894464015960693 seconds\n",
      "Step 800, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 28.576881885528564 seconds\n",
      "Step 900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 32.066569805145264 seconds\n",
      "Step 1000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 35.53994607925415 seconds\n",
      "Step 1100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 39.20879340171814 seconds\n",
      "Step 1200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 42.723153829574585 seconds\n",
      "Step 1300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 46.24322032928467 seconds\n",
      "Step 1400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 49.94549036026001 seconds\n",
      "Step 1500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 53.48367357254028 seconds\n",
      "Step 1600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 56.96287250518799 seconds\n",
      "Step 1700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 60.65591835975647 seconds\n",
      "Step 1800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 64.13478326797485 seconds\n",
      "Step 1900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 67.61118364334106 seconds\n",
      "Step 2000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 71.31025385856628 seconds\n",
      "Step 2100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 74.84526586532593 seconds\n",
      "Step 2200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 78.36668229103088 seconds\n",
      "Step 2300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 82.06448554992676 seconds\n",
      "Step 2400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 85.54982709884644 seconds\n",
      "Step 2500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 89.02849245071411 seconds\n",
      "Step 2600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 92.6963267326355 seconds\n",
      "Step 2700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 96.21251344680786 seconds\n",
      "Step 2800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 99.69396376609802 seconds\n",
      "Step 2900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 103.37020611763 seconds\n",
      "Step 3000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 106.84600782394409 seconds\n",
      "Step 3100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 110.32592678070068 seconds\n",
      "Step 3200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 113.83543038368225 seconds\n",
      "Step 3300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 117.52647018432617 seconds\n",
      "Step 3400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 121.0083339214325 seconds\n",
      "Step 3500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 124.49630737304688 seconds\n",
      "Step 3600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 128.1858229637146 seconds\n",
      "Step 3700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 131.691974401474 seconds\n",
      "Step 3800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 135.17071843147278 seconds\n",
      "Step 3900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 138.86776781082153 seconds\n",
      "Step 4000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 142.32508158683777 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 145.805095911026 seconds\n",
      "Step 4200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 149.46205830574036 seconds\n",
      "Step 4300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 152.9131200313568 seconds\n",
      "Step 4400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 156.35984992980957 seconds\n",
      "Step 4500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 160.00719285011292 seconds\n",
      "Step 4600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 163.4741289615631 seconds\n",
      "Step 4700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 166.95789194107056 seconds\n",
      "Step 4800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 170.6223587989807 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 174.06028270721436 seconds\n",
      "Step 5000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 177.521559715271 seconds\n",
      "Step 5100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 181.16618657112122 seconds\n",
      "Step 5200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 184.65771460533142 seconds\n",
      "Step 5300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 188.16372179985046 seconds\n",
      "Step 5400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 191.81391644477844 seconds\n",
      "Step 5500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 195.27509093284607 seconds\n",
      "Step 5600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 198.82693815231323 seconds\n",
      "Step 5700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 202.33618569374084 seconds\n",
      "Step 5800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 206.00950288772583 seconds\n",
      "Step 5900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 209.48788857460022 seconds\n",
      "Step 6000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 212.96773052215576 seconds\n",
      "Step 6100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 216.65066695213318 seconds\n",
      "Step 6200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 220.1749758720398 seconds\n",
      "Step 6300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 223.71390414237976 seconds\n",
      "Step 6400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 227.41419410705566 seconds\n",
      "Step 6500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 230.9035565853119 seconds\n",
      "Step 6600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 234.37838745117188 seconds\n",
      "Step 6700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 238.07898712158203 seconds\n",
      "Step 6800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 241.57175302505493 seconds\n",
      "Step 6900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 245.04390740394592 seconds\n",
      "Step 7000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 248.73473358154297 seconds\n",
      "Step 7100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 252.25454092025757 seconds\n",
      "Step 7200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 255.74414229393005 seconds\n",
      "Step 7300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 259.4503710269928 seconds\n",
      "Step 7400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 262.9333896636963 seconds\n",
      "Step 7500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 266.4178431034088 seconds\n",
      "Step 7600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 270.09928798675537 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 273.6306300163269 seconds\n",
      "Step 7800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 277.1186845302582 seconds\n",
      "Step 7900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 280.800418138504 seconds\n",
      "Step 8000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 284.33088541030884 seconds\n",
      "Step 8100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 287.86992931365967 seconds\n",
      "Step 8200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 291.4185104370117 seconds\n",
      "Step 8300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 295.09989500045776 seconds\n",
      "Step 8400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 298.6085648536682 seconds\n",
      "Step 8500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 302.0995047092438 seconds\n",
      "Step 8600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 305.8463044166565 seconds\n",
      "Step 8700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 309.3663535118103 seconds\n",
      "Step 8800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 312.85364031791687 seconds\n",
      "Step 8900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 316.5551257133484 seconds\n",
      "Step 9000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 320.08800292015076 seconds\n",
      "Step 9100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 323.6290693283081 seconds\n",
      "Step 9200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 327.3644313812256 seconds\n",
      "Step 9300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 330.904887676239 seconds\n",
      "Step 9400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 334.41829228401184 seconds\n",
      "Step 9500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 338.1361472606659 seconds\n",
      "Step 9600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 341.67314171791077 seconds\n",
      "Step 9700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 345.19550013542175 seconds\n",
      "Step 9800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 348.9121551513672 seconds\n",
      "Step 9900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 352.41315031051636 seconds\n",
      "Step 10000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 355.9139211177826 seconds\n",
      "Step 10100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 359.6572983264923 seconds\n",
      "Step 10200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 363.1560106277466 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 366.65707445144653 seconds\n",
      "Step 10400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 370.365690946579 seconds\n",
      "Step 10500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 373.87670040130615 seconds\n",
      "Step 10600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 377.40275382995605 seconds\n",
      "Step 10700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 380.90455198287964 seconds\n",
      "Step 10800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 384.60561442375183 seconds\n",
      "Step 10900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 388.10788583755493 seconds\n",
      "Step 11000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 391.60197138786316 seconds\n",
      "Step 11100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 395.3529634475708 seconds\n",
      "Step 11200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 398.86226081848145 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 402.3705863952637 seconds\n",
      "Step 11400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 406.12247133255005 seconds\n",
      "Step 11500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 409.63849568367004 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 413.1485016345978 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 416.90062522888184 seconds\n",
      "Step 11800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 420.4361777305603 seconds\n",
      "Step 11900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 423.98126673698425 seconds\n",
      "Step 12000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 427.7879946231842 seconds\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 0.03917956352233887 seconds\n",
      "Step 100, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 3.599332809448242 seconds\n",
      "Step 200, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 7.087504863739014 seconds\n",
      "Step 300, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 10.772325038909912 seconds\n",
      "Step 400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 14.28405499458313 seconds\n",
      "Step 500, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 17.759498596191406 seconds\n",
      "Step 600, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 21.455859899520874 seconds\n",
      "Step 700, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 24.954052209854126 seconds\n",
      "Step 800, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 28.428738117218018 seconds\n",
      "Step 900, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 32.11094522476196 seconds\n",
      "Step 1000, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 35.61266732215881 seconds\n",
      "Step 1100, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 39.09427833557129 seconds\n",
      "Step 1200, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 42.76606774330139 seconds\n",
      "Step 1300, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 46.256988525390625 seconds\n",
      "Step 1400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 49.741068601608276 seconds\n",
      "Step 1500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 53.25114941596985 seconds\n",
      "Step 1600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 56.929227352142334 seconds\n",
      "Step 1700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 60.411842584609985 seconds\n",
      "Step 1800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 63.87916278839111 seconds\n",
      "Step 1900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 67.5567855834961 seconds\n",
      "Step 2000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 71.09097814559937 seconds\n",
      "Step 2100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 74.58026909828186 seconds\n",
      "Step 2200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 78.25717306137085 seconds\n",
      "Step 2300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 81.73761200904846 seconds\n",
      "Step 2400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 85.21355652809143 seconds\n",
      "Step 2500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 88.91281867027283 seconds\n",
      "Step 2600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 92.43589472770691 seconds\n",
      "Step 2700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 95.91253304481506 seconds\n",
      "Step 2800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 99.61626601219177 seconds\n",
      "Step 2900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 103.12596988677979 seconds\n",
      "Step 3000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 106.63895201683044 seconds\n",
      "Step 3100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 110.3128035068512 seconds\n",
      "Step 3200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 113.82288908958435 seconds\n",
      "Step 3300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 117.29971623420715 seconds\n",
      "Step 3400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 121.00194787979126 seconds\n",
      "Step 3500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 124.5243411064148 seconds\n",
      "Step 3600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 128.02576994895935 seconds\n",
      "Step 3700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 131.70156574249268 seconds\n",
      "Step 3800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 135.20853996276855 seconds\n",
      "Step 3900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 138.69164776802063 seconds\n",
      "Step 4000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 142.20641613006592 seconds\n",
      "Step 4100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 145.8674156665802 seconds\n",
      "Step 4200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 149.34751677513123 seconds\n",
      "Step 4300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 152.8327353000641 seconds\n",
      "Step 4400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 156.54815125465393 seconds\n",
      "Step 4500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 160.03087186813354 seconds\n",
      "Step 4600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 163.50415134429932 seconds\n",
      "Step 4700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 167.16532254219055 seconds\n",
      "Step 4800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 170.64391994476318 seconds\n",
      "Step 4900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 174.13819789886475 seconds\n",
      "Step 5000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 177.85611486434937 seconds\n",
      "Step 5100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 181.35542917251587 seconds\n",
      "Step 5200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 184.83235025405884 seconds\n",
      "Step 5300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 188.51766705513 seconds\n",
      "Step 5400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 192.01822209358215 seconds\n",
      "Step 5500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 195.49570751190186 seconds\n",
      "Step 5600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 199.17640900611877 seconds\n",
      "Step 5700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 202.7092800140381 seconds\n",
      "Step 5800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 206.20538353919983 seconds\n",
      "Step 5900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 209.89053606987 seconds\n",
      "Step 6000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 213.41349697113037 seconds\n",
      "Step 6100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 216.961035490036 seconds\n",
      "Step 6200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 220.65654611587524 seconds\n",
      "Step 6300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 224.13643622398376 seconds\n",
      "Step 6400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 227.6291148662567 seconds\n",
      "Step 6500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 231.0987193584442 seconds\n",
      "Step 6600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 234.76510667800903 seconds\n",
      "Step 6700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 238.248188495636 seconds\n",
      "Step 6800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 241.7164182662964 seconds\n",
      "Step 6900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 245.40504097938538 seconds\n",
      "Step 7000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 248.90748286247253 seconds\n",
      "Step 7100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 252.37894010543823 seconds\n",
      "Step 7200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 256.0722391605377 seconds\n",
      "Step 7300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 259.53809118270874 seconds\n",
      "Step 7400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 263.0753390789032 seconds\n",
      "Step 7500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 266.7724268436432 seconds\n",
      "Step 7600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 270.24550223350525 seconds\n",
      "Step 7700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 273.7232744693756 seconds\n",
      "Step 7800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 277.40252017974854 seconds\n",
      "Step 7900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 280.9028594493866 seconds\n",
      "Step 8000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 284.38020944595337 seconds\n",
      "Step 8100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 288.06337237358093 seconds\n",
      "Step 8200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 291.6187138557434 seconds\n",
      "Step 8300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 295.12346601486206 seconds\n",
      "Step 8400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 298.854558467865 seconds\n",
      "Step 8500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 302.35926699638367 seconds\n",
      "Step 8600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 305.852751493454 seconds\n",
      "Step 8700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 309.5617365837097 seconds\n",
      "Step 8800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 313.077840089798 seconds\n",
      "Step 8900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 316.60533356666565 seconds\n",
      "Step 9000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 320.3114140033722 seconds\n",
      "Step 9100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 323.79191398620605 seconds\n",
      "Step 9200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 327.28159642219543 seconds\n",
      "Step 9300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 330.7678966522217 seconds\n",
      "Step 9400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 334.51737332344055 seconds\n",
      "Step 9500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 338.03827476501465 seconds\n",
      "Step 9600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 341.55886602401733 seconds\n",
      "Step 9700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 345.28090143203735 seconds\n",
      "Step 9800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 348.77295660972595 seconds\n",
      "Step 9900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 352.2863314151764 seconds\n",
      "Step 10000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 356.0142230987549 seconds\n",
      "Step 10100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 359.5756871700287 seconds\n",
      "Step 10200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 363.07304406166077 seconds\n",
      "Step 10300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 366.810742855072 seconds\n",
      "Step 10400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 370.36564564704895 seconds\n",
      "Step 10500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 373.8835310935974 seconds\n",
      "Step 10600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 377.60820388793945 seconds\n",
      "Step 10700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 381.16180658340454 seconds\n",
      "Step 10800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 384.67920994758606 seconds\n",
      "Step 10900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 388.42960596084595 seconds\n",
      "Step 11000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 391.9573516845703 seconds\n",
      "Step 11100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 395.4700562953949 seconds\n",
      "Step 11200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 399.1932055950165 seconds\n",
      "Step 11300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 402.7307450771332 seconds\n",
      "Step 11400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 406.25516629219055 seconds\n",
      "Step 11500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 409.99602150917053 seconds\n",
      "Step 11600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 413.51861476898193 seconds\n",
      "Step 11700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 417.061723947525 seconds\n",
      "Step 11800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 420.6460952758789 seconds\n",
      "Step 11900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 424.41165804862976 seconds\n",
      "Step 12000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 428.00989294052124 seconds\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 0.036876678466796875 seconds\n",
      "Step 100, loss: tensor(0.0146, grad_fn=<SubBackward0>), time elapsed: 3.578139066696167 seconds\n",
      "Step 200, loss: tensor(0.0106, grad_fn=<SubBackward0>), time elapsed: 7.293212413787842 seconds\n",
      "Step 300, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 10.788696050643921 seconds\n",
      "Step 400, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 14.270237922668457 seconds\n",
      "Step 500, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 17.929084539413452 seconds\n",
      "Step 600, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 21.43719458580017 seconds\n",
      "Step 700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 24.964884042739868 seconds\n",
      "Step 800, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 28.666215419769287 seconds\n",
      "Step 900, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 32.14519762992859 seconds\n",
      "Step 1000, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 35.617488622665405 seconds\n",
      "Step 1100, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 39.29738807678223 seconds\n",
      "Step 1200, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 42.76779222488403 seconds\n",
      "Step 1300, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 46.244362354278564 seconds\n",
      "Step 1400, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 49.92976474761963 seconds\n",
      "Step 1500, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 53.409828424453735 seconds\n",
      "Step 1600, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 56.93528079986572 seconds\n",
      "Step 1700, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 60.6352813243866 seconds\n",
      "Step 1800, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 64.1452169418335 seconds\n",
      "Step 1900, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 67.63343214988708 seconds\n",
      "Step 2000, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 71.30151224136353 seconds\n",
      "Step 2100, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 74.78969669342041 seconds\n",
      "Step 2200, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 78.2669563293457 seconds\n",
      "Step 2300, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 81.93727469444275 seconds\n",
      "Step 2400, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 85.46178102493286 seconds\n",
      "Step 2500, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 88.96152806282043 seconds\n",
      "Step 2600, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 92.66434621810913 seconds\n",
      "Step 2700, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 96.15922331809998 seconds\n",
      "Step 2800, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 99.64685773849487 seconds\n",
      "Step 2900, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 103.12386441230774 seconds\n",
      "Step 3000, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 106.82316780090332 seconds\n",
      "Step 3100, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 110.35331702232361 seconds\n",
      "Step 3200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 113.83439254760742 seconds\n",
      "Step 3300, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 117.50583100318909 seconds\n",
      "Step 3400, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 120.99796152114868 seconds\n",
      "Step 3500, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 124.47062253952026 seconds\n",
      "Step 3600, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 128.19066667556763 seconds\n",
      "Step 3700, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 131.69181442260742 seconds\n",
      "Step 3800, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 135.1873815059662 seconds\n",
      "Step 3900, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 138.9289972782135 seconds\n",
      "Step 4000, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 142.4419584274292 seconds\n",
      "Step 4100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 145.95418906211853 seconds\n",
      "Step 4200, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 149.67857670783997 seconds\n",
      "Step 4300, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 153.15377950668335 seconds\n",
      "Step 4400, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 156.6281394958496 seconds\n",
      "Step 4500, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 160.3224720954895 seconds\n",
      "Step 4600, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 163.87061738967896 seconds\n",
      "Step 4700, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 167.35420036315918 seconds\n",
      "Step 4800, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 171.04531288146973 seconds\n",
      "Step 4900, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 174.53654074668884 seconds\n",
      "Step 5000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 178.0226378440857 seconds\n",
      "Step 5100, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 181.71537828445435 seconds\n",
      "Step 5200, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 185.24979877471924 seconds\n",
      "Step 5300, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 188.79158329963684 seconds\n",
      "Step 5400, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 192.51925683021545 seconds\n",
      "Step 5500, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 196.00161600112915 seconds\n",
      "Step 5600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 199.5236427783966 seconds\n",
      "Step 5700, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 203.02057147026062 seconds\n",
      "Step 5800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 206.75327920913696 seconds\n",
      "Step 5900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 210.29786467552185 seconds\n",
      "Step 6000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 213.80951356887817 seconds\n",
      "Step 6100, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 217.50425958633423 seconds\n",
      "Step 6200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 220.98590993881226 seconds\n",
      "Step 6300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 224.46088671684265 seconds\n",
      "Step 6400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 228.13195490837097 seconds\n",
      "Step 6500, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 231.61255645751953 seconds\n",
      "Step 6600, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 235.1098394393921 seconds\n",
      "Step 6700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 238.81602954864502 seconds\n",
      "Step 6800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 242.3309292793274 seconds\n",
      "Step 6900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 245.80485701560974 seconds\n",
      "Step 7000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 249.50083684921265 seconds\n",
      "Step 7100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 253.0283133983612 seconds\n",
      "Step 7200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 256.5319743156433 seconds\n",
      "Step 7300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 260.2168381214142 seconds\n",
      "Step 7400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 263.6988000869751 seconds\n",
      "Step 7500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 267.1793677806854 seconds\n",
      "Step 7600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 270.8858222961426 seconds\n",
      "Step 7700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 274.3678810596466 seconds\n",
      "Step 7800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 277.85642194747925 seconds\n",
      "Step 7900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 281.5379891395569 seconds\n",
      "Step 8000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 284.9989204406738 seconds\n",
      "Step 8100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 288.52697014808655 seconds\n",
      "Step 8200, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 292.29019832611084 seconds\n",
      "Step 8300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 295.7577383518219 seconds\n",
      "Step 8400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 299.27239322662354 seconds\n",
      "Step 8500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 302.7698004245758 seconds\n",
      "Step 8600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 306.5093460083008 seconds\n",
      "Step 8700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 310.00808334350586 seconds\n",
      "Step 8800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 313.49172258377075 seconds\n",
      "Step 8900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 317.1749424934387 seconds\n",
      "Step 9000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 320.70380187034607 seconds\n",
      "Step 9100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 324.2511658668518 seconds\n",
      "Step 9200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 327.9529218673706 seconds\n",
      "Step 9300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 331.4487714767456 seconds\n",
      "Step 9400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 334.9569218158722 seconds\n",
      "Step 9500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 338.70392179489136 seconds\n",
      "Step 9600, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 342.19791746139526 seconds\n",
      "Step 9700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 345.67876648902893 seconds\n",
      "Step 9800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 349.3750789165497 seconds\n",
      "Step 9900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 352.9124686717987 seconds\n",
      "Step 10000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 356.43481969833374 seconds\n",
      "Step 10100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 360.1549451351166 seconds\n",
      "Step 10200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 363.6946294307709 seconds\n",
      "Step 10300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 367.1805832386017 seconds\n",
      "Step 10400, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 370.9260494709015 seconds\n",
      "Step 10500, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 374.42461371421814 seconds\n",
      "Step 10600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 377.92901635169983 seconds\n",
      "Step 10700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 381.4247224330902 seconds\n",
      "Step 10800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 385.12650442123413 seconds\n",
      "Step 10900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 388.66642594337463 seconds\n",
      "Step 11000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 392.1893892288208 seconds\n",
      "Step 11100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 395.9190180301666 seconds\n",
      "Step 11200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 399.4336178302765 seconds\n",
      "Step 11300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 402.9537556171417 seconds\n",
      "Step 11400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 406.7313768863678 seconds\n",
      "Step 11500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 410.25585103034973 seconds\n",
      "Step 11600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 413.7783524990082 seconds\n",
      "Step 11700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 417.5316517353058 seconds\n",
      "Step 11800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 421.0902187824249 seconds\n",
      "Step 11900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 424.6736149787903 seconds\n",
      "Step 12000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 428.475305557251 seconds\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 0.035880088806152344 seconds\n",
      "Step 100, loss: tensor(0.0219, grad_fn=<SubBackward0>), time elapsed: 3.544881820678711 seconds\n",
      "Step 200, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 7.061788558959961 seconds\n",
      "Step 300, loss: tensor(0.0127, grad_fn=<SubBackward0>), time elapsed: 10.75009822845459 seconds\n",
      "Step 400, loss: tensor(0.0100, grad_fn=<SubBackward0>), time elapsed: 14.277324914932251 seconds\n",
      "Step 500, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 17.76131296157837 seconds\n",
      "Step 600, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 21.445863962173462 seconds\n",
      "Step 700, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 24.93892192840576 seconds\n",
      "Step 800, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 28.42077088356018 seconds\n",
      "Step 900, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 32.11305499076843 seconds\n",
      "Step 1000, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 35.59862208366394 seconds\n",
      "Step 1100, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 39.08476495742798 seconds\n",
      "Step 1200, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 42.805760860443115 seconds\n",
      "Step 1300, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 46.279381275177 seconds\n",
      "Step 1400, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 49.76122522354126 seconds\n",
      "Step 1500, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 53.45899224281311 seconds\n",
      "Step 1600, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 56.92413926124573 seconds\n",
      "Step 1700, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 60.42526030540466 seconds\n",
      "Step 1800, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 64.09862303733826 seconds\n",
      "Step 1900, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 67.61400151252747 seconds\n",
      "Step 2000, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 71.10084629058838 seconds\n",
      "Step 2100, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 74.5757417678833 seconds\n",
      "Step 2200, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 78.26342177391052 seconds\n",
      "Step 2300, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 81.7567024230957 seconds\n",
      "Step 2400, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 85.23084354400635 seconds\n",
      "Step 2500, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 88.9207808971405 seconds\n",
      "Step 2600, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 92.4027464389801 seconds\n",
      "Step 2700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 95.91419458389282 seconds\n",
      "Step 2800, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 99.61274886131287 seconds\n",
      "Step 2900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 103.0887393951416 seconds\n",
      "Step 3000, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 106.5663251876831 seconds\n",
      "Step 3100, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 110.23340225219727 seconds\n",
      "Step 3200, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 113.75357794761658 seconds\n",
      "Step 3300, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 117.23327875137329 seconds\n",
      "Step 3400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 120.92390727996826 seconds\n",
      "Step 3500, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 124.4546332359314 seconds\n",
      "Step 3600, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 127.96142029762268 seconds\n",
      "Step 3700, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 131.63555598258972 seconds\n",
      "Step 3800, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 135.1076946258545 seconds\n",
      "Step 3900, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 138.62842774391174 seconds\n",
      "Step 4000, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 142.31243705749512 seconds\n",
      "Step 4100, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 145.8264865875244 seconds\n",
      "Step 4200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 149.33257412910461 seconds\n",
      "Step 4300, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 153.00715899467468 seconds\n",
      "Step 4400, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 156.52899384498596 seconds\n",
      "Step 4500, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 160.0209140777588 seconds\n",
      "Step 4600, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 163.69831252098083 seconds\n",
      "Step 4700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 167.16700172424316 seconds\n",
      "Step 4800, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 170.65276265144348 seconds\n",
      "Step 4900, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 174.135027885437 seconds\n",
      "Step 5000, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 177.85633063316345 seconds\n",
      "Step 5100, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 181.40297722816467 seconds\n",
      "Step 5200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 184.90971493721008 seconds\n",
      "Step 5300, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 188.60620498657227 seconds\n",
      "Step 5400, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 192.141371011734 seconds\n",
      "Step 5500, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 195.641371011734 seconds\n",
      "Step 5600, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 199.32057452201843 seconds\n",
      "Step 5700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 202.81336736679077 seconds\n",
      "Step 5800, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 206.30880284309387 seconds\n",
      "Step 5900, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 210.00969195365906 seconds\n",
      "Step 6000, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 213.4905068874359 seconds\n",
      "Step 6100, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 216.97782111167908 seconds\n",
      "Step 6200, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 220.68634390830994 seconds\n",
      "Step 6300, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 224.22298169136047 seconds\n",
      "Step 6400, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 227.7078766822815 seconds\n",
      "Step 6500, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 231.38998007774353 seconds\n",
      "Step 6600, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 234.88321781158447 seconds\n",
      "Step 6700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 238.4005048274994 seconds\n",
      "Step 6800, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 242.09485268592834 seconds\n",
      "Step 6900, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 245.57412576675415 seconds\n",
      "Step 7000, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 249.0622160434723 seconds\n",
      "Step 7100, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 252.75325918197632 seconds\n",
      "Step 7200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 256.2719774246216 seconds\n",
      "Step 7300, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 259.7578561306 seconds\n",
      "Step 7400, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 263.45013761520386 seconds\n",
      "Step 7500, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 266.97536396980286 seconds\n",
      "Step 7600, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 270.538024187088 seconds\n",
      "Step 7700, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 274.0121908187866 seconds\n",
      "Step 7800, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 277.76273703575134 seconds\n",
      "Step 7900, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 281.2411410808563 seconds\n",
      "Step 8000, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 284.7599811553955 seconds\n",
      "Step 8100, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 288.48124074935913 seconds\n",
      "Step 8200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 291.99667620658875 seconds\n",
      "Step 8300, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 295.51038670539856 seconds\n",
      "Step 8400, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 299.21618843078613 seconds\n",
      "Step 8500, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 302.72650599479675 seconds\n",
      "Step 8600, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 306.2608788013458 seconds\n",
      "Step 8700, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 310.0105049610138 seconds\n",
      "Step 8800, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 313.52177691459656 seconds\n",
      "Step 8900, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 317.0073959827423 seconds\n",
      "Step 9000, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 320.70951104164124 seconds\n",
      "Step 9100, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 324.23851323127747 seconds\n",
      "Step 9200, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 327.7843277454376 seconds\n",
      "Step 9300, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 331.477276802063 seconds\n",
      "Step 9400, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 334.9679284095764 seconds\n",
      "Step 9500, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 338.5118598937988 seconds\n",
      "Step 9600, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 342.23075246810913 seconds\n",
      "Step 9700, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 345.71827721595764 seconds\n",
      "Step 9800, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 349.23117685317993 seconds\n",
      "Step 9900, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 352.72050857543945 seconds\n",
      "Step 10000, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 356.4438650608063 seconds\n",
      "Step 10100, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 359.94087958335876 seconds\n",
      "Step 10200, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 363.43914794921875 seconds\n",
      "Step 10300, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 367.1791880130768 seconds\n",
      "Step 10400, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 370.7300386428833 seconds\n",
      "Step 10500, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 374.22950887680054 seconds\n",
      "Step 10600, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 377.9291205406189 seconds\n",
      "Step 10700, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 381.4470522403717 seconds\n",
      "Step 10800, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 384.9894697666168 seconds\n",
      "Step 10900, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 388.7405743598938 seconds\n",
      "Step 11000, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 392.25657773017883 seconds\n",
      "Step 11100, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 395.7651903629303 seconds\n",
      "Step 11200, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 399.50942397117615 seconds\n",
      "Step 11300, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 403.09749126434326 seconds\n",
      "Step 11400, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 406.6389241218567 seconds\n",
      "Step 11500, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 410.379034280777 seconds\n",
      "Step 11600, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 413.94560980796814 seconds\n",
      "Step 11700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 417.48716735839844 seconds\n",
      "Step 11800, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 421.2579917907715 seconds\n",
      "Step 11900, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 424.810364484787 seconds\n",
      "Step 12000, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 428.39433789253235 seconds\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 0.036876678466796875 seconds\n",
      "Step 100, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 3.7562942504882812 seconds\n",
      "Step 200, loss: tensor(0.0250, grad_fn=<SubBackward0>), time elapsed: 7.234121322631836 seconds\n",
      "Step 300, loss: tensor(0.0215, grad_fn=<SubBackward0>), time elapsed: 10.712956190109253 seconds\n",
      "Step 400, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 14.410688161849976 seconds\n",
      "Step 500, loss: tensor(0.0179, grad_fn=<SubBackward0>), time elapsed: 17.879313230514526 seconds\n",
      "Step 600, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 21.362444400787354 seconds\n",
      "Step 700, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 24.84304141998291 seconds\n",
      "Step 800, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 28.56386399269104 seconds\n",
      "Step 900, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 32.10641169548035 seconds\n",
      "Step 1000, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 35.58451843261719 seconds\n",
      "Step 1100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 39.27843880653381 seconds\n",
      "Step 1200, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 42.76937913894653 seconds\n",
      "Step 1300, loss: tensor(0.0128, grad_fn=<SubBackward0>), time elapsed: 46.26774072647095 seconds\n",
      "Step 1400, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 49.98433232307434 seconds\n",
      "Step 1500, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 53.4708411693573 seconds\n",
      "Step 1600, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 56.99096703529358 seconds\n",
      "Step 1700, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 60.66257834434509 seconds\n",
      "Step 1800, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 64.18679404258728 seconds\n",
      "Step 1900, loss: tensor(0.0110, grad_fn=<SubBackward0>), time elapsed: 67.66914057731628 seconds\n",
      "Step 2000, loss: tensor(0.0099, grad_fn=<SubBackward0>), time elapsed: 71.37172603607178 seconds\n",
      "Step 2100, loss: tensor(0.0105, grad_fn=<SubBackward0>), time elapsed: 74.91281771659851 seconds\n",
      "Step 2200, loss: tensor(0.0101, grad_fn=<SubBackward0>), time elapsed: 78.40727543830872 seconds\n",
      "Step 2300, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 82.09154105186462 seconds\n",
      "Step 2400, loss: tensor(0.0102, grad_fn=<SubBackward0>), time elapsed: 85.56916308403015 seconds\n",
      "Step 2500, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 89.0871171951294 seconds\n",
      "Step 2600, loss: tensor(0.0098, grad_fn=<SubBackward0>), time elapsed: 92.78710055351257 seconds\n",
      "Step 2700, loss: tensor(0.0099, grad_fn=<SubBackward0>), time elapsed: 96.2677686214447 seconds\n",
      "Step 2800, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 99.74602794647217 seconds\n",
      "Step 2900, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 103.45114827156067 seconds\n",
      "Step 3000, loss: tensor(0.0098, grad_fn=<SubBackward0>), time elapsed: 106.98959493637085 seconds\n",
      "Step 3100, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 110.47324347496033 seconds\n",
      "Step 3200, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 114.14586329460144 seconds\n",
      "Step 3300, loss: tensor(0.0090, grad_fn=<SubBackward0>), time elapsed: 117.65300798416138 seconds\n",
      "Step 3400, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 121.19792771339417 seconds\n",
      "Step 3500, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 124.67976140975952 seconds\n",
      "Step 3600, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 128.4036226272583 seconds\n",
      "Step 3700, loss: tensor(0.0093, grad_fn=<SubBackward0>), time elapsed: 131.9153790473938 seconds\n",
      "Step 3800, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 135.39797973632812 seconds\n",
      "Step 3900, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 139.10719656944275 seconds\n",
      "Step 4000, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 142.58580541610718 seconds\n",
      "Step 4100, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 146.06317329406738 seconds\n",
      "Step 4200, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 149.76493310928345 seconds\n",
      "Step 4300, loss: tensor(0.0090, grad_fn=<SubBackward0>), time elapsed: 153.27955746650696 seconds\n",
      "Step 4400, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 156.75462293624878 seconds\n",
      "Step 4500, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 160.43878173828125 seconds\n",
      "Step 4600, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 163.92898750305176 seconds\n",
      "Step 4700, loss: tensor(0.0100, grad_fn=<SubBackward0>), time elapsed: 167.42037558555603 seconds\n",
      "Step 4800, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 171.10749697685242 seconds\n",
      "Step 4900, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 174.61925053596497 seconds\n",
      "Step 5000, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 178.1019995212555 seconds\n",
      "Step 5100, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 181.8131091594696 seconds\n",
      "Step 5200, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 185.34053945541382 seconds\n",
      "Step 5300, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 188.8424825668335 seconds\n",
      "Step 5400, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 192.52649211883545 seconds\n",
      "Step 5500, loss: tensor(0.0093, grad_fn=<SubBackward0>), time elapsed: 196.03286361694336 seconds\n",
      "Step 5600, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 199.57060313224792 seconds\n",
      "Step 5700, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 203.30846619606018 seconds\n",
      "Step 5800, loss: tensor(0.0093, grad_fn=<SubBackward0>), time elapsed: 206.81026220321655 seconds\n",
      "Step 5900, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 210.30780625343323 seconds\n",
      "Step 6000, loss: tensor(0.0089, grad_fn=<SubBackward0>), time elapsed: 213.8139729499817 seconds\n",
      "Step 6100, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 217.48631286621094 seconds\n",
      "Step 6200, loss: tensor(0.0089, grad_fn=<SubBackward0>), time elapsed: 220.9695281982422 seconds\n",
      "Step 6300, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 224.43848085403442 seconds\n",
      "Step 6400, loss: tensor(0.0090, grad_fn=<SubBackward0>), time elapsed: 228.1215944290161 seconds\n",
      "Step 6500, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 231.6161985397339 seconds\n",
      "Step 6600, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 235.083087682724 seconds\n",
      "Step 6700, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 238.75120902061462 seconds\n",
      "Step 6800, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 242.25359630584717 seconds\n",
      "Step 6900, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 245.74258303642273 seconds\n",
      "Step 7000, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 249.4300720691681 seconds\n",
      "Step 7100, loss: tensor(0.0090, grad_fn=<SubBackward0>), time elapsed: 252.9420144557953 seconds\n",
      "Step 7200, loss: tensor(0.0086, grad_fn=<SubBackward0>), time elapsed: 256.43004989624023 seconds\n",
      "Step 7300, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 260.14992809295654 seconds\n",
      "Step 7400, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 263.63397574424744 seconds\n",
      "Step 7500, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 267.157265663147 seconds\n",
      "Step 7600, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 270.85480856895447 seconds\n",
      "Step 7700, loss: tensor(0.0080, grad_fn=<SubBackward0>), time elapsed: 274.3366093635559 seconds\n",
      "Step 7800, loss: tensor(0.0083, grad_fn=<SubBackward0>), time elapsed: 277.8149735927582 seconds\n",
      "Step 7900, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 281.49912095069885 seconds\n",
      "Step 8000, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 285.01142024993896 seconds\n",
      "Step 8100, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 288.5292410850525 seconds\n",
      "Step 8200, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 292.270222902298 seconds\n",
      "Step 8300, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 295.743417263031 seconds\n",
      "Step 8400, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 299.24788427352905 seconds\n",
      "Step 8500, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 302.7657778263092 seconds\n",
      "Step 8600, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 306.4524738788605 seconds\n",
      "Step 8700, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 309.940958738327 seconds\n",
      "Step 8800, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 313.41868925094604 seconds\n",
      "Step 8900, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 317.14944219589233 seconds\n",
      "Step 9000, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 320.6353406906128 seconds\n",
      "Step 9100, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 324.12022709846497 seconds\n",
      "Step 9200, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 327.80299735069275 seconds\n",
      "Step 9300, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 331.342955827713 seconds\n",
      "Step 9400, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 334.8331618309021 seconds\n",
      "Step 9500, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 338.5621237754822 seconds\n",
      "Step 9600, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 342.0675950050354 seconds\n",
      "Step 9700, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 345.59729743003845 seconds\n",
      "Step 9800, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 349.3553764820099 seconds\n",
      "Step 9900, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 352.8498604297638 seconds\n",
      "Step 10000, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 356.376473903656 seconds\n",
      "Step 10100, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 360.0841248035431 seconds\n",
      "Step 10200, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 363.6264064311981 seconds\n",
      "Step 10300, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 367.1418390274048 seconds\n",
      "Step 10400, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 370.8674566745758 seconds\n",
      "Step 10500, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 374.4147684574127 seconds\n",
      "Step 10600, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 377.9286127090454 seconds\n",
      "Step 10700, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 381.6529076099396 seconds\n",
      "Step 10800, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 385.1508433818817 seconds\n",
      "Step 10900, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 388.6590187549591 seconds\n",
      "Step 11000, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 392.15799045562744 seconds\n",
      "Step 11100, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 395.9163682460785 seconds\n",
      "Step 11200, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 399.46640181541443 seconds\n",
      "Step 11300, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 403.0253505706787 seconds\n",
      "Step 11400, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 406.7714755535126 seconds\n",
      "Step 11500, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 410.3345685005188 seconds\n",
      "Step 11600, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 413.8530650138855 seconds\n",
      "Step 11700, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 417.57959723472595 seconds\n",
      "Step 11800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 421.13101172447205 seconds\n",
      "Step 11900, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 424.6786332130432 seconds\n",
      "Step 12000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 428.49564838409424 seconds\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0869, grad_fn=<SubBackward0>), time elapsed: 0.03587961196899414 seconds\n",
      "Step 100, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 3.5619943141937256 seconds\n",
      "Step 200, loss: tensor(0.0653, grad_fn=<SubBackward0>), time elapsed: 7.047213792800903 seconds\n",
      "Step 300, loss: tensor(0.0560, grad_fn=<SubBackward0>), time elapsed: 10.756914377212524 seconds\n",
      "Step 400, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 14.281321048736572 seconds\n",
      "Step 500, loss: tensor(0.0454, grad_fn=<SubBackward0>), time elapsed: 17.801019191741943 seconds\n",
      "Step 600, loss: tensor(0.0421, grad_fn=<SubBackward0>), time elapsed: 21.472999334335327 seconds\n",
      "Step 700, loss: tensor(0.0400, grad_fn=<SubBackward0>), time elapsed: 24.943591356277466 seconds\n",
      "Step 800, loss: tensor(0.0393, grad_fn=<SubBackward0>), time elapsed: 28.416939735412598 seconds\n",
      "Step 900, loss: tensor(0.0383, grad_fn=<SubBackward0>), time elapsed: 32.1218159198761 seconds\n",
      "Step 1000, loss: tensor(0.0368, grad_fn=<SubBackward0>), time elapsed: 35.610368967056274 seconds\n",
      "Step 1100, loss: tensor(0.0349, grad_fn=<SubBackward0>), time elapsed: 39.0992476940155 seconds\n",
      "Step 1200, loss: tensor(0.0345, grad_fn=<SubBackward0>), time elapsed: 42.78773903846741 seconds\n",
      "Step 1300, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 46.28389286994934 seconds\n",
      "Step 1400, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 49.76965641975403 seconds\n",
      "Step 1500, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 53.48999905586243 seconds\n",
      "Step 1600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 56.96112060546875 seconds\n",
      "Step 1700, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 60.4416925907135 seconds\n",
      "Step 1800, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 64.1320629119873 seconds\n",
      "Step 1900, loss: tensor(0.0292, grad_fn=<SubBackward0>), time elapsed: 67.63599348068237 seconds\n",
      "Step 2000, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 71.11680459976196 seconds\n",
      "Step 2100, loss: tensor(0.0279, grad_fn=<SubBackward0>), time elapsed: 74.78723526000977 seconds\n",
      "Step 2200, loss: tensor(0.0273, grad_fn=<SubBackward0>), time elapsed: 78.24840807914734 seconds\n",
      "Step 2300, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 81.75924301147461 seconds\n",
      "Step 2400, loss: tensor(0.0277, grad_fn=<SubBackward0>), time elapsed: 85.28568744659424 seconds\n",
      "Step 2500, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 88.94399404525757 seconds\n",
      "Step 2600, loss: tensor(0.0270, grad_fn=<SubBackward0>), time elapsed: 92.41198635101318 seconds\n",
      "Step 2700, loss: tensor(0.0264, grad_fn=<SubBackward0>), time elapsed: 95.93747663497925 seconds\n",
      "Step 2800, loss: tensor(0.0262, grad_fn=<SubBackward0>), time elapsed: 99.65372657775879 seconds\n",
      "Step 2900, loss: tensor(0.0260, grad_fn=<SubBackward0>), time elapsed: 103.14068031311035 seconds\n",
      "Step 3000, loss: tensor(0.0257, grad_fn=<SubBackward0>), time elapsed: 106.60276222229004 seconds\n",
      "Step 3100, loss: tensor(0.0252, grad_fn=<SubBackward0>), time elapsed: 110.2868983745575 seconds\n",
      "Step 3200, loss: tensor(0.0240, grad_fn=<SubBackward0>), time elapsed: 113.76836562156677 seconds\n",
      "Step 3300, loss: tensor(0.0247, grad_fn=<SubBackward0>), time elapsed: 117.23619794845581 seconds\n",
      "Step 3400, loss: tensor(0.0268, grad_fn=<SubBackward0>), time elapsed: 120.93909311294556 seconds\n",
      "Step 3500, loss: tensor(0.0257, grad_fn=<SubBackward0>), time elapsed: 124.46607685089111 seconds\n",
      "Step 3600, loss: tensor(0.0255, grad_fn=<SubBackward0>), time elapsed: 127.99234509468079 seconds\n",
      "Step 3700, loss: tensor(0.0250, grad_fn=<SubBackward0>), time elapsed: 131.6951515674591 seconds\n",
      "Step 3800, loss: tensor(0.0239, grad_fn=<SubBackward0>), time elapsed: 135.17788577079773 seconds\n",
      "Step 3900, loss: tensor(0.0258, grad_fn=<SubBackward0>), time elapsed: 138.67256546020508 seconds\n",
      "Step 4000, loss: tensor(0.0246, grad_fn=<SubBackward0>), time elapsed: 142.40166306495667 seconds\n",
      "Step 4100, loss: tensor(0.0236, grad_fn=<SubBackward0>), time elapsed: 145.93763065338135 seconds\n",
      "Step 4200, loss: tensor(0.0244, grad_fn=<SubBackward0>), time elapsed: 149.4249062538147 seconds\n",
      "Step 4300, loss: tensor(0.0237, grad_fn=<SubBackward0>), time elapsed: 153.14404678344727 seconds\n",
      "Step 4400, loss: tensor(0.0236, grad_fn=<SubBackward0>), time elapsed: 156.62157702445984 seconds\n",
      "Step 4500, loss: tensor(0.0231, grad_fn=<SubBackward0>), time elapsed: 160.13539290428162 seconds\n",
      "Step 4600, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 163.8155643939972 seconds\n",
      "Step 4700, loss: tensor(0.0221, grad_fn=<SubBackward0>), time elapsed: 167.29721879959106 seconds\n",
      "Step 4800, loss: tensor(0.0221, grad_fn=<SubBackward0>), time elapsed: 170.78158330917358 seconds\n",
      "Step 4900, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 174.48805952072144 seconds\n",
      "Step 5000, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 177.96966433525085 seconds\n",
      "Step 5100, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 181.44794869422913 seconds\n",
      "Step 5200, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 185.12463855743408 seconds\n",
      "Step 5300, loss: tensor(0.0195, grad_fn=<SubBackward0>), time elapsed: 188.61186909675598 seconds\n",
      "Step 5400, loss: tensor(0.0201, grad_fn=<SubBackward0>), time elapsed: 192.13136625289917 seconds\n",
      "Step 5500, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 195.6023736000061 seconds\n",
      "Step 5600, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 199.3238639831543 seconds\n",
      "Step 5700, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 202.81017351150513 seconds\n",
      "Step 5800, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 206.2893087863922 seconds\n",
      "Step 5900, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 209.98669242858887 seconds\n",
      "Step 6000, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 213.46344304084778 seconds\n",
      "Step 6100, loss: tensor(0.0175, grad_fn=<SubBackward0>), time elapsed: 216.9360475540161 seconds\n",
      "Step 6200, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 220.62975239753723 seconds\n",
      "Step 6300, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 224.1304247379303 seconds\n",
      "Step 6400, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 227.5947506427765 seconds\n",
      "Step 6500, loss: tensor(0.0176, grad_fn=<SubBackward0>), time elapsed: 231.26902079582214 seconds\n",
      "Step 6600, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 234.74860429763794 seconds\n",
      "Step 6700, loss: tensor(0.0167, grad_fn=<SubBackward0>), time elapsed: 238.2573812007904 seconds\n",
      "Step 6800, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 241.9798285961151 seconds\n",
      "Step 6900, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 245.49833059310913 seconds\n",
      "Step 7000, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 248.9836871623993 seconds\n",
      "Step 7100, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 252.6989302635193 seconds\n",
      "Step 7200, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 256.1828989982605 seconds\n",
      "Step 7300, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 259.66963481903076 seconds\n",
      "Step 7400, loss: tensor(0.0165, grad_fn=<SubBackward0>), time elapsed: 263.3664677143097 seconds\n",
      "Step 7500, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 266.8865044116974 seconds\n",
      "Step 7600, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 270.3719003200531 seconds\n",
      "Step 7700, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 274.05845069885254 seconds\n",
      "Step 7800, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 277.5301423072815 seconds\n",
      "Step 7900, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 281.01897954940796 seconds\n",
      "Step 8000, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 284.69284200668335 seconds\n",
      "Step 8100, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 288.1599373817444 seconds\n",
      "Step 8200, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 291.6561212539673 seconds\n",
      "Step 8300, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 295.12809562683105 seconds\n",
      "Step 8400, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 298.83751225471497 seconds\n",
      "Step 8500, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 302.33132338523865 seconds\n",
      "Step 8600, loss: tensor(0.0168, grad_fn=<SubBackward0>), time elapsed: 305.8208293914795 seconds\n",
      "Step 8700, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 309.55286049842834 seconds\n",
      "Step 8800, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 313.08905506134033 seconds\n",
      "Step 8900, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 316.56944727897644 seconds\n",
      "Step 9000, loss: tensor(0.0159, grad_fn=<SubBackward0>), time elapsed: 320.28811955451965 seconds\n",
      "Step 9100, loss: tensor(0.0167, grad_fn=<SubBackward0>), time elapsed: 323.81486558914185 seconds\n",
      "Step 9200, loss: tensor(0.0165, grad_fn=<SubBackward0>), time elapsed: 327.2962279319763 seconds\n",
      "Step 9300, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 330.98135566711426 seconds\n",
      "Step 9400, loss: tensor(0.0171, grad_fn=<SubBackward0>), time elapsed: 334.4798047542572 seconds\n",
      "Step 9500, loss: tensor(0.0165, grad_fn=<SubBackward0>), time elapsed: 337.95165157318115 seconds\n",
      "Step 9600, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 341.63402247428894 seconds\n",
      "Step 9700, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 345.1311173439026 seconds\n",
      "Step 9800, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 348.636572599411 seconds\n",
      "Step 9900, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 352.3437166213989 seconds\n",
      "Step 10000, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 355.8548672199249 seconds\n",
      "Step 10100, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 359.3752827644348 seconds\n",
      "Step 10200, loss: tensor(0.0165, grad_fn=<SubBackward0>), time elapsed: 363.1113073825836 seconds\n",
      "Step 10300, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 366.60025000572205 seconds\n",
      "Step 10400, loss: tensor(0.0159, grad_fn=<SubBackward0>), time elapsed: 370.1017451286316 seconds\n",
      "Step 10500, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 373.6633071899414 seconds\n",
      "Step 10600, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 377.36343717575073 seconds\n",
      "Step 10700, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 380.92270851135254 seconds\n",
      "Step 10800, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 384.48552942276 seconds\n",
      "Step 10900, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 388.2102997303009 seconds\n",
      "Step 11000, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 391.7348008155823 seconds\n",
      "Step 11100, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 395.23378443717957 seconds\n",
      "Step 11200, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 398.9414253234863 seconds\n",
      "Step 11300, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 402.472252368927 seconds\n",
      "Step 11400, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 406.02549052238464 seconds\n",
      "Step 11500, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 409.7915287017822 seconds\n",
      "Step 11600, loss: tensor(0.0165, grad_fn=<SubBackward0>), time elapsed: 413.3260715007782 seconds\n",
      "Step 11700, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 416.8548274040222 seconds\n",
      "Step 11800, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 420.6242253780365 seconds\n",
      "Step 11900, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 424.1651291847229 seconds\n",
      "Step 12000, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 427.7440309524536 seconds\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1088, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(0.0993, grad_fn=<SubBackward0>), time elapsed: 3.773041248321533 seconds\n",
      "Step 200, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 7.272053480148315 seconds\n",
      "Step 300, loss: tensor(0.0824, grad_fn=<SubBackward0>), time elapsed: 10.760518550872803 seconds\n",
      "Step 400, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 14.493759155273438 seconds\n",
      "Step 500, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 18.014156818389893 seconds\n",
      "Step 600, loss: tensor(0.0681, grad_fn=<SubBackward0>), time elapsed: 21.512830018997192 seconds\n",
      "Step 700, loss: tensor(0.0652, grad_fn=<SubBackward0>), time elapsed: 25.19141411781311 seconds\n",
      "Step 800, loss: tensor(0.0611, grad_fn=<SubBackward0>), time elapsed: 28.66476559638977 seconds\n",
      "Step 900, loss: tensor(0.0581, grad_fn=<SubBackward0>), time elapsed: 32.158485889434814 seconds\n",
      "Step 1000, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 35.861074686050415 seconds\n",
      "Step 1100, loss: tensor(0.0551, grad_fn=<SubBackward0>), time elapsed: 39.322752475738525 seconds\n",
      "Step 1200, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 42.8102445602417 seconds\n",
      "Step 1300, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 46.48662972450256 seconds\n",
      "Step 1400, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 50.00839686393738 seconds\n",
      "Step 1500, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 53.50218224525452 seconds\n",
      "Step 1600, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 57.21256375312805 seconds\n",
      "Step 1700, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 60.69242858886719 seconds\n",
      "Step 1800, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 64.19478464126587 seconds\n",
      "Step 1900, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 67.69536924362183 seconds\n",
      "Step 2000, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 71.3549747467041 seconds\n",
      "Step 2100, loss: tensor(0.0475, grad_fn=<SubBackward0>), time elapsed: 74.83860778808594 seconds\n",
      "Step 2200, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 78.30432224273682 seconds\n",
      "Step 2300, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 82.01018381118774 seconds\n",
      "Step 2400, loss: tensor(0.0454, grad_fn=<SubBackward0>), time elapsed: 85.51931715011597 seconds\n",
      "Step 2500, loss: tensor(0.0437, grad_fn=<SubBackward0>), time elapsed: 88.99313068389893 seconds\n",
      "Step 2600, loss: tensor(0.0439, grad_fn=<SubBackward0>), time elapsed: 92.66580700874329 seconds\n",
      "Step 2700, loss: tensor(0.0424, grad_fn=<SubBackward0>), time elapsed: 96.14313197135925 seconds\n",
      "Step 2800, loss: tensor(0.0404, grad_fn=<SubBackward0>), time elapsed: 99.61622095108032 seconds\n",
      "Step 2900, loss: tensor(0.0398, grad_fn=<SubBackward0>), time elapsed: 103.31478071212769 seconds\n",
      "Step 3000, loss: tensor(0.0394, grad_fn=<SubBackward0>), time elapsed: 106.83007335662842 seconds\n",
      "Step 3100, loss: tensor(0.0368, grad_fn=<SubBackward0>), time elapsed: 110.35733485221863 seconds\n",
      "Step 3200, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 114.07304835319519 seconds\n",
      "Step 3300, loss: tensor(0.0352, grad_fn=<SubBackward0>), time elapsed: 117.55591201782227 seconds\n",
      "Step 3400, loss: tensor(0.0347, grad_fn=<SubBackward0>), time elapsed: 121.09296345710754 seconds\n",
      "Step 3500, loss: tensor(0.0339, grad_fn=<SubBackward0>), time elapsed: 124.82125902175903 seconds\n",
      "Step 3600, loss: tensor(0.0332, grad_fn=<SubBackward0>), time elapsed: 128.32372879981995 seconds\n",
      "Step 3700, loss: tensor(0.0333, grad_fn=<SubBackward0>), time elapsed: 131.8020534515381 seconds\n",
      "Step 3800, loss: tensor(0.0337, grad_fn=<SubBackward0>), time elapsed: 135.50144219398499 seconds\n",
      "Step 3900, loss: tensor(0.0329, grad_fn=<SubBackward0>), time elapsed: 139.0081377029419 seconds\n",
      "Step 4000, loss: tensor(0.0345, grad_fn=<SubBackward0>), time elapsed: 142.54437971115112 seconds\n",
      "Step 4100, loss: tensor(0.0339, grad_fn=<SubBackward0>), time elapsed: 146.22708249092102 seconds\n",
      "Step 4200, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 149.72786021232605 seconds\n",
      "Step 4300, loss: tensor(0.0329, grad_fn=<SubBackward0>), time elapsed: 153.220370054245 seconds\n",
      "Step 4400, loss: tensor(0.0327, grad_fn=<SubBackward0>), time elapsed: 156.89669609069824 seconds\n",
      "Step 4500, loss: tensor(0.0320, grad_fn=<SubBackward0>), time elapsed: 160.372549533844 seconds\n",
      "Step 4600, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 163.8719823360443 seconds\n",
      "Step 4700, loss: tensor(0.0322, grad_fn=<SubBackward0>), time elapsed: 167.54474234580994 seconds\n",
      "Step 4800, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 171.01617622375488 seconds\n",
      "Step 4900, loss: tensor(0.0328, grad_fn=<SubBackward0>), time elapsed: 174.50050616264343 seconds\n",
      "Step 5000, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 178.01382851600647 seconds\n",
      "Step 5100, loss: tensor(0.0325, grad_fn=<SubBackward0>), time elapsed: 181.7206254005432 seconds\n",
      "Step 5200, loss: tensor(0.0327, grad_fn=<SubBackward0>), time elapsed: 185.2118034362793 seconds\n",
      "Step 5300, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 188.68750858306885 seconds\n",
      "Step 5400, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 192.3830578327179 seconds\n",
      "Step 5500, loss: tensor(0.0318, grad_fn=<SubBackward0>), time elapsed: 195.89139080047607 seconds\n",
      "Step 5600, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 199.35596323013306 seconds\n",
      "Step 5700, loss: tensor(0.0323, grad_fn=<SubBackward0>), time elapsed: 203.028639793396 seconds\n",
      "Step 5800, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 206.51033234596252 seconds\n",
      "Step 5900, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 209.9809365272522 seconds\n",
      "Step 6000, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 213.70024752616882 seconds\n",
      "Step 6100, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 217.2056028842926 seconds\n",
      "Step 6200, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 220.68537950515747 seconds\n",
      "Step 6300, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 224.38117957115173 seconds\n",
      "Step 6400, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 227.85906171798706 seconds\n",
      "Step 6500, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 231.38165521621704 seconds\n",
      "Step 6600, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 235.10309767723083 seconds\n",
      "Step 6700, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 238.5872356891632 seconds\n",
      "Step 6800, loss: tensor(0.0325, grad_fn=<SubBackward0>), time elapsed: 242.0793161392212 seconds\n",
      "Step 6900, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 245.77963376045227 seconds\n",
      "Step 7000, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 249.2880094051361 seconds\n",
      "Step 7100, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 252.7620346546173 seconds\n",
      "Step 7200, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 256.4642930030823 seconds\n",
      "Step 7300, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 260.00336599349976 seconds\n",
      "Step 7400, loss: tensor(0.0320, grad_fn=<SubBackward0>), time elapsed: 263.54652857780457 seconds\n",
      "Step 7500, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 267.0195951461792 seconds\n",
      "Step 7600, loss: tensor(0.0321, grad_fn=<SubBackward0>), time elapsed: 270.7460799217224 seconds\n",
      "Step 7700, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 274.2777271270752 seconds\n",
      "Step 7800, loss: tensor(0.0321, grad_fn=<SubBackward0>), time elapsed: 277.8040306568146 seconds\n",
      "Step 7900, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 281.4881374835968 seconds\n",
      "Step 8000, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 284.9659993648529 seconds\n",
      "Step 8100, loss: tensor(0.0320, grad_fn=<SubBackward0>), time elapsed: 288.43185687065125 seconds\n",
      "Step 8200, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 292.15809202194214 seconds\n",
      "Step 8300, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 295.64917850494385 seconds\n",
      "Step 8400, loss: tensor(0.0328, grad_fn=<SubBackward0>), time elapsed: 299.15302085876465 seconds\n",
      "Step 8500, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 302.88206362724304 seconds\n",
      "Step 8600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 306.41516041755676 seconds\n",
      "Step 8700, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 309.9308431148529 seconds\n",
      "Step 8800, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 313.627032995224 seconds\n",
      "Step 8900, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 317.1392357349396 seconds\n",
      "Step 9000, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 320.665150642395 seconds\n",
      "Step 9100, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 324.3741464614868 seconds\n",
      "Step 9200, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 327.86284613609314 seconds\n",
      "Step 9300, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 331.353458404541 seconds\n",
      "Step 9400, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 335.0823817253113 seconds\n",
      "Step 9500, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 338.5740201473236 seconds\n",
      "Step 9600, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 342.0701620578766 seconds\n",
      "Step 9700, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 345.7544732093811 seconds\n",
      "Step 9800, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 349.2625288963318 seconds\n",
      "Step 9900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 352.7343809604645 seconds\n",
      "Step 10000, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 356.28045749664307 seconds\n",
      "Step 10100, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 360.0290904045105 seconds\n",
      "Step 10200, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 363.60711765289307 seconds\n",
      "Step 10300, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 367.12613558769226 seconds\n",
      "Step 10400, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 370.8789219856262 seconds\n",
      "Step 10500, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 374.4204988479614 seconds\n",
      "Step 10600, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 377.93706369400024 seconds\n",
      "Step 10700, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 381.6679012775421 seconds\n",
      "Step 10800, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 385.1734848022461 seconds\n",
      "Step 10900, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 388.68385791778564 seconds\n",
      "Step 11000, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 392.41220116615295 seconds\n",
      "Step 11100, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 395.9383544921875 seconds\n",
      "Step 11200, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 399.4465811252594 seconds\n",
      "Step 11300, loss: tensor(0.0324, grad_fn=<SubBackward0>), time elapsed: 403.20428013801575 seconds\n",
      "Step 11400, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 406.72747111320496 seconds\n",
      "Step 11500, loss: tensor(0.0287, grad_fn=<SubBackward0>), time elapsed: 410.24432015419006 seconds\n",
      "Step 11600, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 413.99574518203735 seconds\n",
      "Step 11700, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 417.57763600349426 seconds\n",
      "Step 11800, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 421.17374777793884 seconds\n",
      "Step 11900, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 424.94711780548096 seconds\n",
      "Step 12000, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 428.52447056770325 seconds\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.2015, grad_fn=<SubBackward0>), time elapsed: 0.03687691688537598 seconds\n",
      "Step 100, loss: tensor(0.1894, grad_fn=<SubBackward0>), time elapsed: 3.6003315448760986 seconds\n",
      "Step 200, loss: tensor(0.1803, grad_fn=<SubBackward0>), time elapsed: 7.292380332946777 seconds\n",
      "Step 300, loss: tensor(0.1633, grad_fn=<SubBackward0>), time elapsed: 10.794636487960815 seconds\n",
      "Step 400, loss: tensor(0.1461, grad_fn=<SubBackward0>), time elapsed: 14.320456981658936 seconds\n",
      "Step 500, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 17.99337077140808 seconds\n",
      "Step 600, loss: tensor(0.1134, grad_fn=<SubBackward0>), time elapsed: 21.47077751159668 seconds\n",
      "Step 700, loss: tensor(0.1089, grad_fn=<SubBackward0>), time elapsed: 24.951905965805054 seconds\n",
      "Step 800, loss: tensor(0.1048, grad_fn=<SubBackward0>), time elapsed: 28.435869693756104 seconds\n",
      "Step 900, loss: tensor(0.0957, grad_fn=<SubBackward0>), time elapsed: 32.15460467338562 seconds\n",
      "Step 1000, loss: tensor(0.0917, grad_fn=<SubBackward0>), time elapsed: 35.634787797927856 seconds\n",
      "Step 1100, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 39.125730752944946 seconds\n",
      "Step 1200, loss: tensor(0.0848, grad_fn=<SubBackward0>), time elapsed: 42.818875789642334 seconds\n",
      "Step 1300, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 46.307124853134155 seconds\n",
      "Step 1400, loss: tensor(0.0790, grad_fn=<SubBackward0>), time elapsed: 49.79448914527893 seconds\n",
      "Step 1500, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 53.47507381439209 seconds\n",
      "Step 1600, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 56.99120330810547 seconds\n",
      "Step 1700, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 60.468921422958374 seconds\n",
      "Step 1800, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 64.17106986045837 seconds\n",
      "Step 1900, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 67.66582179069519 seconds\n",
      "Step 2000, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 71.18515849113464 seconds\n",
      "Step 2100, loss: tensor(0.0721, grad_fn=<SubBackward0>), time elapsed: 74.88327097892761 seconds\n",
      "Step 2200, loss: tensor(0.0711, grad_fn=<SubBackward0>), time elapsed: 78.35632967948914 seconds\n",
      "Step 2300, loss: tensor(0.0715, grad_fn=<SubBackward0>), time elapsed: 81.82822513580322 seconds\n",
      "Step 2400, loss: tensor(0.0715, grad_fn=<SubBackward0>), time elapsed: 85.54828310012817 seconds\n",
      "Step 2500, loss: tensor(0.0709, grad_fn=<SubBackward0>), time elapsed: 89.03128433227539 seconds\n",
      "Step 2600, loss: tensor(0.0690, grad_fn=<SubBackward0>), time elapsed: 92.52457022666931 seconds\n",
      "Step 2700, loss: tensor(0.0685, grad_fn=<SubBackward0>), time elapsed: 96.23887872695923 seconds\n",
      "Step 2800, loss: tensor(0.0692, grad_fn=<SubBackward0>), time elapsed: 99.7676932811737 seconds\n",
      "Step 2900, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 103.25800585746765 seconds\n",
      "Step 3000, loss: tensor(0.0702, grad_fn=<SubBackward0>), time elapsed: 106.9433183670044 seconds\n",
      "Step 3100, loss: tensor(0.0670, grad_fn=<SubBackward0>), time elapsed: 110.45629715919495 seconds\n",
      "Step 3200, loss: tensor(0.0685, grad_fn=<SubBackward0>), time elapsed: 113.96771669387817 seconds\n",
      "Step 3300, loss: tensor(0.0679, grad_fn=<SubBackward0>), time elapsed: 117.43649077415466 seconds\n",
      "Step 3400, loss: tensor(0.0663, grad_fn=<SubBackward0>), time elapsed: 121.09642434120178 seconds\n",
      "Step 3500, loss: tensor(0.0669, grad_fn=<SubBackward0>), time elapsed: 124.57906436920166 seconds\n",
      "Step 3600, loss: tensor(0.0682, grad_fn=<SubBackward0>), time elapsed: 128.08984661102295 seconds\n",
      "Step 3700, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 131.77373027801514 seconds\n",
      "Step 3800, loss: tensor(0.0657, grad_fn=<SubBackward0>), time elapsed: 135.25247406959534 seconds\n",
      "Step 3900, loss: tensor(0.0666, grad_fn=<SubBackward0>), time elapsed: 138.73375248908997 seconds\n",
      "Step 4000, loss: tensor(0.0670, grad_fn=<SubBackward0>), time elapsed: 142.40063571929932 seconds\n",
      "Step 4100, loss: tensor(0.0664, grad_fn=<SubBackward0>), time elapsed: 145.9128520488739 seconds\n",
      "Step 4200, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 149.40066289901733 seconds\n",
      "Step 4300, loss: tensor(0.0637, grad_fn=<SubBackward0>), time elapsed: 153.10866570472717 seconds\n",
      "Step 4400, loss: tensor(0.0633, grad_fn=<SubBackward0>), time elapsed: 156.5950162410736 seconds\n",
      "Step 4500, loss: tensor(0.0637, grad_fn=<SubBackward0>), time elapsed: 160.10453391075134 seconds\n",
      "Step 4600, loss: tensor(0.0665, grad_fn=<SubBackward0>), time elapsed: 163.8174545764923 seconds\n",
      "Step 4700, loss: tensor(0.0649, grad_fn=<SubBackward0>), time elapsed: 167.3066327571869 seconds\n",
      "Step 4800, loss: tensor(0.0630, grad_fn=<SubBackward0>), time elapsed: 170.78230452537537 seconds\n",
      "Step 4900, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 174.48328924179077 seconds\n",
      "Step 5000, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 177.96110081672668 seconds\n",
      "Step 5100, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 181.4703586101532 seconds\n",
      "Step 5200, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 185.1857476234436 seconds\n",
      "Step 5300, loss: tensor(0.0618, grad_fn=<SubBackward0>), time elapsed: 188.7141501903534 seconds\n",
      "Step 5400, loss: tensor(0.0611, grad_fn=<SubBackward0>), time elapsed: 192.2051558494568 seconds\n",
      "Step 5500, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 195.89841485023499 seconds\n",
      "Step 5600, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 199.42774295806885 seconds\n",
      "Step 5700, loss: tensor(0.0568, grad_fn=<SubBackward0>), time elapsed: 202.95057678222656 seconds\n",
      "Step 5800, loss: tensor(0.0562, grad_fn=<SubBackward0>), time elapsed: 206.637104511261 seconds\n",
      "Step 5900, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 210.09954857826233 seconds\n",
      "Step 6000, loss: tensor(0.0560, grad_fn=<SubBackward0>), time elapsed: 213.6169774532318 seconds\n",
      "Step 6100, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 217.0904860496521 seconds\n",
      "Step 6200, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 220.80987548828125 seconds\n",
      "Step 6300, loss: tensor(0.0583, grad_fn=<SubBackward0>), time elapsed: 224.31483364105225 seconds\n",
      "Step 6400, loss: tensor(0.0573, grad_fn=<SubBackward0>), time elapsed: 227.80624175071716 seconds\n",
      "Step 6500, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 231.52043318748474 seconds\n",
      "Step 6600, loss: tensor(0.0559, grad_fn=<SubBackward0>), time elapsed: 235.053040266037 seconds\n",
      "Step 6700, loss: tensor(0.0552, grad_fn=<SubBackward0>), time elapsed: 238.54092478752136 seconds\n",
      "Step 6800, loss: tensor(0.0540, grad_fn=<SubBackward0>), time elapsed: 242.23649764060974 seconds\n",
      "Step 6900, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 245.71205306053162 seconds\n",
      "Step 7000, loss: tensor(0.0552, grad_fn=<SubBackward0>), time elapsed: 249.18530750274658 seconds\n",
      "Step 7100, loss: tensor(0.0552, grad_fn=<SubBackward0>), time elapsed: 252.89419269561768 seconds\n",
      "Step 7200, loss: tensor(0.0542, grad_fn=<SubBackward0>), time elapsed: 256.3761639595032 seconds\n",
      "Step 7300, loss: tensor(0.0563, grad_fn=<SubBackward0>), time elapsed: 259.9021520614624 seconds\n",
      "Step 7400, loss: tensor(0.0553, grad_fn=<SubBackward0>), time elapsed: 263.6087155342102 seconds\n",
      "Step 7500, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 267.1247069835663 seconds\n",
      "Step 7600, loss: tensor(0.0553, grad_fn=<SubBackward0>), time elapsed: 270.6600344181061 seconds\n",
      "Step 7700, loss: tensor(0.0546, grad_fn=<SubBackward0>), time elapsed: 274.3736970424652 seconds\n",
      "Step 7800, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 277.8525342941284 seconds\n",
      "Step 7900, loss: tensor(0.0547, grad_fn=<SubBackward0>), time elapsed: 281.33570432662964 seconds\n",
      "Step 8000, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 285.02207612991333 seconds\n",
      "Step 8100, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 288.55218029022217 seconds\n",
      "Step 8200, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 292.0375919342041 seconds\n",
      "Step 8300, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 295.7260994911194 seconds\n",
      "Step 8400, loss: tensor(0.0546, grad_fn=<SubBackward0>), time elapsed: 299.2528645992279 seconds\n",
      "Step 8500, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 302.75338435173035 seconds\n",
      "Step 8600, loss: tensor(0.0542, grad_fn=<SubBackward0>), time elapsed: 306.2585446834564 seconds\n",
      "Step 8700, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 310.0036609172821 seconds\n",
      "Step 8800, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 313.4901840686798 seconds\n",
      "Step 8900, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 316.96836733818054 seconds\n",
      "Step 9000, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 320.7115330696106 seconds\n",
      "Step 9100, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 324.2073919773102 seconds\n",
      "Step 9200, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 327.69666504859924 seconds\n",
      "Step 9300, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 331.3797948360443 seconds\n",
      "Step 9400, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 334.8755958080292 seconds\n",
      "Step 9500, loss: tensor(0.0441, grad_fn=<SubBackward0>), time elapsed: 338.3930025100708 seconds\n",
      "Step 9600, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 342.11319279670715 seconds\n",
      "Step 9700, loss: tensor(0.0448, grad_fn=<SubBackward0>), time elapsed: 345.665025472641 seconds\n",
      "Step 9800, loss: tensor(0.0474, grad_fn=<SubBackward0>), time elapsed: 349.17984342575073 seconds\n",
      "Step 9900, loss: tensor(0.0445, grad_fn=<SubBackward0>), time elapsed: 352.90472507476807 seconds\n",
      "Step 10000, loss: tensor(0.0436, grad_fn=<SubBackward0>), time elapsed: 356.4390835762024 seconds\n",
      "Step 10100, loss: tensor(0.0442, grad_fn=<SubBackward0>), time elapsed: 359.9258396625519 seconds\n",
      "Step 10200, loss: tensor(0.0453, grad_fn=<SubBackward0>), time elapsed: 363.6385610103607 seconds\n",
      "Step 10300, loss: tensor(0.0453, grad_fn=<SubBackward0>), time elapsed: 367.14230704307556 seconds\n",
      "Step 10400, loss: tensor(0.0409, grad_fn=<SubBackward0>), time elapsed: 370.6459484100342 seconds\n",
      "Step 10500, loss: tensor(0.0442, grad_fn=<SubBackward0>), time elapsed: 374.40724635124207 seconds\n",
      "Step 10600, loss: tensor(0.0432, grad_fn=<SubBackward0>), time elapsed: 377.91633701324463 seconds\n",
      "Step 10700, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 381.4641606807709 seconds\n",
      "Step 10800, loss: tensor(0.0422, grad_fn=<SubBackward0>), time elapsed: 385.1826841831207 seconds\n",
      "Step 10900, loss: tensor(0.0409, grad_fn=<SubBackward0>), time elapsed: 388.7037687301636 seconds\n",
      "Step 11000, loss: tensor(0.0424, grad_fn=<SubBackward0>), time elapsed: 392.22152733802795 seconds\n",
      "Step 11100, loss: tensor(0.0409, grad_fn=<SubBackward0>), time elapsed: 395.7448511123657 seconds\n",
      "Step 11200, loss: tensor(0.0408, grad_fn=<SubBackward0>), time elapsed: 399.4555869102478 seconds\n",
      "Step 11300, loss: tensor(0.0413, grad_fn=<SubBackward0>), time elapsed: 402.981689453125 seconds\n",
      "Step 11400, loss: tensor(0.0392, grad_fn=<SubBackward0>), time elapsed: 406.5026786327362 seconds\n",
      "Step 11500, loss: tensor(0.0397, grad_fn=<SubBackward0>), time elapsed: 410.2211377620697 seconds\n",
      "Step 11600, loss: tensor(0.0398, grad_fn=<SubBackward0>), time elapsed: 413.7435071468353 seconds\n",
      "Step 11700, loss: tensor(0.0414, grad_fn=<SubBackward0>), time elapsed: 417.3099410533905 seconds\n",
      "Step 11800, loss: tensor(0.0411, grad_fn=<SubBackward0>), time elapsed: 421.0946750640869 seconds\n",
      "Step 11900, loss: tensor(0.0424, grad_fn=<SubBackward0>), time elapsed: 424.6517195701599 seconds\n",
      "Step 12000, loss: tensor(0.0373, grad_fn=<SubBackward0>), time elapsed: 428.2661075592041 seconds\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.3271, grad_fn=<SubBackward0>), time elapsed: 0.03687644004821777 seconds\n",
      "Step 100, loss: tensor(0.2989, grad_fn=<SubBackward0>), time elapsed: 3.757495880126953 seconds\n",
      "Step 200, loss: tensor(0.2647, grad_fn=<SubBackward0>), time elapsed: 7.234053373336792 seconds\n",
      "Step 300, loss: tensor(0.2366, grad_fn=<SubBackward0>), time elapsed: 10.74651050567627 seconds\n",
      "Step 400, loss: tensor(0.2132, grad_fn=<SubBackward0>), time elapsed: 14.43767786026001 seconds\n",
      "Step 500, loss: tensor(0.1967, grad_fn=<SubBackward0>), time elapsed: 17.918778896331787 seconds\n",
      "Step 600, loss: tensor(0.1707, grad_fn=<SubBackward0>), time elapsed: 21.38995671272278 seconds\n",
      "Step 700, loss: tensor(0.1610, grad_fn=<SubBackward0>), time elapsed: 25.092495918273926 seconds\n",
      "Step 800, loss: tensor(0.1510, grad_fn=<SubBackward0>), time elapsed: 28.625974893569946 seconds\n",
      "Step 900, loss: tensor(0.1415, grad_fn=<SubBackward0>), time elapsed: 32.13244938850403 seconds\n",
      "Step 1000, loss: tensor(0.1342, grad_fn=<SubBackward0>), time elapsed: 35.82314896583557 seconds\n",
      "Step 1100, loss: tensor(0.1289, grad_fn=<SubBackward0>), time elapsed: 39.35225701332092 seconds\n",
      "Step 1200, loss: tensor(0.1271, grad_fn=<SubBackward0>), time elapsed: 42.86101055145264 seconds\n",
      "Step 1300, loss: tensor(0.1214, grad_fn=<SubBackward0>), time elapsed: 46.53383421897888 seconds\n",
      "Step 1400, loss: tensor(0.1186, grad_fn=<SubBackward0>), time elapsed: 50.01488733291626 seconds\n",
      "Step 1500, loss: tensor(0.1160, grad_fn=<SubBackward0>), time elapsed: 53.46460938453674 seconds\n",
      "Step 1600, loss: tensor(0.1157, grad_fn=<SubBackward0>), time elapsed: 57.1424994468689 seconds\n",
      "Step 1700, loss: tensor(0.1130, grad_fn=<SubBackward0>), time elapsed: 60.604055881500244 seconds\n",
      "Step 1800, loss: tensor(0.1106, grad_fn=<SubBackward0>), time elapsed: 64.09008455276489 seconds\n",
      "Step 1900, loss: tensor(0.1081, grad_fn=<SubBackward0>), time elapsed: 67.73874688148499 seconds\n",
      "Step 2000, loss: tensor(0.1072, grad_fn=<SubBackward0>), time elapsed: 71.22794365882874 seconds\n",
      "Step 2100, loss: tensor(0.1054, grad_fn=<SubBackward0>), time elapsed: 74.69085383415222 seconds\n",
      "Step 2200, loss: tensor(0.1056, grad_fn=<SubBackward0>), time elapsed: 78.34969425201416 seconds\n",
      "Step 2300, loss: tensor(0.1041, grad_fn=<SubBackward0>), time elapsed: 81.84709477424622 seconds\n",
      "Step 2400, loss: tensor(0.1029, grad_fn=<SubBackward0>), time elapsed: 85.30576419830322 seconds\n",
      "Step 2500, loss: tensor(0.1004, grad_fn=<SubBackward0>), time elapsed: 88.80630111694336 seconds\n",
      "Step 2600, loss: tensor(0.0999, grad_fn=<SubBackward0>), time elapsed: 92.44385147094727 seconds\n",
      "Step 2700, loss: tensor(0.0999, grad_fn=<SubBackward0>), time elapsed: 95.8963623046875 seconds\n",
      "Step 2800, loss: tensor(0.1005, grad_fn=<SubBackward0>), time elapsed: 99.3798828125 seconds\n",
      "Step 2900, loss: tensor(0.0978, grad_fn=<SubBackward0>), time elapsed: 103.0738046169281 seconds\n",
      "Step 3000, loss: tensor(0.0981, grad_fn=<SubBackward0>), time elapsed: 106.54976224899292 seconds\n",
      "Step 3100, loss: tensor(0.0986, grad_fn=<SubBackward0>), time elapsed: 109.99352693557739 seconds\n",
      "Step 3200, loss: tensor(0.0978, grad_fn=<SubBackward0>), time elapsed: 113.66174364089966 seconds\n",
      "Step 3300, loss: tensor(0.0993, grad_fn=<SubBackward0>), time elapsed: 117.17177033424377 seconds\n",
      "Step 3400, loss: tensor(0.0967, grad_fn=<SubBackward0>), time elapsed: 120.61980366706848 seconds\n",
      "Step 3500, loss: tensor(0.0970, grad_fn=<SubBackward0>), time elapsed: 124.29747295379639 seconds\n",
      "Step 3600, loss: tensor(0.0963, grad_fn=<SubBackward0>), time elapsed: 127.7514476776123 seconds\n",
      "Step 3700, loss: tensor(0.0973, grad_fn=<SubBackward0>), time elapsed: 131.23510694503784 seconds\n",
      "Step 3800, loss: tensor(0.0971, grad_fn=<SubBackward0>), time elapsed: 134.95332312583923 seconds\n",
      "Step 3900, loss: tensor(0.0956, grad_fn=<SubBackward0>), time elapsed: 138.49475955963135 seconds\n",
      "Step 4000, loss: tensor(0.0965, grad_fn=<SubBackward0>), time elapsed: 141.97409534454346 seconds\n",
      "Step 4100, loss: tensor(0.0968, grad_fn=<SubBackward0>), time elapsed: 145.69397377967834 seconds\n",
      "Step 4200, loss: tensor(0.0949, grad_fn=<SubBackward0>), time elapsed: 149.2082200050354 seconds\n",
      "Step 4300, loss: tensor(0.0938, grad_fn=<SubBackward0>), time elapsed: 152.73016262054443 seconds\n",
      "Step 4400, loss: tensor(0.0953, grad_fn=<SubBackward0>), time elapsed: 156.47632837295532 seconds\n",
      "Step 4500, loss: tensor(0.0952, grad_fn=<SubBackward0>), time elapsed: 160.01514196395874 seconds\n",
      "Step 4600, loss: tensor(0.0917, grad_fn=<SubBackward0>), time elapsed: 163.52149510383606 seconds\n",
      "Step 4700, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 167.22045803070068 seconds\n",
      "Step 4800, loss: tensor(0.0918, grad_fn=<SubBackward0>), time elapsed: 170.70065331459045 seconds\n",
      "Step 4900, loss: tensor(0.0910, grad_fn=<SubBackward0>), time elapsed: 174.22190380096436 seconds\n",
      "Step 5000, loss: tensor(0.0916, grad_fn=<SubBackward0>), time elapsed: 177.94413661956787 seconds\n",
      "Step 5100, loss: tensor(0.0885, grad_fn=<SubBackward0>), time elapsed: 181.4655110836029 seconds\n",
      "Step 5200, loss: tensor(0.0898, grad_fn=<SubBackward0>), time elapsed: 185.00257682800293 seconds\n",
      "Step 5300, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 188.71857142448425 seconds\n",
      "Step 5400, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 192.24916863441467 seconds\n",
      "Step 5500, loss: tensor(0.0899, grad_fn=<SubBackward0>), time elapsed: 195.75028133392334 seconds\n",
      "Step 5600, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 199.2463836669922 seconds\n",
      "Step 5700, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 202.96668410301208 seconds\n",
      "Step 5800, loss: tensor(0.0891, grad_fn=<SubBackward0>), time elapsed: 206.47585916519165 seconds\n",
      "Step 5900, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 209.99464058876038 seconds\n",
      "Step 6000, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 213.73365330696106 seconds\n",
      "Step 6100, loss: tensor(0.0848, grad_fn=<SubBackward0>), time elapsed: 217.22430896759033 seconds\n",
      "Step 6200, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 220.70340394973755 seconds\n",
      "Step 6300, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 224.42201018333435 seconds\n",
      "Step 6400, loss: tensor(0.0831, grad_fn=<SubBackward0>), time elapsed: 227.9211938381195 seconds\n",
      "Step 6500, loss: tensor(0.0850, grad_fn=<SubBackward0>), time elapsed: 231.4284040927887 seconds\n",
      "Step 6600, loss: tensor(0.0833, grad_fn=<SubBackward0>), time elapsed: 235.13138890266418 seconds\n",
      "Step 6700, loss: tensor(0.0838, grad_fn=<SubBackward0>), time elapsed: 238.65404057502747 seconds\n",
      "Step 6800, loss: tensor(0.0830, grad_fn=<SubBackward0>), time elapsed: 242.15638542175293 seconds\n",
      "Step 6900, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 245.8560380935669 seconds\n",
      "Step 7000, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 249.33628034591675 seconds\n",
      "Step 7100, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 252.8618655204773 seconds\n",
      "Step 7200, loss: tensor(0.0789, grad_fn=<SubBackward0>), time elapsed: 256.5558967590332 seconds\n",
      "Step 7300, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 260.0798909664154 seconds\n",
      "Step 7400, loss: tensor(0.0781, grad_fn=<SubBackward0>), time elapsed: 263.5621235370636 seconds\n",
      "Step 7500, loss: tensor(0.0785, grad_fn=<SubBackward0>), time elapsed: 267.25367617607117 seconds\n",
      "Step 7600, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 270.73366045951843 seconds\n",
      "Step 7700, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 274.2268753051758 seconds\n",
      "Step 7800, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 277.9123227596283 seconds\n",
      "Step 7900, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 281.4204123020172 seconds\n",
      "Step 8000, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 284.9055275917053 seconds\n",
      "Step 8100, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 288.39195680618286 seconds\n",
      "Step 8200, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 292.13750171661377 seconds\n",
      "Step 8300, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 295.6619005203247 seconds\n",
      "Step 8400, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 299.20035457611084 seconds\n",
      "Step 8500, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 302.8998193740845 seconds\n",
      "Step 8600, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 306.4498174190521 seconds\n",
      "Step 8700, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 309.9933400154114 seconds\n",
      "Step 8800, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 313.70597982406616 seconds\n",
      "Step 8900, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 317.24566626548767 seconds\n",
      "Step 9000, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 320.7453787326813 seconds\n",
      "Step 9100, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 324.4648151397705 seconds\n",
      "Step 9200, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 328.02026081085205 seconds\n",
      "Step 9300, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 331.51993894577026 seconds\n",
      "Step 9400, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 335.2359149456024 seconds\n",
      "Step 9500, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 338.7615125179291 seconds\n",
      "Step 9600, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 342.28998589515686 seconds\n",
      "Step 9700, loss: tensor(0.0779, grad_fn=<SubBackward0>), time elapsed: 346.0094299316406 seconds\n",
      "Step 9800, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 349.55273604393005 seconds\n",
      "Step 9900, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 353.02544236183167 seconds\n",
      "Step 10000, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 356.72311544418335 seconds\n",
      "Step 10100, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 360.2416055202484 seconds\n",
      "Step 10200, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 363.7447953224182 seconds\n",
      "Step 10300, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 367.4685513973236 seconds\n",
      "Step 10400, loss: tensor(0.0770, grad_fn=<SubBackward0>), time elapsed: 371.0113170146942 seconds\n",
      "Step 10500, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 374.51596784591675 seconds\n",
      "Step 10600, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 378.0410611629486 seconds\n",
      "Step 10700, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 381.79274106025696 seconds\n",
      "Step 10800, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 385.3477146625519 seconds\n",
      "Step 10900, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 388.8530304431915 seconds\n",
      "Step 11000, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 392.5878601074219 seconds\n",
      "Step 11100, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 396.10173535346985 seconds\n",
      "Step 11200, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 399.6084704399109 seconds\n",
      "Step 11300, loss: tensor(0.0720, grad_fn=<SubBackward0>), time elapsed: 403.3668038845062 seconds\n",
      "Step 11400, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 406.90205216407776 seconds\n",
      "Step 11500, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 410.44654846191406 seconds\n",
      "Step 11600, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 414.21003699302673 seconds\n",
      "Step 11700, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 417.82642245292664 seconds\n",
      "Step 11800, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 421.44743180274963 seconds\n",
      "Step 11900, loss: tensor(0.0712, grad_fn=<SubBackward0>), time elapsed: 425.28258442878723 seconds\n",
      "Step 12000, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 428.9039306640625 seconds\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.4868, grad_fn=<SubBackward0>), time elapsed: 0.03986668586730957 seconds\n",
      "Step 100, loss: tensor(0.4339, grad_fn=<SubBackward0>), time elapsed: 3.586085796356201 seconds\n",
      "Step 200, loss: tensor(0.3647, grad_fn=<SubBackward0>), time elapsed: 7.2689292430877686 seconds\n",
      "Step 300, loss: tensor(0.2991, grad_fn=<SubBackward0>), time elapsed: 10.735272884368896 seconds\n",
      "Step 400, loss: tensor(0.2551, grad_fn=<SubBackward0>), time elapsed: 14.209991455078125 seconds\n",
      "Step 500, loss: tensor(0.2274, grad_fn=<SubBackward0>), time elapsed: 17.911296367645264 seconds\n",
      "Step 600, loss: tensor(0.2095, grad_fn=<SubBackward0>), time elapsed: 21.424159288406372 seconds\n",
      "Step 700, loss: tensor(0.1922, grad_fn=<SubBackward0>), time elapsed: 24.943932056427002 seconds\n",
      "Step 800, loss: tensor(0.1832, grad_fn=<SubBackward0>), time elapsed: 28.69083595275879 seconds\n",
      "Step 900, loss: tensor(0.1815, grad_fn=<SubBackward0>), time elapsed: 32.21791887283325 seconds\n",
      "Step 1000, loss: tensor(0.1750, grad_fn=<SubBackward0>), time elapsed: 35.7135853767395 seconds\n",
      "Step 1100, loss: tensor(0.1729, grad_fn=<SubBackward0>), time elapsed: 39.4170618057251 seconds\n",
      "Step 1200, loss: tensor(0.1651, grad_fn=<SubBackward0>), time elapsed: 42.94035220146179 seconds\n",
      "Step 1300, loss: tensor(0.1635, grad_fn=<SubBackward0>), time elapsed: 46.4602792263031 seconds\n",
      "Step 1400, loss: tensor(0.1588, grad_fn=<SubBackward0>), time elapsed: 50.130321979522705 seconds\n",
      "Step 1500, loss: tensor(0.1573, grad_fn=<SubBackward0>), time elapsed: 53.60097146034241 seconds\n",
      "Step 1600, loss: tensor(0.1561, grad_fn=<SubBackward0>), time elapsed: 57.08697438240051 seconds\n",
      "Step 1700, loss: tensor(0.1553, grad_fn=<SubBackward0>), time elapsed: 60.595881938934326 seconds\n",
      "Step 1800, loss: tensor(0.1514, grad_fn=<SubBackward0>), time elapsed: 64.26115131378174 seconds\n",
      "Step 1900, loss: tensor(0.1554, grad_fn=<SubBackward0>), time elapsed: 67.759605884552 seconds\n",
      "Step 2000, loss: tensor(0.1528, grad_fn=<SubBackward0>), time elapsed: 71.23480606079102 seconds\n",
      "Step 2100, loss: tensor(0.1527, grad_fn=<SubBackward0>), time elapsed: 74.94386458396912 seconds\n",
      "Step 2200, loss: tensor(0.1532, grad_fn=<SubBackward0>), time elapsed: 78.49336385726929 seconds\n",
      "Step 2300, loss: tensor(0.1496, grad_fn=<SubBackward0>), time elapsed: 82.04436564445496 seconds\n",
      "Step 2400, loss: tensor(0.1514, grad_fn=<SubBackward0>), time elapsed: 85.72322869300842 seconds\n",
      "Step 2500, loss: tensor(0.1505, grad_fn=<SubBackward0>), time elapsed: 89.24671840667725 seconds\n",
      "Step 2600, loss: tensor(0.1489, grad_fn=<SubBackward0>), time elapsed: 92.77589583396912 seconds\n",
      "Step 2700, loss: tensor(0.1488, grad_fn=<SubBackward0>), time elapsed: 96.47841024398804 seconds\n",
      "Step 2800, loss: tensor(0.1483, grad_fn=<SubBackward0>), time elapsed: 100.00764060020447 seconds\n",
      "Step 2900, loss: tensor(0.1484, grad_fn=<SubBackward0>), time elapsed: 103.48688626289368 seconds\n",
      "Step 3000, loss: tensor(0.1472, grad_fn=<SubBackward0>), time elapsed: 107.19556307792664 seconds\n",
      "Step 3100, loss: tensor(0.1474, grad_fn=<SubBackward0>), time elapsed: 110.73387694358826 seconds\n",
      "Step 3200, loss: tensor(0.1452, grad_fn=<SubBackward0>), time elapsed: 114.24414825439453 seconds\n",
      "Step 3300, loss: tensor(0.1414, grad_fn=<SubBackward0>), time elapsed: 117.91267442703247 seconds\n",
      "Step 3400, loss: tensor(0.1424, grad_fn=<SubBackward0>), time elapsed: 121.40591716766357 seconds\n",
      "Step 3500, loss: tensor(0.1428, grad_fn=<SubBackward0>), time elapsed: 124.88213181495667 seconds\n",
      "Step 3600, loss: tensor(0.1409, grad_fn=<SubBackward0>), time elapsed: 128.5844156742096 seconds\n",
      "Step 3700, loss: tensor(0.1414, grad_fn=<SubBackward0>), time elapsed: 132.05582308769226 seconds\n",
      "Step 3800, loss: tensor(0.1398, grad_fn=<SubBackward0>), time elapsed: 135.56135392189026 seconds\n",
      "Step 3900, loss: tensor(0.1377, grad_fn=<SubBackward0>), time elapsed: 139.2904782295227 seconds\n",
      "Step 4000, loss: tensor(0.1387, grad_fn=<SubBackward0>), time elapsed: 142.82724165916443 seconds\n",
      "Step 4100, loss: tensor(0.1416, grad_fn=<SubBackward0>), time elapsed: 146.3148205280304 seconds\n",
      "Step 4200, loss: tensor(0.1351, grad_fn=<SubBackward0>), time elapsed: 150.01910638809204 seconds\n",
      "Step 4300, loss: tensor(0.1367, grad_fn=<SubBackward0>), time elapsed: 153.5464162826538 seconds\n",
      "Step 4400, loss: tensor(0.1360, grad_fn=<SubBackward0>), time elapsed: 157.09180641174316 seconds\n",
      "Step 4500, loss: tensor(0.1377, grad_fn=<SubBackward0>), time elapsed: 160.78406739234924 seconds\n",
      "Step 4600, loss: tensor(0.1375, grad_fn=<SubBackward0>), time elapsed: 164.24925327301025 seconds\n",
      "Step 4700, loss: tensor(0.1358, grad_fn=<SubBackward0>), time elapsed: 167.74472975730896 seconds\n",
      "Step 4800, loss: tensor(0.1365, grad_fn=<SubBackward0>), time elapsed: 171.4136130809784 seconds\n",
      "Step 4900, loss: tensor(0.1352, grad_fn=<SubBackward0>), time elapsed: 174.88464045524597 seconds\n",
      "Step 5000, loss: tensor(0.1354, grad_fn=<SubBackward0>), time elapsed: 178.37086153030396 seconds\n",
      "Step 5100, loss: tensor(0.1377, grad_fn=<SubBackward0>), time elapsed: 181.90947556495667 seconds\n",
      "Step 5200, loss: tensor(0.1320, grad_fn=<SubBackward0>), time elapsed: 185.63814854621887 seconds\n",
      "Step 5300, loss: tensor(0.1383, grad_fn=<SubBackward0>), time elapsed: 189.13494563102722 seconds\n",
      "Step 5400, loss: tensor(0.1351, grad_fn=<SubBackward0>), time elapsed: 192.6092176437378 seconds\n",
      "Step 5500, loss: tensor(0.1361, grad_fn=<SubBackward0>), time elapsed: 196.34551548957825 seconds\n",
      "Step 5600, loss: tensor(0.1321, grad_fn=<SubBackward0>), time elapsed: 199.86051440238953 seconds\n",
      "Step 5700, loss: tensor(0.1344, grad_fn=<SubBackward0>), time elapsed: 203.33268761634827 seconds\n",
      "Step 5800, loss: tensor(0.1369, grad_fn=<SubBackward0>), time elapsed: 207.00132155418396 seconds\n",
      "Step 5900, loss: tensor(0.1341, grad_fn=<SubBackward0>), time elapsed: 210.52291297912598 seconds\n",
      "Step 6000, loss: tensor(0.1313, grad_fn=<SubBackward0>), time elapsed: 214.0125653743744 seconds\n",
      "Step 6100, loss: tensor(0.1320, grad_fn=<SubBackward0>), time elapsed: 217.70781540870667 seconds\n",
      "Step 6200, loss: tensor(0.1300, grad_fn=<SubBackward0>), time elapsed: 221.2011022567749 seconds\n",
      "Step 6300, loss: tensor(0.1304, grad_fn=<SubBackward0>), time elapsed: 224.70887780189514 seconds\n",
      "Step 6400, loss: tensor(0.1266, grad_fn=<SubBackward0>), time elapsed: 228.39574027061462 seconds\n",
      "Step 6500, loss: tensor(0.1312, grad_fn=<SubBackward0>), time elapsed: 231.92381048202515 seconds\n",
      "Step 6600, loss: tensor(0.1306, grad_fn=<SubBackward0>), time elapsed: 235.41085577011108 seconds\n",
      "Step 6700, loss: tensor(0.1275, grad_fn=<SubBackward0>), time elapsed: 239.11551332473755 seconds\n",
      "Step 6800, loss: tensor(0.1312, grad_fn=<SubBackward0>), time elapsed: 242.62265491485596 seconds\n",
      "Step 6900, loss: tensor(0.1264, grad_fn=<SubBackward0>), time elapsed: 246.11731958389282 seconds\n",
      "Step 7000, loss: tensor(0.1260, grad_fn=<SubBackward0>), time elapsed: 249.8263018131256 seconds\n",
      "Step 7100, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 253.33829164505005 seconds\n",
      "Step 7200, loss: tensor(0.1271, grad_fn=<SubBackward0>), time elapsed: 256.8237009048462 seconds\n",
      "Step 7300, loss: tensor(0.1229, grad_fn=<SubBackward0>), time elapsed: 260.52025985717773 seconds\n",
      "Step 7400, loss: tensor(0.1273, grad_fn=<SubBackward0>), time elapsed: 264.01499342918396 seconds\n",
      "Step 7500, loss: tensor(0.1242, grad_fn=<SubBackward0>), time elapsed: 267.54870343208313 seconds\n",
      "Step 7600, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 271.02920389175415 seconds\n",
      "Step 7700, loss: tensor(0.1254, grad_fn=<SubBackward0>), time elapsed: 274.73249650001526 seconds\n",
      "Step 7800, loss: tensor(0.1203, grad_fn=<SubBackward0>), time elapsed: 278.24168586730957 seconds\n",
      "Step 7900, loss: tensor(0.1226, grad_fn=<SubBackward0>), time elapsed: 281.7585368156433 seconds\n",
      "Step 8000, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 285.44670724868774 seconds\n",
      "Step 8100, loss: tensor(0.1219, grad_fn=<SubBackward0>), time elapsed: 288.98086619377136 seconds\n",
      "Step 8200, loss: tensor(0.1199, grad_fn=<SubBackward0>), time elapsed: 292.49554109573364 seconds\n",
      "Step 8300, loss: tensor(0.1206, grad_fn=<SubBackward0>), time elapsed: 296.20776748657227 seconds\n",
      "Step 8400, loss: tensor(0.1216, grad_fn=<SubBackward0>), time elapsed: 299.7793996334076 seconds\n",
      "Step 8500, loss: tensor(0.1223, grad_fn=<SubBackward0>), time elapsed: 303.3226046562195 seconds\n",
      "Step 8600, loss: tensor(0.1207, grad_fn=<SubBackward0>), time elapsed: 307.0374231338501 seconds\n",
      "Step 8700, loss: tensor(0.1206, grad_fn=<SubBackward0>), time elapsed: 310.58647632598877 seconds\n",
      "Step 8800, loss: tensor(0.1164, grad_fn=<SubBackward0>), time elapsed: 314.07488536834717 seconds\n",
      "Step 8900, loss: tensor(0.1195, grad_fn=<SubBackward0>), time elapsed: 317.84125232696533 seconds\n",
      "Step 9000, loss: tensor(0.1152, grad_fn=<SubBackward0>), time elapsed: 321.3901000022888 seconds\n",
      "Step 9100, loss: tensor(0.1160, grad_fn=<SubBackward0>), time elapsed: 324.9050693511963 seconds\n",
      "Step 9200, loss: tensor(0.1238, grad_fn=<SubBackward0>), time elapsed: 328.62646079063416 seconds\n",
      "Step 9300, loss: tensor(0.1154, grad_fn=<SubBackward0>), time elapsed: 332.1379511356354 seconds\n",
      "Step 9400, loss: tensor(0.1188, grad_fn=<SubBackward0>), time elapsed: 335.62535643577576 seconds\n",
      "Step 9500, loss: tensor(0.1141, grad_fn=<SubBackward0>), time elapsed: 339.3285071849823 seconds\n",
      "Step 9600, loss: tensor(0.1144, grad_fn=<SubBackward0>), time elapsed: 342.8393352031708 seconds\n",
      "Step 9700, loss: tensor(0.1164, grad_fn=<SubBackward0>), time elapsed: 346.37410140037537 seconds\n",
      "Step 9800, loss: tensor(0.1107, grad_fn=<SubBackward0>), time elapsed: 350.0745403766632 seconds\n",
      "Step 9900, loss: tensor(0.1142, grad_fn=<SubBackward0>), time elapsed: 353.5964503288269 seconds\n",
      "Step 10000, loss: tensor(0.1166, grad_fn=<SubBackward0>), time elapsed: 357.08663964271545 seconds\n",
      "Step 10100, loss: tensor(0.1130, grad_fn=<SubBackward0>), time elapsed: 360.6180703639984 seconds\n",
      "Step 10200, loss: tensor(0.1138, grad_fn=<SubBackward0>), time elapsed: 364.34438920021057 seconds\n",
      "Step 10300, loss: tensor(0.1178, grad_fn=<SubBackward0>), time elapsed: 367.82724833488464 seconds\n",
      "Step 10400, loss: tensor(0.1149, grad_fn=<SubBackward0>), time elapsed: 371.32070422172546 seconds\n",
      "Step 10500, loss: tensor(0.1167, grad_fn=<SubBackward0>), time elapsed: 375.0774610042572 seconds\n",
      "Step 10600, loss: tensor(0.1111, grad_fn=<SubBackward0>), time elapsed: 378.58469796180725 seconds\n",
      "Step 10700, loss: tensor(0.1152, grad_fn=<SubBackward0>), time elapsed: 382.0820996761322 seconds\n",
      "Step 10800, loss: tensor(0.1193, grad_fn=<SubBackward0>), time elapsed: 385.8090546131134 seconds\n",
      "Step 10900, loss: tensor(0.1153, grad_fn=<SubBackward0>), time elapsed: 389.3706896305084 seconds\n",
      "Step 11000, loss: tensor(0.1134, grad_fn=<SubBackward0>), time elapsed: 392.8910303115845 seconds\n",
      "Step 11100, loss: tensor(0.1176, grad_fn=<SubBackward0>), time elapsed: 396.6231598854065 seconds\n",
      "Step 11200, loss: tensor(0.1155, grad_fn=<SubBackward0>), time elapsed: 400.16244411468506 seconds\n",
      "Step 11300, loss: tensor(0.1122, grad_fn=<SubBackward0>), time elapsed: 403.6933808326721 seconds\n",
      "Step 11400, loss: tensor(0.1162, grad_fn=<SubBackward0>), time elapsed: 407.46065521240234 seconds\n",
      "Step 11500, loss: tensor(0.1158, grad_fn=<SubBackward0>), time elapsed: 411.0295886993408 seconds\n",
      "Step 11600, loss: tensor(0.1136, grad_fn=<SubBackward0>), time elapsed: 414.55330204963684 seconds\n",
      "Step 11700, loss: tensor(0.1164, grad_fn=<SubBackward0>), time elapsed: 418.3542215824127 seconds\n",
      "Step 11800, loss: tensor(0.1090, grad_fn=<SubBackward0>), time elapsed: 421.91415548324585 seconds\n",
      "Step 11900, loss: tensor(0.1138, grad_fn=<SubBackward0>), time elapsed: 425.46621584892273 seconds\n",
      "Step 12000, loss: tensor(0.1159, grad_fn=<SubBackward0>), time elapsed: 429.2565882205963 seconds\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.6593, grad_fn=<SubBackward0>), time elapsed: 0.03687644004821777 seconds\n",
      "Step 100, loss: tensor(0.5984, grad_fn=<SubBackward0>), time elapsed: 3.567805290222168 seconds\n",
      "Step 200, loss: tensor(0.5332, grad_fn=<SubBackward0>), time elapsed: 7.049844741821289 seconds\n",
      "Step 300, loss: tensor(0.4926, grad_fn=<SubBackward0>), time elapsed: 10.73621654510498 seconds\n",
      "Step 400, loss: tensor(0.4432, grad_fn=<SubBackward0>), time elapsed: 14.272724866867065 seconds\n",
      "Step 500, loss: tensor(0.4078, grad_fn=<SubBackward0>), time elapsed: 17.812486171722412 seconds\n",
      "Step 600, loss: tensor(0.3682, grad_fn=<SubBackward0>), time elapsed: 21.540273189544678 seconds\n",
      "Step 700, loss: tensor(0.3365, grad_fn=<SubBackward0>), time elapsed: 25.027557849884033 seconds\n",
      "Step 800, loss: tensor(0.3140, grad_fn=<SubBackward0>), time elapsed: 28.526965379714966 seconds\n",
      "Step 900, loss: tensor(0.2981, grad_fn=<SubBackward0>), time elapsed: 32.01567029953003 seconds\n",
      "Step 1000, loss: tensor(0.2849, grad_fn=<SubBackward0>), time elapsed: 35.70251774787903 seconds\n",
      "Step 1100, loss: tensor(0.2739, grad_fn=<SubBackward0>), time elapsed: 39.247493743896484 seconds\n",
      "Step 1200, loss: tensor(0.2719, grad_fn=<SubBackward0>), time elapsed: 42.78779602050781 seconds\n",
      "Step 1300, loss: tensor(0.2594, grad_fn=<SubBackward0>), time elapsed: 46.515013694763184 seconds\n",
      "Step 1400, loss: tensor(0.2514, grad_fn=<SubBackward0>), time elapsed: 49.999606132507324 seconds\n",
      "Step 1500, loss: tensor(0.2476, grad_fn=<SubBackward0>), time elapsed: 53.50645899772644 seconds\n",
      "Step 1600, loss: tensor(0.2443, grad_fn=<SubBackward0>), time elapsed: 57.17971992492676 seconds\n",
      "Step 1700, loss: tensor(0.2340, grad_fn=<SubBackward0>), time elapsed: 60.684309244155884 seconds\n",
      "Step 1800, loss: tensor(0.2313, grad_fn=<SubBackward0>), time elapsed: 64.1839611530304 seconds\n",
      "Step 1900, loss: tensor(0.2233, grad_fn=<SubBackward0>), time elapsed: 67.87808322906494 seconds\n",
      "Step 2000, loss: tensor(0.2231, grad_fn=<SubBackward0>), time elapsed: 71.35493659973145 seconds\n",
      "Step 2100, loss: tensor(0.2213, grad_fn=<SubBackward0>), time elapsed: 74.82725286483765 seconds\n",
      "Step 2200, loss: tensor(0.2171, grad_fn=<SubBackward0>), time elapsed: 78.51684403419495 seconds\n",
      "Step 2300, loss: tensor(0.2071, grad_fn=<SubBackward0>), time elapsed: 82.0258378982544 seconds\n",
      "Step 2400, loss: tensor(0.2105, grad_fn=<SubBackward0>), time elapsed: 85.5078809261322 seconds\n",
      "Step 2500, loss: tensor(0.2065, grad_fn=<SubBackward0>), time elapsed: 89.2125563621521 seconds\n",
      "Step 2600, loss: tensor(0.2048, grad_fn=<SubBackward0>), time elapsed: 92.7253258228302 seconds\n",
      "Step 2700, loss: tensor(0.1961, grad_fn=<SubBackward0>), time elapsed: 96.21396517753601 seconds\n",
      "Step 2800, loss: tensor(0.1896, grad_fn=<SubBackward0>), time elapsed: 99.90121912956238 seconds\n",
      "Step 2900, loss: tensor(0.1922, grad_fn=<SubBackward0>), time elapsed: 103.42528009414673 seconds\n",
      "Step 3000, loss: tensor(0.1831, grad_fn=<SubBackward0>), time elapsed: 106.90771818161011 seconds\n",
      "Step 3100, loss: tensor(0.1752, grad_fn=<SubBackward0>), time elapsed: 110.61355972290039 seconds\n",
      "Step 3200, loss: tensor(0.1815, grad_fn=<SubBackward0>), time elapsed: 114.0924756526947 seconds\n",
      "Step 3300, loss: tensor(0.1676, grad_fn=<SubBackward0>), time elapsed: 117.56942677497864 seconds\n",
      "Step 3400, loss: tensor(0.1710, grad_fn=<SubBackward0>), time elapsed: 121.26389670372009 seconds\n",
      "Step 3500, loss: tensor(0.1625, grad_fn=<SubBackward0>), time elapsed: 124.78663349151611 seconds\n",
      "Step 3600, loss: tensor(0.1657, grad_fn=<SubBackward0>), time elapsed: 128.2637016773224 seconds\n",
      "Step 3700, loss: tensor(0.1580, grad_fn=<SubBackward0>), time elapsed: 131.98291778564453 seconds\n",
      "Step 3800, loss: tensor(0.1558, grad_fn=<SubBackward0>), time elapsed: 135.50314331054688 seconds\n",
      "Step 3900, loss: tensor(0.1547, grad_fn=<SubBackward0>), time elapsed: 139.02165269851685 seconds\n",
      "Step 4000, loss: tensor(0.1564, grad_fn=<SubBackward0>), time elapsed: 142.69469499588013 seconds\n",
      "Step 4100, loss: tensor(0.1529, grad_fn=<SubBackward0>), time elapsed: 146.1575801372528 seconds\n",
      "Step 4200, loss: tensor(0.1528, grad_fn=<SubBackward0>), time elapsed: 149.64132928848267 seconds\n",
      "Step 4300, loss: tensor(0.1487, grad_fn=<SubBackward0>), time elapsed: 153.13567805290222 seconds\n",
      "Step 4400, loss: tensor(0.1463, grad_fn=<SubBackward0>), time elapsed: 156.8926224708557 seconds\n",
      "Step 4500, loss: tensor(0.1475, grad_fn=<SubBackward0>), time elapsed: 160.44505214691162 seconds\n",
      "Step 4600, loss: tensor(0.1461, grad_fn=<SubBackward0>), time elapsed: 163.96697854995728 seconds\n",
      "Step 4700, loss: tensor(0.1486, grad_fn=<SubBackward0>), time elapsed: 167.68489742279053 seconds\n",
      "Step 4800, loss: tensor(0.1443, grad_fn=<SubBackward0>), time elapsed: 171.1899013519287 seconds\n",
      "Step 4900, loss: tensor(0.1362, grad_fn=<SubBackward0>), time elapsed: 174.70849323272705 seconds\n",
      "Step 5000, loss: tensor(0.1418, grad_fn=<SubBackward0>), time elapsed: 178.43807673454285 seconds\n",
      "Step 5100, loss: tensor(0.1489, grad_fn=<SubBackward0>), time elapsed: 181.9307336807251 seconds\n",
      "Step 5200, loss: tensor(0.1416, grad_fn=<SubBackward0>), time elapsed: 185.48310732841492 seconds\n",
      "Step 5300, loss: tensor(0.1385, grad_fn=<SubBackward0>), time elapsed: 189.21054530143738 seconds\n",
      "Step 5400, loss: tensor(0.1438, grad_fn=<SubBackward0>), time elapsed: 192.77966809272766 seconds\n",
      "Step 5500, loss: tensor(0.1415, grad_fn=<SubBackward0>), time elapsed: 196.3189342021942 seconds\n",
      "Step 5600, loss: tensor(0.1426, grad_fn=<SubBackward0>), time elapsed: 200.02811288833618 seconds\n",
      "Step 5700, loss: tensor(0.1406, grad_fn=<SubBackward0>), time elapsed: 203.53125047683716 seconds\n",
      "Step 5800, loss: tensor(0.1354, grad_fn=<SubBackward0>), time elapsed: 207.01937580108643 seconds\n",
      "Step 5900, loss: tensor(0.1377, grad_fn=<SubBackward0>), time elapsed: 210.73780512809753 seconds\n",
      "Step 6000, loss: tensor(0.1388, grad_fn=<SubBackward0>), time elapsed: 214.276837348938 seconds\n",
      "Step 6100, loss: tensor(0.1385, grad_fn=<SubBackward0>), time elapsed: 217.75604677200317 seconds\n",
      "Step 6200, loss: tensor(0.1354, grad_fn=<SubBackward0>), time elapsed: 221.45571660995483 seconds\n",
      "Step 6300, loss: tensor(0.1407, grad_fn=<SubBackward0>), time elapsed: 225.00504779815674 seconds\n",
      "Step 6400, loss: tensor(0.1367, grad_fn=<SubBackward0>), time elapsed: 228.5207724571228 seconds\n",
      "Step 6500, loss: tensor(0.1371, grad_fn=<SubBackward0>), time elapsed: 232.20662879943848 seconds\n",
      "Step 6600, loss: tensor(0.1383, grad_fn=<SubBackward0>), time elapsed: 235.702632188797 seconds\n",
      "Step 6700, loss: tensor(0.1349, grad_fn=<SubBackward0>), time elapsed: 239.21787929534912 seconds\n",
      "Step 6800, loss: tensor(0.1400, grad_fn=<SubBackward0>), time elapsed: 242.94325518608093 seconds\n",
      "Step 6900, loss: tensor(0.1351, grad_fn=<SubBackward0>), time elapsed: 246.44634199142456 seconds\n",
      "Step 7000, loss: tensor(0.1360, grad_fn=<SubBackward0>), time elapsed: 249.98569750785828 seconds\n",
      "Step 7100, loss: tensor(0.1415, grad_fn=<SubBackward0>), time elapsed: 253.47008895874023 seconds\n",
      "Step 7200, loss: tensor(0.1305, grad_fn=<SubBackward0>), time elapsed: 257.1942059993744 seconds\n",
      "Step 7300, loss: tensor(0.1413, grad_fn=<SubBackward0>), time elapsed: 260.73434615135193 seconds\n",
      "Step 7400, loss: tensor(0.1394, grad_fn=<SubBackward0>), time elapsed: 264.24916434288025 seconds\n",
      "Step 7500, loss: tensor(0.1383, grad_fn=<SubBackward0>), time elapsed: 267.97081565856934 seconds\n",
      "Step 7600, loss: tensor(0.1338, grad_fn=<SubBackward0>), time elapsed: 271.46873593330383 seconds\n",
      "Step 7700, loss: tensor(0.1376, grad_fn=<SubBackward0>), time elapsed: 275.00872898101807 seconds\n",
      "Step 7800, loss: tensor(0.1393, grad_fn=<SubBackward0>), time elapsed: 278.7160680294037 seconds\n",
      "Step 7900, loss: tensor(0.1380, grad_fn=<SubBackward0>), time elapsed: 282.23928713798523 seconds\n",
      "Step 8000, loss: tensor(0.1330, grad_fn=<SubBackward0>), time elapsed: 285.7201690673828 seconds\n",
      "Step 8100, loss: tensor(0.1344, grad_fn=<SubBackward0>), time elapsed: 289.4337589740753 seconds\n",
      "Step 8200, loss: tensor(0.1367, grad_fn=<SubBackward0>), time elapsed: 292.9292342662811 seconds\n",
      "Step 8300, loss: tensor(0.1401, grad_fn=<SubBackward0>), time elapsed: 296.45745182037354 seconds\n",
      "Step 8400, loss: tensor(0.1374, grad_fn=<SubBackward0>), time elapsed: 300.2210998535156 seconds\n",
      "Step 8500, loss: tensor(0.1344, grad_fn=<SubBackward0>), time elapsed: 303.7520594596863 seconds\n",
      "Step 8600, loss: tensor(0.1394, grad_fn=<SubBackward0>), time elapsed: 307.218962430954 seconds\n",
      "Step 8700, loss: tensor(0.1385, grad_fn=<SubBackward0>), time elapsed: 310.9470067024231 seconds\n",
      "Step 8800, loss: tensor(0.1380, grad_fn=<SubBackward0>), time elapsed: 314.43222308158875 seconds\n",
      "Step 8900, loss: tensor(0.1412, grad_fn=<SubBackward0>), time elapsed: 317.9691390991211 seconds\n",
      "Step 9000, loss: tensor(0.1351, grad_fn=<SubBackward0>), time elapsed: 321.6710386276245 seconds\n",
      "Step 9100, loss: tensor(0.1323, grad_fn=<SubBackward0>), time elapsed: 325.170330286026 seconds\n",
      "Step 9200, loss: tensor(0.1478, grad_fn=<SubBackward0>), time elapsed: 328.68022298812866 seconds\n",
      "Step 9300, loss: tensor(0.1343, grad_fn=<SubBackward0>), time elapsed: 332.44707703590393 seconds\n",
      "Step 9400, loss: tensor(0.1406, grad_fn=<SubBackward0>), time elapsed: 335.96059370040894 seconds\n",
      "Step 9500, loss: tensor(0.1328, grad_fn=<SubBackward0>), time elapsed: 339.4819598197937 seconds\n",
      "Step 9600, loss: tensor(0.1369, grad_fn=<SubBackward0>), time elapsed: 342.98193883895874 seconds\n",
      "Step 9700, loss: tensor(0.1366, grad_fn=<SubBackward0>), time elapsed: 346.73586773872375 seconds\n",
      "Step 9800, loss: tensor(0.1369, grad_fn=<SubBackward0>), time elapsed: 350.29072308540344 seconds\n",
      "Step 9900, loss: tensor(0.1345, grad_fn=<SubBackward0>), time elapsed: 353.7901999950409 seconds\n",
      "Step 10000, loss: tensor(0.1402, grad_fn=<SubBackward0>), time elapsed: 357.50518441200256 seconds\n",
      "Step 10100, loss: tensor(0.1373, grad_fn=<SubBackward0>), time elapsed: 361.02114367485046 seconds\n",
      "Step 10200, loss: tensor(0.1298, grad_fn=<SubBackward0>), time elapsed: 364.54587602615356 seconds\n",
      "Step 10300, loss: tensor(0.1328, grad_fn=<SubBackward0>), time elapsed: 368.3077712059021 seconds\n",
      "Step 10400, loss: tensor(0.1292, grad_fn=<SubBackward0>), time elapsed: 371.8778898715973 seconds\n",
      "Step 10500, loss: tensor(0.1404, grad_fn=<SubBackward0>), time elapsed: 375.4059703350067 seconds\n",
      "Step 10600, loss: tensor(0.1376, grad_fn=<SubBackward0>), time elapsed: 379.14476919174194 seconds\n",
      "Step 10700, loss: tensor(0.1356, grad_fn=<SubBackward0>), time elapsed: 382.66909170150757 seconds\n",
      "Step 10800, loss: tensor(0.1353, grad_fn=<SubBackward0>), time elapsed: 386.20182824134827 seconds\n",
      "Step 10900, loss: tensor(0.1337, grad_fn=<SubBackward0>), time elapsed: 389.929025888443 seconds\n",
      "Step 11000, loss: tensor(0.1359, grad_fn=<SubBackward0>), time elapsed: 393.44461035728455 seconds\n",
      "Step 11100, loss: tensor(0.1360, grad_fn=<SubBackward0>), time elapsed: 396.9776406288147 seconds\n",
      "Step 11200, loss: tensor(0.1317, grad_fn=<SubBackward0>), time elapsed: 400.72133803367615 seconds\n",
      "Step 11300, loss: tensor(0.1396, grad_fn=<SubBackward0>), time elapsed: 404.2368998527527 seconds\n",
      "Step 11400, loss: tensor(0.1340, grad_fn=<SubBackward0>), time elapsed: 407.7816393375397 seconds\n",
      "Step 11500, loss: tensor(0.1321, grad_fn=<SubBackward0>), time elapsed: 411.5545332431793 seconds\n",
      "Step 11600, loss: tensor(0.1380, grad_fn=<SubBackward0>), time elapsed: 415.15143299102783 seconds\n",
      "Step 11700, loss: tensor(0.1325, grad_fn=<SubBackward0>), time elapsed: 418.70069909095764 seconds\n",
      "Step 11800, loss: tensor(0.1286, grad_fn=<SubBackward0>), time elapsed: 422.3243021965027 seconds\n",
      "Step 11900, loss: tensor(0.1400, grad_fn=<SubBackward0>), time elapsed: 426.16963720321655 seconds\n",
      "Step 12000, loss: tensor(0.1335, grad_fn=<SubBackward0>), time elapsed: 429.8011577129364 seconds\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.8437, grad_fn=<SubBackward0>), time elapsed: 0.03488349914550781 seconds\n",
      "Step 100, loss: tensor(0.7659, grad_fn=<SubBackward0>), time elapsed: 3.767207384109497 seconds\n",
      "Step 200, loss: tensor(0.6739, grad_fn=<SubBackward0>), time elapsed: 7.281392574310303 seconds\n",
      "Step 300, loss: tensor(0.5759, grad_fn=<SubBackward0>), time elapsed: 10.817251920700073 seconds\n",
      "Step 400, loss: tensor(0.4964, grad_fn=<SubBackward0>), time elapsed: 14.329386949539185 seconds\n",
      "Step 500, loss: tensor(0.4406, grad_fn=<SubBackward0>), time elapsed: 18.08327031135559 seconds\n",
      "Step 600, loss: tensor(0.3905, grad_fn=<SubBackward0>), time elapsed: 21.618103504180908 seconds\n",
      "Step 700, loss: tensor(0.3482, grad_fn=<SubBackward0>), time elapsed: 25.102728366851807 seconds\n",
      "Step 800, loss: tensor(0.3058, grad_fn=<SubBackward0>), time elapsed: 28.831626415252686 seconds\n",
      "Step 900, loss: tensor(0.2805, grad_fn=<SubBackward0>), time elapsed: 32.33667850494385 seconds\n",
      "Step 1000, loss: tensor(0.2598, grad_fn=<SubBackward0>), time elapsed: 35.84820365905762 seconds\n",
      "Step 1100, loss: tensor(0.2463, grad_fn=<SubBackward0>), time elapsed: 39.55271363258362 seconds\n",
      "Step 1200, loss: tensor(0.2349, grad_fn=<SubBackward0>), time elapsed: 43.072235107421875 seconds\n",
      "Step 1300, loss: tensor(0.2280, grad_fn=<SubBackward0>), time elapsed: 46.56377291679382 seconds\n",
      "Step 1400, loss: tensor(0.2241, grad_fn=<SubBackward0>), time elapsed: 50.293779611587524 seconds\n",
      "Step 1500, loss: tensor(0.2199, grad_fn=<SubBackward0>), time elapsed: 53.78545832633972 seconds\n",
      "Step 1600, loss: tensor(0.2170, grad_fn=<SubBackward0>), time elapsed: 57.26928734779358 seconds\n",
      "Step 1700, loss: tensor(0.2156, grad_fn=<SubBackward0>), time elapsed: 60.98729610443115 seconds\n",
      "Step 1800, loss: tensor(0.2101, grad_fn=<SubBackward0>), time elapsed: 64.53767204284668 seconds\n",
      "Step 1900, loss: tensor(0.2109, grad_fn=<SubBackward0>), time elapsed: 68.05141282081604 seconds\n",
      "Step 2000, loss: tensor(0.2087, grad_fn=<SubBackward0>), time elapsed: 71.73980116844177 seconds\n",
      "Step 2100, loss: tensor(0.2080, grad_fn=<SubBackward0>), time elapsed: 75.23172497749329 seconds\n",
      "Step 2200, loss: tensor(0.2071, grad_fn=<SubBackward0>), time elapsed: 78.72690773010254 seconds\n",
      "Step 2300, loss: tensor(0.2053, grad_fn=<SubBackward0>), time elapsed: 82.43081831932068 seconds\n",
      "Step 2400, loss: tensor(0.2043, grad_fn=<SubBackward0>), time elapsed: 85.91355323791504 seconds\n",
      "Step 2500, loss: tensor(0.2051, grad_fn=<SubBackward0>), time elapsed: 89.41419291496277 seconds\n",
      "Step 2600, loss: tensor(0.2021, grad_fn=<SubBackward0>), time elapsed: 93.13174557685852 seconds\n",
      "Step 2700, loss: tensor(0.2022, grad_fn=<SubBackward0>), time elapsed: 96.6444149017334 seconds\n",
      "Step 2800, loss: tensor(0.2034, grad_fn=<SubBackward0>), time elapsed: 100.13746118545532 seconds\n",
      "Step 2900, loss: tensor(0.1988, grad_fn=<SubBackward0>), time elapsed: 103.82206916809082 seconds\n",
      "Step 3000, loss: tensor(0.2031, grad_fn=<SubBackward0>), time elapsed: 107.35184526443481 seconds\n",
      "Step 3100, loss: tensor(0.1991, grad_fn=<SubBackward0>), time elapsed: 110.86275815963745 seconds\n",
      "Step 3200, loss: tensor(0.2007, grad_fn=<SubBackward0>), time elapsed: 114.59797668457031 seconds\n",
      "Step 3300, loss: tensor(0.1970, grad_fn=<SubBackward0>), time elapsed: 118.09835839271545 seconds\n",
      "Step 3400, loss: tensor(0.1976, grad_fn=<SubBackward0>), time elapsed: 121.58106899261475 seconds\n",
      "Step 3500, loss: tensor(0.1977, grad_fn=<SubBackward0>), time elapsed: 125.31253981590271 seconds\n",
      "Step 3600, loss: tensor(0.1988, grad_fn=<SubBackward0>), time elapsed: 128.83867025375366 seconds\n",
      "Step 3700, loss: tensor(0.1987, grad_fn=<SubBackward0>), time elapsed: 132.3547990322113 seconds\n",
      "Step 3800, loss: tensor(0.1949, grad_fn=<SubBackward0>), time elapsed: 135.83586430549622 seconds\n",
      "Step 3900, loss: tensor(0.1952, grad_fn=<SubBackward0>), time elapsed: 139.5319573879242 seconds\n",
      "Step 4000, loss: tensor(0.1954, grad_fn=<SubBackward0>), time elapsed: 143.0390980243683 seconds\n",
      "Step 4100, loss: tensor(0.1939, grad_fn=<SubBackward0>), time elapsed: 146.56269788742065 seconds\n",
      "Step 4200, loss: tensor(0.1949, grad_fn=<SubBackward0>), time elapsed: 150.28274297714233 seconds\n",
      "Step 4300, loss: tensor(0.1873, grad_fn=<SubBackward0>), time elapsed: 153.7918839454651 seconds\n",
      "Step 4400, loss: tensor(0.1872, grad_fn=<SubBackward0>), time elapsed: 157.27411198616028 seconds\n",
      "Step 4500, loss: tensor(0.1851, grad_fn=<SubBackward0>), time elapsed: 161.01207852363586 seconds\n",
      "Step 4600, loss: tensor(0.1819, grad_fn=<SubBackward0>), time elapsed: 164.5403869152069 seconds\n",
      "Step 4700, loss: tensor(0.1789, grad_fn=<SubBackward0>), time elapsed: 168.03164768218994 seconds\n",
      "Step 4800, loss: tensor(0.1787, grad_fn=<SubBackward0>), time elapsed: 171.76936531066895 seconds\n",
      "Step 4900, loss: tensor(0.1798, grad_fn=<SubBackward0>), time elapsed: 175.3005712032318 seconds\n",
      "Step 5000, loss: tensor(0.1765, grad_fn=<SubBackward0>), time elapsed: 178.80356812477112 seconds\n",
      "Step 5100, loss: tensor(0.1735, grad_fn=<SubBackward0>), time elapsed: 182.5186643600464 seconds\n",
      "Step 5200, loss: tensor(0.1689, grad_fn=<SubBackward0>), time elapsed: 186.03491282463074 seconds\n",
      "Step 5300, loss: tensor(0.1654, grad_fn=<SubBackward0>), time elapsed: 189.54725670814514 seconds\n",
      "Step 5400, loss: tensor(0.1625, grad_fn=<SubBackward0>), time elapsed: 193.27010369300842 seconds\n",
      "Step 5500, loss: tensor(0.1593, grad_fn=<SubBackward0>), time elapsed: 196.81019854545593 seconds\n",
      "Step 5600, loss: tensor(0.1581, grad_fn=<SubBackward0>), time elapsed: 200.3016800880432 seconds\n",
      "Step 5700, loss: tensor(0.1575, grad_fn=<SubBackward0>), time elapsed: 203.99408721923828 seconds\n",
      "Step 5800, loss: tensor(0.1554, grad_fn=<SubBackward0>), time elapsed: 207.51627278327942 seconds\n",
      "Step 5900, loss: tensor(0.1552, grad_fn=<SubBackward0>), time elapsed: 211.03219604492188 seconds\n",
      "Step 6000, loss: tensor(0.1490, grad_fn=<SubBackward0>), time elapsed: 214.75379705429077 seconds\n",
      "Step 6100, loss: tensor(0.1514, grad_fn=<SubBackward0>), time elapsed: 218.2956190109253 seconds\n",
      "Step 6200, loss: tensor(0.1495, grad_fn=<SubBackward0>), time elapsed: 221.81679821014404 seconds\n",
      "Step 6300, loss: tensor(0.1513, grad_fn=<SubBackward0>), time elapsed: 225.5612554550171 seconds\n",
      "Step 6400, loss: tensor(0.1442, grad_fn=<SubBackward0>), time elapsed: 229.087087392807 seconds\n",
      "Step 6500, loss: tensor(0.1482, grad_fn=<SubBackward0>), time elapsed: 232.58561396598816 seconds\n",
      "Step 6600, loss: tensor(0.1482, grad_fn=<SubBackward0>), time elapsed: 236.10739374160767 seconds\n",
      "Step 6700, loss: tensor(0.1433, grad_fn=<SubBackward0>), time elapsed: 239.8465781211853 seconds\n",
      "Step 6800, loss: tensor(0.1513, grad_fn=<SubBackward0>), time elapsed: 243.38784909248352 seconds\n",
      "Step 6900, loss: tensor(0.1493, grad_fn=<SubBackward0>), time elapsed: 246.88524436950684 seconds\n",
      "Step 7000, loss: tensor(0.1542, grad_fn=<SubBackward0>), time elapsed: 250.5992305278778 seconds\n",
      "Step 7100, loss: tensor(0.1466, grad_fn=<SubBackward0>), time elapsed: 254.1381058692932 seconds\n",
      "Step 7200, loss: tensor(0.1476, grad_fn=<SubBackward0>), time elapsed: 257.6369984149933 seconds\n",
      "Step 7300, loss: tensor(0.1483, grad_fn=<SubBackward0>), time elapsed: 261.3806037902832 seconds\n",
      "Step 7400, loss: tensor(0.1462, grad_fn=<SubBackward0>), time elapsed: 264.90884590148926 seconds\n",
      "Step 7500, loss: tensor(0.1492, grad_fn=<SubBackward0>), time elapsed: 268.39642357826233 seconds\n",
      "Step 7600, loss: tensor(0.1513, grad_fn=<SubBackward0>), time elapsed: 272.146507024765 seconds\n",
      "Step 7700, loss: tensor(0.1512, grad_fn=<SubBackward0>), time elapsed: 275.68053579330444 seconds\n",
      "Step 7800, loss: tensor(0.1475, grad_fn=<SubBackward0>), time elapsed: 279.167533159256 seconds\n",
      "Step 7900, loss: tensor(0.1465, grad_fn=<SubBackward0>), time elapsed: 282.90228486061096 seconds\n",
      "Step 8000, loss: tensor(0.1475, grad_fn=<SubBackward0>), time elapsed: 286.4400808811188 seconds\n",
      "Step 8100, loss: tensor(0.1510, grad_fn=<SubBackward0>), time elapsed: 289.9744155406952 seconds\n",
      "Step 8200, loss: tensor(0.1508, grad_fn=<SubBackward0>), time elapsed: 293.7233521938324 seconds\n",
      "Step 8300, loss: tensor(0.1476, grad_fn=<SubBackward0>), time elapsed: 297.24151825904846 seconds\n",
      "Step 8400, loss: tensor(0.1466, grad_fn=<SubBackward0>), time elapsed: 300.7823281288147 seconds\n",
      "Step 8500, loss: tensor(0.1509, grad_fn=<SubBackward0>), time elapsed: 304.52408480644226 seconds\n",
      "Step 8600, loss: tensor(0.1551, grad_fn=<SubBackward0>), time elapsed: 308.0543622970581 seconds\n",
      "Step 8700, loss: tensor(0.1480, grad_fn=<SubBackward0>), time elapsed: 311.57527804374695 seconds\n",
      "Step 8800, loss: tensor(0.1418, grad_fn=<SubBackward0>), time elapsed: 315.28503823280334 seconds\n",
      "Step 8900, loss: tensor(0.1456, grad_fn=<SubBackward0>), time elapsed: 318.803076505661 seconds\n",
      "Step 9000, loss: tensor(0.1556, grad_fn=<SubBackward0>), time elapsed: 322.33108472824097 seconds\n",
      "Step 9100, loss: tensor(0.1450, grad_fn=<SubBackward0>), time elapsed: 325.8468849658966 seconds\n",
      "Step 9200, loss: tensor(0.1445, grad_fn=<SubBackward0>), time elapsed: 329.55722188949585 seconds\n",
      "Step 9300, loss: tensor(0.1432, grad_fn=<SubBackward0>), time elapsed: 333.06920981407166 seconds\n",
      "Step 9400, loss: tensor(0.1492, grad_fn=<SubBackward0>), time elapsed: 336.5897309780121 seconds\n",
      "Step 9500, loss: tensor(0.1459, grad_fn=<SubBackward0>), time elapsed: 340.34105229377747 seconds\n",
      "Step 9600, loss: tensor(0.1484, grad_fn=<SubBackward0>), time elapsed: 343.831622838974 seconds\n",
      "Step 9700, loss: tensor(0.1457, grad_fn=<SubBackward0>), time elapsed: 347.34989619255066 seconds\n",
      "Step 9800, loss: tensor(0.1485, grad_fn=<SubBackward0>), time elapsed: 351.04978489875793 seconds\n",
      "Step 9900, loss: tensor(0.1468, grad_fn=<SubBackward0>), time elapsed: 354.5763807296753 seconds\n",
      "Step 10000, loss: tensor(0.1465, grad_fn=<SubBackward0>), time elapsed: 358.10056257247925 seconds\n",
      "Step 10100, loss: tensor(0.1474, grad_fn=<SubBackward0>), time elapsed: 361.8148937225342 seconds\n",
      "Step 10200, loss: tensor(0.1447, grad_fn=<SubBackward0>), time elapsed: 365.32815623283386 seconds\n",
      "Step 10300, loss: tensor(0.1467, grad_fn=<SubBackward0>), time elapsed: 368.8314187526703 seconds\n",
      "Step 10400, loss: tensor(0.1507, grad_fn=<SubBackward0>), time elapsed: 372.6139051914215 seconds\n",
      "Step 10500, loss: tensor(0.1487, grad_fn=<SubBackward0>), time elapsed: 376.1228926181793 seconds\n",
      "Step 10600, loss: tensor(0.1443, grad_fn=<SubBackward0>), time elapsed: 379.62853145599365 seconds\n",
      "Step 10700, loss: tensor(0.1432, grad_fn=<SubBackward0>), time elapsed: 383.39378237724304 seconds\n",
      "Step 10800, loss: tensor(0.1469, grad_fn=<SubBackward0>), time elapsed: 386.95499563217163 seconds\n",
      "Step 10900, loss: tensor(0.1470, grad_fn=<SubBackward0>), time elapsed: 390.4906234741211 seconds\n",
      "Step 11000, loss: tensor(0.1418, grad_fn=<SubBackward0>), time elapsed: 394.211740732193 seconds\n",
      "Step 11100, loss: tensor(0.1449, grad_fn=<SubBackward0>), time elapsed: 397.77101969718933 seconds\n",
      "Step 11200, loss: tensor(0.1461, grad_fn=<SubBackward0>), time elapsed: 401.33335065841675 seconds\n",
      "Step 11300, loss: tensor(0.1473, grad_fn=<SubBackward0>), time elapsed: 405.1186685562134 seconds\n",
      "Step 11400, loss: tensor(0.1477, grad_fn=<SubBackward0>), time elapsed: 408.66120409965515 seconds\n",
      "Step 11500, loss: tensor(0.1422, grad_fn=<SubBackward0>), time elapsed: 412.2208466529846 seconds\n",
      "Step 11600, loss: tensor(0.1465, grad_fn=<SubBackward0>), time elapsed: 415.745450258255 seconds\n",
      "Step 11700, loss: tensor(0.1481, grad_fn=<SubBackward0>), time elapsed: 419.54737091064453 seconds\n",
      "Step 11800, loss: tensor(0.1432, grad_fn=<SubBackward0>), time elapsed: 423.15106201171875 seconds\n",
      "Step 11900, loss: tensor(0.1473, grad_fn=<SubBackward0>), time elapsed: 426.75894355773926 seconds\n",
      "Step 12000, loss: tensor(0.1421, grad_fn=<SubBackward0>), time elapsed: 430.6036286354065 seconds\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(1.0820, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(1.0281, grad_fn=<SubBackward0>), time elapsed: 3.607551336288452 seconds\n",
      "Step 200, loss: tensor(0.9198, grad_fn=<SubBackward0>), time elapsed: 7.114226341247559 seconds\n",
      "Step 300, loss: tensor(0.7993, grad_fn=<SubBackward0>), time elapsed: 10.823481798171997 seconds\n",
      "Step 400, loss: tensor(0.7043, grad_fn=<SubBackward0>), time elapsed: 14.321379661560059 seconds\n",
      "Step 500, loss: tensor(0.6420, grad_fn=<SubBackward0>), time elapsed: 17.821983814239502 seconds\n",
      "Step 600, loss: tensor(0.5815, grad_fn=<SubBackward0>), time elapsed: 21.5376980304718 seconds\n",
      "Step 700, loss: tensor(0.5366, grad_fn=<SubBackward0>), time elapsed: 25.02492594718933 seconds\n",
      "Step 800, loss: tensor(0.4979, grad_fn=<SubBackward0>), time elapsed: 28.555362939834595 seconds\n",
      "Step 900, loss: tensor(0.4603, grad_fn=<SubBackward0>), time elapsed: 32.307552337646484 seconds\n",
      "Step 1000, loss: tensor(0.4202, grad_fn=<SubBackward0>), time elapsed: 35.825867891311646 seconds\n",
      "Step 1100, loss: tensor(0.3914, grad_fn=<SubBackward0>), time elapsed: 39.30446243286133 seconds\n",
      "Step 1200, loss: tensor(0.3505, grad_fn=<SubBackward0>), time elapsed: 42.991875648498535 seconds\n",
      "Step 1300, loss: tensor(0.3184, grad_fn=<SubBackward0>), time elapsed: 46.47540616989136 seconds\n",
      "Step 1400, loss: tensor(0.2863, grad_fn=<SubBackward0>), time elapsed: 49.9685263633728 seconds\n",
      "Step 1500, loss: tensor(0.2663, grad_fn=<SubBackward0>), time elapsed: 53.69526672363281 seconds\n",
      "Step 1600, loss: tensor(0.2430, grad_fn=<SubBackward0>), time elapsed: 57.21800112724304 seconds\n",
      "Step 1700, loss: tensor(0.2274, grad_fn=<SubBackward0>), time elapsed: 60.759055614471436 seconds\n",
      "Step 1800, loss: tensor(0.2166, grad_fn=<SubBackward0>), time elapsed: 64.47933888435364 seconds\n",
      "Step 1900, loss: tensor(0.2109, grad_fn=<SubBackward0>), time elapsed: 67.98301649093628 seconds\n",
      "Step 2000, loss: tensor(0.2028, grad_fn=<SubBackward0>), time elapsed: 71.49242687225342 seconds\n",
      "Step 2100, loss: tensor(0.2001, grad_fn=<SubBackward0>), time elapsed: 75.22169828414917 seconds\n",
      "Step 2200, loss: tensor(0.1914, grad_fn=<SubBackward0>), time elapsed: 78.71827459335327 seconds\n",
      "Step 2300, loss: tensor(0.1909, grad_fn=<SubBackward0>), time elapsed: 82.21419858932495 seconds\n",
      "Step 2400, loss: tensor(0.1904, grad_fn=<SubBackward0>), time elapsed: 85.89784383773804 seconds\n",
      "Step 2500, loss: tensor(0.1880, grad_fn=<SubBackward0>), time elapsed: 89.4217631816864 seconds\n",
      "Step 2600, loss: tensor(0.1880, grad_fn=<SubBackward0>), time elapsed: 92.93715405464172 seconds\n",
      "Step 2700, loss: tensor(0.1807, grad_fn=<SubBackward0>), time elapsed: 96.61845874786377 seconds\n",
      "Step 2800, loss: tensor(0.1854, grad_fn=<SubBackward0>), time elapsed: 100.13314270973206 seconds\n",
      "Step 2900, loss: tensor(0.1775, grad_fn=<SubBackward0>), time elapsed: 103.64074444770813 seconds\n",
      "Step 3000, loss: tensor(0.1809, grad_fn=<SubBackward0>), time elapsed: 107.3470721244812 seconds\n",
      "Step 3100, loss: tensor(0.1784, grad_fn=<SubBackward0>), time elapsed: 110.81415796279907 seconds\n",
      "Step 3200, loss: tensor(0.1829, grad_fn=<SubBackward0>), time elapsed: 114.33875489234924 seconds\n",
      "Step 3300, loss: tensor(0.1784, grad_fn=<SubBackward0>), time elapsed: 117.83357572555542 seconds\n",
      "Step 3400, loss: tensor(0.1784, grad_fn=<SubBackward0>), time elapsed: 121.51887536048889 seconds\n",
      "Step 3500, loss: tensor(0.1791, grad_fn=<SubBackward0>), time elapsed: 125.04158592224121 seconds\n",
      "Step 3600, loss: tensor(0.1821, grad_fn=<SubBackward0>), time elapsed: 128.5440011024475 seconds\n",
      "Step 3700, loss: tensor(0.1815, grad_fn=<SubBackward0>), time elapsed: 132.25490808486938 seconds\n",
      "Step 3800, loss: tensor(0.1796, grad_fn=<SubBackward0>), time elapsed: 135.76129484176636 seconds\n",
      "Step 3900, loss: tensor(0.1792, grad_fn=<SubBackward0>), time elapsed: 139.24313354492188 seconds\n",
      "Step 4000, loss: tensor(0.1788, grad_fn=<SubBackward0>), time elapsed: 142.94516563415527 seconds\n",
      "Step 4100, loss: tensor(0.1798, grad_fn=<SubBackward0>), time elapsed: 146.42937707901 seconds\n",
      "Step 4200, loss: tensor(0.1815, grad_fn=<SubBackward0>), time elapsed: 149.94210600852966 seconds\n",
      "Step 4300, loss: tensor(0.1813, grad_fn=<SubBackward0>), time elapsed: 153.6495656967163 seconds\n",
      "Step 4400, loss: tensor(0.1814, grad_fn=<SubBackward0>), time elapsed: 157.13827896118164 seconds\n",
      "Step 4500, loss: tensor(0.1825, grad_fn=<SubBackward0>), time elapsed: 160.62580180168152 seconds\n",
      "Step 4600, loss: tensor(0.1832, grad_fn=<SubBackward0>), time elapsed: 164.3379590511322 seconds\n",
      "Step 4700, loss: tensor(0.1832, grad_fn=<SubBackward0>), time elapsed: 167.889888048172 seconds\n",
      "Step 4800, loss: tensor(0.1820, grad_fn=<SubBackward0>), time elapsed: 171.417906999588 seconds\n",
      "Step 4900, loss: tensor(0.1827, grad_fn=<SubBackward0>), time elapsed: 175.13647985458374 seconds\n",
      "Step 5000, loss: tensor(0.1810, grad_fn=<SubBackward0>), time elapsed: 178.63688802719116 seconds\n",
      "Step 5100, loss: tensor(0.1809, grad_fn=<SubBackward0>), time elapsed: 182.13329029083252 seconds\n",
      "Step 5200, loss: tensor(0.1828, grad_fn=<SubBackward0>), time elapsed: 185.86543107032776 seconds\n",
      "Step 5300, loss: tensor(0.1808, grad_fn=<SubBackward0>), time elapsed: 189.38060903549194 seconds\n",
      "Step 5400, loss: tensor(0.1812, grad_fn=<SubBackward0>), time elapsed: 192.91926670074463 seconds\n",
      "Step 5500, loss: tensor(0.1787, grad_fn=<SubBackward0>), time elapsed: 196.64219737052917 seconds\n",
      "Step 5600, loss: tensor(0.1782, grad_fn=<SubBackward0>), time elapsed: 200.15680623054504 seconds\n",
      "Step 5700, loss: tensor(0.1793, grad_fn=<SubBackward0>), time elapsed: 203.66282176971436 seconds\n",
      "Step 5800, loss: tensor(0.1769, grad_fn=<SubBackward0>), time elapsed: 207.41183853149414 seconds\n",
      "Step 5900, loss: tensor(0.1760, grad_fn=<SubBackward0>), time elapsed: 210.9510805606842 seconds\n",
      "Step 6000, loss: tensor(0.1765, grad_fn=<SubBackward0>), time elapsed: 214.49306273460388 seconds\n",
      "Step 6100, loss: tensor(0.1736, grad_fn=<SubBackward0>), time elapsed: 218.0268361568451 seconds\n",
      "Step 6200, loss: tensor(0.1777, grad_fn=<SubBackward0>), time elapsed: 221.73819065093994 seconds\n",
      "Step 6300, loss: tensor(0.1741, grad_fn=<SubBackward0>), time elapsed: 225.26626634597778 seconds\n",
      "Step 6400, loss: tensor(0.1722, grad_fn=<SubBackward0>), time elapsed: 228.80576515197754 seconds\n",
      "Step 6500, loss: tensor(0.1728, grad_fn=<SubBackward0>), time elapsed: 232.5100326538086 seconds\n",
      "Step 6600, loss: tensor(0.1736, grad_fn=<SubBackward0>), time elapsed: 236.01100039482117 seconds\n",
      "Step 6700, loss: tensor(0.1722, grad_fn=<SubBackward0>), time elapsed: 239.50560903549194 seconds\n",
      "Step 6800, loss: tensor(0.1727, grad_fn=<SubBackward0>), time elapsed: 243.2409644126892 seconds\n",
      "Step 6900, loss: tensor(0.1680, grad_fn=<SubBackward0>), time elapsed: 246.732097864151 seconds\n",
      "Step 7000, loss: tensor(0.1688, grad_fn=<SubBackward0>), time elapsed: 250.2585551738739 seconds\n",
      "Step 7100, loss: tensor(0.1705, grad_fn=<SubBackward0>), time elapsed: 253.99259209632874 seconds\n",
      "Step 7200, loss: tensor(0.1664, grad_fn=<SubBackward0>), time elapsed: 257.512943983078 seconds\n",
      "Step 7300, loss: tensor(0.1677, grad_fn=<SubBackward0>), time elapsed: 261.05347299575806 seconds\n",
      "Step 7400, loss: tensor(0.1651, grad_fn=<SubBackward0>), time elapsed: 264.77218651771545 seconds\n",
      "Step 7500, loss: tensor(0.1671, grad_fn=<SubBackward0>), time elapsed: 268.26732301712036 seconds\n",
      "Step 7600, loss: tensor(0.1617, grad_fn=<SubBackward0>), time elapsed: 271.7756578922272 seconds\n",
      "Step 7700, loss: tensor(0.1646, grad_fn=<SubBackward0>), time elapsed: 275.49390506744385 seconds\n",
      "Step 7800, loss: tensor(0.1631, grad_fn=<SubBackward0>), time elapsed: 278.9858841896057 seconds\n",
      "Step 7900, loss: tensor(0.1588, grad_fn=<SubBackward0>), time elapsed: 282.5009677410126 seconds\n",
      "Step 8000, loss: tensor(0.1618, grad_fn=<SubBackward0>), time elapsed: 286.2351839542389 seconds\n",
      "Step 8100, loss: tensor(0.1583, grad_fn=<SubBackward0>), time elapsed: 289.7688777446747 seconds\n",
      "Step 8200, loss: tensor(0.1566, grad_fn=<SubBackward0>), time elapsed: 293.30094146728516 seconds\n",
      "Step 8300, loss: tensor(0.1584, grad_fn=<SubBackward0>), time elapsed: 296.99652123451233 seconds\n",
      "Step 8400, loss: tensor(0.1532, grad_fn=<SubBackward0>), time elapsed: 300.5392861366272 seconds\n",
      "Step 8500, loss: tensor(0.1525, grad_fn=<SubBackward0>), time elapsed: 304.04668045043945 seconds\n",
      "Step 8600, loss: tensor(0.1515, grad_fn=<SubBackward0>), time elapsed: 307.5784742832184 seconds\n",
      "Step 8700, loss: tensor(0.1548, grad_fn=<SubBackward0>), time elapsed: 311.2934834957123 seconds\n",
      "Step 8800, loss: tensor(0.1555, grad_fn=<SubBackward0>), time elapsed: 314.8076345920563 seconds\n",
      "Step 8900, loss: tensor(0.1475, grad_fn=<SubBackward0>), time elapsed: 318.3135275840759 seconds\n",
      "Step 9000, loss: tensor(0.1539, grad_fn=<SubBackward0>), time elapsed: 322.03874945640564 seconds\n",
      "Step 9100, loss: tensor(0.1501, grad_fn=<SubBackward0>), time elapsed: 325.54772543907166 seconds\n",
      "Step 9200, loss: tensor(0.1548, grad_fn=<SubBackward0>), time elapsed: 329.10153222084045 seconds\n",
      "Step 9300, loss: tensor(0.1539, grad_fn=<SubBackward0>), time elapsed: 332.84367656707764 seconds\n",
      "Step 9400, loss: tensor(0.1522, grad_fn=<SubBackward0>), time elapsed: 336.39817810058594 seconds\n",
      "Step 9500, loss: tensor(0.1479, grad_fn=<SubBackward0>), time elapsed: 339.88970041275024 seconds\n",
      "Step 9600, loss: tensor(0.1495, grad_fn=<SubBackward0>), time elapsed: 343.6217269897461 seconds\n",
      "Step 9700, loss: tensor(0.1555, grad_fn=<SubBackward0>), time elapsed: 347.1602694988251 seconds\n",
      "Step 9800, loss: tensor(0.1504, grad_fn=<SubBackward0>), time elapsed: 350.6562306880951 seconds\n",
      "Step 9900, loss: tensor(0.1513, grad_fn=<SubBackward0>), time elapsed: 354.39972734451294 seconds\n",
      "Step 10000, loss: tensor(0.1508, grad_fn=<SubBackward0>), time elapsed: 357.9197664260864 seconds\n",
      "Step 10100, loss: tensor(0.1474, grad_fn=<SubBackward0>), time elapsed: 361.45532178878784 seconds\n",
      "Step 10200, loss: tensor(0.1464, grad_fn=<SubBackward0>), time elapsed: 365.2065734863281 seconds\n",
      "Step 10300, loss: tensor(0.1479, grad_fn=<SubBackward0>), time elapsed: 368.7325119972229 seconds\n",
      "Step 10400, loss: tensor(0.1484, grad_fn=<SubBackward0>), time elapsed: 372.2763521671295 seconds\n",
      "Step 10500, loss: tensor(0.1486, grad_fn=<SubBackward0>), time elapsed: 376.03006410598755 seconds\n",
      "Step 10600, loss: tensor(0.1397, grad_fn=<SubBackward0>), time elapsed: 379.5584759712219 seconds\n",
      "Step 10700, loss: tensor(0.1467, grad_fn=<SubBackward0>), time elapsed: 383.1346755027771 seconds\n",
      "Step 10800, loss: tensor(0.1405, grad_fn=<SubBackward0>), time elapsed: 386.8777129650116 seconds\n",
      "Step 10900, loss: tensor(0.1435, grad_fn=<SubBackward0>), time elapsed: 390.42251682281494 seconds\n",
      "Step 11000, loss: tensor(0.1371, grad_fn=<SubBackward0>), time elapsed: 394.0101969242096 seconds\n",
      "Step 11100, loss: tensor(0.1383, grad_fn=<SubBackward0>), time elapsed: 397.53039169311523 seconds\n",
      "Step 11200, loss: tensor(0.1392, grad_fn=<SubBackward0>), time elapsed: 401.2341375350952 seconds\n",
      "Step 11300, loss: tensor(0.1445, grad_fn=<SubBackward0>), time elapsed: 404.81556844711304 seconds\n",
      "Step 11400, loss: tensor(0.1342, grad_fn=<SubBackward0>), time elapsed: 408.3417365550995 seconds\n",
      "Step 11500, loss: tensor(0.1418, grad_fn=<SubBackward0>), time elapsed: 412.1179325580597 seconds\n",
      "Step 11600, loss: tensor(0.1360, grad_fn=<SubBackward0>), time elapsed: 415.66507482528687 seconds\n",
      "Step 11700, loss: tensor(0.1349, grad_fn=<SubBackward0>), time elapsed: 419.212984085083 seconds\n",
      "Step 11800, loss: tensor(0.1350, grad_fn=<SubBackward0>), time elapsed: 422.9509222507477 seconds\n",
      "Step 11900, loss: tensor(0.1342, grad_fn=<SubBackward0>), time elapsed: 426.54736518859863 seconds\n",
      "Step 12000, loss: tensor(0.1388, grad_fn=<SubBackward0>), time elapsed: 430.15990376472473 seconds\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(1.1435, grad_fn=<SubBackward0>), time elapsed: 0.03488326072692871 seconds\n",
      "Step 100, loss: tensor(1.0219, grad_fn=<SubBackward0>), time elapsed: 3.770111083984375 seconds\n",
      "Step 200, loss: tensor(0.8766, grad_fn=<SubBackward0>), time elapsed: 7.2673351764678955 seconds\n",
      "Step 300, loss: tensor(0.7414, grad_fn=<SubBackward0>), time elapsed: 10.796419858932495 seconds\n",
      "Step 400, loss: tensor(0.6166, grad_fn=<SubBackward0>), time elapsed: 14.47688627243042 seconds\n",
      "Step 500, loss: tensor(0.5142, grad_fn=<SubBackward0>), time elapsed: 17.974830389022827 seconds\n",
      "Step 600, loss: tensor(0.4667, grad_fn=<SubBackward0>), time elapsed: 21.4605815410614 seconds\n",
      "Step 700, loss: tensor(0.4004, grad_fn=<SubBackward0>), time elapsed: 25.186843156814575 seconds\n",
      "Step 800, loss: tensor(0.3382, grad_fn=<SubBackward0>), time elapsed: 28.70294165611267 seconds\n",
      "Step 900, loss: tensor(0.2955, grad_fn=<SubBackward0>), time elapsed: 32.19865679740906 seconds\n",
      "Step 1000, loss: tensor(0.2563, grad_fn=<SubBackward0>), time elapsed: 35.92510437965393 seconds\n",
      "Step 1100, loss: tensor(0.2370, grad_fn=<SubBackward0>), time elapsed: 39.44827127456665 seconds\n",
      "Step 1200, loss: tensor(0.2102, grad_fn=<SubBackward0>), time elapsed: 42.98093509674072 seconds\n",
      "Step 1300, loss: tensor(0.2052, grad_fn=<SubBackward0>), time elapsed: 46.66730761528015 seconds\n",
      "Step 1400, loss: tensor(0.2010, grad_fn=<SubBackward0>), time elapsed: 50.129913330078125 seconds\n",
      "Step 1500, loss: tensor(0.1946, grad_fn=<SubBackward0>), time elapsed: 53.629154920578 seconds\n",
      "Step 1600, loss: tensor(0.1885, grad_fn=<SubBackward0>), time elapsed: 57.32390117645264 seconds\n",
      "Step 1700, loss: tensor(0.1856, grad_fn=<SubBackward0>), time elapsed: 60.828683614730835 seconds\n",
      "Step 1800, loss: tensor(0.1858, grad_fn=<SubBackward0>), time elapsed: 64.31352591514587 seconds\n",
      "Step 1900, loss: tensor(0.1773, grad_fn=<SubBackward0>), time elapsed: 67.97081518173218 seconds\n",
      "Step 2000, loss: tensor(0.1729, grad_fn=<SubBackward0>), time elapsed: 71.44914531707764 seconds\n",
      "Step 2100, loss: tensor(0.1683, grad_fn=<SubBackward0>), time elapsed: 74.92171001434326 seconds\n",
      "Step 2200, loss: tensor(0.1661, grad_fn=<SubBackward0>), time elapsed: 78.59698700904846 seconds\n",
      "Step 2300, loss: tensor(0.1569, grad_fn=<SubBackward0>), time elapsed: 82.05218982696533 seconds\n",
      "Step 2400, loss: tensor(0.1588, grad_fn=<SubBackward0>), time elapsed: 85.55417704582214 seconds\n",
      "Step 2500, loss: tensor(0.1536, grad_fn=<SubBackward0>), time elapsed: 89.23261427879333 seconds\n",
      "Step 2600, loss: tensor(0.1508, grad_fn=<SubBackward0>), time elapsed: 92.69297623634338 seconds\n",
      "Step 2700, loss: tensor(0.1448, grad_fn=<SubBackward0>), time elapsed: 96.19687986373901 seconds\n",
      "Step 2800, loss: tensor(0.1413, grad_fn=<SubBackward0>), time elapsed: 99.6856472492218 seconds\n",
      "Step 2900, loss: tensor(0.1395, grad_fn=<SubBackward0>), time elapsed: 103.33450889587402 seconds\n",
      "Step 3000, loss: tensor(0.1413, grad_fn=<SubBackward0>), time elapsed: 106.8403639793396 seconds\n",
      "Step 3100, loss: tensor(0.1326, grad_fn=<SubBackward0>), time elapsed: 110.3014543056488 seconds\n",
      "Step 3200, loss: tensor(0.1354, grad_fn=<SubBackward0>), time elapsed: 113.98516249656677 seconds\n",
      "Step 3300, loss: tensor(0.1319, grad_fn=<SubBackward0>), time elapsed: 117.47165322303772 seconds\n",
      "Step 3400, loss: tensor(0.1290, grad_fn=<SubBackward0>), time elapsed: 120.97892260551453 seconds\n",
      "Step 3500, loss: tensor(0.1322, grad_fn=<SubBackward0>), time elapsed: 124.68332648277283 seconds\n",
      "Step 3600, loss: tensor(0.1351, grad_fn=<SubBackward0>), time elapsed: 128.1603343486786 seconds\n",
      "Step 3700, loss: tensor(0.1359, grad_fn=<SubBackward0>), time elapsed: 131.63498258590698 seconds\n",
      "Step 3800, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 135.30304598808289 seconds\n",
      "Step 3900, loss: tensor(0.1296, grad_fn=<SubBackward0>), time elapsed: 138.79400658607483 seconds\n",
      "Step 4000, loss: tensor(0.1288, grad_fn=<SubBackward0>), time elapsed: 142.2970678806305 seconds\n",
      "Step 4100, loss: tensor(0.1268, grad_fn=<SubBackward0>), time elapsed: 145.98999333381653 seconds\n",
      "Step 4200, loss: tensor(0.1257, grad_fn=<SubBackward0>), time elapsed: 149.4831883907318 seconds\n",
      "Step 4300, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 152.9892063140869 seconds\n",
      "Step 4400, loss: tensor(0.1267, grad_fn=<SubBackward0>), time elapsed: 156.6753077507019 seconds\n",
      "Step 4500, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 160.16789388656616 seconds\n",
      "Step 4600, loss: tensor(0.1255, grad_fn=<SubBackward0>), time elapsed: 163.63565826416016 seconds\n",
      "Step 4700, loss: tensor(0.1300, grad_fn=<SubBackward0>), time elapsed: 167.29802775382996 seconds\n",
      "Step 4800, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 170.77924060821533 seconds\n",
      "Step 4900, loss: tensor(0.1254, grad_fn=<SubBackward0>), time elapsed: 174.2670271396637 seconds\n",
      "Step 5000, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 177.93798208236694 seconds\n",
      "Step 5100, loss: tensor(0.1210, grad_fn=<SubBackward0>), time elapsed: 181.4082899093628 seconds\n",
      "Step 5200, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 184.88124871253967 seconds\n",
      "Step 5300, loss: tensor(0.1267, grad_fn=<SubBackward0>), time elapsed: 188.5760543346405 seconds\n",
      "Step 5400, loss: tensor(0.1246, grad_fn=<SubBackward0>), time elapsed: 192.0443296432495 seconds\n",
      "Step 5500, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 195.51018691062927 seconds\n",
      "Step 5600, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 198.96115517616272 seconds\n",
      "Step 5700, loss: tensor(0.1246, grad_fn=<SubBackward0>), time elapsed: 202.64636373519897 seconds\n",
      "Step 5800, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 206.16446924209595 seconds\n",
      "Step 5900, loss: tensor(0.1288, grad_fn=<SubBackward0>), time elapsed: 209.66699934005737 seconds\n",
      "Step 6000, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 213.35772943496704 seconds\n",
      "Step 6100, loss: tensor(0.1275, grad_fn=<SubBackward0>), time elapsed: 216.85531330108643 seconds\n",
      "Step 6200, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 220.36945104599 seconds\n",
      "Step 6300, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 224.06240797042847 seconds\n",
      "Step 6400, loss: tensor(0.1236, grad_fn=<SubBackward0>), time elapsed: 227.55681896209717 seconds\n",
      "Step 6500, loss: tensor(0.1269, grad_fn=<SubBackward0>), time elapsed: 231.01854729652405 seconds\n",
      "Step 6600, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 234.71656227111816 seconds\n",
      "Step 6700, loss: tensor(0.1263, grad_fn=<SubBackward0>), time elapsed: 238.19183325767517 seconds\n",
      "Step 6800, loss: tensor(0.1235, grad_fn=<SubBackward0>), time elapsed: 241.6573359966278 seconds\n",
      "Step 6900, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 245.33190941810608 seconds\n",
      "Step 7000, loss: tensor(0.1264, grad_fn=<SubBackward0>), time elapsed: 248.83342146873474 seconds\n",
      "Step 7100, loss: tensor(0.1212, grad_fn=<SubBackward0>), time elapsed: 252.33455801010132 seconds\n",
      "Step 7200, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 256.050683259964 seconds\n",
      "Step 7300, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 259.5264718532562 seconds\n",
      "Step 7400, loss: tensor(0.1265, grad_fn=<SubBackward0>), time elapsed: 263.0013334751129 seconds\n",
      "Step 7500, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 266.68131613731384 seconds\n",
      "Step 7600, loss: tensor(0.1215, grad_fn=<SubBackward0>), time elapsed: 270.1884262561798 seconds\n",
      "Step 7700, loss: tensor(0.1177, grad_fn=<SubBackward0>), time elapsed: 273.70292258262634 seconds\n",
      "Step 7800, loss: tensor(0.1212, grad_fn=<SubBackward0>), time elapsed: 277.37595438957214 seconds\n",
      "Step 7900, loss: tensor(0.1204, grad_fn=<SubBackward0>), time elapsed: 280.83979654312134 seconds\n",
      "Step 8000, loss: tensor(0.1228, grad_fn=<SubBackward0>), time elapsed: 284.3016550540924 seconds\n",
      "Step 8100, loss: tensor(0.1242, grad_fn=<SubBackward0>), time elapsed: 287.7875304222107 seconds\n",
      "Step 8200, loss: tensor(0.1218, grad_fn=<SubBackward0>), time elapsed: 291.48431420326233 seconds\n",
      "Step 8300, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 294.93886280059814 seconds\n",
      "Step 8400, loss: tensor(0.1181, grad_fn=<SubBackward0>), time elapsed: 298.42220425605774 seconds\n",
      "Step 8500, loss: tensor(0.1246, grad_fn=<SubBackward0>), time elapsed: 302.13606667518616 seconds\n",
      "Step 8600, loss: tensor(0.1236, grad_fn=<SubBackward0>), time elapsed: 305.6122934818268 seconds\n",
      "Step 8700, loss: tensor(0.1201, grad_fn=<SubBackward0>), time elapsed: 309.1112959384918 seconds\n",
      "Step 8800, loss: tensor(0.1215, grad_fn=<SubBackward0>), time elapsed: 312.8212661743164 seconds\n",
      "Step 8900, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 316.29765605926514 seconds\n",
      "Step 9000, loss: tensor(0.1213, grad_fn=<SubBackward0>), time elapsed: 319.81809067726135 seconds\n",
      "Step 9100, loss: tensor(0.1163, grad_fn=<SubBackward0>), time elapsed: 323.5240092277527 seconds\n",
      "Step 9200, loss: tensor(0.1219, grad_fn=<SubBackward0>), time elapsed: 327.04846024513245 seconds\n",
      "Step 9300, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 330.5727915763855 seconds\n",
      "Step 9400, loss: tensor(0.1272, grad_fn=<SubBackward0>), time elapsed: 334.2978081703186 seconds\n",
      "Step 9500, loss: tensor(0.1206, grad_fn=<SubBackward0>), time elapsed: 337.8120675086975 seconds\n",
      "Step 9600, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 341.3048930168152 seconds\n",
      "Step 9700, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 345.0132145881653 seconds\n",
      "Step 9800, loss: tensor(0.1218, grad_fn=<SubBackward0>), time elapsed: 348.5397262573242 seconds\n",
      "Step 9900, loss: tensor(0.1211, grad_fn=<SubBackward0>), time elapsed: 352.061155796051 seconds\n",
      "Step 10000, loss: tensor(0.1216, grad_fn=<SubBackward0>), time elapsed: 355.75561141967773 seconds\n",
      "Step 10100, loss: tensor(0.1229, grad_fn=<SubBackward0>), time elapsed: 359.2511451244354 seconds\n",
      "Step 10200, loss: tensor(0.1262, grad_fn=<SubBackward0>), time elapsed: 362.7614703178406 seconds\n",
      "Step 10300, loss: tensor(0.1209, grad_fn=<SubBackward0>), time elapsed: 366.4611189365387 seconds\n",
      "Step 10400, loss: tensor(0.1183, grad_fn=<SubBackward0>), time elapsed: 369.9743173122406 seconds\n",
      "Step 10500, loss: tensor(0.1186, grad_fn=<SubBackward0>), time elapsed: 373.4715096950531 seconds\n",
      "Step 10600, loss: tensor(0.1194, grad_fn=<SubBackward0>), time elapsed: 376.9833312034607 seconds\n",
      "Step 10700, loss: tensor(0.1221, grad_fn=<SubBackward0>), time elapsed: 380.70921778678894 seconds\n",
      "Step 10800, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 384.23450326919556 seconds\n",
      "Step 10900, loss: tensor(0.1214, grad_fn=<SubBackward0>), time elapsed: 387.7155375480652 seconds\n",
      "Step 11000, loss: tensor(0.1170, grad_fn=<SubBackward0>), time elapsed: 391.422504901886 seconds\n",
      "Step 11100, loss: tensor(0.1181, grad_fn=<SubBackward0>), time elapsed: 394.91770219802856 seconds\n",
      "Step 11200, loss: tensor(0.1233, grad_fn=<SubBackward0>), time elapsed: 398.4300289154053 seconds\n",
      "Step 11300, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 402.16988706588745 seconds\n",
      "Step 11400, loss: tensor(0.1196, grad_fn=<SubBackward0>), time elapsed: 405.709276676178 seconds\n",
      "Step 11500, loss: tensor(0.1211, grad_fn=<SubBackward0>), time elapsed: 409.24177527427673 seconds\n",
      "Step 11600, loss: tensor(0.1223, grad_fn=<SubBackward0>), time elapsed: 413.00892448425293 seconds\n",
      "Step 11700, loss: tensor(0.1199, grad_fn=<SubBackward0>), time elapsed: 416.5715181827545 seconds\n",
      "Step 11800, loss: tensor(0.1182, grad_fn=<SubBackward0>), time elapsed: 420.14592719078064 seconds\n",
      "Step 11900, loss: tensor(0.1197, grad_fn=<SubBackward0>), time elapsed: 423.93831157684326 seconds\n",
      "Step 12000, loss: tensor(0.1203, grad_fn=<SubBackward0>), time elapsed: 427.52855229377747 seconds\n"
     ]
    }
   ],
   "source": [
    "#train on diffusion data\n",
    "n, na = 4, 1 # number of data and ancilla qubits\n",
    "T = 20 # number of diffusion steps\n",
    "L = 6 # layer of backward PQC\n",
    "Ndata = 1000 # number of data in the training data set\n",
    "epochs = 12001 # number of training epochs\n",
    "\n",
    "#gen random samples at t = T\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_T = diffModel.HaarSampleGeneration(Ndata, seed = 22)\n",
    "\n",
    "#load diffusion process\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "model.set_diffusionSet(states_diff)\n",
    "\n",
    "params_total = []\n",
    "loss_hist_total = []\n",
    "\n",
    "for t in range(T - 1, -1, -1):\n",
    "    params_tot = np.load('params_total.npy')\n",
    "    for tt in range(t+1, T):\n",
    "        print(tt)\n",
    "        params_tot[tt] = np.load('paramsandloss_squaremodel/params_t%d.npy'%tt)\n",
    "    \n",
    "    params, loss_hist = Training_t(model, t, inputs_T, params_tot, Ndata, epochs)\n",
    "\n",
    "    np.save('paramsandloss_squaremodel/params_t%d'%t, params.detach().numpy())\n",
    "    np.save('paramsandloss_squaremodel/loss_t%d'%t, loss_hist.detach().numpy())\n",
    "    \n",
    "    #params_total = np.append(params_total, params, axis = 1)\n",
    "    #loss_hist_total = np.append(loss_hist_total, loss_hist, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, na = 4, 1\n",
    "T = 20\n",
    "L = 6\n",
    "Ndata = 2000\n",
    "epochs = 20000 + 1\n",
    "\n",
    "params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "loss_tot = np.zeros((T, epochs))\n",
    "f0_tot = np.zeros((T, epochs))\n",
    "\n",
    "for t in range(T):\n",
    "    params_tot[t] = np.load('paramsandloss_squaremodel/params_t%d.npy'%t)\n",
    "    loss_tot[t] = np.load('paramsandloss_squaremodel/loss_t%d.npy'%t)\n",
    "    \n",
    "\n",
    "np.save(\"params_total_2000Ndata_20kEpochs\", params_tot)\n",
    "np.save(\"loss_tot_2000Ndata_20kEpochs\", loss_tot)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fd8a623620>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create random 16 pixel data\n",
    "Ndata = 50\n",
    "n = 4\n",
    "random_images = np.zeros((Ndata, 4, 4))\n",
    "amplitude_vals = np.zeros(Ndata)\n",
    "\n",
    "for i in range(0, Ndata):\n",
    "    random_images[i] = np.random.rand(4,4)\n",
    "    amplitude_vals[i] = np.sum(random_images[i] ** 2) ** 0.5\n",
    "\n",
    "plt.imshow(random_images[1], cmap = 'grey', interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn image data into qubits\n",
    "random_images_qubits = np.zeros((Ndata, 2**n)) + 1j * np.zeros((Ndata, 2**n))\n",
    "for i in range(0, Ndata):\n",
    "    random_images_qubits[i] = np.ravel(random_images[i] / amplitude_vals[i] + 0j)\n",
    "\n",
    "#print(random_images_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Diffuse our test image\n",
    "n = 4\n",
    "T = 20\n",
    "Ndata = 50\n",
    "\n",
    "diff_hs = np.linspace(0.5, 4., T)\n",
    "\n",
    "model_diff = DiffusionModel(n, T, Ndata)\n",
    "\n",
    "diffuse_square = np.zeros((Ndata, 2**n)) + 1j * np.zeros((Ndata, 2**n))\n",
    "for i in range(0, Ndata):\n",
    "    diffuse_square[i] = np.array(temp_test / (12**0.5)) + 1j * np.zeros(2**n)\n",
    "\n",
    "X = torch.from_numpy(diffuse_square)\n",
    "#X = torch.from_numpy(random_images_qubits)\n",
    "\n",
    "Xout = np.zeros((T+1, Ndata, 2**n), dtype = np.complex64)\n",
    "Xout[0] = X\n",
    "\n",
    "for t in range(1, T+1):\n",
    "    Xout[t] = model_diff.set_diffusionData_t(t, X, diff_hs[:t], seed = t).numpy()\n",
    "    print(t)\n",
    "\n",
    "np.save(\"Xout_testimage\", Xout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use model to predict what original image was from diffused image\n",
    "diffused_images = np.load('Xout_testimage.npy')\n",
    "test_data_T20 = torch.tensor(diffused_images[20], dtype = torch.complex64)\n",
    "#print(Xout[0])\n",
    "n, na = 4, 1\n",
    "T = 20\n",
    "L = 6\n",
    "Ndata = 50\n",
    "\n",
    "params_tot = np.load('params_total_2000Ndata_20kEpochs.npy')\n",
    "\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_te = diffModel.HaarSampleGeneration(Ndata, seed=22)\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "\n",
    "\n",
    "#data_te = model.backDataGeneration(test_data_T20, params_tot, Ndata)[:, :, :2**n].numpy()\n",
    "data_te = model.backDataGeneration(torch.from_numpy(random_images_qubits), params_tot, Ndata)[:, :, :2**n].numpy()\n",
    "\n",
    "np.save(\"test_backwardsgen\", data_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "backwards_gen = np.load('test_backwardsgen.npy')\n",
    "print(np.ndim(backwards_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from backwards_gen get the nxn array for the image\n",
    "backwards_gen = np.load('test_backwardsgen.npy')\n",
    "dim = 4#int((final_output_flattened.size) ** 0.5)\n",
    "final_output_nxn = np.zeros((T + 1, Ndata, dim, dim))\n",
    "\n",
    "for z in range(0, 21):\n",
    "    final_output_flattened = backwards_gen[z]\n",
    "    final_output_flattened = np.abs(final_output_flattened)\n",
    "    multiplier = np.max(final_output_flattened)\n",
    "    for i in range(0, np.size(amplitude_vals)):\n",
    "        final_output_flattened[:][i] *= amplitude_vals[i]\n",
    "    #final_output_flattened = final_output_flattened.flatten()\n",
    "\n",
    "    for i in range(0, dim):\n",
    "        for j in range(0, dim):\n",
    "            for nth_data in range(0, Ndata):\n",
    "                final_output_nxn[z][nth_data][i][j] = final_output_flattened[nth_data][(i*dim) + j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.37881529 0.9924829  0.36502409 0.56354952]\n",
      "  [0.98291945 0.00184423 0.54083985 0.43845654]\n",
      "  [0.24158737 0.39088732 0.26326311 0.05454385]\n",
      "  [0.72323108 0.09985524 0.9247148  0.93785793]]\n",
      "\n",
      " [[0.77530569 0.75433999 0.99660558 0.51560795]\n",
      "  [0.9976033  0.87785959 0.40256074 0.58681762]\n",
      "  [0.04788043 0.48599678 0.06118194 0.59382915]\n",
      "  [0.34558684 0.65512115 0.2689738  0.97009063]]\n",
      "\n",
      " [[0.32765019 0.00206653 0.6361618  0.7916128 ]\n",
      "  [0.39878047 0.72215533 0.76918352 0.93433154]\n",
      "  [0.26971069 0.18723755 0.83239204 0.98767221]\n",
      "  [0.07457087 0.06002143 0.69135612 0.61806995]]\n",
      "\n",
      " [[0.70742476 0.36684838 0.52213907 0.27367702]\n",
      "  [0.43233776 0.53962862 0.90716076 0.85649383]\n",
      "  [0.25310919 0.50950313 0.9356643  0.15290259]\n",
      "  [0.3181062  0.91900724 0.91043442 0.84843618]]\n",
      "\n",
      " [[0.91217428 0.40718284 0.04979086 0.24673502]\n",
      "  [0.42381477 0.49834833 0.74655819 0.02233062]\n",
      "  [0.14369847 0.69338936 0.29580325 0.47495016]\n",
      "  [0.60008198 0.00984897 0.08537296 0.4252688 ]]\n",
      "\n",
      " [[0.94392872 0.53389758 0.13651323 0.13436911]\n",
      "  [0.56450093 0.19090731 0.57029605 0.98093152]\n",
      "  [0.83678383 0.49993974 0.58780068 0.92433953]\n",
      "  [0.40551353 0.08223825 0.04319717 0.6098541 ]]\n",
      "\n",
      " [[0.28742051 0.34532976 0.21404608 0.60211462]\n",
      "  [0.70272672 0.74673742 0.6710701  0.09899734]\n",
      "  [0.44125986 0.19920859 0.03002127 0.13557471]\n",
      "  [0.7241106  0.26673901 0.12120044 0.66388881]]\n",
      "\n",
      " [[0.57310414 0.12365697 0.50937492 0.10160585]\n",
      "  [0.72435451 0.83609825 0.27040091 0.64159644]\n",
      "  [0.13779105 0.61785609 0.89983338 0.59557253]\n",
      "  [0.79322064 0.35751191 0.54403937 0.19222994]]\n",
      "\n",
      " [[0.17791373 0.76582891 0.83205658 0.46025205]\n",
      "  [0.73820162 0.44904655 0.24523026 0.63252127]\n",
      "  [0.82971245 0.82810134 0.15001157 0.03096274]\n",
      "  [0.47856331 0.7251507  0.11983383 0.20495854]]\n",
      "\n",
      " [[0.49576914 0.6521011  0.36139065 0.81863517]\n",
      "  [0.49761084 0.54216635 0.52061313 0.55630088]\n",
      "  [0.47872826 0.17047544 0.42717889 0.68971777]\n",
      "  [0.11796382 0.33973861 0.4570522  0.8402819 ]]\n",
      "\n",
      " [[0.09019609 0.89049554 0.24190119 0.37088209]\n",
      "  [0.13731943 0.55064183 0.79198456 0.36857709]\n",
      "  [0.87506419 0.37600714 0.98335654 0.19462101]\n",
      "  [0.48449942 0.94543684 0.95441294 0.57872856]]\n",
      "\n",
      " [[0.19026044 0.06730666 0.60693169 0.43025315]\n",
      "  [0.66564471 0.92841917 0.50632077 0.65605128]\n",
      "  [0.8337723  0.08536644 0.60405087 0.46241853]\n",
      "  [0.16272788 0.65501595 0.72571486 0.84884822]]\n",
      "\n",
      " [[0.89242035 0.48227742 0.00778067 0.27929503]\n",
      "  [0.67863798 0.32574329 0.87780255 0.71111554]\n",
      "  [0.02500384 0.52680576 0.29041648 0.38301861]\n",
      "  [0.31585646 0.78476501 0.11837065 0.1752325 ]]\n",
      "\n",
      " [[0.35494557 0.17763937 0.48088309 0.66186994]\n",
      "  [0.10780302 0.58382565 0.5048573  0.64067346]\n",
      "  [0.7079891  0.49404863 0.800237   0.93388176]\n",
      "  [0.73953295 0.52631348 0.98080111 0.53627694]]\n",
      "\n",
      " [[0.97825503 0.89128798 0.91302478 0.75250047]\n",
      "  [0.17569204 0.84431702 0.27092704 0.82721269]\n",
      "  [0.60755861 0.93791771 0.74460775 0.67303342]\n",
      "  [0.02732699 0.76209778 0.87342185 0.3223162 ]]\n",
      "\n",
      " [[0.69132215 0.73089767 0.36378089 0.0638739 ]\n",
      "  [0.43750858 0.3388657  0.15137853 0.34505135]\n",
      "  [0.11165826 0.93076903 0.75773168 0.77011287]\n",
      "  [0.05912503 0.94127762 0.1452724  0.20581588]]\n",
      "\n",
      " [[0.32098091 0.53093612 0.04511486 0.02912952]\n",
      "  [0.24201165 0.45417836 0.38893282 0.86025035]\n",
      "  [0.41823667 0.7568149  0.48969913 0.85367107]\n",
      "  [0.68736112 0.62011993 0.0079197  0.51383525]]\n",
      "\n",
      " [[0.19851094 0.7406894  0.6195944  0.91546255]\n",
      "  [0.58258617 0.85935104 0.88341898 0.0206975 ]\n",
      "  [0.78678548 0.22800121 0.21185717 0.94240981]\n",
      "  [0.4615154  0.58010173 0.47263479 0.51215625]]\n",
      "\n",
      " [[0.23543577 0.24171072 0.32754046 0.0567745 ]\n",
      "  [0.09088328 0.67911369 0.32272887 0.62651128]\n",
      "  [0.45356044 0.33933553 0.04706914 0.91353631]\n",
      "  [0.22221851 0.30627769 0.62376833 0.00653045]]\n",
      "\n",
      " [[0.11429156 0.73581219 0.92930961 0.07626185]\n",
      "  [0.52500099 0.8211934  0.39759061 0.00626311]\n",
      "  [0.8316735  0.57180762 0.98535007 0.16674298]\n",
      "  [0.43925202 0.35475042 0.18157551 0.7842291 ]]\n",
      "\n",
      " [[0.67561954 0.84871483 0.52159029 0.69273877]\n",
      "  [0.77229154 0.52713543 0.79323554 0.23792607]\n",
      "  [0.28597185 0.04654152 0.88388729 0.45221674]\n",
      "  [0.33408284 0.08277649 0.7621448  0.19786483]]\n",
      "\n",
      " [[0.33138484 0.23806418 0.79714185 0.84069556]\n",
      "  [0.99886322 0.65702677 0.42505509 0.25115377]\n",
      "  [0.37169111 0.82980376 0.17128533 0.39873564]\n",
      "  [0.05539724 0.43042552 0.5615226  0.55012614]]\n",
      "\n",
      " [[0.79736972 0.50675118 0.9854461  0.9836852 ]\n",
      "  [0.79518765 0.76980823 0.0639073  0.85439652]\n",
      "  [0.82430482 0.23291959 0.26494774 0.47937426]\n",
      "  [0.88309932 0.09798981 0.15967664 0.00873009]]\n",
      "\n",
      " [[0.64830667 0.18531236 0.80744952 0.74693573]\n",
      "  [0.13575475 0.42826152 0.53975219 0.3911905 ]\n",
      "  [0.66767204 0.8886106  0.07643053 0.19626462]\n",
      "  [0.4122574  0.14397965 0.94955951 0.45764551]]\n",
      "\n",
      " [[0.66932899 0.46499583 0.97470832 0.83061206]\n",
      "  [0.84970355 0.10127059 0.98304725 0.64130753]\n",
      "  [0.63389766 0.32225636 0.79701257 0.09375318]\n",
      "  [0.51993942 0.07354158 0.37055641 0.03567447]]\n",
      "\n",
      " [[0.13086963 0.79706472 0.33830297 0.04939571]\n",
      "  [0.15938832 0.57610732 0.8968206  0.08540734]\n",
      "  [0.62453359 0.09051166 0.57208985 0.07803547]\n",
      "  [0.11335953 0.76353723 0.91527826 0.66800231]]\n",
      "\n",
      " [[0.00668302 0.6636225  0.15324938 0.26766875]\n",
      "  [0.81796771 0.97817791 0.25610399 0.81619245]\n",
      "  [0.98433846 0.41825515 0.09568481 0.71519488]\n",
      "  [0.12294914 0.76463687 0.54615724 0.78257316]]\n",
      "\n",
      " [[0.36080092 0.35576099 0.95527297 0.13392751]\n",
      "  [0.35546732 0.81049657 0.45973644 0.30200225]\n",
      "  [0.02619072 0.58286881 0.58456534 0.71341079]\n",
      "  [0.17248134 0.72103333 0.94721073 0.80207622]]\n",
      "\n",
      " [[0.71737897 0.8022638  0.60246551 0.10775306]\n",
      "  [0.10884299 0.05656232 0.81704456 0.54600662]\n",
      "  [0.32687822 0.01969781 0.30708262 0.01373302]\n",
      "  [0.66456252 0.10480341 0.50878942 0.54484552]]\n",
      "\n",
      " [[0.79812485 0.06685904 0.72183305 0.83298743]\n",
      "  [0.89214373 0.2867116  0.88752961 0.66310161]\n",
      "  [0.03290473 0.90447098 0.38254759 0.0888771 ]\n",
      "  [0.298408   0.20430119 0.67451775 0.97408664]]\n",
      "\n",
      " [[0.29091039 0.66801935 0.94010329 0.40016115]\n",
      "  [0.35776165 0.43959016 0.70532775 0.58470136]\n",
      "  [0.89507258 0.78499931 0.85325521 0.42600527]\n",
      "  [0.54093379 0.53545433 0.89378834 0.44706112]]\n",
      "\n",
      " [[0.73872089 0.23475742 0.12191223 0.99089384]\n",
      "  [0.72553527 0.62380207 0.71601921 0.50534487]\n",
      "  [0.83169556 0.83038658 0.84930712 0.821639  ]\n",
      "  [0.36862877 0.9471494  0.84890914 0.40731266]]\n",
      "\n",
      " [[0.84624654 0.99753213 0.26691094 0.64821279]\n",
      "  [0.61855149 0.37233171 0.88580251 0.43827215]\n",
      "  [0.6483385  0.76523906 0.84892184 0.24301861]\n",
      "  [0.8160035  0.03192436 0.83156312 0.39587122]]\n",
      "\n",
      " [[0.35915321 0.77238053 0.35129935 0.97464973]\n",
      "  [0.53804916 0.28343573 0.75950092 0.0243925 ]\n",
      "  [0.1513354  0.02792527 0.73325378 0.08808689]\n",
      "  [0.6382615  0.75044483 0.25202221 0.67575455]]\n",
      "\n",
      " [[0.18807869 0.42573169 0.26824117 0.77980316]\n",
      "  [0.90347701 0.42060748 0.86033952 0.19651364]\n",
      "  [0.22369656 0.85442168 0.61685294 0.17611666]\n",
      "  [0.82243681 0.49875119 0.19905719 0.54078448]]\n",
      "\n",
      " [[0.82965755 0.1303274  0.45018724 0.66766047]\n",
      "  [0.04758186 0.15782194 0.50053614 0.93843651]\n",
      "  [0.52748382 0.25455302 0.73981267 0.14144459]\n",
      "  [0.32353267 0.35433543 0.28805637 0.25968534]]\n",
      "\n",
      " [[0.73328274 0.27684328 0.70708627 0.04056906]\n",
      "  [0.72130251 0.3449308  0.95655954 0.538104  ]\n",
      "  [0.68280286 0.30532855 0.29621324 0.56301987]\n",
      "  [0.4046354  0.98875791 0.81285477 0.96520454]]\n",
      "\n",
      " [[0.38050786 0.85531598 0.56625557 0.73762047]\n",
      "  [0.18351206 0.80082077 0.32414991 0.32268465]\n",
      "  [0.53281891 0.33929294 0.37539747 0.83233362]\n",
      "  [0.18650234 0.83585864 0.30191562 0.0557368 ]]\n",
      "\n",
      " [[0.72985715 0.16314226 0.02538464 0.8847239 ]\n",
      "  [0.02449129 0.49819839 0.34221542 0.76529312]\n",
      "  [0.36908168 0.49641833 0.99477571 0.34645969]\n",
      "  [0.04236459 0.4444961  0.50089121 0.12853836]]\n",
      "\n",
      " [[0.60281229 0.94696295 0.04997711 0.5514676 ]\n",
      "  [0.52292424 0.42976788 0.46915865 0.17949319]\n",
      "  [0.19993453 0.30132973 0.29548106 0.85531718]\n",
      "  [0.30991489 0.78851235 0.98193163 0.20397472]]\n",
      "\n",
      " [[0.82076889 0.43140689 0.30635005 0.86763728]\n",
      "  [0.16989309 0.97153336 0.4547511  0.97900385]\n",
      "  [0.0331415  0.16026624 0.82512498 0.80352068]\n",
      "  [0.27283525 0.53559858 0.08799794 0.96443737]]\n",
      "\n",
      " [[0.01890707 0.78246766 0.7509802  0.47817045]\n",
      "  [0.81111294 0.58958113 0.75028455 0.26242948]\n",
      "  [0.28026491 0.9597528  0.35404009 0.57006907]\n",
      "  [0.79395062 0.80870408 0.73219162 0.45573071]]\n",
      "\n",
      " [[0.42459023 0.47666118 0.06298977 0.07942055]\n",
      "  [0.35908526 0.95591384 0.26759717 0.97939563]\n",
      "  [0.63972169 0.53568548 0.20179786 0.23620228]\n",
      "  [0.46223405 0.73199177 0.75206304 0.32574722]]\n",
      "\n",
      " [[0.81638801 0.01770343 0.41415891 0.91380256]\n",
      "  [0.58742464 0.00744564 0.6680479  0.48017076]\n",
      "  [0.92174888 0.06323449 0.02947869 0.87525147]\n",
      "  [0.62388915 0.36406165 0.05736397 0.82393056]]\n",
      "\n",
      " [[0.08067896 0.02588105 0.96119606 0.42321587]\n",
      "  [0.69628966 0.07389161 0.86615461 0.52308464]\n",
      "  [0.63598669 0.48048976 0.56287181 0.37675723]\n",
      "  [0.88005453 0.85317647 0.17384283 0.46792766]]\n",
      "\n",
      " [[0.00929353 0.82124519 0.24620999 0.94172955]\n",
      "  [0.46619058 0.25081143 0.11004604 0.75729334]\n",
      "  [0.18135141 0.93915236 0.6745615  0.3367756 ]\n",
      "  [0.77967662 0.32654476 0.41161555 0.60196525]]\n",
      "\n",
      " [[0.81210744 0.0123015  0.39969707 0.91191143]\n",
      "  [0.50358552 0.17598978 0.92268711 0.21133974]\n",
      "  [0.19241758 0.5892095  0.96710777 0.18837839]\n",
      "  [0.13402931 0.14867532 0.3290644  0.77277577]]\n",
      "\n",
      " [[0.515549   0.25996059 0.82229084 0.49425548]\n",
      "  [0.23818931 0.36425105 0.53367674 0.39665034]\n",
      "  [0.30437663 0.19715226 0.05725509 0.78747201]\n",
      "  [0.32646236 0.68304145 0.11330929 0.04211123]]\n",
      "\n",
      " [[0.27986637 0.77404642 0.8886742  0.09264336]\n",
      "  [0.56410414 0.50684291 0.74163181 0.85559821]\n",
      "  [0.05028905 0.99903589 0.84124786 0.63651884]\n",
      "  [0.03555544 0.85335344 0.71163154 0.4058463 ]]\n",
      "\n",
      " [[0.88234144 0.80759943 0.15752333 0.33673957]\n",
      "  [0.67529678 0.77875566 0.10552063 0.72588968]\n",
      "  [0.17864759 0.67434639 0.10204618 0.84765953]\n",
      "  [0.27763227 0.40544745 0.19122334 0.83399266]]]\n"
     ]
    }
   ],
   "source": [
    "print(final_output_nxn[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "test_this = np.array([\n",
    "    [[1, 1], [1, 1]], \n",
    "    [[2, 2], [2, 2]], \n",
    "    [[3, 3], [3, 3]]\n",
    "    ])\n",
    "print(np.sum(test_this[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22799259e-03-5.62238682e-04j -7.19844305e-04-4.64098906e-04j\n",
      "  -1.35706831e-03-7.58030103e-04j -1.30663137e-03-4.72220650e-04j\n",
      "  -1.57542247e-03-5.63659589e-04j -5.29263169e-04+3.46412089e-05j\n",
      "   1.72907079e-04-1.31795998e-04j -1.25336205e-03-3.08553048e-04j\n",
      "  -1.37835613e-03-4.98039939e-04j -5.00446731e-05+1.25281367e-05j\n",
      "  -5.53752325e-05-1.76545960e-04j -2.05164868e-03-4.98054840e-04j\n",
      "  -1.00558077e-03-5.27329510e-04j -1.39092852e-03-4.53577784e-04j\n",
      "  -1.29212707e-03-8.87922361e-04j -1.51073944e-03-8.35879706e-04j]\n",
      " [ 5.35857223e-04+3.26519745e-04j  5.12739702e-04-1.21991965e-04j\n",
      "   7.65251461e-04-3.60648468e-04j  8.86915019e-04-6.04025663e-05j\n",
      "   8.00232869e-04+3.53649142e-04j  7.19198899e-04-5.86880313e-04j\n",
      "  -1.81068608e-04-8.30121426e-05j  1.11819524e-03+1.12879985e-04j\n",
      "   8.91579082e-04+3.21587722e-04j  4.88190883e-04-6.53182564e-04j\n",
      "  -1.53223620e-04-2.40429908e-05j  1.09090388e-03+3.69306662e-07j\n",
      "   4.78762347e-04+5.29663404e-04j  7.17572751e-04+1.57418144e-05j\n",
      "   7.36924354e-04-4.38001443e-04j  8.88152514e-04-2.41683563e-04j]\n",
      " [ 6.54576637e-04+2.30252088e-04j  5.98553743e-04+2.89238495e-04j\n",
      "   7.26999075e-04-1.11667621e-04j  4.87147830e-04+3.37303325e-04j\n",
      "   4.41646640e-04+2.45734293e-04j  1.06553962e-05+1.69930019e-04j\n",
      "   2.42929345e-05+5.30573852e-05j  3.37331818e-04-7.30297470e-05j\n",
      "   5.06299140e-04+4.56955837e-04j  1.70052284e-04+2.61715555e-04j\n",
      "   4.71543899e-06-2.58189015e-04j  3.65440850e-04+3.50821880e-04j\n",
      "   3.68923793e-04+4.50445223e-04j  4.28277708e-04+4.38363990e-04j\n",
      "   4.12650552e-04+5.97884820e-04j  5.16023894e-04-5.45792223e-04j]\n",
      " [-2.73179059e-04-2.67761148e-04j -3.14664765e-04-4.21961537e-04j\n",
      "   3.62844497e-04-3.74133233e-04j  1.71252817e-04-5.94872516e-04j\n",
      "   3.08134157e-04+2.74612306e-04j -4.71983294e-05+5.48612152e-04j\n",
      "  -2.82064895e-04-4.02923615e-05j  8.40425200e-05-2.72898033e-04j\n",
      "  -1.60442345e-04-1.12405949e-04j -4.20969387e-04-1.18831435e-04j\n",
      "   3.61199112e-04+7.87757890e-05j -2.19933339e-04-7.88356701e-04j\n",
      "   7.02104357e-04+3.28016176e-04j -4.77966969e-05+2.24160867e-05j\n",
      "  -2.03195068e-05-2.08025798e-04j -6.63384344e-05-6.95985451e-04j]\n",
      " [-1.50508864e-03-1.15198753e-04j  1.03389507e-03-8.45093862e-04j\n",
      "  -5.61964873e-04-1.36913627e-03j -4.00889345e-04+7.28529485e-05j\n",
      "  -1.95578177e-04-1.56244612e-03j -2.29966070e-04+4.48638515e-04j\n",
      "   5.69049269e-04-4.43146215e-04j  4.93367079e-05-5.22756367e-04j\n",
      "  -7.00117671e-04-8.82074470e-04j -8.38016858e-05+1.78114118e-04j\n",
      "  -6.51322480e-04-2.79089872e-04j -1.47107159e-04-6.54700329e-04j\n",
      "  -2.27159180e-04-9.99240787e-04j  5.18340792e-04-1.10097892e-04j\n",
      "   1.24153812e-04-1.94164563e-03j -6.35812932e-04-1.62340250e-04j]\n",
      " [ 7.06756720e-04+2.19898968e-04j  9.51047696e-04-1.14323178e-04j\n",
      "  -2.10775790e-04+8.80114385e-05j  4.77403228e-04+4.55284753e-04j\n",
      "  -2.59820226e-04+1.36815629e-03j  5.23176219e-04-5.44595707e-04j\n",
      "   8.13934748e-05+3.51439259e-04j  6.85889740e-04+2.08148369e-04j\n",
      "   3.58953461e-04+7.82942574e-04j -7.91952043e-05+3.32235475e-04j\n",
      "  -3.54650430e-04-3.65440501e-04j  4.71522362e-04+1.24187896e-03j\n",
      "  -2.47956312e-04+7.96267006e-04j  1.00526726e-03+5.01792587e-04j\n",
      "   1.26302405e-03+1.35474186e-03j  1.08995126e-03-7.31824490e-04j]\n",
      " [-8.24160583e-04+9.17679747e-04j  3.42710438e-04-2.20363709e-05j\n",
      "   5.68467251e-04-1.65898341e-03j  3.82181024e-04+3.75405594e-04j\n",
      "   5.99676336e-04-1.02032127e-03j  1.64982266e-04+1.92867112e-04j\n",
      "   4.31035878e-04+3.01141299e-05j  4.21887846e-04-4.73221560e-04j\n",
      "  -1.20671197e-04-4.63416945e-04j  2.72522302e-04-2.16836779e-04j\n",
      "  -1.03475014e-03-9.75585368e-04j -5.46911790e-04-9.93730719e-05j\n",
      "   7.69740611e-04-8.06771277e-04j -4.92055842e-04-7.81662297e-04j\n",
      "  -1.77695940e-04-1.39238371e-03j -7.11256289e-05+7.00709061e-04j]\n",
      " [ 7.01402954e-04-3.03673005e-04j  4.28707805e-04-3.65778978e-04j\n",
      "  -4.91765677e-04+7.31631939e-04j  2.64000002e-04+4.12242953e-04j\n",
      "   2.96554557e-04+4.49273590e-04j  2.95815000e-04-1.40302902e-04j\n",
      "  -1.37397234e-04+1.03687309e-03j  6.54446252e-04-2.85668502e-04j\n",
      "  -3.55274096e-05+2.15133958e-04j  2.37952161e-04+4.33589419e-04j\n",
      "   4.62995115e-04-4.78062662e-04j -3.25529865e-04-3.47658279e-05j\n",
      "   5.17643522e-04+9.75056842e-04j -1.28602856e-04+9.84502491e-04j\n",
      "   2.68045347e-04-2.30882040e-04j  1.34367976e-04-3.10043542e-04j]\n",
      " [-1.24758983e-04-7.58010210e-05j  1.73320761e-04+7.62630778e-04j\n",
      "  -8.09735968e-04+6.39011050e-05j  3.01329710e-04-1.56201175e-04j\n",
      "   2.74751248e-04-1.18027483e-05j -4.63714852e-04+5.50474098e-04j\n",
      "   2.93380959e-04-2.60930072e-04j  4.45585232e-04-3.94478557e-04j\n",
      "  -1.33284935e-04+6.05991925e-04j  1.19114869e-04-5.05143544e-04j\n",
      "   1.71959895e-04-3.44459462e-04j  3.97552649e-04-2.66231626e-04j\n",
      "   7.03245110e-04+1.06216270e-04j -2.53824779e-04+8.61091772e-04j\n",
      "  -4.03147627e-04+1.28339583e-04j -6.09656599e-05+3.49926500e-04j]\n",
      " [ 7.18069728e-04-7.53231507e-05j -6.78277924e-04-9.33855248e-04j\n",
      "   3.23618791e-04+3.21466330e-04j  5.46164694e-04+2.10990202e-05j\n",
      "   5.77516214e-04-2.51352816e-04j -1.13801514e-04+3.96528369e-04j\n",
      "  -5.38819120e-04+3.68681445e-04j  3.29547911e-04-2.17308290e-04j\n",
      "   2.52240752e-05-1.99458445e-04j -1.08289583e-04-2.82342080e-04j\n",
      "  -5.72884222e-04-1.24239945e-04j -6.40768732e-04-2.65735202e-04j\n",
      "  -2.88560754e-04-1.86742400e-04j  4.02412319e-04-2.13690248e-04j\n",
      "   6.38409401e-04+6.98991760e-04j  3.27721034e-04-2.69628654e-04j]\n",
      " [-3.23449582e-04+1.10380515e-03j -3.13770317e-04+4.96037712e-04j\n",
      "   7.24345155e-04-2.59207096e-04j -9.52226517e-04-1.03472953e-03j\n",
      "   8.43452057e-04+2.78310945e-05j  7.07202125e-04-3.49143433e-04j\n",
      "   4.45942278e-04-3.52389325e-04j -1.29062639e-04-2.37773158e-04j\n",
      "  -3.01934371e-04+1.86277364e-04j -6.61555459e-05-4.72322223e-04j\n",
      "   7.34357582e-06-2.88590090e-04j  3.40627186e-04-1.10971865e-04j\n",
      "   1.20464494e-04+3.60242673e-04j -5.72684396e-04-3.66049208e-04j\n",
      "  -2.27295677e-04+4.16042400e-04j -3.01133667e-04-6.52369054e-04j]\n",
      " [ 2.99742416e-04+1.92339750e-04j  6.94456248e-05+4.87973921e-05j\n",
      "   6.51978015e-04+5.21028298e-04j  2.88649870e-04+6.47809182e-04j\n",
      "  -2.40423426e-04+4.37560484e-05j -1.18867911e-05+1.03482674e-03j\n",
      "   6.28487207e-04+7.54927576e-04j -3.80898491e-05+3.32160125e-04j\n",
      "   5.71965866e-05-9.81337274e-04j -5.04358264e-04+3.63650091e-04j\n",
      "   4.51800770e-05-3.44519853e-04j -2.59445696e-05-7.08617619e-04j\n",
      "   7.58910784e-04+2.49556993e-04j -2.08528581e-05-2.27504701e-04j\n",
      "  -2.49330362e-04+3.56754928e-04j -1.01655931e-03-2.72289530e-04j]\n",
      " [-3.67801200e-04+4.43728670e-04j  6.16963080e-04+2.13422187e-04j\n",
      "  -9.65065847e-04-1.00450392e-03j -8.27529439e-05+7.30302185e-04j\n",
      "  -1.59923893e-05+2.75467173e-04j -8.43145244e-05+2.81503133e-04j\n",
      "  -6.03979744e-04+2.44144554e-04j -3.04057816e-04+9.08258371e-04j\n",
      "  -7.32763147e-04-2.91969976e-04j -9.58382094e-04+6.05583773e-04j\n",
      "  -2.07789722e-04-2.71537545e-04j -2.37695094e-05-8.13113234e-04j\n",
      "   4.17915347e-04-2.04999305e-05j -2.42801761e-04+8.24657618e-04j\n",
      "   4.93641528e-05-4.73387307e-04j -6.74584298e-04-3.33849748e-04j]\n",
      " [-1.13737956e-03-3.22508509e-04j  5.45610732e-04+6.80178404e-04j\n",
      "   3.84165382e-04-1.81914060e-04j -1.30634848e-03-7.50789826e-04j\n",
      "   2.38007307e-03+9.42565210e-04j  1.89160591e-03+1.20434607e-03j\n",
      "   3.12241027e-04-9.92133049e-04j -1.20131031e-03+2.13941233e-03j\n",
      "   1.43241545e-04-7.40321178e-04j -3.03491019e-03-1.25681912e-03j\n",
      "  -1.38230133e-03+1.47951418e-03j  9.88555257e-05-1.54793053e-03j\n",
      "   7.65024917e-04-1.17156550e-03j -1.82959833e-03+5.76432329e-04j\n",
      "  -3.18183447e-04+1.77404960e-03j  9.27388377e-04-1.54794182e-03j]\n",
      " [ 2.88314768e-04+4.10214474e-04j  3.77076765e-04-6.09723618e-04j\n",
      "  -2.56992662e-05+6.07378897e-05j -1.05746684e-03-1.77888386e-03j\n",
      "  -4.58035705e-04-9.05908237e-04j -1.41410122e-03-5.13467239e-04j\n",
      "   8.22485832e-04+1.24441565e-03j  8.59508174e-04-6.80129568e-04j\n",
      "  -5.68697287e-04-9.95013281e-04j -1.78214163e-03-1.48657674e-03j\n",
      "  -1.01997401e-03+3.74998385e-03j  1.90403091e-03-1.42270420e-03j\n",
      "  -1.67329763e-05+1.34554598e-03j -1.88074573e-05+1.64802466e-03j\n",
      "  -9.25703091e-04+8.86842841e-04j  1.61499262e-03-7.91975530e-04j]\n",
      " [-7.83650554e-04+1.66699733e-03j  1.15568854e-03+1.75909343e-04j\n",
      "   7.38305447e-04-5.23717317e-04j  1.89873608e-04+7.54675246e-04j\n",
      "   1.52567029e-03-1.08141534e-03j -2.88301473e-03-1.63086434e-03j\n",
      "   1.36570213e-03-1.87835132e-03j -1.24224124e-03-7.02245336e-04j\n",
      "  -6.03156164e-04+7.17460762e-06j  3.65733431e-04-1.86030741e-03j\n",
      "   1.44754013e-03-2.22482020e-03j -2.43082945e-03+7.49389728e-05j\n",
      "   8.96144018e-04-1.52338087e-03j -1.50145171e-03-3.77353077e-04j\n",
      "   1.07231596e-03+4.53914370e-04j -1.55921897e-03-1.74264144e-03j]\n",
      " [-3.57476201e-05-7.77593872e-04j  1.82054835e-04+9.72832262e-04j\n",
      "  -3.31919087e-04-4.89312282e-04j  1.70850742e-03-1.83001813e-03j\n",
      "   3.24849934e-05+8.79887783e-04j -1.03184150e-03-9.65491112e-04j\n",
      "  -2.78845746e-05-1.86165352e-03j  1.11410953e-03+6.40038168e-04j\n",
      "  -2.98278232e-04-1.08219194e-03j -7.68052356e-04+1.64710655e-04j\n",
      "  -5.75036451e-04-5.28615376e-04j  3.40803480e-03-2.28086719e-03j\n",
      "  -5.17874898e-04+1.32583675e-03j  1.34807045e-03-8.19552923e-04j\n",
      "   1.14952505e-03-4.44328500e-04j  2.44013104e-03-6.08092640e-04j]\n",
      " [-2.00243387e-03-4.89099941e-04j -1.15394429e-03-3.96170071e-04j\n",
      "  -3.21214064e-03+1.47822045e-03j -2.63850699e-04+5.00546768e-04j\n",
      "  -1.41904270e-03+3.50443515e-05j  2.61905981e-04+1.14833389e-03j\n",
      "  -5.87112829e-03+2.92295264e-03j  2.43942603e-03-5.71620942e-04j\n",
      "  -1.39704288e-03+1.60009484e-03j  1.97236193e-03-1.23695121e-03j\n",
      "  -3.68109584e-04+2.93900399e-03j -1.58393232e-03+3.41608352e-03j\n",
      "  -7.88678648e-04+8.89889523e-03j -2.11817925e-04+4.47608298e-03j\n",
      "   3.85502144e-03+4.30910429e-03j  2.03974475e-03+8.63340916e-04j]\n",
      " [ 1.91131863e-03-3.12800473e-03j  1.14618684e-03+3.77145107e-03j\n",
      "  -2.16141227e-03+6.98045420e-04j -1.06091553e-03-2.65189609e-03j\n",
      "   4.29900596e-04-3.71911586e-03j  3.90253736e-05-1.75490661e-03j\n",
      "  -2.20019626e-03-1.69607822e-03j  5.23076812e-03+3.97570711e-03j\n",
      "  -1.21953293e-04-5.01257554e-03j -2.29984988e-04+5.66541869e-03j\n",
      "  -1.00827101e-03+5.03580179e-03j -2.84227403e-03+2.96144676e-03j\n",
      "  -1.02911808e-03+3.66512965e-03j -3.15433019e-03-7.01088400e-04j\n",
      "   2.95630330e-03+1.06796611e-03j -9.98151023e-04+1.70415326e-04j]\n",
      " [ 5.70536451e-03-4.14368277e-03j  1.79578946e-03-1.79071154e-03j\n",
      "   3.31964460e-03-5.11474861e-03j -2.27331067e-03-1.71687210e-03j\n",
      "   1.51243270e-03-3.92498076e-03j -3.76769411e-03+1.53619109e-03j\n",
      "   6.31527859e-04+4.78876551e-04j -2.30143755e-03-2.35294783e-03j\n",
      "   1.99083169e-03-5.26341563e-03j  3.68450303e-04+2.44160020e-03j\n",
      "   2.01138714e-03-4.14258707e-03j  7.19568692e-04+1.19193632e-03j\n",
      "   3.88804264e-03+3.60252010e-03j -3.07003327e-04+1.80490955e-04j\n",
      "  -2.42232019e-03+9.57435463e-04j  2.52721272e-03-2.67008576e-03j]\n",
      " [ 5.49055496e-03+0.00000000e+00j  5.45750745e-03+0.00000000e+00j\n",
      "   5.29341726e-03+0.00000000e+00j  5.56062674e-03+0.00000000e+00j\n",
      "   5.50424308e-03+0.00000000e+00j  5.63780311e-03+0.00000000e+00j\n",
      "   6.26635831e-03+0.00000000e+00j  5.67421736e-03+0.00000000e+00j\n",
      "   4.86466568e-03+0.00000000e+00j  5.31084044e-03+0.00000000e+00j\n",
      "   5.49384067e-03+0.00000000e+00j  5.28760254e-03+0.00000000e+00j\n",
      "   4.54842532e-03+0.00000000e+00j  5.44680143e-03+0.00000000e+00j\n",
      "   5.34294965e-03+0.00000000e+00j  5.47792763e-03+0.00000000e+00j]]\n",
      "1.0082647069802675\n",
      "0.9935114765578962\n",
      "0.99558554268548\n",
      "0.9986661099664925\n",
      "1.0005109166933936\n",
      "0.9953385532438566\n",
      "0.9984824882613693\n",
      "0.9976271726726568\n",
      "0.9986652702653257\n",
      "0.9976756861143192\n",
      "0.9995708732486196\n",
      "0.9986524543689894\n",
      "1.0003487722291127\n",
      "0.9993757844993147\n",
      "0.9979451361831131\n",
      "1.0004720909145028\n",
      "0.9931752912795951\n",
      "1.0014198120651137\n",
      "0.9990994263430214\n",
      "0.991066945507235\n",
      "0.9620302344987712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'T')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#calculate mean square error\n",
    "#Xout[T][Ndata][2**n]\n",
    "#backwards_gen[T][Ndata][2**n]\n",
    "mse_calc = np.zeros((21, 16))\n",
    "avg_backwards_vector = np.sum(backwards_gen, axis = 1)/Ndata\n",
    "print(avg_backwards_vector)\n",
    "\n",
    "orig_vals = np.sum((np.load('training_data.npy')), axis = 0) / 2000\n",
    "#print(orig_vals)\n",
    "for i in range(0, 21):\n",
    "    temp1 = 0\n",
    "    for j in range(0, 2**n):\n",
    "        temp1 += (np.abs(orig_vals[j] - avg_backwards_vector[i][j]))**2\n",
    "    print(temp1)\n",
    "    mse_calc[i] = temp1/ 16\n",
    "\n",
    "#mse_calc = np.abs(np.sum(mse_calc, axis = 1)) / 16\n",
    "\n",
    "plt.plot(range(0, 21), mse_calc)\n",
    "plt.title(\"Mean Square Error between Forward and Backward Diffusion\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.xlabel(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize final_output_nxn for each data\n",
    "#final_output_nxn[T][Ndata][4][4]\n",
    "#print_this_1 is average output of model\n",
    "#fig, axs = plt.subplots(1, 1, figsize = (10, 20))\n",
    "num1 = np.max(np.abs(avg_backwards_vector))\n",
    "print_this = np.abs(avg_backwards_vector) /num1\n",
    "print_this_1 = np.zeros((4,4))\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 4):\n",
    "        print_this_1[i][j] = print_this[0][4*i + j]\n",
    "\n",
    "\n",
    "for z in range(0, 1):\n",
    "    #print(final_output_nxn[z])\n",
    "    plt.imshow(\n",
    "        #final_output_nxn[0][z]\n",
    "        print_this_1, cmap = 'grey', interpolation = 'nearest')\n",
    "    plt.title('Average Output at T = %d'%0)\n",
    "\n",
    "#plt.suptitle('Backwards Diffusion At T = 0')\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
