{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import unitary_group\n",
    "from skimage.measure import block_reduce\n",
    "from opt_einsum import contract\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from src.QDDPM_torch import DiffusionModel, QDDPM, naturalDistance\n",
    "import src.ImageEncode as ie\n",
    "rc('text', usetex=True)\n",
    "rc('axes', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Torch will use CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Torch will use CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_encode(image_values: np.array, npixels: int):\n",
    "    '''\n",
    "    Args:\n",
    "        image_values: 2-dimensional array (non hairy) of image data\n",
    "    '''\n",
    "    norm: float = np.linalg.norm(image_values)\n",
    "    normalized_image_values: np.array = image_values.flatten()/norm\n",
    "    return normalized_image_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.33\n",
      "0.66\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "pixelValues = []\n",
    "for i in range(4):\n",
    "    value = 0.33 * i\n",
    "    pixelValues.append(value)\n",
    "    print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.33]\n",
      " [0.66 0.99]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.26726124 0.53452248 0.80178373]\n"
     ]
    }
   ],
   "source": [
    "values = np.array([pixelValues[0:2], pixelValues[2:4]])\n",
    "print(values)\n",
    "normalized = ie.normalize(values)\n",
    "plt.imshow(values, cmap='grey', interpolation='nearest')\n",
    "plt.show()\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training(values: np.array, n_train: int, scale: float, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    n = values.size\n",
    "    noise = np.random.randn(n_train,n)+0j*np.random.randn(n_train,n) \n",
    "    print(noise.shape)\n",
    "    print(values.shape)\n",
    "    states = (noise*scale) + values\n",
    "    states/=np.tile(np.linalg.norm(states, axis=1).reshape(1,n_train), (n,1)).T\n",
    "    return states\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "(4,)\n",
      "[[ 0.57783984+0.j  0.30807145+0.j  0.57899018+0.j  0.48576072+0.j]\n",
      " [-0.77941512+0.j  0.59356852+0.j -0.07397829+0.j  0.1863215 +0.j]\n",
      " [-0.41666856+0.j -0.11343743+0.j -0.29341698+0.j  0.85289257+0.j]\n",
      " [ 0.12438246+0.j -0.15386701+0.j -0.43745441+0.j  0.87720441+0.j]\n",
      " [-0.68917202+0.j  0.64908543+0.j  0.25434139+0.j  0.19758665+0.j]\n",
      " [-0.38452304+0.j  0.45745944+0.j -0.57573788+0.j  0.55803116+0.j]\n",
      " [ 0.6995513 +0.j  0.07191031+0.j -0.40464609+0.j  0.58456687+0.j]\n",
      " [ 0.04138612+0.j -0.12783116+0.j  0.16412136+0.j  0.97724642+0.j]\n",
      " [ 0.27240794+0.j  0.43180913+0.j  0.05038003+0.j  0.85836859+0.j]\n",
      " [-0.34383161+0.j -0.86575846+0.j -0.33624253+0.j -0.13850299+0.j]\n",
      " [-0.09001471+0.j -0.18527585+0.j  0.96718869+0.j  0.14871532+0.j]\n",
      " [-0.0441544 +0.j -0.36436842+0.j -0.0676421 +0.j  0.9277449 +0.j]\n",
      " [-0.09444859+0.j -0.60093677+0.j -0.00527805+0.j  0.79367916+0.j]\n",
      " [-0.78927571+0.j -0.43674775+0.j  0.36772953+0.j  0.22598728+0.j]\n",
      " [-0.07844488+0.j  0.71819645+0.j  0.60881626+0.j  0.32769348+0.j]\n",
      " [-0.5406571 +0.j  0.50961627+0.j -0.66850631+0.j  0.03287035+0.j]\n",
      " [-0.49138116+0.j -0.30026126+0.j  0.55053483+0.j  0.60439981+0.j]\n",
      " [-0.11086236+0.j -0.42136924+0.j -0.51146009+0.j -0.74065247+0.j]\n",
      " [-0.88168016+0.j  0.31885995+0.j -0.33727597+0.j  0.0849314 +0.j]\n",
      " [ 0.93731136+0.j  0.02164974+0.j  0.0386597 +0.j -0.34566476+0.j]\n",
      " [ 0.26652531+0.j -0.54418502+0.j  0.64670087+0.j  0.46325469+0.j]\n",
      " [-0.11746803+0.j  0.23968381+0.j  0.89585367+0.j -0.35524518+0.j]\n",
      " [ 0.53089806+0.j  0.83961473+0.j -0.10090264+0.j -0.05489089+0.j]\n",
      " [-0.57058824+0.j  0.53583517+0.j  0.39846267+0.j  0.47805568+0.j]\n",
      " [-0.38302145+0.j -0.20807308+0.j -0.84981243+0.j  0.29634271+0.j]\n",
      " [-0.32386211+0.j  0.77905971+0.j  0.04328822+0.j -0.53507516+0.j]\n",
      " [ 0.34074951+0.j  0.17716862+0.j  0.61801049+0.j  0.68597674+0.j]\n",
      " [ 0.57495253+0.j  0.58117093+0.j  0.04314719+0.j  0.57428936+0.j]\n",
      " [ 0.76432323+0.j -0.56685321+0.j -0.21263917+0.j -0.22197302+0.j]\n",
      " [ 0.13629186+0.j  0.39445971+0.j  0.65980309+0.j  0.62488875+0.j]\n",
      " [-0.74170378+0.j -0.33649618+0.j  0.33673427+0.j  0.47249959+0.j]\n",
      " [-0.32945619+0.j -0.02376138+0.j -0.84520984+0.j -0.42013609+0.j]\n",
      " [-0.39277639+0.j  0.27432758+0.j -0.23168305+0.j  0.84663691+0.j]\n",
      " [ 0.45445489+0.j  0.41131492+0.j -0.61850916+0.j  0.49166778+0.j]\n",
      " [-0.18900773+0.j -0.4476638 +0.j  0.86753543+0.j -0.1060919 +0.j]\n",
      " [ 0.02614114+0.j  0.36927483+0.j  0.9050776 +0.j -0.20925412+0.j]\n",
      " [ 0.57791729+0.j  0.33310957+0.j -0.29650571+0.j -0.68347201+0.j]\n",
      " [ 0.1922205 +0.j  0.86051276+0.j -0.36820802+0.j  0.29494395+0.j]\n",
      " [ 0.06328253+0.j -0.48804436+0.j -0.71914735+0.j  0.49054572+0.j]\n",
      " [-0.30278292+0.j  0.92676565+0.j -0.1922538 +0.j  0.11165307+0.j]\n",
      " [-0.92656765+0.j -0.09625279+0.j  0.26744584+0.j -0.24633415+0.j]\n",
      " [ 0.11296725+0.j  0.76439086+0.j -0.00808306+0.j  0.63472803+0.j]\n",
      " [-0.45784563+0.j  0.40577354+0.j -0.77446219+0.j  0.1610389 +0.j]\n",
      " [-0.13415345+0.j  0.35143117+0.j  0.90924952+0.j -0.1782254 +0.j]\n",
      " [-0.49773596+0.j -0.80028792+0.j  0.27794977+0.j  0.185855  +0.j]\n",
      " [-0.01314209+0.j -0.50513071+0.j  0.82352745+0.j  0.25782319+0.j]\n",
      " [ 0.1978455 +0.j  0.59300387+0.j  0.30896069+0.j  0.71676136+0.j]\n",
      " [-0.75446211+0.j -0.14480546+0.j  0.51087879+0.j -0.38577345+0.j]\n",
      " [ 0.05540263+0.j  0.78877409+0.j  0.1786162 +0.j  0.58554439+0.j]\n",
      " [ 0.07220581+0.j  0.8174774 +0.j  0.55113567+0.j -0.15088572+0.j]\n",
      " [-0.37935221+0.j  0.28002637+0.j -0.13852084+0.j  0.87091281+0.j]\n",
      " [ 0.64395448+0.j  0.34878086+0.j -0.50217503+0.j  0.45988561+0.j]\n",
      " [-0.27140453+0.j  0.51862911+0.j -0.25633031+0.j  0.76919321+0.j]\n",
      " [ 0.05356704+0.j  0.28973819+0.j -0.21043226+0.j -0.93214839+0.j]\n",
      " [ 0.43779613+0.j -0.71392071+0.j  0.47556405+0.j  0.26924078+0.j]\n",
      " [ 0.56445191+0.j -0.05917215+0.j  0.3014806 +0.j  0.76616066+0.j]\n",
      " [-0.35066136+0.j  0.50849953+0.j -0.15037469+0.j  0.77191469+0.j]\n",
      " [-0.04885237+0.j  0.43917575+0.j  0.44104771+0.j  0.78116261+0.j]\n",
      " [-0.58741715+0.j  0.16218265+0.j -0.42145319+0.j  0.67157657+0.j]\n",
      " [ 0.50285998+0.j -0.47680784+0.j  0.49825153+0.j -0.52108688+0.j]\n",
      " [ 0.31233169+0.j  0.1265175 +0.j -0.63442189+0.j  0.69566595+0.j]\n",
      " [-0.70347997+0.j  0.05246661+0.j  0.69450998+0.j  0.14148874+0.j]\n",
      " [ 0.32246969+0.j  0.54064616+0.j  0.57443194+0.j  0.52320453+0.j]\n",
      " [ 0.94396215+0.j  0.26013209+0.j -0.20287845+0.j  0.01034861+0.j]\n",
      " [-0.8469547 +0.j -0.02478034+0.j  0.39377331+0.j  0.35636534+0.j]\n",
      " [ 0.45188376+0.j  0.7086318 +0.j -0.52762097+0.j  0.12352386+0.j]\n",
      " [-0.60255976+0.j -0.11853418+0.j -0.78693149+0.j -0.06008515+0.j]\n",
      " [-0.25379642+0.j -0.41229426+0.j -0.30329395+0.j  0.82073967+0.j]\n",
      " [-0.29280622+0.j  0.50264251+0.j  0.60089405+0.j  0.54821653+0.j]\n",
      " [-0.36274435+0.j -0.57143985+0.j -0.66359935+0.j  0.31860466+0.j]\n",
      " [-0.42345398+0.j  0.00352289+0.j  0.56248189+0.j  0.71013269+0.j]\n",
      " [ 0.8054218 +0.j -0.0220022 +0.j  0.17548075+0.j  0.56570146+0.j]\n",
      " [ 0.696207  +0.j  0.57531071+0.j  0.24526556+0.j -0.3523609 +0.j]\n",
      " [-0.77342261+0.j -0.45620992+0.j -0.39087876+0.j  0.2022468 +0.j]\n",
      " [-0.82182386+0.j -0.31467168+0.j  0.30119029+0.j  0.3672488 +0.j]\n",
      " [ 0.2685487 +0.j -0.79363495+0.j -0.19209296+0.j  0.51100435+0.j]\n",
      " [-0.40254748+0.j -0.56267195+0.j -0.48833372+0.j  0.53187026+0.j]\n",
      " [ 0.93302739+0.j  0.0704565 +0.j -0.11889035+0.j  0.33220604+0.j]\n",
      " [-0.60329371+0.j -0.32385501+0.j  0.61738465+0.j  0.38728649+0.j]\n",
      " [-0.15232109+0.j  0.86050394+0.j -0.2433026 +0.j -0.42087421+0.j]\n",
      " [ 0.81724008+0.j  0.39374977+0.j -0.24920227+0.j  0.33908407+0.j]\n",
      " [ 0.04958289+0.j  0.1370628 +0.j  0.28347506+0.j  0.94783818+0.j]\n",
      " [ 0.29070387+0.j -0.18116863+0.j  0.44194934+0.j  0.82906572+0.j]\n",
      " [-0.21167789+0.j  0.65350137+0.j -0.22475624+0.j  0.69109555+0.j]\n",
      " [ 0.73381945+0.j  0.52187732+0.j  0.09897572+0.j -0.42350548+0.j]\n",
      " [-0.51934271+0.j  0.61132965+0.j  0.05063649+0.j -0.59497492+0.j]\n",
      " [-0.87924843+0.j  0.20863703+0.j -0.42312214+0.j -0.06603372+0.j]\n",
      " [-0.25334655+0.j -0.21784715+0.j -0.49465846+0.j  0.80229119+0.j]\n",
      " [-0.66330239+0.j -0.34072462+0.j  0.63046764+0.j  0.21551617+0.j]\n",
      " [ 0.84196054+0.j -0.04699366+0.j  0.40334758+0.j  0.35525312+0.j]\n",
      " [-0.15311041+0.j -0.71063261+0.j  0.21140838+0.j  0.65334906+0.j]\n",
      " [ 0.69381417+0.j -0.44528265+0.j  0.15392416+0.j  0.54465825+0.j]\n",
      " [-0.31589859+0.j  0.62404857+0.j  0.6933783 +0.j -0.17319929+0.j]\n",
      " [ 0.43019309+0.j  0.29612267+0.j  0.30644139+0.j -0.79582595+0.j]\n",
      " [-0.70168097+0.j  0.01578422+0.j  0.44204849+0.j  0.55855869+0.j]\n",
      " [ 0.28237005+0.j  0.76931048+0.j  0.15209125+0.j  0.55253668+0.j]\n",
      " [-0.01767105+0.j  0.44725608+0.j -0.11141609+0.j  0.88726331+0.j]\n",
      " [ 0.76258445+0.j -0.19275379+0.j  0.40473586+0.j -0.46636876+0.j]\n",
      " [ 0.67397846+0.j -0.24312197+0.j  0.55074189+0.j  0.42816832+0.j]\n",
      " [-0.48402184+0.j  0.37873804+0.j  0.20121667+0.j  0.76275305+0.j]]\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0000000000000002\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0000000000000002\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0000000000000002\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999998\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0000000000000002\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999998\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = generate_training(normalized, 100, 2)\n",
    "print(a)\n",
    "for i in a:\n",
    "    print(np.linalg.norm(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "(4,)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#generate diffusion data\n",
    "n = 2\n",
    "T = 20\n",
    "Ndata = 100\n",
    "\n",
    "diff_hs = np.linspace(0.5, 4., T)\n",
    "\n",
    "model_diff = DiffusionModel(n, T, Ndata)\n",
    "X = torch.from_numpy(generate_training(normalized, Ndata, 2))\n",
    "\n",
    "Xout = np.zeros((T+1, Ndata, 2**n), dtype = np.complex64)\n",
    "Xout[0] = X\n",
    "\n",
    "for t in range(1, T+1):\n",
    "    Xout[t] = model_diff.set_diffusionData_t(t, X, diff_hs[:t], seed = t).numpy()\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_t(model, t, inputs_T, params_tot, Ndata, epochs):\n",
    "    '''\n",
    "    the trianing for the backward PQC at step t\n",
    "    input_tplus1: the output from step t+1, as the role of input at step t\n",
    "    Args:\n",
    "    model: the QDDPM model\n",
    "    t: the diffusion step\n",
    "    inputs_T: the input data at step t=T\n",
    "    params_tot: collection of PQC parameters before step t\n",
    "    Ndata: number of samples in dataset\n",
    "    epochs: the number of iterations\n",
    "    '''\n",
    "    input_tplus1 = model.prepareInput_t(inputs_T, params_tot, t, Ndata) # prepare input\n",
    "    states_diff = model.states_diff\n",
    "    loss_hist = [] # record of training history\n",
    "\n",
    "    # initialize parameters\n",
    "    np.random.seed()\n",
    "    params_t = torch.tensor(np.random.normal(size=2 * model.n_tot * model.L), requires_grad=True)\n",
    "    # set optimizer and learning rate decay\n",
    "    optimizer = torch.optim.Adam([params_t], lr=0.0005)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for step in range(epochs):\n",
    "        indices = np.random.choice(states_diff.shape[1], size=Ndata, replace=False)\n",
    "        true_data = states_diff[t, indices]\n",
    "\n",
    "        output_t = model.backwardOutput_t(input_tplus1, params_t)\n",
    "        loss = naturalDistance(output_t, true_data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_hist.append(loss) # record the current loss\n",
    "        \n",
    "        if step%100 == 0:\n",
    "            loss_value = loss_hist[-1]\n",
    "            print(\"Step %s, loss: %s, time elapsed: %s seconds\"%(step, loss_value, time.time() - t0))\n",
    "\n",
    "    return params_t, torch.stack(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: tensor(0.1279, grad_fn=<SubBackward0>), time elapsed: 0.015958547592163086 seconds\n",
      "Step 100, loss: tensor(0.1005, grad_fn=<SubBackward0>), time elapsed: 1.057617425918579 seconds\n",
      "Step 200, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 2.0956339836120605 seconds\n",
      "Step 300, loss: tensor(0.0178, grad_fn=<SubBackward0>), time elapsed: 3.1342480182647705 seconds\n",
      "Step 400, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 4.163017749786377 seconds\n",
      "Step 500, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 5.191228866577148 seconds\n",
      "Step 600, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 6.219494104385376 seconds\n",
      "Step 700, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 7.637280464172363 seconds\n",
      "Step 800, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 8.668456792831421 seconds\n",
      "Step 900, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 9.70641040802002 seconds\n",
      "Step 1000, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 10.740382194519043 seconds\n",
      "Step 1100, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 11.779857158660889 seconds\n",
      "Step 1200, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 12.812556743621826 seconds\n",
      "Step 1300, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 13.836163759231567 seconds\n",
      "Step 1400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 14.85661768913269 seconds\n",
      "Step 1500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 16.046195030212402 seconds\n",
      "Step 1600, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 17.05966806411743 seconds\n",
      "Step 1700, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 18.08948254585266 seconds\n",
      "Step 1800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 19.110957622528076 seconds\n",
      "Step 1900, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 20.1309597492218 seconds\n",
      "Step 2000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 21.17396354675293 seconds\n",
      "19\n",
      "Step 0, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 0.010200023651123047 seconds\n",
      "Step 100, loss: tensor(0.0640, grad_fn=<SubBackward0>), time elapsed: 1.0414345264434814 seconds\n",
      "Step 200, loss: tensor(0.0285, grad_fn=<SubBackward0>), time elapsed: 2.0731289386749268 seconds\n",
      "Step 300, loss: tensor(0.0202, grad_fn=<SubBackward0>), time elapsed: 3.0860984325408936 seconds\n",
      "Step 400, loss: tensor(0.0128, grad_fn=<SubBackward0>), time elapsed: 4.286565542221069 seconds\n",
      "Step 500, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 5.317123651504517 seconds\n",
      "Step 600, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 6.357370138168335 seconds\n",
      "Step 700, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 7.397172451019287 seconds\n",
      "Step 800, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 8.424487829208374 seconds\n",
      "Step 900, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 9.458362340927124 seconds\n",
      "Step 1000, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 10.494171380996704 seconds\n",
      "Step 1100, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 11.528680562973022 seconds\n",
      "Step 1200, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 12.722903490066528 seconds\n",
      "Step 1300, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 13.731892824172974 seconds\n",
      "Step 1400, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 14.749794483184814 seconds\n",
      "Step 1500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 15.77418065071106 seconds\n",
      "Step 1600, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 16.807339668273926 seconds\n",
      "Step 1700, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 17.835204124450684 seconds\n",
      "Step 1800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 18.859150409698486 seconds\n",
      "Step 1900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 19.88641905784607 seconds\n",
      "Step 2000, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 20.913761854171753 seconds\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1280, grad_fn=<SubBackward0>), time elapsed: 0.01300048828125 seconds\n",
      "Step 100, loss: tensor(0.0779, grad_fn=<SubBackward0>), time elapsed: 1.2419705390930176 seconds\n",
      "Step 200, loss: tensor(0.0370, grad_fn=<SubBackward0>), time elapsed: 2.269727945327759 seconds\n",
      "Step 300, loss: tensor(0.0318, grad_fn=<SubBackward0>), time elapsed: 3.2848944664001465 seconds\n",
      "Step 400, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 4.309795141220093 seconds\n",
      "Step 500, loss: tensor(0.0229, grad_fn=<SubBackward0>), time elapsed: 5.335830450057983 seconds\n",
      "Step 600, loss: tensor(0.0101, grad_fn=<SubBackward0>), time elapsed: 6.356605529785156 seconds\n",
      "Step 700, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 7.376486778259277 seconds\n",
      "Step 800, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 8.395771026611328 seconds\n",
      "Step 900, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 9.413889408111572 seconds\n",
      "Step 1000, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 10.602774143218994 seconds\n",
      "Step 1100, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 11.613810539245605 seconds\n",
      "Step 1200, loss: tensor(0.0080, grad_fn=<SubBackward0>), time elapsed: 12.634810209274292 seconds\n",
      "Step 1300, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 13.655501365661621 seconds\n",
      "Step 1400, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 14.684531927108765 seconds\n",
      "Step 1500, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 15.706712007522583 seconds\n",
      "Step 1600, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 16.731087684631348 seconds\n",
      "Step 1700, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 17.75506830215454 seconds\n",
      "Step 1800, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 18.943925380706787 seconds\n",
      "Step 1900, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 19.95401954650879 seconds\n",
      "Step 2000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 20.9867205619812 seconds\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1365, grad_fn=<SubBackward0>), time elapsed: 0.012003183364868164 seconds\n",
      "Step 100, loss: tensor(0.0982, grad_fn=<SubBackward0>), time elapsed: 1.0315840244293213 seconds\n",
      "Step 200, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 2.0583484172821045 seconds\n",
      "Step 300, loss: tensor(0.0381, grad_fn=<SubBackward0>), time elapsed: 3.085966110229492 seconds\n",
      "Step 400, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 4.11215615272522 seconds\n",
      "Step 500, loss: tensor(0.0227, grad_fn=<SubBackward0>), time elapsed: 5.130238771438599 seconds\n",
      "Step 600, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 6.152007818222046 seconds\n",
      "Step 700, loss: tensor(0.0167, grad_fn=<SubBackward0>), time elapsed: 7.346480131149292 seconds\n",
      "Step 800, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 8.36116075515747 seconds\n",
      "Step 900, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 9.38782262802124 seconds\n",
      "Step 1000, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 10.41233515739441 seconds\n",
      "Step 1100, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 11.43619704246521 seconds\n",
      "Step 1200, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 12.458301782608032 seconds\n",
      "Step 1300, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 13.479439735412598 seconds\n",
      "Step 1400, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 14.50497055053711 seconds\n",
      "Step 1500, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 15.524922609329224 seconds\n",
      "Step 1600, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 16.716777324676514 seconds\n",
      "Step 1700, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 17.732168197631836 seconds\n",
      "Step 1800, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 18.755085229873657 seconds\n",
      "Step 1900, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 19.79628014564514 seconds\n",
      "Step 2000, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 20.827141046524048 seconds\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0573, grad_fn=<SubBackward0>), time elapsed: 0.012017011642456055 seconds\n",
      "Step 100, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 1.0460572242736816 seconds\n",
      "Step 200, loss: tensor(0.0237, grad_fn=<SubBackward0>), time elapsed: 2.0749258995056152 seconds\n",
      "Step 300, loss: tensor(0.0205, grad_fn=<SubBackward0>), time elapsed: 3.0999832153320312 seconds\n",
      "Step 400, loss: tensor(0.0147, grad_fn=<SubBackward0>), time elapsed: 4.303574085235596 seconds\n",
      "Step 500, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 5.314378261566162 seconds\n",
      "Step 600, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 6.335665225982666 seconds\n",
      "Step 700, loss: tensor(0.0089, grad_fn=<SubBackward0>), time elapsed: 7.358018398284912 seconds\n",
      "Step 800, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 8.386138916015625 seconds\n",
      "Step 900, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 9.411913633346558 seconds\n",
      "Step 1000, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 10.437336921691895 seconds\n",
      "Step 1100, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 11.460057497024536 seconds\n",
      "Step 1200, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 12.4800124168396 seconds\n",
      "Step 1300, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 13.667108535766602 seconds\n",
      "Step 1400, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 14.684557914733887 seconds\n",
      "Step 1500, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 15.702630996704102 seconds\n",
      "Step 1600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 16.735799551010132 seconds\n",
      "Step 1700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 17.76087260246277 seconds\n",
      "Step 1800, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 18.7895987033844 seconds\n",
      "Step 1900, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 19.807888984680176 seconds\n",
      "Step 2000, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 20.832054615020752 seconds\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0655, grad_fn=<SubBackward0>), time elapsed: 0.011548280715942383 seconds\n",
      "Step 100, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 1.247631549835205 seconds\n",
      "Step 200, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 2.2767789363861084 seconds\n",
      "Step 300, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 3.3023719787597656 seconds\n",
      "Step 400, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 4.332223653793335 seconds\n",
      "Step 500, loss: tensor(0.0101, grad_fn=<SubBackward0>), time elapsed: 5.358864784240723 seconds\n",
      "Step 600, loss: tensor(0.0083, grad_fn=<SubBackward0>), time elapsed: 6.383825302124023 seconds\n",
      "Step 700, loss: tensor(0.0076, grad_fn=<SubBackward0>), time elapsed: 7.416670322418213 seconds\n",
      "Step 800, loss: tensor(0.0077, grad_fn=<SubBackward0>), time elapsed: 8.45171856880188 seconds\n",
      "Step 900, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 9.469780683517456 seconds\n",
      "Step 1000, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 10.650680303573608 seconds\n",
      "Step 1100, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 11.667904376983643 seconds\n",
      "Step 1200, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 12.69924259185791 seconds\n",
      "Step 1300, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 13.726902961730957 seconds\n",
      "Step 1400, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 14.747792959213257 seconds\n",
      "Step 1500, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 15.771163940429688 seconds\n",
      "Step 1600, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 16.79716181755066 seconds\n",
      "Step 1700, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 17.83171582221985 seconds\n",
      "Step 1800, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 18.847829818725586 seconds\n",
      "Step 1900, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 20.023725509643555 seconds\n",
      "Step 2000, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 21.038652658462524 seconds\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0664, grad_fn=<SubBackward0>), time elapsed: 0.011999845504760742 seconds\n",
      "Step 100, loss: tensor(0.0350, grad_fn=<SubBackward0>), time elapsed: 1.0488612651824951 seconds\n",
      "Step 200, loss: tensor(0.0239, grad_fn=<SubBackward0>), time elapsed: 2.084056854248047 seconds\n",
      "Step 300, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 3.1126410961151123 seconds\n",
      "Step 400, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 4.15314793586731 seconds\n",
      "Step 500, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 5.177137136459351 seconds\n",
      "Step 600, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 6.207223415374756 seconds\n",
      "Step 700, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 7.391499042510986 seconds\n",
      "Step 800, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 8.400648832321167 seconds\n",
      "Step 900, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 9.418954610824585 seconds\n",
      "Step 1000, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 10.4489905834198 seconds\n",
      "Step 1100, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 11.483159065246582 seconds\n",
      "Step 1200, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 12.51285457611084 seconds\n",
      "Step 1300, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 13.539036750793457 seconds\n",
      "Step 1400, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 14.563213109970093 seconds\n",
      "Step 1500, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 15.587266683578491 seconds\n",
      "Step 1600, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 16.775150537490845 seconds\n",
      "Step 1700, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 17.791515350341797 seconds\n",
      "Step 1800, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 18.811371326446533 seconds\n",
      "Step 1900, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 19.82994794845581 seconds\n",
      "Step 2000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 20.85121250152588 seconds\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1334, grad_fn=<SubBackward0>), time elapsed: 0.010100364685058594 seconds\n",
      "Step 100, loss: tensor(0.0683, grad_fn=<SubBackward0>), time elapsed: 1.041438102722168 seconds\n",
      "Step 200, loss: tensor(0.0674, grad_fn=<SubBackward0>), time elapsed: 2.0676703453063965 seconds\n",
      "Step 300, loss: tensor(0.0457, grad_fn=<SubBackward0>), time elapsed: 3.085813522338867 seconds\n",
      "Step 400, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 4.287287712097168 seconds\n",
      "Step 500, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 5.300792455673218 seconds\n",
      "Step 600, loss: tensor(0.0200, grad_fn=<SubBackward0>), time elapsed: 6.31961464881897 seconds\n",
      "Step 700, loss: tensor(0.0196, grad_fn=<SubBackward0>), time elapsed: 7.340375185012817 seconds\n",
      "Step 800, loss: tensor(0.0145, grad_fn=<SubBackward0>), time elapsed: 8.36552906036377 seconds\n",
      "Step 900, loss: tensor(0.0172, grad_fn=<SubBackward0>), time elapsed: 9.390660524368286 seconds\n",
      "Step 1000, loss: tensor(0.0146, grad_fn=<SubBackward0>), time elapsed: 10.419198274612427 seconds\n",
      "Step 1100, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 11.438832998275757 seconds\n",
      "Step 1200, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 12.471539735794067 seconds\n",
      "Step 1300, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 13.655755281448364 seconds\n",
      "Step 1400, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 14.670381307601929 seconds\n",
      "Step 1500, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 15.697889804840088 seconds\n",
      "Step 1600, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 16.72605037689209 seconds\n",
      "Step 1700, loss: tensor(0.0105, grad_fn=<SubBackward0>), time elapsed: 17.747794151306152 seconds\n",
      "Step 1800, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 18.77452826499939 seconds\n",
      "Step 1900, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 19.80096697807312 seconds\n",
      "Step 2000, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 20.829558849334717 seconds\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1116, grad_fn=<SubBackward0>), time elapsed: 0.010999917984008789 seconds\n",
      "Step 100, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 1.0499155521392822 seconds\n",
      "Step 200, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 2.2688889503479004 seconds\n",
      "Step 300, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 3.293503761291504 seconds\n",
      "Step 400, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 4.326627254486084 seconds\n",
      "Step 500, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 5.365272283554077 seconds\n",
      "Step 600, loss: tensor(0.0243, grad_fn=<SubBackward0>), time elapsed: 6.396981477737427 seconds\n",
      "Step 700, loss: tensor(0.0221, grad_fn=<SubBackward0>), time elapsed: 7.42206883430481 seconds\n",
      "Step 800, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 8.445137977600098 seconds\n",
      "Step 900, loss: tensor(0.0253, grad_fn=<SubBackward0>), time elapsed: 9.479105949401855 seconds\n",
      "Step 1000, loss: tensor(0.0211, grad_fn=<SubBackward0>), time elapsed: 10.668306589126587 seconds\n",
      "Step 1100, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 11.681926012039185 seconds\n",
      "Step 1200, loss: tensor(0.0106, grad_fn=<SubBackward0>), time elapsed: 12.711371898651123 seconds\n",
      "Step 1300, loss: tensor(0.0100, grad_fn=<SubBackward0>), time elapsed: 13.735063076019287 seconds\n",
      "Step 1400, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 14.763993978500366 seconds\n",
      "Step 1500, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 15.805416822433472 seconds\n",
      "Step 1600, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 16.824413537979126 seconds\n",
      "Step 1700, loss: tensor(0.0099, grad_fn=<SubBackward0>), time elapsed: 17.850916147232056 seconds\n",
      "Step 1800, loss: tensor(0.0080, grad_fn=<SubBackward0>), time elapsed: 18.877334117889404 seconds\n",
      "Step 1900, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 20.071606636047363 seconds\n",
      "Step 2000, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 21.088024854660034 seconds\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1489, grad_fn=<SubBackward0>), time elapsed: 0.010576486587524414 seconds\n",
      "Step 100, loss: tensor(0.0830, grad_fn=<SubBackward0>), time elapsed: 1.035933494567871 seconds\n",
      "Step 200, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 2.0750598907470703 seconds\n",
      "Step 300, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 3.117555856704712 seconds\n",
      "Step 400, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 4.148307800292969 seconds\n",
      "Step 500, loss: tensor(0.0270, grad_fn=<SubBackward0>), time elapsed: 5.169841527938843 seconds\n",
      "Step 600, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 6.192577123641968 seconds\n",
      "Step 700, loss: tensor(0.0265, grad_fn=<SubBackward0>), time elapsed: 7.385142803192139 seconds\n",
      "Step 800, loss: tensor(0.0248, grad_fn=<SubBackward0>), time elapsed: 8.40353512763977 seconds\n",
      "Step 900, loss: tensor(0.0210, grad_fn=<SubBackward0>), time elapsed: 9.420701742172241 seconds\n",
      "Step 1000, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 10.445991039276123 seconds\n",
      "Step 1100, loss: tensor(0.0266, grad_fn=<SubBackward0>), time elapsed: 11.461804389953613 seconds\n",
      "Step 1200, loss: tensor(0.0211, grad_fn=<SubBackward0>), time elapsed: 12.48788046836853 seconds\n",
      "Step 1300, loss: tensor(0.0204, grad_fn=<SubBackward0>), time elapsed: 13.520739793777466 seconds\n",
      "Step 1400, loss: tensor(0.0217, grad_fn=<SubBackward0>), time elapsed: 14.553340435028076 seconds\n",
      "Step 1500, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 15.576063871383667 seconds\n",
      "Step 1600, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 16.762418746948242 seconds\n",
      "Step 1700, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 17.782755851745605 seconds\n",
      "Step 1800, loss: tensor(0.0110, grad_fn=<SubBackward0>), time elapsed: 18.80382466316223 seconds\n",
      "Step 1900, loss: tensor(0.0104, grad_fn=<SubBackward0>), time elapsed: 19.82929539680481 seconds\n",
      "Step 2000, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 20.863786458969116 seconds\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1303, grad_fn=<SubBackward0>), time elapsed: 0.009996891021728516 seconds\n",
      "Step 100, loss: tensor(0.0898, grad_fn=<SubBackward0>), time elapsed: 1.0486717224121094 seconds\n",
      "Step 200, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 2.083660125732422 seconds\n",
      "Step 300, loss: tensor(0.0414, grad_fn=<SubBackward0>), time elapsed: 3.1042447090148926 seconds\n",
      "Step 400, loss: tensor(0.0224, grad_fn=<SubBackward0>), time elapsed: 4.122981309890747 seconds\n",
      "Step 500, loss: tensor(0.0252, grad_fn=<SubBackward0>), time elapsed: 5.309542655944824 seconds\n",
      "Step 600, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 6.336927890777588 seconds\n",
      "Step 700, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 7.369407415390015 seconds\n",
      "Step 800, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 8.39395022392273 seconds\n",
      "Step 900, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 9.428956031799316 seconds\n",
      "Step 1000, loss: tensor(0.0098, grad_fn=<SubBackward0>), time elapsed: 10.452131271362305 seconds\n",
      "Step 1100, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 11.480417966842651 seconds\n",
      "Step 1200, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 12.506579399108887 seconds\n",
      "Step 1300, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 13.692533016204834 seconds\n",
      "Step 1400, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 14.701189517974854 seconds\n",
      "Step 1500, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 15.7275710105896 seconds\n",
      "Step 1600, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 16.76339888572693 seconds\n",
      "Step 1700, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 17.78809380531311 seconds\n",
      "Step 1800, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 18.813443660736084 seconds\n",
      "Step 1900, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 19.85383129119873 seconds\n",
      "Step 2000, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 20.87253737449646 seconds\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.2853, grad_fn=<SubBackward0>), time elapsed: 0.01100015640258789 seconds\n",
      "Step 100, loss: tensor(0.2291, grad_fn=<SubBackward0>), time elapsed: 1.0541586875915527 seconds\n",
      "Step 200, loss: tensor(0.1373, grad_fn=<SubBackward0>), time elapsed: 2.2833757400512695 seconds\n",
      "Step 300, loss: tensor(0.1221, grad_fn=<SubBackward0>), time elapsed: 3.3021230697631836 seconds\n",
      "Step 400, loss: tensor(0.1094, grad_fn=<SubBackward0>), time elapsed: 4.329243898391724 seconds\n",
      "Step 500, loss: tensor(0.1205, grad_fn=<SubBackward0>), time elapsed: 5.366986036300659 seconds\n",
      "Step 600, loss: tensor(0.0843, grad_fn=<SubBackward0>), time elapsed: 6.393982648849487 seconds\n",
      "Step 700, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 7.4347429275512695 seconds\n",
      "Step 800, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 8.460036516189575 seconds\n",
      "Step 900, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 9.483824014663696 seconds\n",
      "Step 1000, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 10.691730976104736 seconds\n",
      "Step 1100, loss: tensor(0.0604, grad_fn=<SubBackward0>), time elapsed: 11.710232734680176 seconds\n",
      "Step 1200, loss: tensor(0.0640, grad_fn=<SubBackward0>), time elapsed: 12.734249353408813 seconds\n",
      "Step 1300, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 13.764049768447876 seconds\n",
      "Step 1400, loss: tensor(0.0615, grad_fn=<SubBackward0>), time elapsed: 14.792875528335571 seconds\n",
      "Step 1500, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 15.82781457901001 seconds\n",
      "Step 1600, loss: tensor(0.0442, grad_fn=<SubBackward0>), time elapsed: 16.852139711380005 seconds\n",
      "Step 1700, loss: tensor(0.0455, grad_fn=<SubBackward0>), time elapsed: 17.880096673965454 seconds\n",
      "Step 1800, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 18.899847507476807 seconds\n",
      "Step 1900, loss: tensor(0.0447, grad_fn=<SubBackward0>), time elapsed: 20.086926698684692 seconds\n",
      "Step 2000, loss: tensor(0.0410, grad_fn=<SubBackward0>), time elapsed: 21.118919849395752 seconds\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1930, grad_fn=<SubBackward0>), time elapsed: 0.012000083923339844 seconds\n",
      "Step 100, loss: tensor(0.1393, grad_fn=<SubBackward0>), time elapsed: 1.0413777828216553 seconds\n",
      "Step 200, loss: tensor(0.0973, grad_fn=<SubBackward0>), time elapsed: 2.07433819770813 seconds\n",
      "Step 300, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 3.102006435394287 seconds\n",
      "Step 400, loss: tensor(0.0721, grad_fn=<SubBackward0>), time elapsed: 4.1282875537872314 seconds\n",
      "Step 500, loss: tensor(0.0687, grad_fn=<SubBackward0>), time elapsed: 5.151849269866943 seconds\n",
      "Step 600, loss: tensor(0.0476, grad_fn=<SubBackward0>), time elapsed: 6.175885200500488 seconds\n",
      "Step 700, loss: tensor(0.0416, grad_fn=<SubBackward0>), time elapsed: 7.36531925201416 seconds\n",
      "Step 800, loss: tensor(0.0441, grad_fn=<SubBackward0>), time elapsed: 8.377804040908813 seconds\n",
      "Step 900, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 9.419761896133423 seconds\n",
      "Step 1000, loss: tensor(0.0376, grad_fn=<SubBackward0>), time elapsed: 10.448932647705078 seconds\n",
      "Step 1100, loss: tensor(0.0401, grad_fn=<SubBackward0>), time elapsed: 11.493886470794678 seconds\n",
      "Step 1200, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 12.521020889282227 seconds\n",
      "Step 1300, loss: tensor(0.0272, grad_fn=<SubBackward0>), time elapsed: 13.545867443084717 seconds\n",
      "Step 1400, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 14.592476844787598 seconds\n",
      "Step 1500, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 15.61992335319519 seconds\n",
      "Step 1600, loss: tensor(0.0238, grad_fn=<SubBackward0>), time elapsed: 16.80687403678894 seconds\n",
      "Step 1700, loss: tensor(0.0232, grad_fn=<SubBackward0>), time elapsed: 17.823013067245483 seconds\n",
      "Step 1800, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 18.8463716506958 seconds\n",
      "Step 1900, loss: tensor(0.0233, grad_fn=<SubBackward0>), time elapsed: 19.885820150375366 seconds\n",
      "Step 2000, loss: tensor(0.0244, grad_fn=<SubBackward0>), time elapsed: 20.91464138031006 seconds\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1936, grad_fn=<SubBackward0>), time elapsed: 0.010019063949584961 seconds\n",
      "Step 100, loss: tensor(0.1185, grad_fn=<SubBackward0>), time elapsed: 1.0432465076446533 seconds\n",
      "Step 200, loss: tensor(0.1280, grad_fn=<SubBackward0>), time elapsed: 2.0710248947143555 seconds\n",
      "Step 300, loss: tensor(0.1342, grad_fn=<SubBackward0>), time elapsed: 3.1003851890563965 seconds\n",
      "Step 400, loss: tensor(0.1444, grad_fn=<SubBackward0>), time elapsed: 4.306182384490967 seconds\n",
      "Step 500, loss: tensor(0.1788, grad_fn=<SubBackward0>), time elapsed: 5.3131139278411865 seconds\n",
      "Step 600, loss: tensor(0.1212, grad_fn=<SubBackward0>), time elapsed: 6.332702159881592 seconds\n",
      "Step 700, loss: tensor(0.1669, grad_fn=<SubBackward0>), time elapsed: 7.356356143951416 seconds\n",
      "Step 800, loss: tensor(0.1031, grad_fn=<SubBackward0>), time elapsed: 8.389400005340576 seconds\n",
      "Step 900, loss: tensor(0.1128, grad_fn=<SubBackward0>), time elapsed: 9.413083553314209 seconds\n",
      "Step 1000, loss: tensor(0.1259, grad_fn=<SubBackward0>), time elapsed: 10.435097932815552 seconds\n",
      "Step 1100, loss: tensor(0.1103, grad_fn=<SubBackward0>), time elapsed: 11.462202548980713 seconds\n",
      "Step 1200, loss: tensor(0.0946, grad_fn=<SubBackward0>), time elapsed: 12.485044956207275 seconds\n",
      "Step 1300, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 13.680708885192871 seconds\n",
      "Step 1400, loss: tensor(0.0612, grad_fn=<SubBackward0>), time elapsed: 14.699596166610718 seconds\n",
      "Step 1500, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 15.724270582199097 seconds\n",
      "Step 1600, loss: tensor(0.0784, grad_fn=<SubBackward0>), time elapsed: 16.76620841026306 seconds\n",
      "Step 1700, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 17.798650979995728 seconds\n",
      "Step 1800, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 18.8398597240448 seconds\n",
      "Step 1900, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 19.86531400680542 seconds\n",
      "Step 2000, loss: tensor(0.0606, grad_fn=<SubBackward0>), time elapsed: 20.898635864257812 seconds\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.3764, grad_fn=<SubBackward0>), time elapsed: 0.01100015640258789 seconds\n",
      "Step 100, loss: tensor(0.2781, grad_fn=<SubBackward0>), time elapsed: 1.2412290573120117 seconds\n",
      "Step 200, loss: tensor(0.1905, grad_fn=<SubBackward0>), time elapsed: 2.2735402584075928 seconds\n",
      "Step 300, loss: tensor(0.1543, grad_fn=<SubBackward0>), time elapsed: 3.2940104007720947 seconds\n",
      "Step 400, loss: tensor(0.1588, grad_fn=<SubBackward0>), time elapsed: 4.317279577255249 seconds\n",
      "Step 500, loss: tensor(0.1275, grad_fn=<SubBackward0>), time elapsed: 5.344388008117676 seconds\n",
      "Step 600, loss: tensor(0.1003, grad_fn=<SubBackward0>), time elapsed: 6.3716185092926025 seconds\n",
      "Step 700, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 7.414200782775879 seconds\n",
      "Step 800, loss: tensor(0.1022, grad_fn=<SubBackward0>), time elapsed: 8.444161891937256 seconds\n",
      "Step 900, loss: tensor(0.0985, grad_fn=<SubBackward0>), time elapsed: 9.463188171386719 seconds\n",
      "Step 1000, loss: tensor(0.0828, grad_fn=<SubBackward0>), time elapsed: 10.650282144546509 seconds\n",
      "Step 1100, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 11.664243936538696 seconds\n",
      "Step 1200, loss: tensor(0.0696, grad_fn=<SubBackward0>), time elapsed: 12.691188335418701 seconds\n",
      "Step 1300, loss: tensor(0.0708, grad_fn=<SubBackward0>), time elapsed: 13.716930150985718 seconds\n",
      "Step 1400, loss: tensor(0.0620, grad_fn=<SubBackward0>), time elapsed: 14.74273681640625 seconds\n",
      "Step 1500, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 15.775148153305054 seconds\n",
      "Step 1600, loss: tensor(0.0685, grad_fn=<SubBackward0>), time elapsed: 16.80504012107849 seconds\n",
      "Step 1700, loss: tensor(0.0563, grad_fn=<SubBackward0>), time elapsed: 17.842220067977905 seconds\n",
      "Step 1800, loss: tensor(0.0628, grad_fn=<SubBackward0>), time elapsed: 18.867844820022583 seconds\n",
      "Step 1900, loss: tensor(0.0610, grad_fn=<SubBackward0>), time elapsed: 20.051153421401978 seconds\n",
      "Step 2000, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 21.068073272705078 seconds\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1447, grad_fn=<SubBackward0>), time elapsed: 0.0103302001953125 seconds\n",
      "Step 100, loss: tensor(0.1312, grad_fn=<SubBackward0>), time elapsed: 1.047374963760376 seconds\n",
      "Step 200, loss: tensor(0.1236, grad_fn=<SubBackward0>), time elapsed: 2.0761466026306152 seconds\n",
      "Step 300, loss: tensor(0.1633, grad_fn=<SubBackward0>), time elapsed: 3.0999412536621094 seconds\n",
      "Step 400, loss: tensor(0.1287, grad_fn=<SubBackward0>), time elapsed: 4.119400978088379 seconds\n",
      "Step 500, loss: tensor(0.1443, grad_fn=<SubBackward0>), time elapsed: 5.140653371810913 seconds\n",
      "Step 600, loss: tensor(0.1127, grad_fn=<SubBackward0>), time elapsed: 6.174041986465454 seconds\n",
      "Step 700, loss: tensor(0.1389, grad_fn=<SubBackward0>), time elapsed: 7.36420464515686 seconds\n",
      "Step 800, loss: tensor(0.1115, grad_fn=<SubBackward0>), time elapsed: 8.38047981262207 seconds\n",
      "Step 900, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 9.404594659805298 seconds\n",
      "Step 1000, loss: tensor(0.1122, grad_fn=<SubBackward0>), time elapsed: 10.430838584899902 seconds\n",
      "Step 1100, loss: tensor(0.0972, grad_fn=<SubBackward0>), time elapsed: 11.458957195281982 seconds\n",
      "Step 1200, loss: tensor(0.0998, grad_fn=<SubBackward0>), time elapsed: 12.481886148452759 seconds\n",
      "Step 1300, loss: tensor(0.1131, grad_fn=<SubBackward0>), time elapsed: 13.50143313407898 seconds\n",
      "Step 1400, loss: tensor(0.0978, grad_fn=<SubBackward0>), time elapsed: 14.521260023117065 seconds\n",
      "Step 1500, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 15.550427675247192 seconds\n",
      "Step 1600, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 16.74344253540039 seconds\n",
      "Step 1700, loss: tensor(0.1036, grad_fn=<SubBackward0>), time elapsed: 17.759500741958618 seconds\n",
      "Step 1800, loss: tensor(0.0918, grad_fn=<SubBackward0>), time elapsed: 18.782525777816772 seconds\n",
      "Step 1900, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 19.798415660858154 seconds\n",
      "Step 2000, loss: tensor(0.1197, grad_fn=<SubBackward0>), time elapsed: 20.834988832473755 seconds\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1934, grad_fn=<SubBackward0>), time elapsed: 0.011508703231811523 seconds\n",
      "Step 100, loss: tensor(0.1789, grad_fn=<SubBackward0>), time elapsed: 1.039128303527832 seconds\n",
      "Step 200, loss: tensor(0.1550, grad_fn=<SubBackward0>), time elapsed: 2.0598809719085693 seconds\n",
      "Step 300, loss: tensor(0.1378, grad_fn=<SubBackward0>), time elapsed: 3.0759549140930176 seconds\n",
      "Step 400, loss: tensor(0.1288, grad_fn=<SubBackward0>), time elapsed: 4.29172158241272 seconds\n",
      "Step 500, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 5.30836033821106 seconds\n",
      "Step 600, loss: tensor(0.1206, grad_fn=<SubBackward0>), time elapsed: 6.325976610183716 seconds\n",
      "Step 700, loss: tensor(0.1142, grad_fn=<SubBackward0>), time elapsed: 7.353329658508301 seconds\n",
      "Step 800, loss: tensor(0.1177, grad_fn=<SubBackward0>), time elapsed: 8.379655361175537 seconds\n",
      "Step 900, loss: tensor(0.0997, grad_fn=<SubBackward0>), time elapsed: 9.402849674224854 seconds\n",
      "Step 1000, loss: tensor(0.0987, grad_fn=<SubBackward0>), time elapsed: 10.435529232025146 seconds\n",
      "Step 1100, loss: tensor(0.1079, grad_fn=<SubBackward0>), time elapsed: 11.460031986236572 seconds\n",
      "Step 1200, loss: tensor(0.0861, grad_fn=<SubBackward0>), time elapsed: 12.47447156906128 seconds\n",
      "Step 1300, loss: tensor(0.1180, grad_fn=<SubBackward0>), time elapsed: 13.657655715942383 seconds\n",
      "Step 1400, loss: tensor(0.0815, grad_fn=<SubBackward0>), time elapsed: 14.67491364479065 seconds\n",
      "Step 1500, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 15.691399097442627 seconds\n",
      "Step 1600, loss: tensor(0.0828, grad_fn=<SubBackward0>), time elapsed: 16.708171844482422 seconds\n",
      "Step 1700, loss: tensor(0.0832, grad_fn=<SubBackward0>), time elapsed: 17.749884605407715 seconds\n",
      "Step 1800, loss: tensor(0.0807, grad_fn=<SubBackward0>), time elapsed: 18.772091150283813 seconds\n",
      "Step 1900, loss: tensor(0.0937, grad_fn=<SubBackward0>), time elapsed: 19.80957579612732 seconds\n",
      "Step 2000, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 20.839190244674683 seconds\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.4470, grad_fn=<SubBackward0>), time elapsed: 0.010976076126098633 seconds\n",
      "Step 100, loss: tensor(0.3756, grad_fn=<SubBackward0>), time elapsed: 1.2436456680297852 seconds\n",
      "Step 200, loss: tensor(0.2998, grad_fn=<SubBackward0>), time elapsed: 2.2707231044769287 seconds\n",
      "Step 300, loss: tensor(0.2263, grad_fn=<SubBackward0>), time elapsed: 3.2949249744415283 seconds\n",
      "Step 400, loss: tensor(0.1708, grad_fn=<SubBackward0>), time elapsed: 4.329944372177124 seconds\n",
      "Step 500, loss: tensor(0.1266, grad_fn=<SubBackward0>), time elapsed: 5.359202146530151 seconds\n",
      "Step 600, loss: tensor(0.1166, grad_fn=<SubBackward0>), time elapsed: 6.384023189544678 seconds\n",
      "Step 700, loss: tensor(0.1226, grad_fn=<SubBackward0>), time elapsed: 7.406362533569336 seconds\n",
      "Step 800, loss: tensor(0.0951, grad_fn=<SubBackward0>), time elapsed: 8.438935995101929 seconds\n",
      "Step 900, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 9.462148666381836 seconds\n",
      "Step 1000, loss: tensor(0.0874, grad_fn=<SubBackward0>), time elapsed: 10.653125286102295 seconds\n",
      "Step 1100, loss: tensor(0.0711, grad_fn=<SubBackward0>), time elapsed: 11.671620845794678 seconds\n",
      "Step 1200, loss: tensor(0.0730, grad_fn=<SubBackward0>), time elapsed: 12.695846557617188 seconds\n",
      "Step 1300, loss: tensor(0.0839, grad_fn=<SubBackward0>), time elapsed: 13.723796129226685 seconds\n",
      "Step 1400, loss: tensor(0.0700, grad_fn=<SubBackward0>), time elapsed: 14.75672721862793 seconds\n",
      "Step 1500, loss: tensor(0.0643, grad_fn=<SubBackward0>), time elapsed: 15.776862144470215 seconds\n",
      "Step 1600, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 16.798951864242554 seconds\n",
      "Step 1700, loss: tensor(0.0632, grad_fn=<SubBackward0>), time elapsed: 17.815141916275024 seconds\n",
      "Step 1800, loss: tensor(0.0690, grad_fn=<SubBackward0>), time elapsed: 18.849886894226074 seconds\n",
      "Step 1900, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 20.036783695220947 seconds\n",
      "Step 2000, loss: tensor(0.0582, grad_fn=<SubBackward0>), time elapsed: 21.05858588218689 seconds\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.4080, grad_fn=<SubBackward0>), time elapsed: 0.009508371353149414 seconds\n",
      "Step 100, loss: tensor(0.2706, grad_fn=<SubBackward0>), time elapsed: 1.0456371307373047 seconds\n",
      "Step 200, loss: tensor(0.2211, grad_fn=<SubBackward0>), time elapsed: 2.0746145248413086 seconds\n",
      "Step 300, loss: tensor(0.1891, grad_fn=<SubBackward0>), time elapsed: 3.1130595207214355 seconds\n",
      "Step 400, loss: tensor(0.2169, grad_fn=<SubBackward0>), time elapsed: 4.139135360717773 seconds\n",
      "Step 500, loss: tensor(0.1481, grad_fn=<SubBackward0>), time elapsed: 5.17361855506897 seconds\n",
      "Step 600, loss: tensor(0.1374, grad_fn=<SubBackward0>), time elapsed: 6.196174383163452 seconds\n",
      "Step 700, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 7.394021987915039 seconds\n",
      "Step 800, loss: tensor(0.1368, grad_fn=<SubBackward0>), time elapsed: 8.409232378005981 seconds\n",
      "Step 900, loss: tensor(0.1212, grad_fn=<SubBackward0>), time elapsed: 9.430415630340576 seconds\n",
      "Step 1000, loss: tensor(0.1132, grad_fn=<SubBackward0>), time elapsed: 10.45900273323059 seconds\n",
      "Step 1100, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 11.506539344787598 seconds\n",
      "Step 1200, loss: tensor(0.1055, grad_fn=<SubBackward0>), time elapsed: 12.544712543487549 seconds\n",
      "Step 1300, loss: tensor(0.1195, grad_fn=<SubBackward0>), time elapsed: 13.581639766693115 seconds\n",
      "Step 1400, loss: tensor(0.1100, grad_fn=<SubBackward0>), time elapsed: 14.606959104537964 seconds\n",
      "Step 1500, loss: tensor(0.1040, grad_fn=<SubBackward0>), time elapsed: 15.627816677093506 seconds\n",
      "Step 1600, loss: tensor(0.1084, grad_fn=<SubBackward0>), time elapsed: 16.809272527694702 seconds\n",
      "Step 1700, loss: tensor(0.0965, grad_fn=<SubBackward0>), time elapsed: 17.8314208984375 seconds\n",
      "Step 1800, loss: tensor(0.0955, grad_fn=<SubBackward0>), time elapsed: 18.853947639465332 seconds\n",
      "Step 1900, loss: tensor(0.0886, grad_fn=<SubBackward0>), time elapsed: 19.8720383644104 seconds\n",
      "Step 2000, loss: tensor(0.0992, grad_fn=<SubBackward0>), time elapsed: 20.898561716079712 seconds\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.3696, grad_fn=<SubBackward0>), time elapsed: 0.010018110275268555 seconds\n",
      "Step 100, loss: tensor(0.2362, grad_fn=<SubBackward0>), time elapsed: 1.0499475002288818 seconds\n",
      "Step 200, loss: tensor(0.1606, grad_fn=<SubBackward0>), time elapsed: 2.091799259185791 seconds\n",
      "Step 300, loss: tensor(0.1218, grad_fn=<SubBackward0>), time elapsed: 3.1128041744232178 seconds\n",
      "Step 400, loss: tensor(0.1152, grad_fn=<SubBackward0>), time elapsed: 4.31142520904541 seconds\n",
      "Step 500, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 5.3229756355285645 seconds\n",
      "Step 600, loss: tensor(0.1143, grad_fn=<SubBackward0>), time elapsed: 6.353042364120483 seconds\n",
      "Step 700, loss: tensor(0.1185, grad_fn=<SubBackward0>), time elapsed: 7.378042697906494 seconds\n",
      "Step 800, loss: tensor(0.0994, grad_fn=<SubBackward0>), time elapsed: 8.401234865188599 seconds\n",
      "Step 900, loss: tensor(0.1108, grad_fn=<SubBackward0>), time elapsed: 9.423174619674683 seconds\n",
      "Step 1000, loss: tensor(0.1162, grad_fn=<SubBackward0>), time elapsed: 10.449430704116821 seconds\n",
      "Step 1100, loss: tensor(0.1470, grad_fn=<SubBackward0>), time elapsed: 11.485125064849854 seconds\n",
      "Step 1200, loss: tensor(0.1219, grad_fn=<SubBackward0>), time elapsed: 12.514345645904541 seconds\n",
      "Step 1300, loss: tensor(0.1254, grad_fn=<SubBackward0>), time elapsed: 13.697916746139526 seconds\n",
      "Step 1400, loss: tensor(0.1167, grad_fn=<SubBackward0>), time elapsed: 14.721850156784058 seconds\n",
      "Step 1500, loss: tensor(0.1848, grad_fn=<SubBackward0>), time elapsed: 15.74963116645813 seconds\n",
      "Step 1600, loss: tensor(0.1369, grad_fn=<SubBackward0>), time elapsed: 16.78676676750183 seconds\n",
      "Step 1700, loss: tensor(0.1321, grad_fn=<SubBackward0>), time elapsed: 17.814948320388794 seconds\n",
      "Step 1800, loss: tensor(0.1327, grad_fn=<SubBackward0>), time elapsed: 18.83895969390869 seconds\n",
      "Step 1900, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 19.866875886917114 seconds\n",
      "Step 2000, loss: tensor(0.0983, grad_fn=<SubBackward0>), time elapsed: 20.894116640090942 seconds\n"
     ]
    }
   ],
   "source": [
    "#train on diffusion data\n",
    "n, na = 2, 1 # number of data and ancilla qubits\n",
    "T = 20 # number of diffusion steps\n",
    "L = 6 # layer of backward PQC\n",
    "Ndata = 100 # number of data in the training data set\n",
    "epochs = 2001 # number of training epochs\n",
    "\n",
    "#gen random samples at t = T\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_T = diffModel.HaarSampleGeneration(Ndata, seed = 22)\n",
    "\n",
    "#load diffusion process\n",
    "states_diff = Xout\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "model.set_diffusionSet(states_diff)\n",
    "\n",
    "params_total = []\n",
    "loss_hist_total = []\n",
    "\n",
    "for t in range(T - 1, -1, -1):\n",
    "    params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "    for tt in range(t+1, 20):\n",
    "        print(tt)\n",
    "        params_tot[tt] = np.load('data/cluster/n2/QDDPMcluster0params_n2na1T20L6_t%d_mmd.npy'%tt)\n",
    "    params, loss_hist = Training_t(model, t, inputs_T, params_tot, Ndata, epochs)\n",
    "    params = params.detach().numpy()\n",
    "    loss_hist = loss_hist.detach().numpy()\n",
    "    np.save('params_T%d'%t, params)\n",
    "\n",
    "    \n",
    "    np.save('loss_hist_T%d'%t, loss_hist)\n",
    "    #params_total = np.append(params_total, params, axis = 1)\n",
    "    #loss_hist_total = np.append(loss_hist_total, loss_hist, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.33]\n",
      " [0.66 0.99]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        +0.j 0.26726124+0.j 0.53452248+0.j 0.80178373+0.j]\n"
     ]
    }
   ],
   "source": [
    "values = np.array([pixelValues[0:2], pixelValues[2:4]])\n",
    "print(values)\n",
    "normalized = ie.normalize(values)\n",
    "plt.imshow(values, cmap='grey', interpolation='nearest')\n",
    "plt.show()\n",
    "normalized = normalized.ravel() + 0j*np.zeros(4)\n",
    "print(normalized)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
