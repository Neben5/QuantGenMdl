{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import unitary_group\n",
    "from skimage.measure import block_reduce\n",
    "from opt_einsum import contract\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from src.QDDPM_torch import DiffusionModel, QDDPM, naturalDistance\n",
    "import src.ImageEncode as ie\n",
    "rc('text', usetex=False)\n",
    "rc('axes', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Torch will use CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Torch will use CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_encode(image_values: np.array, npixels: int):\n",
    "    '''\n",
    "    Args:\n",
    "        image_values: 2-dimensional array (non hairy) of image data\n",
    "    '''\n",
    "    norm: float = np.linalg.norm(image_values)\n",
    "    normalized_image_values: np.array = image_values.flatten()/norm\n",
    "    return normalized_image_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.33\n",
      "0.66\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "pixelValues = []\n",
    "for i in range(4):\n",
    "    value = 0.33 * i\n",
    "    pixelValues.append(value)\n",
    "    print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.33]\n",
      " [0.66 0.99]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.26726124 0.53452248 0.80178373]\n"
     ]
    }
   ],
   "source": [
    "values = np.array([pixelValues[0:2], pixelValues[2:4]])\n",
    "print(values)\n",
    "normalized = ie.normalize(values)\n",
    "plt.imshow(values, cmap='grey', interpolation='nearest')\n",
    "plt.show()\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training(values: np.array, n_train: int, scale: float, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    n = values.size\n",
    "    noise = np.random.randn(n_train,n)+0j*np.random.randn(n_train,n) \n",
    "    print(noise.shape)\n",
    "    print(values.shape)\n",
    "    states = (noise*scale) + values\n",
    "    states/=np.tile(np.linalg.norm(states, axis=1).reshape(1,n_train), (n,1)).T\n",
    "    return states\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "(4,)\n",
      "[[ 2.04543051e-01+0.j -1.80815269e-01+0.j -6.20780375e-01+0.j\n",
      "  -7.34914760e-01+0.j]\n",
      " [-3.15818491e-01+0.j  2.28187356e-01+0.j  8.46549592e-01+0.j\n",
      "  -3.62688571e-01+0.j]\n",
      " [-2.39716674e-01+0.j -8.16197019e-03+0.j  7.56994361e-01+0.j\n",
      "   6.07806577e-01+0.j]\n",
      " [ 9.13570309e-01+0.j  3.86007055e-01+0.j  2.95149849e-02+0.j\n",
      "   1.24566083e-01+0.j]\n",
      " [-4.28905815e-01+0.j -4.50670535e-01+0.j  4.72613020e-01+0.j\n",
      "  -6.24157676e-01+0.j]\n",
      " [ 3.77093394e-01+0.j -8.36091296e-01+0.j  7.06478729e-02+0.j\n",
      "  -3.92123444e-01+0.j]\n",
      " [-4.07752528e-01+0.j -6.96920436e-01+0.j -9.02186049e-04+0.j\n",
      "   5.89948275e-01+0.j]\n",
      " [ 7.63116866e-01+0.j -1.42132320e-01+0.j  6.00591829e-01+0.j\n",
      "  -1.91678137e-01+0.j]\n",
      " [ 1.69959099e-01+0.j  3.54638156e-01+0.j -9.12853935e-01+0.j\n",
      "  -1.09742322e-01+0.j]\n",
      " [-5.74739008e-01+0.j  7.86301294e-02+0.j  1.77917603e-01+0.j\n",
      "  -7.94882194e-01+0.j]\n",
      " [-2.46541625e-01+0.j -7.90741933e-01+0.j  7.35090651e-02+0.j\n",
      "   5.55464526e-01+0.j]\n",
      " [-6.19567212e-01+0.j  4.08300285e-01+0.j -6.25723373e-02+0.j\n",
      "   6.67466890e-01+0.j]\n",
      " [-9.02671248e-01+0.j  1.61990586e-01+0.j  3.96753576e-01+0.j\n",
      "   3.91186316e-02+0.j]\n",
      " [-5.81775982e-01+0.j  6.34298754e-01+0.j -4.89829262e-01+0.j\n",
      "   1.38813157e-01+0.j]\n",
      " [ 7.81391305e-01+0.j -1.43558535e-03+0.j  5.05415692e-01+0.j\n",
      "  -3.66033530e-01+0.j]\n",
      " [-3.12606686e-01+0.j -5.45854158e-01+0.j  5.28309957e-01+0.j\n",
      "  -5.70270890e-01+0.j]\n",
      " [-2.42100160e-01+0.j  2.93910532e-01+0.j  9.21605789e-01+0.j\n",
      "  -7.51457326e-02+0.j]\n",
      " [ 6.40355304e-01+0.j  1.29824605e-01+0.j -6.32464072e-01+0.j\n",
      "  -4.16028671e-01+0.j]\n",
      " [-6.78835727e-01+0.j  9.75833496e-03+0.j -2.32582809e-01+0.j\n",
      "   6.96413719e-01+0.j]\n",
      " [ 3.36873444e-01+0.j  1.21968657e-02+0.j -6.52251370e-01+0.j\n",
      "   6.78922432e-01+0.j]\n",
      " [-2.75886644e-01+0.j  8.84585692e-01+0.j -3.70626291e-01+0.j\n",
      "   6.34890937e-02+0.j]\n",
      " [-9.24580111e-01+0.j -2.10951229e-01+0.j -3.06473101e-01+0.j\n",
      "  -8.20087549e-02+0.j]\n",
      " [-7.42132249e-01+0.j -1.98803795e-01+0.j  6.36051495e-01+0.j\n",
      "   7.18002193e-02+0.j]\n",
      " [-8.91005587e-01+0.j -4.19223296e-01+0.j  1.69481390e-01+0.j\n",
      "  -4.04589971e-02+0.j]\n",
      " [ 6.90159223e-02+0.j  9.84501787e-01+0.j  2.63877141e-02+0.j\n",
      "   1.59049436e-01+0.j]\n",
      " [-7.84138231e-02+0.j  9.77708256e-01+0.j -1.92747793e-01+0.j\n",
      "   2.80379652e-02+0.j]\n",
      " [ 6.88572252e-01+0.j -8.81956875e-02+0.j -6.71628840e-01+0.j\n",
      "   2.58852229e-01+0.j]\n",
      " [ 2.70163222e-01+0.j -5.41371493e-01+0.j -3.07553874e-01+0.j\n",
      "   7.34397273e-01+0.j]\n",
      " [-2.07503687e-01+0.j -4.70107104e-02+0.j  5.75800466e-01+0.j\n",
      "   7.89421330e-01+0.j]\n",
      " [ 5.19087564e-01+0.j -1.85965681e-01+0.j -6.90251179e-01+0.j\n",
      "   4.68527669e-01+0.j]\n",
      " [-2.18230148e-01+0.j  9.20107400e-01+0.j  3.24546730e-01+0.j\n",
      "   2.11516993e-02+0.j]\n",
      " [ 8.79301984e-01+0.j  3.38482978e-01+0.j  1.93948032e-01+0.j\n",
      "  -2.73205884e-01+0.j]\n",
      " [-5.19785642e-01+0.j -1.39035053e-01+0.j  6.56970072e-01+0.j\n",
      "   5.28093235e-01+0.j]\n",
      " [-8.94591880e-03+0.j -8.12507204e-01+0.j  3.46455544e-01+0.j\n",
      "   4.68743608e-01+0.j]\n",
      " [-8.08199888e-01+0.j -1.87884992e-01+0.j  2.66646820e-01+0.j\n",
      "  -4.90317901e-01+0.j]\n",
      " [ 4.08039505e-01+0.j  2.01241036e-01+0.j -8.89646330e-01+0.j\n",
      "  -3.91818327e-02+0.j]\n",
      " [-6.06981611e-02+0.j  3.45412104e-01+0.j -3.05827157e-01+0.j\n",
      "   8.85141775e-01+0.j]\n",
      " [-3.57756366e-01+0.j  8.64599636e-01+0.j  3.47236257e-01+0.j\n",
      "  -6.24886694e-02+0.j]\n",
      " [ 2.00219380e-02+0.j -2.93093416e-01+0.j  7.49186468e-01+0.j\n",
      "   5.93645523e-01+0.j]\n",
      " [ 6.05347230e-02+0.j  5.59949229e-01+0.j -6.53971049e-01+0.j\n",
      "   5.05088384e-01+0.j]\n",
      " [ 3.77138357e-01+0.j -1.34313095e-01+0.j -9.12922279e-01+0.j\n",
      "  -7.93697999e-02+0.j]\n",
      " [ 1.19215277e-01+0.j  3.73726578e-01+0.j -6.34493776e-01+0.j\n",
      "  -6.65983342e-01+0.j]\n",
      " [ 1.79000679e-01+0.j  1.74305165e-01+0.j  8.08895041e-01+0.j\n",
      "   5.32226717e-01+0.j]\n",
      " [-9.35434597e-01+0.j -1.13302189e-01+0.j -8.95710296e-02+0.j\n",
      "  -3.22648043e-01+0.j]\n",
      " [ 8.20815415e-01+0.j -3.12130368e-01+0.j  3.86711609e-01+0.j\n",
      "   2.81586255e-01+0.j]\n",
      " [ 3.63091111e-01+0.j  4.63320064e-01+0.j  3.27183889e-01+0.j\n",
      "   7.39222609e-01+0.j]\n",
      " [-1.63860389e-01+0.j  8.56588681e-01+0.j -4.67156852e-01+0.j\n",
      "   1.45499418e-01+0.j]\n",
      " [ 6.07926630e-01+0.j  4.53783087e-01+0.j -4.94420845e-01+0.j\n",
      "  -4.24327881e-01+0.j]\n",
      " [-5.18068663e-01+0.j  2.53934135e-01+0.j  6.00670701e-01+0.j\n",
      "   5.53459144e-01+0.j]\n",
      " [-7.86797020e-01+0.j -4.58463586e-01+0.j  3.21418374e-01+0.j\n",
      "   2.59714880e-01+0.j]\n",
      " [ 2.07120284e-01+0.j  4.91592244e-02+0.j -5.81126228e-01+0.j\n",
      "   7.85478749e-01+0.j]\n",
      " [ 9.98855429e-01+0.j -7.47370312e-03+0.j  4.48387949e-02+0.j\n",
      "  -1.48815034e-02+0.j]\n",
      " [ 7.84978787e-02+0.j -7.37193206e-01+0.j -5.95450948e-01+0.j\n",
      "  -3.09551980e-01+0.j]\n",
      " [ 5.04726631e-01+0.j -8.22074177e-01+0.j  2.57117795e-01+0.j\n",
      "  -5.77539188e-02+0.j]\n",
      " [ 1.37220967e-02+0.j -3.70647007e-01+0.j  7.11245518e-01+0.j\n",
      "   5.97128390e-01+0.j]\n",
      " [-4.10450853e-01+0.j -4.87627338e-01+0.j -4.38717549e-01+0.j\n",
      "   6.33463960e-01+0.j]\n",
      " [ 7.47908494e-01+0.j -6.95362787e-02+0.j  6.59095496e-01+0.j\n",
      "   3.72923258e-02+0.j]\n",
      " [ 5.19758401e-01+0.j  4.15065132e-01+0.j  6.21349350e-01+0.j\n",
      "   4.14122115e-01+0.j]\n",
      " [ 2.20705143e-01+0.j -3.23857163e-01+0.j -9.19935245e-01+0.j\n",
      "  -1.11769195e-02+0.j]\n",
      " [-3.07725673e-01+0.j -5.24161094e-01+0.j -7.44690652e-01+0.j\n",
      "   2.75673522e-01+0.j]\n",
      " [ 2.99808019e-01+0.j -1.81143958e-01+0.j  2.74478682e-01+0.j\n",
      "   8.95524132e-01+0.j]\n",
      " [ 8.93055635e-01+0.j  4.35015116e-01+0.j -1.13470772e-01+0.j\n",
      "   1.83811335e-02+0.j]\n",
      " [ 2.18341657e-01+0.j  3.28251135e-01+0.j -9.09405919e-01+0.j\n",
      "   1.32510329e-01+0.j]\n",
      " [ 1.51847972e-01+0.j -5.89746027e-01+0.j  7.84840366e-01+0.j\n",
      "  -1.14749370e-01+0.j]\n",
      " [-5.39888826e-01+0.j -4.09141291e-02+0.j  6.96806090e-01+0.j\n",
      "  -4.70433165e-01+0.j]\n",
      " [-1.89727421e-01+0.j  8.19325158e-01+0.j  5.40487618e-01+0.j\n",
      "   2.41438589e-02+0.j]\n",
      " [ 7.10330055e-01+0.j  5.28736322e-01+0.j  8.17887338e-02+0.j\n",
      "   4.57361693e-01+0.j]\n",
      " [ 5.37172064e-01+0.j  1.22504645e-02+0.j  5.47062996e-01+0.j\n",
      "  -6.41886421e-01+0.j]\n",
      " [ 8.64979755e-01+0.j -4.16779647e-01+0.j  2.79282138e-01+0.j\n",
      "   1.03071333e-02+0.j]\n",
      " [ 8.96417988e-02+0.j -7.59041377e-01+0.j -6.19595468e-01+0.j\n",
      "   1.78667266e-01+0.j]\n",
      " [ 3.78253123e-01+0.j  4.44044096e-01+0.j  3.38388642e-01+0.j\n",
      "   7.38405406e-01+0.j]\n",
      " [ 7.07015109e-01+0.j -9.44109005e-03+0.j  2.63567370e-01+0.j\n",
      "   6.56180420e-01+0.j]\n",
      " [ 3.42267439e-02+0.j -9.71384077e-01+0.j -9.58786449e-02+0.j\n",
      "   2.14589818e-01+0.j]\n",
      " [ 1.27500341e-01+0.j  1.09957943e-01+0.j  6.68385162e-01+0.j\n",
      "   7.24509620e-01+0.j]\n",
      " [-7.16231508e-01+0.j -3.07677926e-01+0.j -3.25159581e-01+0.j\n",
      "   5.35367133e-01+0.j]\n",
      " [-6.22694935e-01+0.j -4.51442392e-01+0.j -3.16533793e-01+0.j\n",
      "   5.55209098e-01+0.j]\n",
      " [-4.43026534e-01+0.j  8.48422645e-01+0.j -2.28086596e-01+0.j\n",
      "  -1.78558143e-01+0.j]\n",
      " [ 1.19147102e-01+0.j -4.31957232e-01+0.j -7.32470633e-01+0.j\n",
      "   5.12546280e-01+0.j]\n",
      " [-3.46952065e-01+0.j  1.70376258e-01+0.j  6.40565373e-01+0.j\n",
      "  -6.63530103e-01+0.j]\n",
      " [-1.43409300e-01+0.j  6.60758534e-02+0.j  6.48833527e-01+0.j\n",
      "  -7.44367388e-01+0.j]\n",
      " [ 1.10252255e-01+0.j  1.84137415e-01+0.j  6.31205619e-01+0.j\n",
      "   7.45330342e-01+0.j]\n",
      " [ 5.15176819e-01+0.j  3.62817761e-01+0.j  6.88722272e-01+0.j\n",
      "   3.58633167e-01+0.j]\n",
      " [-2.77574829e-01+0.j  8.11948036e-01+0.j  1.09628701e-01+0.j\n",
      "  -5.01671355e-01+0.j]\n",
      " [-7.40151114e-01+0.j  3.54007784e-01+0.j -4.27300560e-01+0.j\n",
      "   3.79827657e-01+0.j]\n",
      " [-5.49729032e-01+0.j  2.22093096e-01+0.j -2.95585254e-01+0.j\n",
      "  -7.49067424e-01+0.j]\n",
      " [-6.72394059e-01+0.j -1.09526361e-01+0.j -7.17728410e-01+0.j\n",
      "  -1.44069896e-01+0.j]\n",
      " [ 4.05663239e-01+0.j  3.88190076e-01+0.j  7.92835250e-01+0.j\n",
      "   2.36976935e-01+0.j]\n",
      " [-4.68944398e-01+0.j  2.47673302e-01+0.j  2.82428625e-01+0.j\n",
      "   7.99364222e-01+0.j]\n",
      " [-4.57563319e-01+0.j  7.38643254e-01+0.j  4.51112383e-02+0.j\n",
      "   4.92957329e-01+0.j]\n",
      " [ 5.45569626e-01+0.j  1.67784282e-01+0.j  6.38868617e-01+0.j\n",
      "   5.15799484e-01+0.j]\n",
      " [-2.86960592e-01+0.j -2.99845587e-01+0.j  8.90149899e-01+0.j\n",
      "   1.88094125e-01+0.j]\n",
      " [-5.11294024e-01+0.j  2.48295195e-01+0.j  1.36274330e-01+0.j\n",
      "  -8.11392152e-01+0.j]\n",
      " [ 6.96218489e-01+0.j  5.58895145e-01+0.j -6.81549819e-02+0.j\n",
      "  -4.45276241e-01+0.j]\n",
      " [-5.50735570e-01+0.j  3.20491276e-01+0.j  7.64994747e-01+0.j\n",
      "   9.35879833e-02+0.j]\n",
      " [ 1.55307946e-01+0.j  9.83558244e-01+0.j  9.13571101e-02+0.j\n",
      "   1.21037496e-02+0.j]\n",
      " [ 1.07059697e-01+0.j  6.60845946e-01+0.j  7.42797426e-01+0.j\n",
      "   8.53473717e-03+0.j]\n",
      " [ 3.82139047e-03+0.j  6.94921055e-01+0.j -7.10511998e-01+0.j\n",
      "  -1.10647296e-01+0.j]\n",
      " [-8.38569684e-02+0.j -9.69671486e-01+0.j -2.17731670e-01+0.j\n",
      "  -7.27883079e-02+0.j]\n",
      " [ 7.39741328e-01+0.j -4.45111551e-01+0.j  9.41093267e-02+0.j\n",
      "  -4.95784136e-01+0.j]\n",
      " [-8.21659988e-02+0.j -3.82178578e-01+0.j  4.50954864e-01+0.j\n",
      "   8.02388929e-01+0.j]]\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0000000000000002\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999998\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999998\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999998\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0000000000000002\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "0.9999999999999998\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999998\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999998\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0000000000000002\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999998\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0000000000000002\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0000000000000002\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = generate_training(normalized, 100, 2)\n",
    "print(a)\n",
    "for i in a:\n",
    "    print(np.linalg.norm(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4)\n",
      "(4,)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#generate diffusion data\n",
    "n = 2\n",
    "T = 20\n",
    "Ndata = 400\n",
    "\n",
    "diff_hs = np.linspace(0.5, 4., T)\n",
    "\n",
    "model_diff = DiffusionModel(n, T, Ndata)\n",
    "X = torch.from_numpy(generate_training(normalized, Ndata, 2))\n",
    "\n",
    "Xout = np.zeros((T+1, Ndata, 2**n), dtype = np.complex64)\n",
    "Xout[0] = X\n",
    "\n",
    "for t in range(1, T+1):\n",
    "    Xout[t] = model_diff.set_diffusionData_t(t, X, diff_hs[:t], seed = t).numpy()\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_t(model, t, inputs_T, params_tot, Ndata, epochs):\n",
    "    '''\n",
    "    the trianing for the backward PQC at step t\n",
    "    input_tplus1: the output from step t+1, as the role of input at step t\n",
    "    Args:\n",
    "    model: the QDDPM model\n",
    "    t: the diffusion step\n",
    "    inputs_T: the input data at step t=T\n",
    "    params_tot: collection of PQC parameters before step t\n",
    "    Ndata: number of samples in dataset\n",
    "    epochs: the number of iterations\n",
    "    '''\n",
    "    input_tplus1 = model.prepareInput_t(inputs_T, params_tot, t, Ndata) # prepare input\n",
    "    states_diff = model.states_diff\n",
    "    loss_hist = [] # record of training history\n",
    "\n",
    "    # initialize parameters\n",
    "    np.random.seed()\n",
    "    params_t = torch.tensor(np.random.normal(size=2 * model.n_tot * model.L), requires_grad=True)\n",
    "    # set optimizer and learning rate decay\n",
    "    optimizer = torch.optim.Adam([params_t], lr=0.0005)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for step in range(epochs):\n",
    "        indices = np.random.choice(states_diff.shape[1], size=Ndata, replace=False)\n",
    "        true_data = states_diff[t, indices]\n",
    "\n",
    "        output_t = model.backwardOutput_t(input_tplus1, params_t)\n",
    "        loss = naturalDistance(output_t, true_data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_hist.append(loss) # record the current loss\n",
    "        \n",
    "        if step%100 == 0:\n",
    "            loss_value = loss_hist[-1]\n",
    "            print(\"Step %s, loss: %s, time elapsed: %s seconds\"%(step, loss_value, time.time() - t0))\n",
    "\n",
    "    return params_t, torch.stack(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: tensor(0.1064, grad_fn=<SubBackward0>), time elapsed: 0.06264543533325195 seconds\n",
      "Step 100, loss: tensor(0.0725, grad_fn=<SubBackward0>), time elapsed: 6.182528734207153 seconds\n",
      "Step 200, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 12.631946325302124 seconds\n",
      "Step 300, loss: tensor(0.0418, grad_fn=<SubBackward0>), time elapsed: 19.06618905067444 seconds\n",
      "Step 400, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 26.45806097984314 seconds\n",
      "Step 500, loss: tensor(0.0171, grad_fn=<SubBackward0>), time elapsed: 33.10645914077759 seconds\n",
      "Step 600, loss: tensor(0.0199, grad_fn=<SubBackward0>), time elapsed: 39.55510330200195 seconds\n",
      "Step 700, loss: tensor(0.0100, grad_fn=<SubBackward0>), time elapsed: 46.13269257545471 seconds\n",
      "Step 800, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 52.43871736526489 seconds\n",
      "Step 900, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 59.36052703857422 seconds\n",
      "Step 1000, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 65.86505198478699 seconds\n",
      "Step 1100, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 72.01320886611938 seconds\n",
      "Step 1200, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 78.16330075263977 seconds\n",
      "Step 1300, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 84.76418542861938 seconds\n",
      "Step 1400, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 91.10073518753052 seconds\n",
      "Step 1500, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 97.20229816436768 seconds\n",
      "Step 1600, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 103.74063301086426 seconds\n",
      "Step 1700, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 110.3501615524292 seconds\n",
      "Step 1800, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 116.65885639190674 seconds\n",
      "Step 1900, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 123.16902589797974 seconds\n",
      "Step 2000, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 129.60345435142517 seconds\n",
      "19\n",
      "Step 0, loss: tensor(0.0870, grad_fn=<SubBackward0>), time elapsed: 0.051011085510253906 seconds\n",
      "Step 100, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 6.979634046554565 seconds\n",
      "Step 200, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 13.310032367706299 seconds\n",
      "Step 300, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 20.815019845962524 seconds\n",
      "Step 400, loss: tensor(0.0248, grad_fn=<SubBackward0>), time elapsed: 27.01721477508545 seconds\n",
      "Step 500, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 33.96754789352417 seconds\n",
      "Step 600, loss: tensor(0.0125, grad_fn=<SubBackward0>), time elapsed: 40.81656241416931 seconds\n",
      "Step 700, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 47.870763540267944 seconds\n",
      "Step 800, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 55.53788113594055 seconds\n",
      "Step 900, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 62.9039671421051 seconds\n",
      "Step 1000, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 69.92671155929565 seconds\n",
      "Step 1100, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 76.5674057006836 seconds\n",
      "Step 1200, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 83.31623482704163 seconds\n",
      "Step 1300, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 90.68185091018677 seconds\n",
      "Step 1400, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 97.42049789428711 seconds\n",
      "Step 1500, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 103.9086434841156 seconds\n",
      "Step 1600, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 110.92782282829285 seconds\n",
      "Step 1700, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 117.67032527923584 seconds\n",
      "Step 1800, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 124.67160677909851 seconds\n",
      "Step 1900, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 132.14028906822205 seconds\n",
      "Step 2000, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 139.10718417167664 seconds\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 0.05856823921203613 seconds\n",
      "Step 100, loss: tensor(0.0199, grad_fn=<SubBackward0>), time elapsed: 6.868046045303345 seconds\n",
      "Step 200, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 13.403804779052734 seconds\n",
      "Step 300, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 20.633007287979126 seconds\n",
      "Step 400, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 28.96066641807556 seconds\n",
      "Step 500, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 35.435123682022095 seconds\n",
      "Step 600, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 42.19383716583252 seconds\n",
      "Step 700, loss: tensor(0.0107, grad_fn=<SubBackward0>), time elapsed: 48.8583288192749 seconds\n",
      "Step 800, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 56.24790644645691 seconds\n",
      "Step 900, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 63.42526364326477 seconds\n",
      "Step 1000, loss: tensor(0.0083, grad_fn=<SubBackward0>), time elapsed: 69.67316675186157 seconds\n",
      "Step 1100, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 76.04043936729431 seconds\n",
      "Step 1200, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 83.011559009552 seconds\n",
      "Step 1300, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 90.5131721496582 seconds\n",
      "Step 1400, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 97.91275382041931 seconds\n",
      "Step 1500, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 104.21195554733276 seconds\n",
      "Step 1600, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 110.81282639503479 seconds\n",
      "Step 1700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 117.92982864379883 seconds\n",
      "Step 1800, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 125.67981958389282 seconds\n",
      "Step 1900, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 132.74579191207886 seconds\n",
      "Step 2000, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 141.28354692459106 seconds\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0262, grad_fn=<SubBackward0>), time elapsed: 0.05449986457824707 seconds\n",
      "Step 100, loss: tensor(0.0110, grad_fn=<SubBackward0>), time elapsed: 7.3728861808776855 seconds\n",
      "Step 200, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 14.270445823669434 seconds\n",
      "Step 300, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 21.063010454177856 seconds\n",
      "Step 400, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 27.618669271469116 seconds\n",
      "Step 500, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 34.89936375617981 seconds\n",
      "Step 600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 41.60016942024231 seconds\n",
      "Step 700, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 47.98053789138794 seconds\n",
      "Step 800, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 55.47369694709778 seconds\n",
      "Step 900, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 63.015554666519165 seconds\n",
      "Step 1000, loss: tensor(0.0125, grad_fn=<SubBackward0>), time elapsed: 70.31885862350464 seconds\n",
      "Step 1100, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 77.17390656471252 seconds\n",
      "Step 1200, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 84.1295485496521 seconds\n",
      "Step 1300, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 90.76345133781433 seconds\n",
      "Step 1400, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 97.4863669872284 seconds\n",
      "Step 1500, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 104.95543313026428 seconds\n",
      "Step 1600, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 113.16486835479736 seconds\n",
      "Step 1700, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 119.9324312210083 seconds\n",
      "Step 1800, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 126.60040044784546 seconds\n",
      "Step 1900, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 133.4620122909546 seconds\n",
      "Step 2000, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 140.13252353668213 seconds\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0925, grad_fn=<SubBackward0>), time elapsed: 0.05301356315612793 seconds\n",
      "Step 100, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 6.449378967285156 seconds\n",
      "Step 200, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 12.679349184036255 seconds\n",
      "Step 300, loss: tensor(0.0392, grad_fn=<SubBackward0>), time elapsed: 18.95345664024353 seconds\n",
      "Step 400, loss: tensor(0.0219, grad_fn=<SubBackward0>), time elapsed: 25.52216362953186 seconds\n",
      "Step 500, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 31.882687091827393 seconds\n",
      "Step 600, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 39.02375030517578 seconds\n",
      "Step 700, loss: tensor(0.0108, grad_fn=<SubBackward0>), time elapsed: 45.80115294456482 seconds\n",
      "Step 800, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 52.36631751060486 seconds\n",
      "Step 900, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 59.01642823219299 seconds\n",
      "Step 1000, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 65.5796754360199 seconds\n",
      "Step 1100, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 72.92476630210876 seconds\n",
      "Step 1200, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 79.59231758117676 seconds\n",
      "Step 1300, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 85.56669640541077 seconds\n",
      "Step 1400, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 91.78572273254395 seconds\n",
      "Step 1500, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 98.5484447479248 seconds\n",
      "Step 1600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 105.42712354660034 seconds\n",
      "Step 1700, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 112.53870725631714 seconds\n",
      "Step 1800, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 118.7767903804779 seconds\n",
      "Step 1900, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 124.93682670593262 seconds\n",
      "Step 2000, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 131.60610604286194 seconds\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 0.055066823959350586 seconds\n",
      "Step 100, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 6.154043912887573 seconds\n",
      "Step 200, loss: tensor(0.0239, grad_fn=<SubBackward0>), time elapsed: 12.244909048080444 seconds\n",
      "Step 300, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 18.490572452545166 seconds\n",
      "Step 400, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 24.877339363098145 seconds\n",
      "Step 500, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 31.10073947906494 seconds\n",
      "Step 600, loss: tensor(0.0080, grad_fn=<SubBackward0>), time elapsed: 37.09774899482727 seconds\n",
      "Step 700, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 43.12410521507263 seconds\n",
      "Step 800, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 49.58577251434326 seconds\n",
      "Step 900, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 56.04171895980835 seconds\n",
      "Step 1000, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 62.70262694358826 seconds\n",
      "Step 1100, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 68.77027344703674 seconds\n",
      "Step 1200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 74.97898077964783 seconds\n",
      "Step 1300, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 81.15552091598511 seconds\n",
      "Step 1400, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 87.87025809288025 seconds\n",
      "Step 1500, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 96.01528477668762 seconds\n",
      "Step 1600, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 102.80319094657898 seconds\n",
      "Step 1700, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 109.17496800422668 seconds\n",
      "Step 1800, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 115.67521834373474 seconds\n",
      "Step 1900, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 123.1730284690857 seconds\n",
      "Step 2000, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 130.269535779953 seconds\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1193, grad_fn=<SubBackward0>), time elapsed: 0.05856966972351074 seconds\n",
      "Step 100, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 5.934926509857178 seconds\n",
      "Step 200, loss: tensor(0.0372, grad_fn=<SubBackward0>), time elapsed: 14.40038013458252 seconds\n",
      "Step 300, loss: tensor(0.0221, grad_fn=<SubBackward0>), time elapsed: 21.131529092788696 seconds\n",
      "Step 400, loss: tensor(0.0170, grad_fn=<SubBackward0>), time elapsed: 27.309236764907837 seconds\n",
      "Step 500, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 33.48192501068115 seconds\n",
      "Step 600, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 39.67593502998352 seconds\n",
      "Step 700, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 46.799208879470825 seconds\n",
      "Step 800, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 54.68489933013916 seconds\n",
      "Step 900, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 64.0099949836731 seconds\n",
      "Step 1000, loss: tensor(0.0076, grad_fn=<SubBackward0>), time elapsed: 71.35809350013733 seconds\n",
      "Step 1100, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 78.77659487724304 seconds\n",
      "Step 1200, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 85.93260431289673 seconds\n",
      "Step 1300, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 92.36431527137756 seconds\n",
      "Step 1400, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 98.92542600631714 seconds\n",
      "Step 1500, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 105.48621034622192 seconds\n",
      "Step 1600, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 112.29311227798462 seconds\n",
      "Step 1700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 118.93993926048279 seconds\n",
      "Step 1800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 125.7157883644104 seconds\n",
      "Step 1900, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 132.6211760044098 seconds\n",
      "Step 2000, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 139.09662246704102 seconds\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0997, grad_fn=<SubBackward0>), time elapsed: 0.06356000900268555 seconds\n",
      "Step 100, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 6.332423686981201 seconds\n",
      "Step 200, loss: tensor(0.0592, grad_fn=<SubBackward0>), time elapsed: 13.139791250228882 seconds\n",
      "Step 300, loss: tensor(0.0332, grad_fn=<SubBackward0>), time elapsed: 19.92488741874695 seconds\n",
      "Step 400, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 26.811323642730713 seconds\n",
      "Step 500, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 33.22218585014343 seconds\n",
      "Step 600, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 39.63541316986084 seconds\n",
      "Step 700, loss: tensor(0.0098, grad_fn=<SubBackward0>), time elapsed: 46.57087421417236 seconds\n",
      "Step 800, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 53.122058629989624 seconds\n",
      "Step 900, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 59.68424463272095 seconds\n",
      "Step 1000, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 66.17421817779541 seconds\n",
      "Step 1100, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 72.47194957733154 seconds\n",
      "Step 1200, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 78.82815313339233 seconds\n",
      "Step 1300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 85.2256019115448 seconds\n",
      "Step 1400, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 91.82204246520996 seconds\n",
      "Step 1500, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 99.20836067199707 seconds\n",
      "Step 1600, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 105.92382025718689 seconds\n",
      "Step 1700, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 112.46509575843811 seconds\n",
      "Step 1800, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 118.8158609867096 seconds\n",
      "Step 1900, loss: tensor(0.0105, grad_fn=<SubBackward0>), time elapsed: 125.69087147712708 seconds\n",
      "Step 2000, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 132.3140606880188 seconds\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1303, grad_fn=<SubBackward0>), time elapsed: 0.05459952354431152 seconds\n",
      "Step 100, loss: tensor(0.0731, grad_fn=<SubBackward0>), time elapsed: 6.815820217132568 seconds\n",
      "Step 200, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 13.739612340927124 seconds\n",
      "Step 300, loss: tensor(0.0427, grad_fn=<SubBackward0>), time elapsed: 20.659104585647583 seconds\n",
      "Step 400, loss: tensor(0.0244, grad_fn=<SubBackward0>), time elapsed: 26.932790756225586 seconds\n",
      "Step 500, loss: tensor(0.0210, grad_fn=<SubBackward0>), time elapsed: 33.1747944355011 seconds\n",
      "Step 600, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 39.2851357460022 seconds\n",
      "Step 700, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 46.715962171554565 seconds\n",
      "Step 800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 53.93516206741333 seconds\n",
      "Step 900, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 60.330870628356934 seconds\n",
      "Step 1000, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 66.67574620246887 seconds\n",
      "Step 1100, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 74.4054639339447 seconds\n",
      "Step 1200, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 80.85462379455566 seconds\n",
      "Step 1300, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 87.70841002464294 seconds\n",
      "Step 1400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 94.98037457466125 seconds\n",
      "Step 1500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 102.45641374588013 seconds\n",
      "Step 1600, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 109.07806491851807 seconds\n",
      "Step 1700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 117.13747501373291 seconds\n",
      "Step 1800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 124.04035902023315 seconds\n",
      "Step 1900, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 131.64892554283142 seconds\n",
      "Step 2000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 138.14063692092896 seconds\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 0.05554628372192383 seconds\n",
      "Step 100, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 6.842125177383423 seconds\n",
      "Step 200, loss: tensor(0.0273, grad_fn=<SubBackward0>), time elapsed: 13.475139379501343 seconds\n",
      "Step 300, loss: tensor(0.0234, grad_fn=<SubBackward0>), time elapsed: 21.013764142990112 seconds\n",
      "Step 400, loss: tensor(0.0196, grad_fn=<SubBackward0>), time elapsed: 27.939512014389038 seconds\n",
      "Step 500, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 34.19852352142334 seconds\n",
      "Step 600, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 40.9032998085022 seconds\n",
      "Step 700, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 47.84011173248291 seconds\n",
      "Step 800, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 54.25709843635559 seconds\n",
      "Step 900, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 60.82120728492737 seconds\n",
      "Step 1000, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 68.1150484085083 seconds\n",
      "Step 1100, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 75.15257000923157 seconds\n",
      "Step 1200, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 82.05205631256104 seconds\n",
      "Step 1300, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 89.42233467102051 seconds\n",
      "Step 1400, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 97.29171800613403 seconds\n",
      "Step 1500, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 104.54744124412537 seconds\n",
      "Step 1600, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 111.87511587142944 seconds\n",
      "Step 1700, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 118.01993870735168 seconds\n",
      "Step 1800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 124.21713900566101 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 130.61973595619202 seconds\n",
      "Step 2000, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 136.85129928588867 seconds\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0582, grad_fn=<SubBackward0>), time elapsed: 0.059021949768066406 seconds\n",
      "Step 100, loss: tensor(0.0288, grad_fn=<SubBackward0>), time elapsed: 6.016624689102173 seconds\n",
      "Step 200, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 13.254039287567139 seconds\n",
      "Step 300, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 20.564388036727905 seconds\n",
      "Step 400, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 28.14752721786499 seconds\n",
      "Step 500, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 34.39537262916565 seconds\n",
      "Step 600, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 40.443764448165894 seconds\n",
      "Step 700, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 46.49152755737305 seconds\n",
      "Step 800, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 52.81594443321228 seconds\n",
      "Step 900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 58.84722399711609 seconds\n",
      "Step 1000, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 65.30622386932373 seconds\n",
      "Step 1100, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 71.89426779747009 seconds\n",
      "Step 1200, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 78.66152048110962 seconds\n",
      "Step 1300, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 84.9458863735199 seconds\n",
      "Step 1400, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 91.31176543235779 seconds\n",
      "Step 1500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 97.58232402801514 seconds\n",
      "Step 1600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 103.7626485824585 seconds\n",
      "Step 1700, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 109.96363592147827 seconds\n",
      "Step 1800, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 116.680100440979 seconds\n",
      "Step 1900, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 122.76630425453186 seconds\n",
      "Step 2000, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 128.68637371063232 seconds\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 0.05934572219848633 seconds\n",
      "Step 100, loss: tensor(0.0454, grad_fn=<SubBackward0>), time elapsed: 5.937403917312622 seconds\n",
      "Step 200, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 11.88431715965271 seconds\n",
      "Step 300, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 18.454998254776 seconds\n",
      "Step 400, loss: tensor(0.0191, grad_fn=<SubBackward0>), time elapsed: 25.821260452270508 seconds\n",
      "Step 500, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 32.18317723274231 seconds\n",
      "Step 600, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 38.638476848602295 seconds\n",
      "Step 700, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 44.53428316116333 seconds\n",
      "Step 800, loss: tensor(0.0127, grad_fn=<SubBackward0>), time elapsed: 51.4700083732605 seconds\n",
      "Step 900, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 58.11829948425293 seconds\n",
      "Step 1000, loss: tensor(0.0145, grad_fn=<SubBackward0>), time elapsed: 64.8189697265625 seconds\n",
      "Step 1100, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 71.31243181228638 seconds\n",
      "Step 1200, loss: tensor(0.0076, grad_fn=<SubBackward0>), time elapsed: 78.0741217136383 seconds\n",
      "Step 1300, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 85.33127450942993 seconds\n",
      "Step 1400, loss: tensor(0.0075, grad_fn=<SubBackward0>), time elapsed: 92.49767303466797 seconds\n",
      "Step 1500, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 99.48086166381836 seconds\n",
      "Step 1600, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 106.83427405357361 seconds\n",
      "Step 1700, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 113.91039204597473 seconds\n",
      "Step 1800, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 120.34520840644836 seconds\n",
      "Step 1900, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 126.73965692520142 seconds\n",
      "Step 2000, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 134.02349400520325 seconds\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0705, grad_fn=<SubBackward0>), time elapsed: 0.05410909652709961 seconds\n",
      "Step 100, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 6.877963066101074 seconds\n",
      "Step 200, loss: tensor(0.0332, grad_fn=<SubBackward0>), time elapsed: 14.659358263015747 seconds\n",
      "Step 300, loss: tensor(0.0167, grad_fn=<SubBackward0>), time elapsed: 21.34014129638672 seconds\n",
      "Step 400, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 28.114498615264893 seconds\n",
      "Step 500, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 35.10132932662964 seconds\n",
      "Step 600, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 42.35932207107544 seconds\n",
      "Step 700, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 49.5407600402832 seconds\n",
      "Step 800, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 56.56327962875366 seconds\n",
      "Step 900, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 64.55773162841797 seconds\n",
      "Step 1000, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 71.66257905960083 seconds\n",
      "Step 1100, loss: tensor(0.0076, grad_fn=<SubBackward0>), time elapsed: 77.9564220905304 seconds\n",
      "Step 1200, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 84.56777167320251 seconds\n",
      "Step 1300, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 91.21386122703552 seconds\n",
      "Step 1400, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 98.03762984275818 seconds\n",
      "Step 1500, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 104.8410952091217 seconds\n",
      "Step 1600, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 111.92012810707092 seconds\n",
      "Step 1700, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 120.00914239883423 seconds\n",
      "Step 1800, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 127.43189334869385 seconds\n",
      "Step 1900, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 134.59203171730042 seconds\n",
      "Step 2000, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 141.16702318191528 seconds\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1167, grad_fn=<SubBackward0>), time elapsed: 0.055997371673583984 seconds\n",
      "Step 100, loss: tensor(0.0572, grad_fn=<SubBackward0>), time elapsed: 6.158188819885254 seconds\n",
      "Step 200, loss: tensor(0.0410, grad_fn=<SubBackward0>), time elapsed: 12.582093000411987 seconds\n",
      "Step 300, loss: tensor(0.0269, grad_fn=<SubBackward0>), time elapsed: 18.723398447036743 seconds\n",
      "Step 400, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 24.93632411956787 seconds\n",
      "Step 500, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 31.42665386199951 seconds\n",
      "Step 600, loss: tensor(0.0076, grad_fn=<SubBackward0>), time elapsed: 37.516764879226685 seconds\n",
      "Step 700, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 44.25111746788025 seconds\n",
      "Step 800, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 50.85124135017395 seconds\n",
      "Step 900, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 57.44333600997925 seconds\n",
      "Step 1000, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 65.49726343154907 seconds\n",
      "Step 1100, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 72.85055541992188 seconds\n",
      "Step 1200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 79.12995076179504 seconds\n",
      "Step 1300, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 85.79988622665405 seconds\n",
      "Step 1400, loss: tensor(0.0077, grad_fn=<SubBackward0>), time elapsed: 91.94867134094238 seconds\n",
      "Step 1500, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 98.14408564567566 seconds\n",
      "Step 1600, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 104.49479627609253 seconds\n",
      "Step 1700, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 110.60884284973145 seconds\n",
      "Step 1800, loss: tensor(0.0101, grad_fn=<SubBackward0>), time elapsed: 116.87639570236206 seconds\n",
      "Step 1900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 122.95709133148193 seconds\n",
      "Step 2000, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 129.18854689598083 seconds\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0599, grad_fn=<SubBackward0>), time elapsed: 0.051995277404785156 seconds\n",
      "Step 100, loss: tensor(0.0322, grad_fn=<SubBackward0>), time elapsed: 6.669811010360718 seconds\n",
      "Step 200, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 12.611693859100342 seconds\n",
      "Step 300, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 18.56026601791382 seconds\n",
      "Step 400, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 24.494513988494873 seconds\n",
      "Step 500, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 30.420207738876343 seconds\n",
      "Step 600, loss: tensor(0.0089, grad_fn=<SubBackward0>), time elapsed: 37.55762577056885 seconds\n",
      "Step 700, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 42.999950885772705 seconds\n",
      "Step 800, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 47.827444553375244 seconds\n",
      "Step 900, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 52.97391939163208 seconds\n",
      "Step 1000, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 57.943540811538696 seconds\n",
      "Step 1100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 64.6439163684845 seconds\n",
      "Step 1200, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 71.0172266960144 seconds\n",
      "Step 1300, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 77.31233191490173 seconds\n",
      "Step 1400, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 83.23129343986511 seconds\n",
      "Step 1500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 89.25977635383606 seconds\n",
      "Step 1600, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 94.3039300441742 seconds\n",
      "Step 1700, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 102.0139548778534 seconds\n",
      "Step 1800, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 108.68235945701599 seconds\n",
      "Step 1900, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 114.57860517501831 seconds\n",
      "Step 2000, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 119.63228678703308 seconds\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 0.07079291343688965 seconds\n",
      "Step 100, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 6.4542059898376465 seconds\n",
      "Step 200, loss: tensor(0.0393, grad_fn=<SubBackward0>), time elapsed: 12.32209849357605 seconds\n",
      "Step 300, loss: tensor(0.0328, grad_fn=<SubBackward0>), time elapsed: 19.671441555023193 seconds\n",
      "Step 400, loss: tensor(0.0243, grad_fn=<SubBackward0>), time elapsed: 26.280737161636353 seconds\n",
      "Step 500, loss: tensor(0.0206, grad_fn=<SubBackward0>), time elapsed: 33.38781237602234 seconds\n",
      "Step 600, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 40.16454744338989 seconds\n",
      "Step 700, loss: tensor(0.0177, grad_fn=<SubBackward0>), time elapsed: 45.96168088912964 seconds\n",
      "Step 800, loss: tensor(0.0205, grad_fn=<SubBackward0>), time elapsed: 53.19469952583313 seconds\n",
      "Step 900, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 59.963775634765625 seconds\n",
      "Step 1000, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 66.89361929893494 seconds\n",
      "Step 1100, loss: tensor(0.0107, grad_fn=<SubBackward0>), time elapsed: 72.60986852645874 seconds\n",
      "Step 1200, loss: tensor(0.0077, grad_fn=<SubBackward0>), time elapsed: 77.72629523277283 seconds\n",
      "Step 1300, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 84.68785762786865 seconds\n",
      "Step 1400, loss: tensor(0.0080, grad_fn=<SubBackward0>), time elapsed: 90.98007678985596 seconds\n",
      "Step 1500, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 97.29134607315063 seconds\n",
      "Step 1600, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 103.42443990707397 seconds\n",
      "Step 1700, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 108.55159044265747 seconds\n",
      "Step 1800, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 115.52344489097595 seconds\n",
      "Step 1900, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 120.62479066848755 seconds\n",
      "Step 2000, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 125.32579302787781 seconds\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 0.0657050609588623 seconds\n",
      "Step 100, loss: tensor(0.0467, grad_fn=<SubBackward0>), time elapsed: 7.080952882766724 seconds\n",
      "Step 200, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 13.89536738395691 seconds\n",
      "Step 300, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 20.116600275039673 seconds\n",
      "Step 400, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 26.443259477615356 seconds\n",
      "Step 500, loss: tensor(0.0159, grad_fn=<SubBackward0>), time elapsed: 32.6427001953125 seconds\n",
      "Step 600, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 38.07653212547302 seconds\n",
      "Step 700, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 44.16192412376404 seconds\n",
      "Step 800, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 50.270304918289185 seconds\n",
      "Step 900, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 55.791175842285156 seconds\n",
      "Step 1000, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 61.06874680519104 seconds\n",
      "Step 1100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 66.09518074989319 seconds\n",
      "Step 1200, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 71.03312683105469 seconds\n",
      "Step 1300, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 76.08420991897583 seconds\n",
      "Step 1400, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 81.12203431129456 seconds\n",
      "Step 1500, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 86.51648426055908 seconds\n",
      "Step 1600, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 92.58422350883484 seconds\n",
      "Step 1700, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 98.93769454956055 seconds\n",
      "Step 1800, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 105.69816946983337 seconds\n",
      "Step 1900, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 111.61021637916565 seconds\n",
      "Step 2000, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 117.03691244125366 seconds\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1071, grad_fn=<SubBackward0>), time elapsed: 0.05136299133300781 seconds\n",
      "Step 100, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 5.014185667037964 seconds\n",
      "Step 200, loss: tensor(0.0437, grad_fn=<SubBackward0>), time elapsed: 10.244663715362549 seconds\n",
      "Step 300, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 15.350551128387451 seconds\n",
      "Step 400, loss: tensor(0.0172, grad_fn=<SubBackward0>), time elapsed: 22.06060218811035 seconds\n",
      "Step 500, loss: tensor(0.0329, grad_fn=<SubBackward0>), time elapsed: 28.67783546447754 seconds\n",
      "Step 600, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 34.96017527580261 seconds\n",
      "Step 700, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 40.385915756225586 seconds\n",
      "Step 800, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 47.58681583404541 seconds\n",
      "Step 900, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 54.20718336105347 seconds\n",
      "Step 1000, loss: tensor(0.0099, grad_fn=<SubBackward0>), time elapsed: 59.79329967498779 seconds\n",
      "Step 1100, loss: tensor(0.0108, grad_fn=<SubBackward0>), time elapsed: 65.24227929115295 seconds\n",
      "Step 1200, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 70.24560236930847 seconds\n",
      "Step 1300, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 76.02424764633179 seconds\n",
      "Step 1400, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 81.48038411140442 seconds\n",
      "Step 1500, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 87.22139883041382 seconds\n",
      "Step 1600, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 92.11542582511902 seconds\n",
      "Step 1700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 96.63865685462952 seconds\n",
      "Step 1800, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 101.25466418266296 seconds\n",
      "Step 1900, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 106.35395240783691 seconds\n",
      "Step 2000, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 112.5067732334137 seconds\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0659, grad_fn=<SubBackward0>), time elapsed: 0.04952812194824219 seconds\n",
      "Step 100, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 5.055065870285034 seconds\n",
      "Step 200, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 9.582640409469604 seconds\n",
      "Step 300, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 16.105062007904053 seconds\n",
      "Step 400, loss: tensor(0.0147, grad_fn=<SubBackward0>), time elapsed: 21.725656986236572 seconds\n",
      "Step 500, loss: tensor(0.0102, grad_fn=<SubBackward0>), time elapsed: 27.946167707443237 seconds\n",
      "Step 600, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 34.22363209724426 seconds\n",
      "Step 700, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 40.15774202346802 seconds\n",
      "Step 800, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 46.14426040649414 seconds\n",
      "Step 900, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 51.67254114151001 seconds\n",
      "Step 1000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 58.222835063934326 seconds\n",
      "Step 1100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 65.02423334121704 seconds\n",
      "Step 1200, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 71.25657391548157 seconds\n",
      "Step 1300, loss: tensor(0.0076, grad_fn=<SubBackward0>), time elapsed: 78.28262257575989 seconds\n",
      "Step 1400, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 84.19670414924622 seconds\n",
      "Step 1500, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 89.70721673965454 seconds\n",
      "Step 1600, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 95.47569918632507 seconds\n",
      "Step 1700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 101.11364269256592 seconds\n",
      "Step 1800, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 107.8100917339325 seconds\n",
      "Step 1900, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 113.11995148658752 seconds\n",
      "Step 2000, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 119.68669939041138 seconds\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1086, grad_fn=<SubBackward0>), time elapsed: 0.04161667823791504 seconds\n",
      "Step 100, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 6.542376279830933 seconds\n",
      "Step 200, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 13.847973823547363 seconds\n",
      "Step 300, loss: tensor(0.0413, grad_fn=<SubBackward0>), time elapsed: 19.591962337493896 seconds\n",
      "Step 400, loss: tensor(0.0244, grad_fn=<SubBackward0>), time elapsed: 25.396374702453613 seconds\n",
      "Step 500, loss: tensor(0.0335, grad_fn=<SubBackward0>), time elapsed: 30.760822534561157 seconds\n",
      "Step 600, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 35.735788106918335 seconds\n",
      "Step 700, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 41.12170958518982 seconds\n",
      "Step 800, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 46.09988069534302 seconds\n",
      "Step 900, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 51.12998676300049 seconds\n",
      "Step 1000, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 57.06030535697937 seconds\n",
      "Step 1100, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 63.46220088005066 seconds\n",
      "Step 1200, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 68.7500467300415 seconds\n",
      "Step 1300, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 73.95473861694336 seconds\n",
      "Step 1400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 79.66816735267639 seconds\n",
      "Step 1500, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 84.83608317375183 seconds\n",
      "Step 1600, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 89.73949193954468 seconds\n",
      "Step 1700, loss: tensor(0.0105, grad_fn=<SubBackward0>), time elapsed: 94.72698879241943 seconds\n",
      "Step 1800, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 99.59790921211243 seconds\n",
      "Step 1900, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 104.87894749641418 seconds\n",
      "Step 2000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 109.93699193000793 seconds\n"
     ]
    }
   ],
   "source": [
    "#train on diffusion data\n",
    "n, na = 2, 1 # number of data and ancilla qubits\n",
    "T = 20 # number of diffusion steps\n",
    "L = 6 # layer of backward PQC\n",
    "Ndata = 400 # number of data in the training data set\n",
    "epochs = 2001 # number of training epochs\n",
    "\n",
    "#gen random samples at t = T\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_T = diffModel.HaarSampleGeneration(Ndata, seed = 22)\n",
    "\n",
    "#load diffusion process\n",
    "states_diff = Xout\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "model.set_diffusionSet(states_diff)\n",
    "\n",
    "params_total = []\n",
    "loss_hist_total = []\n",
    "\n",
    "for t in range(T - 1, -1, -1):\n",
    "    params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "    for tt in range(t+1, 20):\n",
    "        print(tt)\n",
    "        params_tot[tt] = np.load('data/cluster/n2/QDDPMcluster0params_n2na1T20L6_t%d_mmd.npy'%tt)\n",
    "    params, loss_hist = Training_t(model, t, inputs_T, params_tot, Ndata, epochs)\n",
    "    params = params.detach().numpy()\n",
    "    loss_hist = loss_hist.detach().numpy()\n",
    "    np.save('data/cluster/n2/QDDPMcluster0params_n2na1T20L6_t%d_mmd.npy'%t, params)\n",
    "    np.save('data/cluster/n2/QDDPMcluster0loss_n2na1T20L6_t%d_mmd.npy'%t, loss_hist)\n",
    "    \n",
    "    #params_total = np.append(params_total, params, axis = 1)\n",
    "    #loss_hist_total = np.append(loss_hist_total, loss_hist, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, na = 2, 1\n",
    "T = 20\n",
    "L = 6\n",
    "Ndata = 100\n",
    "epochs = 2001\n",
    "\n",
    "params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "loss_tot = np.zeros((T, epochs))\n",
    "\n",
    "for t in range(T):\n",
    "    params_tot[t] = np.load('data/cluster/n2/QDDPMcluster0params_n2na1T20L6_t%d_mmd.npy'%t)\n",
    "    loss_tot[t] = np.load('data/cluster/n2/QDDPMcluster0loss_n2na1T20L6_t%d_mmd.npy'%t)\n",
    "\n",
    "np.save('data/cluster/n2/QDDPMcluster0params_n2na1T20L6_mmd.npy', params_tot)\n",
    "np.save('data/cluster/n2/QDDPMcluster0loss_n2na1T20L6_mmd.npy', loss_tot)\n",
    "\n",
    "#Define inputs here\n",
    "#Training input\n",
    "#Testing input\n",
    "values = np.array([pixelValues[0:2], pixelValues[2:4]])\n",
    "normalized = ie.normalize(values)\n",
    "normalized = normalized.ravel() + 0j*np.zeros(4)\n",
    "\n",
    "inputs_T_tr = []\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "\n",
    "data_tr = model.backDataGeneration(inputs_T_tr, params_tot, Ndata)[:, :, :2**n].numpy()\n",
    "data_te = model.backDataGeneration(normalized, params_tot, Ndata)[:, :, :2**n].numpy()\n",
    "\n",
    "np.save('data/cluster/n2/QDDPMcluster0trainGen_n2na1T20L6_mmd.npy', data_tr)\n",
    "np.save('data/cluster/n2/QDDPMcluster0testGen_n2na1T20L6_mmd.npy', data_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02723781 0.02737894 0.02737894 ... 0.02822571 0.02780233 0.02794345]\n",
      "[[0.02723781 0.02737894 0.02737894 0.02752007]\n",
      " [0.0276612  0.0276612  0.02780233 0.02780233]\n",
      " [0.02780233 0.02780233 0.02794345 0.0276612 ]\n",
      " ...\n",
      " [0.02328621 0.02526201 0.02737894 0.02808458]\n",
      " [0.02610878 0.02624991 0.02709668 0.02794345]\n",
      " [0.02822571 0.02822571 0.02780233 0.02794345]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22734b9b350>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "scale = 40\n",
    "img = Image.open('dog.jpg')\n",
    "img_greyscale = img.convert(\"L\")\n",
    "img_resized = img_greyscale.resize((scale, scale))\n",
    "img_array = np.array(img_resized, ndmin = 2)\n",
    "img_resized.save('dog_resized_greyscale.jpg')\n",
    "img_final_array = np.zeros((scale, scale), dtype = object)\n",
    "\n",
    "\n",
    "normalized_img_array = ie.normalize(img_array)\n",
    "normalized_img_array = normalized_img_array.ravel() + 0*j*np.zeros(scale*scale)\n",
    "test_data = np.zeros((int(normalized_img_array.size / 4), 4))\n",
    "for i in range(0, normalized_img_array.size):\n",
    "    test_data[int(i / 4)][i % 4] = normalized_img_array[i]\n",
    "\n",
    "print(normalized_img_array)\n",
    "print(test_data)\n",
    "\n",
    "plt.imshow(img_array, cmap='grey',interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.33]\n",
      " [0.66 0.99]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        +0.j 0.26726124+0.j 0.53452248+0.j 0.80178373+0.j]\n"
     ]
    }
   ],
   "source": [
    "values = np.array([pixelValues[0:2], pixelValues[2:4]])\n",
    "print(values)\n",
    "normalized = ie.normalize(values)\n",
    "plt.imshow(values, cmap='grey', interpolation='nearest')\n",
    "plt.show()\n",
    "normalized = normalized.ravel() + 0j*np.zeros(4)\n",
    "print(normalized)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
