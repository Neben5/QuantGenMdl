{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import unitary_group\n",
    "from skimage.measure import block_reduce\n",
    "from opt_einsum import contract\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from src.QDDPM_torch import DiffusionModel, QDDPM, naturalDistance\n",
    "import src.ImageEncode as ie\n",
    "rc('text', usetex=False)\n",
    "rc('axes', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Torch will use CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Torch will use CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training(values: np.array, n_train: int, scale: float, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    n = values.size\n",
    "    noise = abs(np.random.randn(n_train,n))+0j*np.random.randn(n_train,n) \n",
    "    print(noise.shape)\n",
    "    print(values.shape)\n",
    "    states = (noise*scale) + values\n",
    "    states/=np.tile(np.linalg.norm(states, axis=1).reshape(1,n_train), (n,1)).T\n",
    "    return states\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#will not work with mixed state probabilities\n",
    "from PIL import Image\n",
    "scale = 40\n",
    "img = Image.open('images.png')\n",
    "img_greyscale = img.convert(\"L\")\n",
    "img_resized = img_greyscale.resize((scale, scale))\n",
    "\n",
    "img_array = np.array(img_resized, ndmin = 2)\n",
    "img_resized.save('dog_resized_greyscale.jpg')\n",
    "\n",
    "collapsed_array = np.zeros(img_array.size)\n",
    "\n",
    "for i in range(0, img_array.size):\n",
    "    collapsed_array[i] = img_array[int(i/scale)][i%scale]\n",
    "\n",
    "test_data = np.zeros((int(img_array.size / 4), 4)) + 1j * np.zeros((int(img_array.size / 4), 4))\n",
    "temp_normalize = max(collapsed_array) * 4\n",
    "for i in range(0, int(img_array.size / 4)):\n",
    "    test_data[i] = collapsed_array[4*i : 4*i+4] / temp_normalize + 1j * np.zeros(4)\n",
    "\n",
    "plt.imshow(img_array, cmap='grey',interpolation = 'nearest')\n",
    "\n",
    "# Find where NaN values are\n",
    "print(np.isnan(test_data).any())\n",
    "\n",
    "# Get the indices of the NaN values\n",
    "#nan_indices = np.where(img_array)\n",
    "\n",
    "#print(nan_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Square 4 by 4 to add noise and turn into training data\n",
    "temp_test = np.array([[1, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1], [1, 1, 1, 1]])\n",
    "plt.imshow(temp_test, cmap='grey',interpolation = 'nearest')\n",
    "temp_test = temp_test.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "#generate training data for diffusion\n",
    "n = 4\n",
    "T = 20\n",
    "Ndata = 2000\n",
    "\n",
    "diff_hs = torch.from_numpy(np.linspace(0.5, 4., T))\n",
    "\n",
    "model_diff = DiffusionModel(n, T, Ndata)\n",
    "\n",
    "X = torch.from_numpy(generate_training(temp_test, Ndata, 0.05))\n",
    "\n",
    "np.save('training_data', np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2800x1400 with 50 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize training data before they are forward diffused\n",
    "training_temp = np.abs(np.array(X))\n",
    "multiplier = np.max(training_temp)\n",
    "training_temp /= multiplier\n",
    "fig, axs = plt.subplots(5, 10, figsize = (28, 14))\n",
    "for index_1 in range(0, 50):\n",
    "    training_images = training_temp[index_1]\n",
    "\n",
    "    picture_training  = np.zeros((4, 4))\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            picture_training[i][j] = training_images[i*4 + j]\n",
    "\n",
    "    axs[int(index_1 / 10)][index_1 % 10].imshow(picture_training, cmap='grey',interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#diffuse training data\n",
    "Xout = np.zeros((T+1, Ndata, 2**n), dtype = np.complex64)\n",
    "Xout[0] = X.numpy()\n",
    "\n",
    "for t in range(1, T+1):\n",
    "    Xout[t] = model_diff.set_diffusionData_t(t, X, diff_hs[:t], seed = t).numpy()\n",
    "    print(t)\n",
    "\n",
    "np.save(\"states_diff\", Xout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         1.         ... 1.         0.99999988 1.        ]\n",
      " [0.95379411 0.94283223 0.95989051 ... 0.96230815 0.93162313 0.94562588]\n",
      " [0.93342679 0.94968429 0.80878709 ... 0.84761103 0.76824997 0.73139394]\n",
      " ...\n",
      " [0.03011609 0.10262213 0.04946043 ... 0.09197472 0.0871414  0.150233  ]\n",
      " [0.05082321 0.06157269 0.08411071 ... 0.16418842 0.0980667  0.00735945]\n",
      " [0.08787546 0.0059782  0.01721599 ... 0.04829054 0.00670319 0.0607175 ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "fidelity = np.zeros((T + 1, Ndata))\n",
    "for i in range(0, T + 1):\n",
    "    for j in range(0, Ndata):\n",
    "        #different calculations for mixed state fidelity\n",
    "        fidelity[i][j] = np.abs(np.vdot(states_diff[i][j], states_diff[0][j])) ** 2\n",
    "\n",
    "\n",
    "fidelity_mean = np.mean(fidelity, axis = 1)\n",
    "print(fidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the fidelity decay in the diffusion process\n",
    "n = 2\n",
    "T = 20\n",
    "Ndata = 600\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "indices = np.random.permutation(Ndata)\n",
    "\n",
    "ax.plot(range(T+1), fidelity_mean, 'o--', markersize=8, lw=2, c='r')\n",
    "ax.plot(range(T+1), 0.25*np.ones(T+1), '--', lw=2, c='gold')\n",
    "ax.set_ylabel(r'$F_0$', fontsize=30)\n",
    "ax.set_xlabel(r'$t$', fontsize=30)\n",
    "ax.set_ylim(0,1)\n",
    "ax.tick_params(direction='in', length=10, width=3, top='on', right='on', labelsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_t(model, t, inputs_T, params_tot, Ndata, epochs):\n",
    "    '''\n",
    "    the trianing for the backward PQC at step t\n",
    "    input_tplus1: the output from step t+1, as the role of input at step t\n",
    "    Args:\n",
    "    model: the QDDPM model\n",
    "    t: the diffusion step\n",
    "    inputs_T: the input data at step t=T\n",
    "    params_tot: collection of PQC parameters before step t\n",
    "    Ndata: number of samples in dataset\n",
    "    epochs: the number of iterations\n",
    "    '''\n",
    "    input_tplus1 = model.prepareInput_t(inputs_T, params_tot, t, Ndata) # prepare input\n",
    "    states_diff = model.states_diff\n",
    "    loss_hist = [] # record of training history\n",
    "\n",
    "    # initialize parameters\n",
    "    np.random.seed()\n",
    "    params_t = torch.tensor(np.random.normal(size=2 * model.n_tot * model.L), requires_grad=True)\n",
    "    # set optimizer and learning rate decay\n",
    "    optimizer = torch.optim.Adam([params_t], lr=0.0005)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for step in range(epochs):\n",
    "        indices = np.random.choice(states_diff.shape[1], size=Ndata, replace=False)\n",
    "        true_data = states_diff[t, indices]\n",
    "\n",
    "        output_t = model.backwardOutput_t(input_tplus1, params_t)\n",
    "        loss = naturalDistance(output_t, true_data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_hist.append(loss) # record the current loss\n",
    "        \n",
    "        if step%100 == 0:\n",
    "            loss_value = loss_hist[-1]\n",
    "            print(\"Step %s, loss: %s, time elapsed: %s seconds\"%(step, loss_value, time.time() - t0))\n",
    "\n",
    "    return params_t, torch.stack(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: tensor(0.0207, grad_fn=<SubBackward0>), time elapsed: 0.08776545524597168 seconds\n",
      "Step 100, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 5.974656581878662 seconds\n",
      "Step 200, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 11.656195878982544 seconds\n",
      "Step 300, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 17.42595076560974 seconds\n",
      "Step 400, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 23.094358682632446 seconds\n",
      "Step 500, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 28.875133991241455 seconds\n",
      "Step 600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 34.505762815475464 seconds\n",
      "Step 700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 40.16259956359863 seconds\n",
      "Step 800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 45.904818058013916 seconds\n",
      "Step 900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 51.573132038116455 seconds\n",
      "Step 1000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 57.461182832717896 seconds\n",
      "Step 1100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 63.14864706993103 seconds\n",
      "Step 1200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 73.8249843120575 seconds\n",
      "Step 1300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 84.26455616950989 seconds\n",
      "Step 1400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 90.52780675888062 seconds\n",
      "Step 1500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 96.37097215652466 seconds\n",
      "Step 1600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 102.04809379577637 seconds\n",
      "Step 1700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 107.82140445709229 seconds\n",
      "Step 1800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 113.50526857376099 seconds\n",
      "Step 1900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 119.28499579429626 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 124.91267013549805 seconds\n",
      "Step 2100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 130.5266933441162 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 136.22830533981323 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 141.84679102897644 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 147.64419674873352 seconds\n",
      "Step 2500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 153.37086033821106 seconds\n",
      "Step 2600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 159.22691679000854 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 164.91325569152832 seconds\n",
      "Step 2800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 170.6092870235443 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 176.402024269104 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 182.05828619003296 seconds\n",
      "Step 3100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 187.77012276649475 seconds\n",
      "Step 3200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 193.3716254234314 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 198.9502112865448 seconds\n",
      "Step 3400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 204.69045686721802 seconds\n",
      "Step 3500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 210.27309131622314 seconds\n",
      "Step 3600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 216.0008246898651 seconds\n",
      "Step 3700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 221.58672618865967 seconds\n",
      "Step 3800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 227.2997806072235 seconds\n",
      "Step 3900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 232.8690001964569 seconds\n",
      "Step 4000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 238.43914341926575 seconds\n",
      "Step 4100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 244.17954397201538 seconds\n",
      "Step 4200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 249.7433729171753 seconds\n",
      "Step 4300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 255.45767426490784 seconds\n",
      "Step 4400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 261.04651141166687 seconds\n",
      "Step 4500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 266.7733254432678 seconds\n",
      "Step 4600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 272.38298058509827 seconds\n",
      "Step 4700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 277.95547699928284 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 283.71546053886414 seconds\n",
      "Step 4900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 289.3171441555023 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 295.043607711792 seconds\n",
      "Step 5100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 300.6451301574707 seconds\n",
      "Step 5200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 306.2850778102875 seconds\n",
      "Step 5300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 312.0064699649811 seconds\n",
      "Step 5400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 317.58821272850037 seconds\n",
      "Step 5500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 323.2793040275574 seconds\n",
      "Step 5600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 328.86541628837585 seconds\n",
      "Step 5700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 334.60022234916687 seconds\n",
      "Step 5800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 340.198618888855 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 345.77422547340393 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 351.5049104690552 seconds\n",
      "Step 6100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 357.09384059906006 seconds\n",
      "Step 6200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 362.8831944465637 seconds\n",
      "Step 6300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 368.5614335536957 seconds\n",
      "Step 6400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 374.37548446655273 seconds\n",
      "Step 6500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 380.0462725162506 seconds\n",
      "Step 6600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 385.62692856788635 seconds\n",
      "Step 6700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 391.36196851730347 seconds\n",
      "Step 6800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 396.97758412361145 seconds\n",
      "Step 6900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 402.6984622478485 seconds\n",
      "Step 7000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 408.2749478816986 seconds\n",
      "Step 7100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 413.84303545951843 seconds\n",
      "Step 7200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 419.5717387199402 seconds\n",
      "Step 7300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 425.16720056533813 seconds\n",
      "Step 7400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 430.87031865119934 seconds\n",
      "Step 7500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 436.4744963645935 seconds\n",
      "Step 7600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 442.2051031589508 seconds\n",
      "Step 7700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 447.7859218120575 seconds\n",
      "Step 7800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 453.3707320690155 seconds\n",
      "Step 7900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 459.1055762767792 seconds\n",
      "Step 8000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 464.6851496696472 seconds\n",
      "Step 8100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 470.4130356311798 seconds\n",
      "Step 8200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 475.98518466949463 seconds\n",
      "Step 8300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 481.7326707839966 seconds\n",
      "Step 8400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 487.3258566856384 seconds\n",
      "Step 8500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 492.90542697906494 seconds\n",
      "Step 8600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 498.6289699077606 seconds\n",
      "Step 8700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 504.21269726753235 seconds\n",
      "Step 8800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 509.90289330482483 seconds\n",
      "Step 8900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 515.4835481643677 seconds\n",
      "Step 9000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 521.0700507164001 seconds\n",
      "Step 9100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 526.80335688591 seconds\n",
      "Step 9200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 532.3881828784943 seconds\n",
      "Step 9300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 538.1210653781891 seconds\n",
      "Step 9400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 543.716493844986 seconds\n",
      "Step 9500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 549.4710667133331 seconds\n",
      "Step 9600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 555.0423882007599 seconds\n",
      "Step 9700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 560.6240239143372 seconds\n",
      "Step 9800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 566.3434481620789 seconds\n",
      "Step 9900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 571.939670085907 seconds\n",
      "Step 10000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 577.7028732299805 seconds\n",
      "Step 10100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 583.2835233211517 seconds\n",
      "Step 10200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 588.8662810325623 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 594.5845608711243 seconds\n",
      "Step 10400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 600.1444575786591 seconds\n",
      "Step 10500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 605.8670387268066 seconds\n",
      "Step 10600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 611.4726901054382 seconds\n",
      "Step 10700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 617.2210702896118 seconds\n",
      "Step 10800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 622.8272202014923 seconds\n",
      "Step 10900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 628.4271876811981 seconds\n",
      "Step 11000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 634.1830296516418 seconds\n",
      "Step 11100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 639.8001091480255 seconds\n",
      "Step 11200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 645.5154976844788 seconds\n",
      "Step 11300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 651.0804150104523 seconds\n",
      "Step 11400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 656.6432869434357 seconds\n",
      "Step 11500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 662.3532481193542 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 667.9591240882874 seconds\n",
      "Step 11700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 673.7064137458801 seconds\n",
      "Step 11800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 679.3329601287842 seconds\n",
      "Step 11900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 685.0793371200562 seconds\n",
      "Step 12000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 690.6391694545746 seconds\n",
      "Step 12100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 696.1969494819641 seconds\n",
      "Step 12200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 701.9360990524292 seconds\n",
      "Step 12300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 707.4968898296356 seconds\n",
      "Step 12400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 713.204799413681 seconds\n",
      "Step 12500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 718.7854459285736 seconds\n",
      "Step 12600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 724.3671233654022 seconds\n",
      "Step 12700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 730.1178212165833 seconds\n",
      "Step 12800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 735.6859042644501 seconds\n",
      "Step 12900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 741.4041991233826 seconds\n",
      "Step 13000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 746.9848494529724 seconds\n",
      "Step 13100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 752.7046675682068 seconds\n",
      "Step 13200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 758.3015847206116 seconds\n",
      "Step 13300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 763.9196977615356 seconds\n",
      "Step 13400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 769.6747081279755 seconds\n",
      "Step 13500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 775.276083946228 seconds\n",
      "Step 13600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 781.0153167247772 seconds\n",
      "Step 13700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 786.6000807285309 seconds\n",
      "Step 13800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 792.1765446662903 seconds\n",
      "Step 13900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 797.9010469913483 seconds\n",
      "Step 14000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 803.4837744235992 seconds\n",
      "Step 14100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 809.2062485218048 seconds\n",
      "Step 14200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 814.8213429450989 seconds\n",
      "Step 14300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 820.5678265094757 seconds\n",
      "Step 14400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 826.1275656223297 seconds\n",
      "Step 14500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 831.7125403881073 seconds\n",
      "Step 14600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 837.4349126815796 seconds\n",
      "Step 14700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 843.0207612514496 seconds\n",
      "Step 14800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 848.7557277679443 seconds\n",
      "Step 14900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 854.3478269577026 seconds\n",
      "Step 15000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 859.9119589328766 seconds\n",
      "Step 15100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 865.6436989307404 seconds\n",
      "Step 15200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 871.2274353504181 seconds\n",
      "Step 15300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 876.9709184169769 seconds\n",
      "Step 15400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 882.5847780704498 seconds\n",
      "Step 15500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 888.3250193595886 seconds\n",
      "Step 15600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 893.9045903682709 seconds\n",
      "Step 15700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 899.4852366447449 seconds\n",
      "Step 15800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 905.1910164356232 seconds\n",
      "Step 15900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 910.7841885089874 seconds\n",
      "Step 16000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 916.5568814277649 seconds\n",
      "Step 16100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 922.1634564399719 seconds\n",
      "Step 16200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 927.7377715110779 seconds\n",
      "Step 16300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 933.4446671009064 seconds\n",
      "Step 16400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 939.0129773616791 seconds\n",
      "Step 16500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 944.7353339195251 seconds\n",
      "Step 16600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 950.3034207820892 seconds\n",
      "Step 16700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 956.0426437854767 seconds\n",
      "Step 16800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 961.6284630298615 seconds\n",
      "Step 16900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 967.2288956642151 seconds\n",
      "Step 17000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 972.9847331047058 seconds\n",
      "Step 17100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 978.5642747879028 seconds\n",
      "Step 17200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 984.3002684116364 seconds\n",
      "Step 17300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 989.8821139335632 seconds\n",
      "Step 17400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 995.4491639137268 seconds\n",
      "Step 17500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1001.2258477210999 seconds\n",
      "Step 17600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1006.8232746124268 seconds\n",
      "Step 17700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1012.545661687851 seconds\n",
      "Step 17800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1018.1429851055145 seconds\n",
      "Step 17900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1023.7028107643127 seconds\n",
      "Step 18000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1029.4398577213287 seconds\n",
      "Step 18100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1035.0393619537354 seconds\n",
      "Step 18200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1040.7409782409668 seconds\n",
      "Step 18300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1046.339313030243 seconds\n",
      "Step 18400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1052.083645105362 seconds\n",
      "Step 18500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1057.6707282066345 seconds\n",
      "Step 18600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1063.2555565834045 seconds\n",
      "Step 18700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1069.0028676986694 seconds\n",
      "Step 18800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1074.5782589912415 seconds\n",
      "Step 18900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1080.2851340770721 seconds\n",
      "Step 19000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1085.8449602127075 seconds\n",
      "Step 19100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1091.4256162643433 seconds\n",
      "Step 19200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1097.1845240592957 seconds\n",
      "Step 19300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1102.7705788612366 seconds\n",
      "Step 19400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1108.5231728553772 seconds\n",
      "Step 19500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1114.0860579013824 seconds\n",
      "Step 19600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1119.645866394043 seconds\n",
      "Step 19700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1125.3850240707397 seconds\n",
      "Step 19800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1130.9645643234253 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1136.6808953285217 seconds\n",
      "Step 20000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1142.2438867092133 seconds\n",
      "19\n",
      "Step 0, loss: tensor(0.0104, grad_fn=<SubBackward0>), time elapsed: 0.058374881744384766 seconds\n",
      "Step 100, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 5.801647424697876 seconds\n",
      "Step 200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 11.42495846748352 seconds\n",
      "Step 300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 17.071259021759033 seconds\n",
      "Step 400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 22.814573764801025 seconds\n",
      "Step 500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 28.391058444976807 seconds\n",
      "Step 600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 34.088496685028076 seconds\n",
      "Step 700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 39.69000840187073 seconds\n",
      "Step 800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 45.45019221305847 seconds\n",
      "Step 900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 51.03490948677063 seconds\n",
      "Step 1000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 56.62801694869995 seconds\n",
      "Step 1100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 62.36923670768738 seconds\n",
      "Step 1200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 67.95630884170532 seconds\n",
      "Step 1300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 73.66719365119934 seconds\n",
      "Step 1400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 79.24682450294495 seconds\n",
      "Step 1500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 84.78997111320496 seconds\n",
      "Step 1600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 90.51026940345764 seconds\n",
      "Step 1700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 96.08256483078003 seconds\n",
      "Step 1800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 101.79669046401978 seconds\n",
      "Step 1900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 107.39629673957825 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 113.10710000991821 seconds\n",
      "Step 2100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 118.68465971946716 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 124.2715630531311 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 129.9815800189972 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 135.55382990837097 seconds\n",
      "Step 2500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 141.2119631767273 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 146.7509047985077 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 152.41690349578857 seconds\n",
      "Step 2800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 157.93621492385864 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 163.46743059158325 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 169.148024559021 seconds\n",
      "Step 3100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 174.65367937088013 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 180.31016373634338 seconds\n",
      "Step 3300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 185.80308055877686 seconds\n",
      "Step 3400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 191.32202553749084 seconds\n",
      "Step 3500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 196.9888575077057 seconds\n",
      "Step 3600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 202.51524376869202 seconds\n",
      "Step 3700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 208.2020902633667 seconds\n",
      "Step 3800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 213.71729397773743 seconds\n",
      "Step 3900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 219.3866970539093 seconds\n",
      "Step 4000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 224.8956959247589 seconds\n",
      "Step 4100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 230.4185950756073 seconds\n",
      "Step 4200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 236.07876300811768 seconds\n",
      "Step 4300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 241.58919095993042 seconds\n",
      "Step 4400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 247.2524573802948 seconds\n",
      "Step 4500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 252.7995743751526 seconds\n",
      "Step 4600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 258.49875354766846 seconds\n",
      "Step 4700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 264.02956795692444 seconds\n",
      "Step 4800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 269.55335426330566 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 275.2741873264313 seconds\n",
      "Step 5000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 280.8207392692566 seconds\n",
      "Step 5100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 286.50381875038147 seconds\n",
      "Step 5200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 292.0773093700409 seconds\n",
      "Step 5300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 298.2805278301239 seconds\n",
      "Step 5400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 303.80914282798767 seconds\n",
      "Step 5500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 309.316472530365 seconds\n",
      "Step 5600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 314.9686539173126 seconds\n",
      "Step 5700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 320.5187096595764 seconds\n",
      "Step 5800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 326.18671011924744 seconds\n",
      "Step 5900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 331.72266006469727 seconds\n",
      "Step 6000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 337.23882031440735 seconds\n",
      "Step 6100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 342.9203288555145 seconds\n",
      "Step 6200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 348.4531433582306 seconds\n",
      "Step 6300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 354.10708355903625 seconds\n",
      "Step 6400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 359.6233592033386 seconds\n",
      "Step 6500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 365.3012685775757 seconds\n",
      "Step 6600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 370.8075361251831 seconds\n",
      "Step 6700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 376.3182158470154 seconds\n",
      "Step 6800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 382.0273394584656 seconds\n",
      "Step 6900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 387.5771391391754 seconds\n",
      "Step 7000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 393.21324944496155 seconds\n",
      "Step 7100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 398.75151658058167 seconds\n",
      "Step 7200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 404.4300117492676 seconds\n",
      "Step 7300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 409.9746880531311 seconds\n",
      "Step 7400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 415.51358819007874 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 421.6877932548523 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 427.23864555358887 seconds\n",
      "Step 7700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 432.9344983100891 seconds\n",
      "Step 7800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 438.4451639652252 seconds\n",
      "Step 7900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 443.9789729118347 seconds\n",
      "Step 8000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 449.64538526535034 seconds\n",
      "Step 8100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 455.1568660736084 seconds\n",
      "Step 8200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 460.8026900291443 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 466.3437066078186 seconds\n",
      "Step 8400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 472.04855465888977 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 477.5796058177948 seconds\n",
      "Step 8600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 483.1040585041046 seconds\n",
      "Step 8700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 488.7916717529297 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 494.3114278316498 seconds\n",
      "Step 8900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 499.97879338264465 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 505.52699065208435 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 511.0474889278412 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 517.0570693016052 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 522.5619921684265 seconds\n",
      "Step 9400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 528.223531961441 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 533.7782607078552 seconds\n",
      "Step 9600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 539.4412994384766 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 544.9314336776733 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 550.4685337543488 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 556.1628398895264 seconds\n",
      "Step 10000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 561.7453455924988 seconds\n",
      "Step 10100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 567.5853319168091 seconds\n",
      "Step 10200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 573.1457998752594 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 578.6866915225983 seconds\n",
      "Step 10400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 584.3572628498077 seconds\n",
      "Step 10500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 589.8765025138855 seconds\n",
      "Step 10600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 595.5953402519226 seconds\n",
      "Step 10700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 601.794201374054 seconds\n",
      "Step 10800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 607.7107000350952 seconds\n",
      "Step 10900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 613.3706028461456 seconds\n",
      "Step 11000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 619.0325446128845 seconds\n",
      "Step 11100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 624.8671395778656 seconds\n",
      "Step 11200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 630.4649994373322 seconds\n",
      "Step 11300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 636.1563482284546 seconds\n",
      "Step 11400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 641.7118434906006 seconds\n",
      "Step 11500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 647.2883071899414 seconds\n",
      "Step 11600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 653.010769367218 seconds\n",
      "Step 11700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 658.6101982593536 seconds\n",
      "Step 11800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 664.3516283035278 seconds\n",
      "Step 11900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 669.9404547214508 seconds\n",
      "Step 12000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 675.6589121818542 seconds\n",
      "Step 12100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 681.2310450077057 seconds\n",
      "Step 12200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 686.8409245014191 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 692.5696141719818 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 698.160760641098 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 703.9040834903717 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 709.5180451869965 seconds\n",
      "Step 12700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 715.1278913021088 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 720.8964087963104 seconds\n",
      "Step 12900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 726.4895005226135 seconds\n",
      "Step 13000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 732.2285516262054 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 737.8300592899323 seconds\n",
      "Step 13200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 743.6026475429535 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 749.2417891025543 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 754.851667881012 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 760.5833697319031 seconds\n",
      "Step 13600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 766.1713087558746 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 771.933349609375 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 777.5286271572113 seconds\n",
      "Step 13900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 783.1134610176086 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 788.8672134876251 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 794.4501094818115 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 800.189094543457 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 805.9508016109467 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 811.7497093677521 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 817.3398704528809 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 822.8912694454193 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 828.6137325763702 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 834.2079782485962 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 839.9962565898895 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 845.9523403644562 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 851.511935710907 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 857.2512607574463 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 862.8485758304596 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 868.6002421379089 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 874.185010433197 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 880.5038776397705 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 886.088648557663 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 891.6694524288177 seconds\n",
      "Step 15900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 897.4574053287506 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 903.0611670017242 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 908.9273829460144 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 914.5716750621796 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 920.1690411567688 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 925.9290316104889 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 931.5294466018677 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 937.2696888446808 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 942.8670382499695 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 948.6144218444824 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 954.1837546825409 seconds\n",
      "Step 17000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 959.7643678188324 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 965.4836752414703 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 971.0499148368835 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 976.7836513519287 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 982.3572070598602 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 987.9544520378113 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 993.6977257728577 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 999.290864944458 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1005.0133445262909 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1010.6106743812561 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1016.1998291015625 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1021.9513595104218 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1027.5320234298706 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1033.2857480049133 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1038.8893601894379 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1044.6462273597717 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1050.2570781707764 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1055.8419208526611 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1061.563381433487 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1067.130455493927 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1072.8986976146698 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1078.5075600147247 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1084.1487710475922 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1089.9087979793549 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1095.4883630275726 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1101.2108438014984 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1106.7914848327637 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1112.380485534668 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1118.1072130203247 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1123.6962678432465 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1129.4437868595123 seconds\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0089, grad_fn=<SubBackward0>), time elapsed: 0.07213568687438965 seconds\n",
      "Step 100, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 5.702676773071289 seconds\n",
      "Step 200, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 11.432360410690308 seconds\n",
      "Step 300, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 17.02261996269226 seconds\n",
      "Step 400, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 22.58644461631775 seconds\n",
      "Step 500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 28.31742525100708 seconds\n",
      "Step 600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 33.90096640586853 seconds\n",
      "Step 700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 39.603694915771484 seconds\n",
      "Step 800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 45.1989209651947 seconds\n",
      "Step 900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 50.94024586677551 seconds\n",
      "Step 1000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 56.504154205322266 seconds\n",
      "Step 1100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 62.07868552207947 seconds\n",
      "Step 1200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 67.742835521698 seconds\n",
      "Step 1300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 73.31718039512634 seconds\n",
      "Step 1400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 79.0425820350647 seconds\n",
      "Step 1500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 84.62322425842285 seconds\n",
      "Step 1600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 90.20921802520752 seconds\n",
      "Step 1700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 95.89614868164062 seconds\n",
      "Step 1800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 101.46222853660583 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 107.18639063835144 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 112.76535034179688 seconds\n",
      "Step 2100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 118.4712917804718 seconds\n",
      "Step 2200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 124.03617334365845 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 129.6032645702362 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 135.36324501037598 seconds\n",
      "Step 2500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 140.9647672176361 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 146.6705687046051 seconds\n",
      "Step 2700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 152.23658204078674 seconds\n",
      "Step 2800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 157.93215131759644 seconds\n",
      "Step 2900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 163.50429129600525 seconds\n",
      "Step 3000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 169.11848998069763 seconds\n",
      "Step 3100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 174.85750198364258 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 180.43701457977295 seconds\n",
      "Step 3300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 186.14391112327576 seconds\n",
      "Step 3400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 191.72472882270813 seconds\n",
      "Step 3500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 197.42504858970642 seconds\n",
      "Step 3600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 203.0027937889099 seconds\n",
      "Step 3700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 208.6000781059265 seconds\n",
      "Step 3800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 214.30727100372314 seconds\n",
      "Step 3900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 219.87411737442017 seconds\n",
      "Step 4000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 225.60625433921814 seconds\n",
      "Step 4100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 231.202054977417 seconds\n",
      "Step 4200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 236.78688645362854 seconds\n",
      "Step 4300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 242.50936102867126 seconds\n",
      "Step 4400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 248.09725069999695 seconds\n",
      "Step 4500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 253.78753018379211 seconds\n",
      "Step 4600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 259.3639278411865 seconds\n",
      "Step 4700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 265.0781292915344 seconds\n",
      "Step 4800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 270.6754114627838 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 276.2435607910156 seconds\n",
      "Step 5000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 281.9745125770569 seconds\n",
      "Step 5100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 287.5633451938629 seconds\n",
      "Step 5200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 293.29723501205444 seconds\n",
      "Step 5300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 298.87903237342834 seconds\n",
      "Step 5400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 304.6056230068207 seconds\n",
      "Step 5500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 310.17392015457153 seconds\n",
      "Step 5600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 315.7972083091736 seconds\n",
      "Step 5700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 321.9440426826477 seconds\n",
      "Step 5800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 327.62495732307434 seconds\n",
      "Step 5900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 333.83731031417847 seconds\n",
      "Step 6000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 339.48271799087524 seconds\n",
      "Step 6100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 345.09772968292236 seconds\n",
      "Step 6200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 350.8420581817627 seconds\n",
      "Step 6300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 356.4874620437622 seconds\n",
      "Step 6400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 362.2355601787567 seconds\n",
      "Step 6500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 368.91233801841736 seconds\n",
      "Step 6600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 374.61380410194397 seconds\n",
      "Step 6700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 380.13073110580444 seconds\n",
      "Step 6800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 385.6799032688141 seconds\n",
      "Step 6900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 391.30949878692627 seconds\n",
      "Step 7000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 396.8428235054016 seconds\n",
      "Step 7100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 402.50303506851196 seconds\n",
      "Step 7200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 408.03455114364624 seconds\n",
      "Step 7300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 413.7323684692383 seconds\n",
      "Step 7400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 419.2495596408844 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 424.77498483657837 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 430.412823677063 seconds\n",
      "Step 7700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 435.95179176330566 seconds\n",
      "Step 7800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 441.6513900756836 seconds\n",
      "Step 7900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 447.199355840683 seconds\n",
      "Step 8000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 452.73592877388 seconds\n",
      "Step 8100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 460.0047376155853 seconds\n",
      "Step 8200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 465.51171708106995 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 471.1926112174988 seconds\n",
      "Step 8400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 476.7417731285095 seconds\n",
      "Step 8500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 482.42599272727966 seconds\n",
      "Step 8600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 488.7171206474304 seconds\n",
      "Step 8700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 494.2690713405609 seconds\n",
      "Step 8800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 499.9383523464203 seconds\n",
      "Step 8900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 505.50703024864197 seconds\n",
      "Step 9000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 511.1655933856964 seconds\n",
      "Step 9100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 516.6693913936615 seconds\n",
      "Step 9200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 522.1936485767365 seconds\n",
      "Step 9300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 527.8661193847656 seconds\n",
      "Step 9400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 533.4165332317352 seconds\n",
      "Step 9500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 539.1052796840668 seconds\n",
      "Step 9600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 544.6478817462921 seconds\n",
      "Step 9700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 550.3583629131317 seconds\n",
      "Step 9800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 555.8918981552124 seconds\n",
      "Step 9900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 561.410961151123 seconds\n",
      "Step 10000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 567.1268889904022 seconds\n",
      "Step 10100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 572.6700189113617 seconds\n",
      "Step 10200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 578.3548314571381 seconds\n",
      "Step 10300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 584.6038529872894 seconds\n",
      "Step 10400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 590.1354458332062 seconds\n",
      "Step 10500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 595.8037419319153 seconds\n",
      "Step 10600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 601.3213684558868 seconds\n",
      "Step 10700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 606.9813573360443 seconds\n",
      "Step 10800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 612.5085990428925 seconds\n",
      "Step 10900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 618.170820236206 seconds\n",
      "Step 11000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 623.6957125663757 seconds\n",
      "Step 11100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 629.2658047676086 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 634.9730806350708 seconds\n",
      "Step 11300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 640.5438039302826 seconds\n",
      "Step 11400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 646.2682814598083 seconds\n",
      "Step 11500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 652.1674718856812 seconds\n",
      "Step 11600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 657.7212634086609 seconds\n",
      "Step 11700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 663.5137190818787 seconds\n",
      "Step 11800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 669.0915653705597 seconds\n",
      "Step 11900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 674.8000164031982 seconds\n",
      "Step 12000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 680.7899897098541 seconds\n",
      "Step 12100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 686.4711961746216 seconds\n",
      "Step 12200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 692.024816274643 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 697.5640442371368 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 703.277664899826 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 708.8495781421661 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 714.9754374027252 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 720.5669205188751 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 726.1101379394531 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 731.7787818908691 seconds\n",
      "Step 13000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 737.3340086936951 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 743.0262529850006 seconds\n",
      "Step 13200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 748.5793573856354 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 754.3052518367767 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 759.8499412536621 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 765.4170308113098 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 771.1133246421814 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 776.6520760059357 seconds\n",
      "Step 13800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 782.3717305660248 seconds\n",
      "Step 13900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 788.3748669624329 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 793.8731474876404 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 799.5488178730011 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 805.0909724235535 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 810.8368880748749 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 816.3797569274902 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 822.0870585441589 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 827.6078948974609 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 833.1421041488647 seconds\n",
      "Step 14800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 838.9936938285828 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 845.0170059204102 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 850.7043809890747 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 856.3578882217407 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 861.8811047077179 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 867.5721724033356 seconds\n",
      "Step 15400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 873.2960715293884 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 879.3264861106873 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 884.835547208786 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 890.447329044342 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 896.1392221450806 seconds\n",
      "Step 15900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 901.9465727806091 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 908.0802664756775 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 913.9507586956024 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 919.6411929130554 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 925.239275932312 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 930.7642331123352 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 936.8066973686218 seconds\n",
      "Step 16600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 942.3281955718994 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 948.0843377113342 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 953.6345121860504 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 959.1827626228333 seconds\n",
      "Step 17000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 965.2668035030365 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 970.8107781410217 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 977.1673505306244 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 982.7004380226135 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 988.3624258041382 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 993.8855166435242 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 999.4374523162842 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1005.2706332206726 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1010.7855477333069 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1017.4338436126709 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1022.9700982570648 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1028.497498512268 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1034.1771285533905 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1039.7039353847504 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1045.3899302482605 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1050.9129424095154 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1056.4646155834198 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1062.1542375087738 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1067.6971607208252 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1073.3832304477692 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1078.915206670761 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1084.6644222736359 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1090.225491285324 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1095.7653496265411 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1101.4763867855072 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1107.0297546386719 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1112.7675850391388 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1118.7667953968048 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1124.3330490589142 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1130.0458707809448 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1135.6064257621765 seconds\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0252, grad_fn=<SubBackward0>), time elapsed: 0.06104612350463867 seconds\n",
      "Step 100, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 5.734208106994629 seconds\n",
      "Step 200, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 11.243655681610107 seconds\n",
      "Step 300, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 16.90635848045349 seconds\n",
      "Step 400, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 22.405108213424683 seconds\n",
      "Step 500, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 27.919087886810303 seconds\n",
      "Step 600, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 33.56527519226074 seconds\n",
      "Step 700, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 39.0777690410614 seconds\n",
      "Step 800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 44.70958733558655 seconds\n",
      "Step 900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 50.22515296936035 seconds\n",
      "Step 1000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 55.87300491333008 seconds\n",
      "Step 1100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 61.37632870674133 seconds\n",
      "Step 1200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 66.89590001106262 seconds\n",
      "Step 1300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 72.55580139160156 seconds\n",
      "Step 1400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 78.10654592514038 seconds\n",
      "Step 1500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 83.76004600524902 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 89.26222348213196 seconds\n",
      "Step 1700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 94.7647716999054 seconds\n",
      "Step 1800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 100.39011716842651 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 105.9054548740387 seconds\n",
      "Step 2000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 111.5659122467041 seconds\n",
      "Step 2100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 117.08203721046448 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 122.74281072616577 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 128.22500729560852 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 133.71634936332703 seconds\n",
      "Step 2500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 139.36959624290466 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 144.87011861801147 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 150.5031237602234 seconds\n",
      "Step 2800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 156.02144527435303 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 161.67922067642212 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 167.1741099357605 seconds\n",
      "Step 3100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 172.69362664222717 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 178.32041931152344 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 183.8279025554657 seconds\n",
      "Step 3400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 189.45561265945435 seconds\n",
      "Step 3500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 194.95166087150574 seconds\n",
      "Step 3600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 200.4891641139984 seconds\n",
      "Step 3700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 206.13868761062622 seconds\n",
      "Step 3800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 211.67858266830444 seconds\n",
      "Step 3900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 217.33155989646912 seconds\n",
      "Step 4000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 222.84700298309326 seconds\n",
      "Step 4100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 228.48756742477417 seconds\n",
      "Step 4200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 234.02415776252747 seconds\n",
      "Step 4300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 239.53168082237244 seconds\n",
      "Step 4400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 245.175635099411 seconds\n",
      "Step 4500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 250.69162678718567 seconds\n",
      "Step 4600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 256.3186237812042 seconds\n",
      "Step 4700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 261.85623121261597 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 267.52527141571045 seconds\n",
      "Step 4900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 273.0266168117523 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 278.5254530906677 seconds\n",
      "Step 5100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 284.14933824539185 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.69140362739563 seconds\n",
      "Step 5300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 295.3877422809601 seconds\n",
      "Step 5400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 300.91175532341003 seconds\n",
      "Step 5500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 306.4403042793274 seconds\n",
      "Step 5600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 312.1159977912903 seconds\n",
      "Step 5700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 317.66580390930176 seconds\n",
      "Step 5800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 323.3741171360016 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 328.9233949184418 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 334.5922694206238 seconds\n",
      "Step 6100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 340.1170918941498 seconds\n",
      "Step 6200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 345.66538524627686 seconds\n",
      "Step 6300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 351.3634717464447 seconds\n",
      "Step 6400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 356.8904733657837 seconds\n",
      "Step 6500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 362.5532171726227 seconds\n",
      "Step 6600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 368.07183027267456 seconds\n",
      "Step 6700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 373.7512695789337 seconds\n",
      "Step 6800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 379.2815718650818 seconds\n",
      "Step 6900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 384.8277907371521 seconds\n",
      "Step 7000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 390.498074054718 seconds\n",
      "Step 7100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 396.02922320365906 seconds\n",
      "Step 7200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 401.69940423965454 seconds\n",
      "Step 7300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 407.2379856109619 seconds\n",
      "Step 7400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 412.76017713546753 seconds\n",
      "Step 7500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 418.4042887687683 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 423.9156403541565 seconds\n",
      "Step 7700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 429.56330704689026 seconds\n",
      "Step 7800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 435.06485056877136 seconds\n",
      "Step 7900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 440.760849237442 seconds\n",
      "Step 8000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 446.3157515525818 seconds\n",
      "Step 8100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 451.87577724456787 seconds\n",
      "Step 8200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 457.56143736839294 seconds\n",
      "Step 8300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 463.17198944091797 seconds\n",
      "Step 8400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 468.9332127571106 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 474.55150151252747 seconds\n",
      "Step 8600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 480.25520038604736 seconds\n",
      "Step 8700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 486.53687143325806 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 492.05949783325195 seconds\n",
      "Step 8900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 497.74524569511414 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 503.27477741241455 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 508.9784564971924 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 514.4897887706757 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 519.9938724040985 seconds\n",
      "Step 9400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 525.666699886322 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 531.2187542915344 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 536.911702632904 seconds\n",
      "Step 9700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 542.457923412323 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 550.9693074226379 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 556.53067445755 seconds\n",
      "Step 10000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 562.0457453727722 seconds\n",
      "Step 10100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 567.7209243774414 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 573.2366614341736 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 578.8986282348633 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 584.4017679691315 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 589.9175248146057 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 595.5809977054596 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 601.0804109573364 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 606.7408878803253 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 612.2701210975647 seconds\n",
      "Step 11000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 617.9665439128876 seconds\n",
      "Step 11100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 623.4912898540497 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 629.0605225563049 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 634.7732992172241 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 640.3244531154633 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 646.0181910991669 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 651.5625641345978 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 657.6509099006653 seconds\n",
      "Step 11800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 663.4806363582611 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 669.013991355896 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 674.6753304004669 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 680.1907546520233 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 685.8702132701874 seconds\n",
      "Step 12300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 691.4003713130951 seconds\n",
      "Step 12400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 697.212644815445 seconds\n",
      "Step 12500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 702.8921728134155 seconds\n",
      "Step 12600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 708.4204947948456 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 714.1057593822479 seconds\n",
      "Step 12800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 719.6401913166046 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 725.1491310596466 seconds\n",
      "Step 13000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 730.8135621547699 seconds\n",
      "Step 13100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 736.3296029567719 seconds\n",
      "Step 13200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 742.0219097137451 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 747.5938534736633 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 753.3035175800323 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 758.8491270542145 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 764.3891711235046 seconds\n",
      "Step 13700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 770.0951337814331 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 775.6590394973755 seconds\n",
      "Step 13900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 781.3513419628143 seconds\n",
      "Step 14000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 786.9010636806488 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 792.6151530742645 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 798.330073595047 seconds\n",
      "Step 14300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 803.9007883071899 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 809.6133894920349 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 815.7921347618103 seconds\n",
      "Step 14600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 821.302880525589 seconds\n",
      "Step 14700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 826.962860584259 seconds\n",
      "Step 14800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 832.4860274791718 seconds\n",
      "Step 14900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 838.1615381240845 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 843.7139041423798 seconds\n",
      "Step 15100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 849.4184215068817 seconds\n",
      "Step 15200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 854.9286379814148 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 860.4413890838623 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 866.0908088684082 seconds\n",
      "Step 15500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 871.6336581707001 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 877.2795188426971 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 882.786422252655 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 888.3053781986237 seconds\n",
      "Step 15900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 894.018976688385 seconds\n",
      "Step 16000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 899.546947479248 seconds\n",
      "Step 16100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 905.2550935745239 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 910.7914497852325 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 916.48561835289 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 921.9959180355072 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 927.5466601848602 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 933.2862815856934 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 938.833646774292 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 944.5360510349274 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 950.0856931209564 seconds\n",
      "Step 17000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 955.6079568862915 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 961.3405108451843 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 966.8856563568115 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 972.5947830677032 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 978.1390635967255 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 983.8699791431427 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 989.4102246761322 seconds\n",
      "Step 17700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 994.9983336925507 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1000.8181262016296 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1006.3431444168091 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1012.0386373996735 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1017.5774040222168 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1023.7795171737671 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1029.4762136936188 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1034.9983923435211 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1040.67418050766 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1046.195878982544 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1051.7172181606293 seconds\n",
      "Step 18800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1057.3927767276764 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1062.9259150028229 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1068.598335981369 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1074.1397347450256 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1079.8325889110565 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1085.3824725151062 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1090.9379818439484 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1096.6433193683624 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1102.1663262844086 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1108.1260755062103 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1113.6901490688324 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1119.2373585700989 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1124.994999885559 seconds\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 0.062408447265625 seconds\n",
      "Step 100, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 5.696375370025635 seconds\n",
      "Step 200, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 11.356139898300171 seconds\n",
      "Step 300, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 16.874448776245117 seconds\n",
      "Step 400, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 22.549752712249756 seconds\n",
      "Step 500, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 28.05535912513733 seconds\n",
      "Step 600, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 33.564220666885376 seconds\n",
      "Step 700, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 39.20470595359802 seconds\n",
      "Step 800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 44.69365906715393 seconds\n",
      "Step 900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 50.32076048851013 seconds\n",
      "Step 1000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 55.82115364074707 seconds\n",
      "Step 1100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 61.33740735054016 seconds\n",
      "Step 1200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 66.94639563560486 seconds\n",
      "Step 1300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 72.43548440933228 seconds\n",
      "Step 1400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 78.05605030059814 seconds\n",
      "Step 1500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 83.5745849609375 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 89.18767786026001 seconds\n",
      "Step 1700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 94.65889430046082 seconds\n",
      "Step 1800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 100.16338777542114 seconds\n",
      "Step 1900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 105.81823682785034 seconds\n",
      "Step 2000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 111.33467555046082 seconds\n",
      "Step 2100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 117.00308394432068 seconds\n",
      "Step 2200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 122.5227313041687 seconds\n",
      "Step 2300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 128.17369866371155 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 133.6724145412445 seconds\n",
      "Step 2500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 139.1772608757019 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 144.83270025253296 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 150.35105776786804 seconds\n",
      "Step 2800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 155.9951992034912 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 161.5031032562256 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 167.16948318481445 seconds\n",
      "Step 3100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 172.6698796749115 seconds\n",
      "Step 3200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 178.2107813358307 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 183.8490424156189 seconds\n",
      "Step 3400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 189.34838724136353 seconds\n",
      "Step 3500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 194.99189949035645 seconds\n",
      "Step 3600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 200.49588656425476 seconds\n",
      "Step 3700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 206.0148115158081 seconds\n",
      "Step 3800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 211.67961025238037 seconds\n",
      "Step 3900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 217.21962904930115 seconds\n",
      "Step 4000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 222.84956073760986 seconds\n",
      "Step 4100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 228.3407280445099 seconds\n",
      "Step 4200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 233.98933792114258 seconds\n",
      "Step 4300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 239.49882793426514 seconds\n",
      "Step 4400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 244.9912793636322 seconds\n",
      "Step 4500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 250.60625171661377 seconds\n",
      "Step 4600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 256.1503973007202 seconds\n",
      "Step 4700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 261.8335111141205 seconds\n",
      "Step 4800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 267.3791344165802 seconds\n",
      "Step 4900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 273.0315291881561 seconds\n",
      "Step 5000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 278.54478788375854 seconds\n",
      "Step 5100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 284.051869392395 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.6912434101105 seconds\n",
      "Step 5300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 295.2274441719055 seconds\n",
      "Step 5400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 301.0412838459015 seconds\n",
      "Step 5500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 306.63375067710876 seconds\n",
      "Step 5600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 312.36538100242615 seconds\n",
      "Step 5700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 318.01928544044495 seconds\n",
      "Step 5800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 323.6002857685089 seconds\n",
      "Step 5900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 329.2669520378113 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 335.10569739341736 seconds\n",
      "Step 6100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 340.7737216949463 seconds\n",
      "Step 6200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 346.3189344406128 seconds\n",
      "Step 6300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 351.8970708847046 seconds\n",
      "Step 6400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 357.5411503314972 seconds\n",
      "Step 6500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 363.60115242004395 seconds\n",
      "Step 6600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 369.2760157585144 seconds\n",
      "Step 6700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 374.77955651283264 seconds\n",
      "Step 6800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 380.4307096004486 seconds\n",
      "Step 6900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 385.94662284851074 seconds\n",
      "Step 7000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 391.5105664730072 seconds\n",
      "Step 7100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 397.2053031921387 seconds\n",
      "Step 7200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 402.73500084877014 seconds\n",
      "Step 7300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 409.0199065208435 seconds\n",
      "Step 7400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 414.5445201396942 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 420.08415818214417 seconds\n",
      "Step 7600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 425.7440912723541 seconds\n",
      "Step 7700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 431.2938895225525 seconds\n",
      "Step 7800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 437.077285528183 seconds\n",
      "Step 7900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 442.6989121437073 seconds\n",
      "Step 8000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 448.4052336215973 seconds\n",
      "Step 8100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 454.1779203414917 seconds\n",
      "Step 8200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 459.6638250350952 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 465.3725280761719 seconds\n",
      "Step 8400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 470.89377331733704 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 476.59131383895874 seconds\n",
      "Step 8600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 482.1314516067505 seconds\n",
      "Step 8700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 488.12305974960327 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 493.63715386390686 seconds\n",
      "Step 8900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 499.23192381858826 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 504.88069701194763 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 510.6939241886139 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 516.3896825313568 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 522.0544168949127 seconds\n",
      "Step 9400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 527.6032629013062 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 533.5042760372162 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 539.0410859584808 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 544.8525738716125 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 550.3919503688812 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 556.363445520401 seconds\n",
      "Step 10000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 561.968624830246 seconds\n",
      "Step 10100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 567.5850212574005 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 573.4762668609619 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 579.0196993350983 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 585.4795236587524 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 591.0104584693909 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 596.5257363319397 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 602.2011423110962 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 607.7054314613342 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 613.398533821106 seconds\n",
      "Step 11000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 618.9451117515564 seconds\n",
      "Step 11100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 624.6660213470459 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 630.2028577327728 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 635.7669126987457 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 642.2804756164551 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 647.8170714378357 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 653.476095199585 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 659.0052664279938 seconds\n",
      "Step 11800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 664.7066004276276 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 670.512247800827 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 676.0521655082703 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 682.4996745586395 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 688.0339295864105 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 693.757485628128 seconds\n",
      "Step 12400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 699.279896736145 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 704.8117897510529 seconds\n",
      "Step 12600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 710.4937098026276 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 716.0311918258667 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 721.7476274967194 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 727.3132901191711 seconds\n",
      "Step 13000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 732.8407299518585 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 738.5507116317749 seconds\n",
      "Step 13200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 744.0761578083038 seconds\n",
      "Step 13300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 749.7764909267426 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 755.3499977588654 seconds\n",
      "Step 13500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 761.059056520462 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 766.5749368667603 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 772.1401298046112 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 777.8281714916229 seconds\n",
      "Step 13900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 783.3666870594025 seconds\n",
      "Step 14000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 789.0803029537201 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 794.6418609619141 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 800.1977670192719 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 805.9134199619293 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 812.5471160411835 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 818.2593183517456 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 823.7959759235382 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 829.5334892272949 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 835.0652041435242 seconds\n",
      "Step 14900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 840.6293125152588 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 846.3370172977448 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 852.6689028739929 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 858.3804221153259 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 863.8879086971283 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 869.3940896987915 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 875.0835852622986 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 880.6328010559082 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 886.3191637992859 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 891.8448326587677 seconds\n",
      "Step 15900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 897.5148179531097 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 903.0317509174347 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 908.5612268447876 seconds\n",
      "Step 16200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 914.202761888504 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 919.7247595787048 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 925.4048388004303 seconds\n",
      "Step 16500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 930.9378428459167 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 936.4750442504883 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 942.184654712677 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 947.7223062515259 seconds\n",
      "Step 16900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 953.4235413074493 seconds\n",
      "Step 17000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 958.9469783306122 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 964.495992898941 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 970.2228078842163 seconds\n",
      "Step 17300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 975.9410753250122 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 981.6579887866974 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 987.3075630664825 seconds\n",
      "Step 17600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 993.0174686908722 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 998.5818662643433 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1004.1122608184814 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1009.9207649230957 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1015.4350085258484 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1021.2610034942627 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1026.776380777359 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1032.660828113556 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1038.3512954711914 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1043.8595705032349 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1049.548374414444 seconds\n",
      "Step 18700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1055.0907137393951 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1060.6728491783142 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1066.396125793457 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1071.949861049652 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1077.701786994934 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1083.9018244743347 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1089.6335468292236 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1095.1713914871216 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1100.731683731079 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1106.5035297870636 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1112.1160607337952 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1117.8304777145386 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1123.6561300754547 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1129.2146844863892 seconds\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 0.05976462364196777 seconds\n",
      "Step 100, loss: tensor(0.0083, grad_fn=<SubBackward0>), time elapsed: 5.809586524963379 seconds\n",
      "Step 200, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 11.32264757156372 seconds\n",
      "Step 300, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 16.962754487991333 seconds\n",
      "Step 400, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 22.47853684425354 seconds\n",
      "Step 500, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 28.122157096862793 seconds\n",
      "Step 600, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 33.64934706687927 seconds\n",
      "Step 700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 39.155890226364136 seconds\n",
      "Step 800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 44.795947551727295 seconds\n",
      "Step 900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 50.298255443573 seconds\n",
      "Step 1000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 55.91584014892578 seconds\n",
      "Step 1100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 61.42700242996216 seconds\n",
      "Step 1200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 67.08045244216919 seconds\n",
      "Step 1300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 72.58123207092285 seconds\n",
      "Step 1400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 78.10072040557861 seconds\n",
      "Step 1500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 83.7224509716034 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 89.22169899940491 seconds\n",
      "Step 1700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 94.8899896144867 seconds\n",
      "Step 1800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 100.3969612121582 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 105.90123867988586 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 111.55184674263 seconds\n",
      "Step 2100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 117.06472325325012 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 122.74810338020325 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 128.26601195335388 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 133.9359667301178 seconds\n",
      "Step 2500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 139.42031359672546 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 144.92477250099182 seconds\n",
      "Step 2700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 150.55643343925476 seconds\n",
      "Step 2800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 156.08165979385376 seconds\n",
      "Step 2900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 161.72348046302795 seconds\n",
      "Step 3000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 167.2283239364624 seconds\n",
      "Step 3100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 172.8768548965454 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 178.37858819961548 seconds\n",
      "Step 3300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 183.90256023406982 seconds\n",
      "Step 3400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 189.52807712554932 seconds\n",
      "Step 3500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 195.03188014030457 seconds\n",
      "Step 3600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 200.6740539073944 seconds\n",
      "Step 3700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 206.1911153793335 seconds\n",
      "Step 3800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 211.84776282310486 seconds\n",
      "Step 3900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 217.3700668811798 seconds\n",
      "Step 4000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 222.87488913536072 seconds\n",
      "Step 4100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 228.51852774620056 seconds\n",
      "Step 4200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 234.01812863349915 seconds\n",
      "Step 4300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 239.66209411621094 seconds\n",
      "Step 4400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 245.19171023368835 seconds\n",
      "Step 4500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 250.71590948104858 seconds\n",
      "Step 4600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 256.37010407447815 seconds\n",
      "Step 4700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 261.9004592895508 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 267.5456736087799 seconds\n",
      "Step 4900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 273.0667414665222 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 278.7213418483734 seconds\n",
      "Step 5100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 284.2301585674286 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.74536323547363 seconds\n",
      "Step 5300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 295.3888351917267 seconds\n",
      "Step 5400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 300.8954334259033 seconds\n",
      "Step 5500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 306.5755937099457 seconds\n",
      "Step 5600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 312.1050035953522 seconds\n",
      "Step 5700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 317.7699990272522 seconds\n",
      "Step 5800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 323.25768280029297 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 328.78535199165344 seconds\n",
      "Step 6000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 334.45333766937256 seconds\n",
      "Step 6100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 339.950156211853 seconds\n",
      "Step 6200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 345.5843434333801 seconds\n",
      "Step 6300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 351.0767834186554 seconds\n",
      "Step 6400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 356.57354402542114 seconds\n",
      "Step 6500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 362.22276043891907 seconds\n",
      "Step 6600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 367.73783230781555 seconds\n",
      "Step 6700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 373.3661983013153 seconds\n",
      "Step 6800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 378.874351978302 seconds\n",
      "Step 6900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 384.508811712265 seconds\n",
      "Step 7000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 390.0481927394867 seconds\n",
      "Step 7100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 395.5883390903473 seconds\n",
      "Step 7200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 401.2607045173645 seconds\n",
      "Step 7300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 406.7698905467987 seconds\n",
      "Step 7400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 412.4196426868439 seconds\n",
      "Step 7500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 417.949515581131 seconds\n",
      "Step 7600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 423.6239261627197 seconds\n",
      "Step 7700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 429.1613030433655 seconds\n",
      "Step 7800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 434.6909170150757 seconds\n",
      "Step 7900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 440.34901785850525 seconds\n",
      "Step 8000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 445.8953266143799 seconds\n",
      "Step 8100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 451.5611057281494 seconds\n",
      "Step 8200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 457.10882568359375 seconds\n",
      "Step 8300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 462.6309506893158 seconds\n",
      "Step 8400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 468.28206610679626 seconds\n",
      "Step 8500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 473.7941725254059 seconds\n",
      "Step 8600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 479.46640038490295 seconds\n",
      "Step 8700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 485.00726199150085 seconds\n",
      "Step 8800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 490.70447301864624 seconds\n",
      "Step 8900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 496.2120637893677 seconds\n",
      "Step 9000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 501.74134635925293 seconds\n",
      "Step 9100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 507.4003572463989 seconds\n",
      "Step 9200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 512.91335105896 seconds\n",
      "Step 9300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 518.5798783302307 seconds\n",
      "Step 9400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 524.1065108776093 seconds\n",
      "Step 9500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 529.6429951190948 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 535.3118045330048 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 540.8547384738922 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 546.5418450832367 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 552.1296229362488 seconds\n",
      "Step 10000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 557.814975976944 seconds\n",
      "Step 10100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 563.3574223518372 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 568.9001142978668 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 574.6053383350372 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 580.1724507808685 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 585.8739233016968 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 591.6204581260681 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 597.1847190856934 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 603.0862953662872 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 608.6527094841003 seconds\n",
      "Step 11000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 614.3305974006653 seconds\n",
      "Step 11100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 619.8812961578369 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 625.5981063842773 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 631.4706566333771 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 636.9842014312744 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 642.6877584457397 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 648.1973164081573 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 653.8606839179993 seconds\n",
      "Step 11800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 659.3813669681549 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 664.9133498668671 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 670.6357507705688 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 676.1817166805267 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 681.8598594665527 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 687.753500699997 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 693.42937541008 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 698.944155216217 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 704.4872894287109 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 710.147319316864 seconds\n",
      "Step 12800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 715.6894958019257 seconds\n",
      "Step 12900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 721.37824177742 seconds\n",
      "Step 13000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 726.9223260879517 seconds\n",
      "Step 13100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 732.5756614208221 seconds\n",
      "Step 13200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 738.3206334114075 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 743.8849663734436 seconds\n",
      "Step 13400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 749.5849361419678 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 755.2100439071655 seconds\n",
      "Step 13600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 761.2333796024323 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 766.8138699531555 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 772.3450062274933 seconds\n",
      "Step 13900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 778.0236189365387 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 783.5857994556427 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 789.3931787014008 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 794.9230906963348 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 800.4816346168518 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 806.4294850826263 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 811.9610040187836 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 817.6270923614502 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 823.1551222801208 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 828.8125762939453 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 834.3238263130188 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 839.8569347858429 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 845.7151257991791 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 851.2468118667603 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 856.9983921051025 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 862.9393923282623 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 868.4626414775848 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 874.1329071521759 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 879.639327287674 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 885.3315443992615 seconds\n",
      "Step 15900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 890.8633298873901 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 896.5782256126404 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 902.1314287185669 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 907.681314945221 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 913.6124680042267 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 919.1493232250214 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 924.8423783779144 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 930.37637591362 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 935.9592838287354 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 942.3118922710419 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 947.9203164577484 seconds\n",
      "Step 17000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 953.6250727176666 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 959.1420073509216 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 964.6573147773743 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 970.3173906803131 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 975.8465120792389 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 981.5108513832092 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 987.0350692272186 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 992.7310254573822 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 998.3001310825348 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1003.8164455890656 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1009.5417423248291 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1015.0845959186554 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1020.7926044464111 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1026.3616268634796 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1031.934430360794 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1037.676941871643 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1043.2564463615417 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1048.97469496727 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1054.5201840400696 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1060.1435840129852 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1065.8962247371674 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1071.4344387054443 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1077.132113456726 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1083.4435346126556 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1089.176728963852 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1094.6822063922882 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1100.236656665802 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1105.9095611572266 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1111.442922115326 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1117.1323268413544 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1122.678377866745 seconds\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0250, grad_fn=<SubBackward0>), time elapsed: 0.061799049377441406 seconds\n",
      "Step 100, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 5.660330295562744 seconds\n",
      "Step 200, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 11.341418266296387 seconds\n",
      "Step 300, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 16.84400200843811 seconds\n",
      "Step 400, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 22.479936599731445 seconds\n",
      "Step 500, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 27.972734928131104 seconds\n",
      "Step 600, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 33.606866121292114 seconds\n",
      "Step 700, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 39.133134841918945 seconds\n",
      "Step 800, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 44.65788245201111 seconds\n",
      "Step 900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 50.2734580039978 seconds\n",
      "Step 1000, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 55.77788162231445 seconds\n",
      "Step 1100, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 61.40591788291931 seconds\n",
      "Step 1200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 66.91237020492554 seconds\n",
      "Step 1300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 72.57054924964905 seconds\n",
      "Step 1400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 78.0735821723938 seconds\n",
      "Step 1500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 83.5859203338623 seconds\n",
      "Step 1600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 89.21130442619324 seconds\n",
      "Step 1700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 94.71198868751526 seconds\n",
      "Step 1800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 100.3637261390686 seconds\n",
      "Step 1900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 105.8610270023346 seconds\n",
      "Step 2000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 111.35819053649902 seconds\n",
      "Step 2100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 116.98390054702759 seconds\n",
      "Step 2200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 122.49946522712708 seconds\n",
      "Step 2300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 128.14981269836426 seconds\n",
      "Step 2400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 133.6891098022461 seconds\n",
      "Step 2500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 139.33144164085388 seconds\n",
      "Step 2600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 144.8365375995636 seconds\n",
      "Step 2700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 150.34799909591675 seconds\n",
      "Step 2800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 155.9775846004486 seconds\n",
      "Step 2900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 161.5023069381714 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 167.14320492744446 seconds\n",
      "Step 3100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 172.64663672447205 seconds\n",
      "Step 3200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 178.31255960464478 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 183.83059668540955 seconds\n",
      "Step 3400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 189.34862685203552 seconds\n",
      "Step 3500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 195.01672267913818 seconds\n",
      "Step 3600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 200.52543878555298 seconds\n",
      "Step 3700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 206.1606080532074 seconds\n",
      "Step 3800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 211.6711084842682 seconds\n",
      "Step 3900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 217.31331181526184 seconds\n",
      "Step 4000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 222.8430380821228 seconds\n",
      "Step 4100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 228.35319757461548 seconds\n",
      "Step 4200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 234.0042462348938 seconds\n",
      "Step 4300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 239.52733278274536 seconds\n",
      "Step 4400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 245.1570703983307 seconds\n",
      "Step 4500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 250.675213098526 seconds\n",
      "Step 4600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 256.18557834625244 seconds\n",
      "Step 4700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 261.8235104084015 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 267.3484137058258 seconds\n",
      "Step 4900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 272.9973909854889 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 278.51500701904297 seconds\n",
      "Step 5100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 284.19960737228394 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.7401623725891 seconds\n",
      "Step 5300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 295.460077047348 seconds\n",
      "Step 5400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 301.21001982688904 seconds\n",
      "Step 5500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 307.832879781723 seconds\n",
      "Step 5600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 313.52512669563293 seconds\n",
      "Step 5700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 319.0414996147156 seconds\n",
      "Step 5800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 324.6967785358429 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 330.19734382629395 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 335.7126712799072 seconds\n",
      "Step 6100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 341.3656198978424 seconds\n",
      "Step 6200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 346.90833568573 seconds\n",
      "Step 6300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 352.56276631355286 seconds\n",
      "Step 6400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 358.0993494987488 seconds\n",
      "Step 6500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 363.63742232322693 seconds\n",
      "Step 6600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 370.0353240966797 seconds\n",
      "Step 6700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 375.5560111999512 seconds\n",
      "Step 6800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 381.19084191322327 seconds\n",
      "Step 6900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 386.7077338695526 seconds\n",
      "Step 7000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 392.3817820549011 seconds\n",
      "Step 7100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 397.89482402801514 seconds\n",
      "Step 7200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 403.4397830963135 seconds\n",
      "Step 7300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 409.11099553108215 seconds\n",
      "Step 7400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 414.64569306373596 seconds\n",
      "Step 7500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 420.32745337486267 seconds\n",
      "Step 7600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 425.8335521221161 seconds\n",
      "Step 7700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 431.5099992752075 seconds\n",
      "Step 7800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 437.07602071762085 seconds\n",
      "Step 7900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 442.6216678619385 seconds\n",
      "Step 8000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 448.31846475601196 seconds\n",
      "Step 8100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 453.88017892837524 seconds\n",
      "Step 8200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 459.55634784698486 seconds\n",
      "Step 8300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 465.12262654304504 seconds\n",
      "Step 8400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 470.7962911128998 seconds\n",
      "Step 8500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 477.00541591644287 seconds\n",
      "Step 8600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 482.57770109176636 seconds\n",
      "Step 8700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 488.2362823486328 seconds\n",
      "Step 8800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 493.74257946014404 seconds\n",
      "Step 8900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 499.4379527568817 seconds\n",
      "Step 9000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 504.9460859298706 seconds\n",
      "Step 9100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 510.4827947616577 seconds\n",
      "Step 9200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 516.1157612800598 seconds\n",
      "Step 9300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 521.6283257007599 seconds\n",
      "Step 9400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 528.0898678302765 seconds\n",
      "Step 9500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 533.608996629715 seconds\n",
      "Step 9600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 539.1386592388153 seconds\n",
      "Step 9700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 544.810733795166 seconds\n",
      "Step 9800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 550.3305387496948 seconds\n",
      "Step 9900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 556.0183525085449 seconds\n",
      "Step 10000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 561.539644241333 seconds\n",
      "Step 10100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 567.1914527416229 seconds\n",
      "Step 10200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 572.7143774032593 seconds\n",
      "Step 10300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 578.275105714798 seconds\n",
      "Step 10400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 583.989669084549 seconds\n",
      "Step 10500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 589.5851624011993 seconds\n",
      "Step 10600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 595.2860250473022 seconds\n",
      "Step 10700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 600.824627161026 seconds\n",
      "Step 10800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 606.3589744567871 seconds\n",
      "Step 10900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 612.0505800247192 seconds\n",
      "Step 11000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 617.6011657714844 seconds\n",
      "Step 11100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 623.7899308204651 seconds\n",
      "Step 11200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 629.3196806907654 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 634.9811079502106 seconds\n",
      "Step 11400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 640.5906221866608 seconds\n",
      "Step 11500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 646.1348209381104 seconds\n",
      "Step 11600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 651.8354620933533 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 657.3812236785889 seconds\n",
      "Step 11800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 663.0984754562378 seconds\n",
      "Step 11900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 668.714750289917 seconds\n",
      "Step 12000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 674.2591607570648 seconds\n",
      "Step 12100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 680.37104845047 seconds\n",
      "Step 12200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 685.8985962867737 seconds\n",
      "Step 12300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 691.5675346851349 seconds\n",
      "Step 12400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 697.1311497688293 seconds\n",
      "Step 12500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 702.8532121181488 seconds\n",
      "Step 12600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 708.4333341121674 seconds\n",
      "Step 12700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 713.9993612766266 seconds\n",
      "Step 12800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 719.7555305957794 seconds\n",
      "Step 12900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 725.34046626091 seconds\n",
      "Step 13000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 731.0693123340607 seconds\n",
      "Step 13100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 737.2306094169617 seconds\n",
      "Step 13200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 742.7955956459045 seconds\n",
      "Step 13300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 748.4694221019745 seconds\n",
      "Step 13400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 754.0130026340485 seconds\n",
      "Step 13500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 759.6879122257233 seconds\n",
      "Step 13600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 765.2382304668427 seconds\n",
      "Step 13700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 770.9484333992004 seconds\n",
      "Step 13800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 776.5252594947815 seconds\n",
      "Step 13900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 782.0712270736694 seconds\n",
      "Step 14000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 787.7569055557251 seconds\n",
      "Step 14100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 793.280636548996 seconds\n",
      "Step 14200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 798.9899642467499 seconds\n",
      "Step 14300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 804.5455746650696 seconds\n",
      "Step 14400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 810.1033136844635 seconds\n",
      "Step 14500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 815.7945032119751 seconds\n",
      "Step 14600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 821.3442947864532 seconds\n",
      "Step 14700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 827.0399839878082 seconds\n",
      "Step 14800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 832.6054513454437 seconds\n",
      "Step 14900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 838.3030550479889 seconds\n",
      "Step 15000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 844.1448571681976 seconds\n",
      "Step 15100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 849.7440888881683 seconds\n",
      "Step 15200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 855.4814622402191 seconds\n",
      "Step 15300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 861.0702221393585 seconds\n",
      "Step 15400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 867.111118555069 seconds\n",
      "Step 15500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 872.6824307441711 seconds\n",
      "Step 15600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 878.2149593830109 seconds\n",
      "Step 15700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 883.9048316478729 seconds\n",
      "Step 15800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 890.0393249988556 seconds\n",
      "Step 15900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 895.7453844547272 seconds\n",
      "Step 16000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 901.2815432548523 seconds\n",
      "Step 16100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 906.959956407547 seconds\n",
      "Step 16200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 912.6565179824829 seconds\n",
      "Step 16300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 918.59921002388 seconds\n",
      "Step 16400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 924.3131613731384 seconds\n",
      "Step 16500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 929.8558692932129 seconds\n",
      "Step 16600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 935.5197038650513 seconds\n",
      "Step 16700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 941.0682702064514 seconds\n",
      "Step 16800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 946.6302216053009 seconds\n",
      "Step 16900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 952.3437156677246 seconds\n",
      "Step 17000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 958.3360795974731 seconds\n",
      "Step 17100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 964.0122828483582 seconds\n",
      "Step 17200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 969.6032409667969 seconds\n",
      "Step 17300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 975.2918267250061 seconds\n",
      "Step 17400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 980.8757512569427 seconds\n",
      "Step 17500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 986.6980590820312 seconds\n",
      "Step 17600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 992.4060673713684 seconds\n",
      "Step 17700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 998.0267913341522 seconds\n",
      "Step 17800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1003.7246916294098 seconds\n",
      "Step 17900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1009.3897104263306 seconds\n",
      "Step 18000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1014.928740978241 seconds\n",
      "Step 18100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1021.4289302825928 seconds\n",
      "Step 18200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1026.961228132248 seconds\n",
      "Step 18300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1032.6658635139465 seconds\n",
      "Step 18400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1038.1978125572205 seconds\n",
      "Step 18500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1043.7438399791718 seconds\n",
      "Step 18600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1049.419662952423 seconds\n",
      "Step 18700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1054.962968826294 seconds\n",
      "Step 18800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1060.6817548274994 seconds\n",
      "Step 18900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1066.2147316932678 seconds\n",
      "Step 19000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1071.9323184490204 seconds\n",
      "Step 19100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1077.5120632648468 seconds\n",
      "Step 19200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1083.0840439796448 seconds\n",
      "Step 19300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1088.809496164322 seconds\n",
      "Step 19400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1095.9188432693481 seconds\n",
      "Step 19500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1101.6735444068909 seconds\n",
      "Step 19600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1107.2107396125793 seconds\n",
      "Step 19700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1112.755295753479 seconds\n",
      "Step 19800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1118.449541568756 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1124.0046863555908 seconds\n",
      "Step 20000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1129.7078301906586 seconds\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 0.06112265586853027 seconds\n",
      "Step 100, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 5.6908814907073975 seconds\n",
      "Step 200, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 11.253300905227661 seconds\n",
      "Step 300, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 16.877676963806152 seconds\n",
      "Step 400, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 22.41338610649109 seconds\n",
      "Step 500, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 28.03304672241211 seconds\n",
      "Step 600, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 33.51479983329773 seconds\n",
      "Step 700, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 39.17566895484924 seconds\n",
      "Step 800, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 44.6731059551239 seconds\n",
      "Step 900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 50.18402409553528 seconds\n",
      "Step 1000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 55.817137241363525 seconds\n",
      "Step 1100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 61.31541442871094 seconds\n",
      "Step 1200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 66.97697710990906 seconds\n",
      "Step 1300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 72.50171113014221 seconds\n",
      "Step 1400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 78.15946841239929 seconds\n",
      "Step 1500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 83.6525981426239 seconds\n",
      "Step 1600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 89.15808701515198 seconds\n",
      "Step 1700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 94.79078030586243 seconds\n",
      "Step 1800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 100.31637978553772 seconds\n",
      "Step 1900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 105.96645379066467 seconds\n",
      "Step 2000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 111.49436521530151 seconds\n",
      "Step 2100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 117.1558837890625 seconds\n",
      "Step 2200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 122.63043403625488 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 128.1326949596405 seconds\n",
      "Step 2400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 133.76214003562927 seconds\n",
      "Step 2500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 139.2493588924408 seconds\n",
      "Step 2600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 144.87441444396973 seconds\n",
      "Step 2700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 150.392094373703 seconds\n",
      "Step 2800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 155.90868210792542 seconds\n",
      "Step 2900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 161.58039355278015 seconds\n",
      "Step 3000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 167.081556558609 seconds\n",
      "Step 3100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 172.73565459251404 seconds\n",
      "Step 3200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 178.22690439224243 seconds\n",
      "Step 3300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 183.86473536491394 seconds\n",
      "Step 3400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 189.35118961334229 seconds\n",
      "Step 3500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 194.90471696853638 seconds\n",
      "Step 3600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 200.542720079422 seconds\n",
      "Step 3700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 206.06767988204956 seconds\n",
      "Step 3800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 211.70816946029663 seconds\n",
      "Step 3900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 217.23409056663513 seconds\n",
      "Step 4000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 222.9183418750763 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 228.41643166542053 seconds\n",
      "Step 4200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 233.9362223148346 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 239.57948565483093 seconds\n",
      "Step 4400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 245.09160661697388 seconds\n",
      "Step 4500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 250.73205494880676 seconds\n",
      "Step 4600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 256.28328251838684 seconds\n",
      "Step 4700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 261.94808173179626 seconds\n",
      "Step 4800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 267.4451491832733 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 272.942346572876 seconds\n",
      "Step 5000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 278.7833602428436 seconds\n",
      "Step 5100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 284.31241631507874 seconds\n",
      "Step 5200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 289.96419072151184 seconds\n",
      "Step 5300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 295.4799373149872 seconds\n",
      "Step 5400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 301.03256821632385 seconds\n",
      "Step 5500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 306.71249198913574 seconds\n",
      "Step 5600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 313.09924149513245 seconds\n",
      "Step 5700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 318.74930572509766 seconds\n",
      "Step 5800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 324.25660014152527 seconds\n",
      "Step 5900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 329.9054524898529 seconds\n",
      "Step 6000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 335.40904927253723 seconds\n",
      "Step 6100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 340.9369385242462 seconds\n",
      "Step 6200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 346.62188267707825 seconds\n",
      "Step 6300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 352.1423535346985 seconds\n",
      "Step 6400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 357.7920687198639 seconds\n",
      "Step 6500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 363.3174641132355 seconds\n",
      "Step 6600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 368.9721426963806 seconds\n",
      "Step 6700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 374.4760067462921 seconds\n",
      "Step 6800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 379.99012088775635 seconds\n",
      "Step 6900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 385.643741607666 seconds\n",
      "Step 7000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 391.1918406486511 seconds\n",
      "Step 7100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 396.8559534549713 seconds\n",
      "Step 7200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 402.3805627822876 seconds\n",
      "Step 7300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 407.9272060394287 seconds\n",
      "Step 7400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 413.5831370353699 seconds\n",
      "Step 7500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 419.0840301513672 seconds\n",
      "Step 7600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 424.73356533050537 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 430.26845383644104 seconds\n",
      "Step 7800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 435.96921730041504 seconds\n",
      "Step 7900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 442.13990020751953 seconds\n",
      "Step 8000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 447.80983304977417 seconds\n",
      "Step 8100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 453.5088768005371 seconds\n",
      "Step 8200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 459.06612610816956 seconds\n",
      "Step 8300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 464.7229058742523 seconds\n",
      "Step 8400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 470.28172612190247 seconds\n",
      "Step 8500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 475.9722673892975 seconds\n",
      "Step 8600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 481.5122084617615 seconds\n",
      "Step 8700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 487.07589197158813 seconds\n",
      "Step 8800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 492.75380539894104 seconds\n",
      "Step 8900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 498.312997341156 seconds\n",
      "Step 9000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 504.0104477405548 seconds\n",
      "Step 9100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 509.52489590644836 seconds\n",
      "Step 9200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 515.1092052459717 seconds\n",
      "Step 9300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 520.8030064105988 seconds\n",
      "Step 9400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 526.3554470539093 seconds\n",
      "Step 9500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 532.043062210083 seconds\n",
      "Step 9600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 537.5941445827484 seconds\n",
      "Step 9700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 543.3008749485016 seconds\n",
      "Step 9800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 548.8534915447235 seconds\n",
      "Step 9900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 554.3870849609375 seconds\n",
      "Step 10000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 560.0944769382477 seconds\n",
      "Step 10100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 565.6457297801971 seconds\n",
      "Step 10200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 571.3359162807465 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 576.8693356513977 seconds\n",
      "Step 10400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 582.4192280769348 seconds\n",
      "Step 10500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 588.1041910648346 seconds\n",
      "Step 10600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 593.6185824871063 seconds\n",
      "Step 10700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 599.3015546798706 seconds\n",
      "Step 10800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 604.8150551319122 seconds\n",
      "Step 10900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 610.4908089637756 seconds\n",
      "Step 11000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 616.0177667140961 seconds\n",
      "Step 11100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 621.5889620780945 seconds\n",
      "Step 11200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 627.2767026424408 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 632.8309011459351 seconds\n",
      "Step 11400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 638.529034614563 seconds\n",
      "Step 11500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 644.5270462036133 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 650.2873976230621 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 655.9683110713959 seconds\n",
      "Step 11800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 661.5073118209839 seconds\n",
      "Step 11900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 667.2088575363159 seconds\n",
      "Step 12000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 672.777704000473 seconds\n",
      "Step 12100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 678.4796140193939 seconds\n",
      "Step 12200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 684.0217189788818 seconds\n",
      "Step 12300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 689.5660378932953 seconds\n",
      "Step 12400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 695.2492597103119 seconds\n",
      "Step 12500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 700.7891080379486 seconds\n",
      "Step 12600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 706.4721493721008 seconds\n",
      "Step 12700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 712.0518372058868 seconds\n",
      "Step 12800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 717.6230647563934 seconds\n",
      "Step 12900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 724.5221168994904 seconds\n",
      "Step 13000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 730.2406938076019 seconds\n",
      "Step 13100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 735.9319496154785 seconds\n",
      "Step 13200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 741.4710423946381 seconds\n",
      "Step 13300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 747.1395437717438 seconds\n",
      "Step 13400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 752.6574084758759 seconds\n",
      "Step 13500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 758.1944925785065 seconds\n",
      "Step 13600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 763.8698241710663 seconds\n",
      "Step 13700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 769.3798594474792 seconds\n",
      "Step 13800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 775.0618126392365 seconds\n",
      "Step 13900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 780.5935757160187 seconds\n",
      "Step 14000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 786.1173238754272 seconds\n",
      "Step 14100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 791.8119449615479 seconds\n",
      "Step 14200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 797.345109462738 seconds\n",
      "Step 14300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 803.06614112854 seconds\n",
      "Step 14400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 808.6055293083191 seconds\n",
      "Step 14500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 814.2987298965454 seconds\n",
      "Step 14600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 819.8265981674194 seconds\n",
      "Step 14700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 825.3613367080688 seconds\n",
      "Step 14800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 831.0511152744293 seconds\n",
      "Step 14900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 836.5973334312439 seconds\n",
      "Step 15000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 842.3046855926514 seconds\n",
      "Step 15100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 847.8324384689331 seconds\n",
      "Step 15200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 853.3770852088928 seconds\n",
      "Step 15300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 859.0734372138977 seconds\n",
      "Step 15400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 864.630752325058 seconds\n",
      "Step 15500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 870.3100626468658 seconds\n",
      "Step 15600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 875.8544249534607 seconds\n",
      "Step 15700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 881.5466902256012 seconds\n",
      "Step 15800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 887.1100120544434 seconds\n",
      "Step 15900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 892.7027177810669 seconds\n",
      "Step 16000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 898.4006206989288 seconds\n",
      "Step 16100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 903.9397990703583 seconds\n",
      "Step 16200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 909.6600046157837 seconds\n",
      "Step 16300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 915.2060735225677 seconds\n",
      "Step 16400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 920.7318696975708 seconds\n",
      "Step 16500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 926.4629645347595 seconds\n",
      "Step 16600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 932.350263595581 seconds\n",
      "Step 16700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 938.0861051082611 seconds\n",
      "Step 16800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 943.6199135780334 seconds\n",
      "Step 16900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 949.1571185588837 seconds\n",
      "Step 17000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 954.8675410747528 seconds\n",
      "Step 17100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 960.387957572937 seconds\n",
      "Step 17200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 966.0365252494812 seconds\n",
      "Step 17300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 971.5534858703613 seconds\n",
      "Step 17400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 977.6738111972809 seconds\n",
      "Step 17500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 983.2184689044952 seconds\n",
      "Step 17600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 988.7656257152557 seconds\n",
      "Step 17700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 994.4567379951477 seconds\n",
      "Step 17800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 999.9815113544464 seconds\n",
      "Step 17900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1005.6468670368195 seconds\n",
      "Step 18000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1011.1681079864502 seconds\n",
      "Step 18100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1016.7290353775024 seconds\n",
      "Step 18200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1022.4385027885437 seconds\n",
      "Step 18300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1027.9890275001526 seconds\n",
      "Step 18400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1033.7388770580292 seconds\n",
      "Step 18500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1039.3075449466705 seconds\n",
      "Step 18600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1045.0683472156525 seconds\n",
      "Step 18700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1051.0265078544617 seconds\n",
      "Step 18800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1056.5481066703796 seconds\n",
      "Step 18900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1062.230807542801 seconds\n",
      "Step 19000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1067.7657644748688 seconds\n",
      "Step 19100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1073.4520344734192 seconds\n",
      "Step 19200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1079.0016479492188 seconds\n",
      "Step 19300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1084.5529115200043 seconds\n",
      "Step 19400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1090.2863986492157 seconds\n",
      "Step 19500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1095.8261258602142 seconds\n",
      "Step 19600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1101.5595638751984 seconds\n",
      "Step 19700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1107.5469899177551 seconds\n",
      "Step 19800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1113.3721668720245 seconds\n",
      "Step 19900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1119.0896763801575 seconds\n",
      "Step 20000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1124.6610758304596 seconds\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0268, grad_fn=<SubBackward0>), time elapsed: 0.06131720542907715 seconds\n",
      "Step 100, loss: tensor(0.0200, grad_fn=<SubBackward0>), time elapsed: 5.839563608169556 seconds\n",
      "Step 200, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 11.38443398475647 seconds\n",
      "Step 300, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 17.046024322509766 seconds\n",
      "Step 400, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 22.55169177055359 seconds\n",
      "Step 500, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 28.0757577419281 seconds\n",
      "Step 600, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 33.7706573009491 seconds\n",
      "Step 700, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 39.29278326034546 seconds\n",
      "Step 800, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 44.92242741584778 seconds\n",
      "Step 900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 50.447142124176025 seconds\n",
      "Step 1000, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 56.091723680496216 seconds\n",
      "Step 1100, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 61.616432905197144 seconds\n",
      "Step 1200, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 67.13268160820007 seconds\n",
      "Step 1300, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 72.78461956977844 seconds\n",
      "Step 1400, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 78.31994485855103 seconds\n",
      "Step 1500, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 83.9662516117096 seconds\n",
      "Step 1600, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 89.45564222335815 seconds\n",
      "Step 1700, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 94.95372128486633 seconds\n",
      "Step 1800, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 100.57231950759888 seconds\n",
      "Step 1900, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 106.08648037910461 seconds\n",
      "Step 2000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 111.7385573387146 seconds\n",
      "Step 2100, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 117.25725102424622 seconds\n",
      "Step 2200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 122.90696167945862 seconds\n",
      "Step 2300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 128.42523550987244 seconds\n",
      "Step 2400, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 133.92213582992554 seconds\n",
      "Step 2500, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 139.5522484779358 seconds\n",
      "Step 2600, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 145.06816172599792 seconds\n",
      "Step 2700, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 150.7189700603485 seconds\n",
      "Step 2800, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 156.23301005363464 seconds\n",
      "Step 2900, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 161.88010048866272 seconds\n",
      "Step 3000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 167.40494179725647 seconds\n",
      "Step 3100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 172.93349647521973 seconds\n",
      "Step 3200, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 178.55584120750427 seconds\n",
      "Step 3300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 184.07965397834778 seconds\n",
      "Step 3400, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 189.7369782924652 seconds\n",
      "Step 3500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 195.2349066734314 seconds\n",
      "Step 3600, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 200.91847276687622 seconds\n",
      "Step 3700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 206.42460346221924 seconds\n",
      "Step 3800, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 211.95678758621216 seconds\n",
      "Step 3900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 217.5994291305542 seconds\n",
      "Step 4000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 223.1082468032837 seconds\n",
      "Step 4100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 228.78735089302063 seconds\n",
      "Step 4200, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 234.29518055915833 seconds\n",
      "Step 4300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 239.77857637405396 seconds\n",
      "Step 4400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 245.40501761436462 seconds\n",
      "Step 4500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 250.90009760856628 seconds\n",
      "Step 4600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 256.5462644100189 seconds\n",
      "Step 4700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 262.0764195919037 seconds\n",
      "Step 4800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 267.86612939834595 seconds\n",
      "Step 4900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 273.42171907424927 seconds\n",
      "Step 5000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 279.6571328639984 seconds\n",
      "Step 5100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 285.3035960197449 seconds\n",
      "Step 5200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 290.8480770587921 seconds\n",
      "Step 5300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 296.5155580043793 seconds\n",
      "Step 5400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 302.0284686088562 seconds\n",
      "Step 5500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 307.6857075691223 seconds\n",
      "Step 5600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 313.2034375667572 seconds\n",
      "Step 5700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 318.7359399795532 seconds\n",
      "Step 5800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 325.1374452114105 seconds\n",
      "Step 5900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 330.65584897994995 seconds\n",
      "Step 6000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 336.310396194458 seconds\n",
      "Step 6100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 341.8141996860504 seconds\n",
      "Step 6200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 347.33398246765137 seconds\n",
      "Step 6300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 353.04497957229614 seconds\n",
      "Step 6400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 358.5775668621063 seconds\n",
      "Step 6500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 364.2717969417572 seconds\n",
      "Step 6600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 369.8028004169464 seconds\n",
      "Step 6700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 375.46446776390076 seconds\n",
      "Step 6800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 381.0394403934479 seconds\n",
      "Step 6900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 386.57348370552063 seconds\n",
      "Step 7000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 392.2561309337616 seconds\n",
      "Step 7100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 397.8068974018097 seconds\n",
      "Step 7200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 403.48610067367554 seconds\n",
      "Step 7300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 409.18378925323486 seconds\n",
      "Step 7400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 414.88511538505554 seconds\n",
      "Step 7500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 421.13614797592163 seconds\n",
      "Step 7600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 426.6577410697937 seconds\n",
      "Step 7700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 432.33440923690796 seconds\n",
      "Step 7800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 437.86584639549255 seconds\n",
      "Step 7900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 443.57696747779846 seconds\n",
      "Step 8000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 449.1400074958801 seconds\n",
      "Step 8100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 454.8627619743347 seconds\n",
      "Step 8200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 460.3916103839874 seconds\n",
      "Step 8300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 465.9793174266815 seconds\n",
      "Step 8400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 471.6895558834076 seconds\n",
      "Step 8500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 477.9176287651062 seconds\n",
      "Step 8600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 483.5991325378418 seconds\n",
      "Step 8700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 489.1567442417145 seconds\n",
      "Step 8800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 494.6742241382599 seconds\n",
      "Step 8900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 500.3187026977539 seconds\n",
      "Step 9000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 505.9400351047516 seconds\n",
      "Step 9100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 511.67027020454407 seconds\n",
      "Step 9200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 517.1941056251526 seconds\n",
      "Step 9300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 522.9372363090515 seconds\n",
      "Step 9400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 528.4955642223358 seconds\n",
      "Step 9500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 535.0454368591309 seconds\n",
      "Step 9600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 540.7260000705719 seconds\n",
      "Step 9700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 546.2485568523407 seconds\n",
      "Step 9800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 551.8997542858124 seconds\n",
      "Step 9900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 557.413946390152 seconds\n",
      "Step 10000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 562.9285840988159 seconds\n",
      "Step 10100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 568.6069912910461 seconds\n",
      "Step 10200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 574.1355328559875 seconds\n",
      "Step 10300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 579.8190762996674 seconds\n",
      "Step 10400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 585.3363392353058 seconds\n",
      "Step 10500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 591.0576856136322 seconds\n",
      "Step 10600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 596.5686287879944 seconds\n",
      "Step 10700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 602.0881972312927 seconds\n",
      "Step 10800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 608.197968006134 seconds\n",
      "Step 10900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 613.7399406433105 seconds\n",
      "Step 11000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 619.4419963359833 seconds\n",
      "Step 11100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 624.9993886947632 seconds\n",
      "Step 11200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 630.5330069065094 seconds\n",
      "Step 11300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 636.1982622146606 seconds\n",
      "Step 11400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 641.7926287651062 seconds\n",
      "Step 11500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 647.4810819625854 seconds\n",
      "Step 11600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 653.00790143013 seconds\n",
      "Step 11700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 658.753500699997 seconds\n",
      "Step 11800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 664.3609337806702 seconds\n",
      "Step 11900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 670.6820402145386 seconds\n",
      "Step 12000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 676.4153277873993 seconds\n",
      "Step 12100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 681.9458644390106 seconds\n",
      "Step 12200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 687.6461405754089 seconds\n",
      "Step 12300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 693.1773157119751 seconds\n",
      "Step 12400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 698.7187836170197 seconds\n",
      "Step 12500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 704.4116010665894 seconds\n",
      "Step 12600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 709.9348213672638 seconds\n",
      "Step 12700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 715.6187319755554 seconds\n",
      "Step 12800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 721.2064707279205 seconds\n",
      "Step 12900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 726.8809566497803 seconds\n",
      "Step 13000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 732.410964012146 seconds\n",
      "Step 13100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 737.9129366874695 seconds\n",
      "Step 13200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 743.6152212619781 seconds\n",
      "Step 13300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 749.1899085044861 seconds\n",
      "Step 13400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 754.9095878601074 seconds\n",
      "Step 13500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 760.4685769081116 seconds\n",
      "Step 13600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 766.0448834896088 seconds\n",
      "Step 13700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 771.7226023674011 seconds\n",
      "Step 13800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 777.2999401092529 seconds\n",
      "Step 13900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 783.0130889415741 seconds\n",
      "Step 14000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 788.5712673664093 seconds\n",
      "Step 14100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 794.7644157409668 seconds\n",
      "Step 14200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 800.462112903595 seconds\n",
      "Step 14300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 806.0109021663666 seconds\n",
      "Step 14400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 811.7483701705933 seconds\n",
      "Step 14500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 817.27734541893 seconds\n",
      "Step 14600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 822.9725635051727 seconds\n",
      "Step 14700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 828.5277671813965 seconds\n",
      "Step 14800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 834.1033406257629 seconds\n",
      "Step 14900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 839.8630497455597 seconds\n",
      "Step 15000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 845.4546074867249 seconds\n",
      "Step 15100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 851.2190372943878 seconds\n",
      "Step 15200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 856.861421585083 seconds\n",
      "Step 15300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 862.5458006858826 seconds\n",
      "Step 15400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 868.2785699367523 seconds\n",
      "Step 15500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 873.9349203109741 seconds\n",
      "Step 15600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 880.2749540805817 seconds\n",
      "Step 15700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 885.7829241752625 seconds\n",
      "Step 15800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 891.4853868484497 seconds\n",
      "Step 15900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 896.9966342449188 seconds\n",
      "Step 16000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 902.5558412075043 seconds\n",
      "Step 16100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 908.2156422138214 seconds\n",
      "Step 16200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 913.7549374103546 seconds\n",
      "Step 16300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 919.4506907463074 seconds\n",
      "Step 16400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 924.9835822582245 seconds\n",
      "Step 16500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 931.0790424346924 seconds\n",
      "Step 16600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 936.8122601509094 seconds\n",
      "Step 16700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 942.3285694122314 seconds\n",
      "Step 16800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 948.0137917995453 seconds\n",
      "Step 16900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 953.6754026412964 seconds\n",
      "Step 17000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 959.4469728469849 seconds\n",
      "Step 17100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 965.0073657035828 seconds\n",
      "Step 17200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 971.2529299259186 seconds\n",
      "Step 17300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 976.9622611999512 seconds\n",
      "Step 17400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 982.5097014904022 seconds\n",
      "Step 17500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 988.1981782913208 seconds\n",
      "Step 17600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 993.75603556633 seconds\n",
      "Step 17700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 999.3110389709473 seconds\n",
      "Step 17800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1005.0701427459717 seconds\n",
      "Step 17900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1010.6450095176697 seconds\n",
      "Step 18000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1016.3832488059998 seconds\n",
      "Step 18100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1021.9711904525757 seconds\n",
      "Step 18200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1028.5229308605194 seconds\n",
      "Step 18300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1034.0670578479767 seconds\n",
      "Step 18400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1039.5846590995789 seconds\n",
      "Step 18500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1045.258157491684 seconds\n",
      "Step 18600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1050.7683365345001 seconds\n",
      "Step 18700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1056.4794862270355 seconds\n",
      "Step 18800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1062.010468006134 seconds\n",
      "Step 18900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1067.5431339740753 seconds\n",
      "Step 19000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1073.2394423484802 seconds\n",
      "Step 19100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1078.7638280391693 seconds\n",
      "Step 19200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1084.469209909439 seconds\n",
      "Step 19300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1089.9995205402374 seconds\n",
      "Step 19400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1095.524052619934 seconds\n",
      "Step 19500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1101.2212352752686 seconds\n",
      "Step 19600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1106.7711369991302 seconds\n",
      "Step 19700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1112.5334780216217 seconds\n",
      "Step 19800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1118.0958774089813 seconds\n",
      "Step 19900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1123.8363943099976 seconds\n",
      "Step 20000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1129.4083988666534 seconds\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0203, grad_fn=<SubBackward0>), time elapsed: 0.06705641746520996 seconds\n",
      "Step 100, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 5.6780173778533936 seconds\n",
      "Step 200, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 11.353258848190308 seconds\n",
      "Step 300, loss: tensor(0.0090, grad_fn=<SubBackward0>), time elapsed: 16.883031845092773 seconds\n",
      "Step 400, loss: tensor(0.0077, grad_fn=<SubBackward0>), time elapsed: 22.55505633354187 seconds\n",
      "Step 500, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 28.061743021011353 seconds\n",
      "Step 600, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 33.571738481521606 seconds\n",
      "Step 700, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 39.20498561859131 seconds\n",
      "Step 800, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 44.7062246799469 seconds\n",
      "Step 900, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 50.34315514564514 seconds\n",
      "Step 1000, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 55.83336138725281 seconds\n",
      "Step 1100, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 61.48166298866272 seconds\n",
      "Step 1200, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 66.98461055755615 seconds\n",
      "Step 1300, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 72.49153590202332 seconds\n",
      "Step 1400, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 78.11459541320801 seconds\n",
      "Step 1500, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 83.6406421661377 seconds\n",
      "Step 1600, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 89.26758170127869 seconds\n",
      "Step 1700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 94.77395558357239 seconds\n",
      "Step 1800, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 100.4152364730835 seconds\n",
      "Step 1900, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 105.91846442222595 seconds\n",
      "Step 2000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 111.46080112457275 seconds\n",
      "Step 2100, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 117.10937023162842 seconds\n",
      "Step 2200, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 122.6124975681305 seconds\n",
      "Step 2300, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 128.24838399887085 seconds\n",
      "Step 2400, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 133.73201513290405 seconds\n",
      "Step 2500, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 139.3998851776123 seconds\n",
      "Step 2600, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 144.90452218055725 seconds\n",
      "Step 2700, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 150.4198956489563 seconds\n",
      "Step 2800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 156.06393885612488 seconds\n",
      "Step 2900, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 161.58394026756287 seconds\n",
      "Step 3000, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 167.22536993026733 seconds\n",
      "Step 3100, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 172.76625752449036 seconds\n",
      "Step 3200, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 178.25373315811157 seconds\n",
      "Step 3300, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 183.88597893714905 seconds\n",
      "Step 3400, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 189.40049266815186 seconds\n",
      "Step 3500, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 195.047607421875 seconds\n",
      "Step 3600, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 200.57535767555237 seconds\n",
      "Step 3700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 206.21688771247864 seconds\n",
      "Step 3800, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 211.70524215698242 seconds\n",
      "Step 3900, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 217.22563338279724 seconds\n",
      "Step 4000, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 222.84887552261353 seconds\n",
      "Step 4100, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 228.35923981666565 seconds\n",
      "Step 4200, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 234.01105642318726 seconds\n",
      "Step 4300, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 239.51981902122498 seconds\n",
      "Step 4400, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 245.1728298664093 seconds\n",
      "Step 4500, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 250.69545769691467 seconds\n",
      "Step 4600, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 256.22567868232727 seconds\n",
      "Step 4700, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 261.890841960907 seconds\n",
      "Step 4800, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 267.3979504108429 seconds\n",
      "Step 4900, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 273.0474591255188 seconds\n",
      "Step 5000, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 278.55936193466187 seconds\n",
      "Step 5100, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 284.07015323638916 seconds\n",
      "Step 5200, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 289.743070602417 seconds\n",
      "Step 5300, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 295.28937005996704 seconds\n",
      "Step 5400, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 300.950875043869 seconds\n",
      "Step 5500, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 306.463796377182 seconds\n",
      "Step 5600, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 312.0977261066437 seconds\n",
      "Step 5700, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 317.6065058708191 seconds\n",
      "Step 5800, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 323.13708114624023 seconds\n",
      "Step 5900, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 328.7541253566742 seconds\n",
      "Step 6000, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 334.26903009414673 seconds\n",
      "Step 6100, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 339.903391122818 seconds\n",
      "Step 6200, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 345.4150776863098 seconds\n",
      "Step 6300, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 351.07388186454773 seconds\n",
      "Step 6400, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 356.5990719795227 seconds\n",
      "Step 6500, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 362.09383034706116 seconds\n",
      "Step 6600, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 367.71875190734863 seconds\n",
      "Step 6700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 373.25284218788147 seconds\n",
      "Step 6800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 379.30522775650024 seconds\n",
      "Step 6900, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 384.84716725349426 seconds\n",
      "Step 7000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 390.35397505760193 seconds\n",
      "Step 7100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 396.04254055023193 seconds\n",
      "Step 7200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 401.5557379722595 seconds\n",
      "Step 7300, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 407.2283375263214 seconds\n",
      "Step 7400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 412.7572479248047 seconds\n",
      "Step 7500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 418.4595868587494 seconds\n",
      "Step 7600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 424.3928482532501 seconds\n",
      "Step 7700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 429.92211079597473 seconds\n",
      "Step 7800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 435.61729621887207 seconds\n",
      "Step 7900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 441.161062002182 seconds\n",
      "Step 8000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 446.83539724349976 seconds\n",
      "Step 8100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 452.39935779571533 seconds\n",
      "Step 8200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 458.0862741470337 seconds\n",
      "Step 8300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 463.56697130203247 seconds\n",
      "Step 8400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 469.1097493171692 seconds\n",
      "Step 8500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 474.7859275341034 seconds\n",
      "Step 8600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 480.47344160079956 seconds\n",
      "Step 8700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 486.1473078727722 seconds\n",
      "Step 8800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 491.66640400886536 seconds\n",
      "Step 8900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 497.2025189399719 seconds\n",
      "Step 9000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 502.8779878616333 seconds\n",
      "Step 9100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 508.42483139038086 seconds\n",
      "Step 9200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 514.3481080532074 seconds\n",
      "Step 9300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 519.8548171520233 seconds\n",
      "Step 9400, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 525.5374224185944 seconds\n",
      "Step 9500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 531.0579614639282 seconds\n",
      "Step 9600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 536.6481621265411 seconds\n",
      "Step 9700, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 542.3035864830017 seconds\n",
      "Step 9800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 548.3883180618286 seconds\n",
      "Step 9900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 554.1057894229889 seconds\n",
      "Step 10000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 559.7074346542358 seconds\n",
      "Step 10100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 565.3239862918854 seconds\n",
      "Step 10200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 571.1528384685516 seconds\n",
      "Step 10300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 576.7831814289093 seconds\n",
      "Step 10400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 582.6955370903015 seconds\n",
      "Step 10500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 588.3415598869324 seconds\n",
      "Step 10600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 594.0365252494812 seconds\n",
      "Step 10700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 599.6039750576019 seconds\n",
      "Step 10800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 605.2446012496948 seconds\n",
      "Step 10900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 611.0337474346161 seconds\n",
      "Step 11000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 617.4653191566467 seconds\n",
      "Step 11100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 623.1607673168182 seconds\n",
      "Step 11200, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 628.7004923820496 seconds\n",
      "Step 11300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 634.2120769023895 seconds\n",
      "Step 11400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 639.8757696151733 seconds\n",
      "Step 11500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 645.423020362854 seconds\n",
      "Step 11600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 651.0747210979462 seconds\n",
      "Step 11700, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 656.6125495433807 seconds\n",
      "Step 11800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 662.3138673305511 seconds\n",
      "Step 11900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 667.8371443748474 seconds\n",
      "Step 12000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 673.3654294013977 seconds\n",
      "Step 12100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 679.0387227535248 seconds\n",
      "Step 12200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 684.5854635238647 seconds\n",
      "Step 12300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 690.3098349571228 seconds\n",
      "Step 12400, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 695.8421823978424 seconds\n",
      "Step 12500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 701.3863387107849 seconds\n",
      "Step 12600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 707.0748591423035 seconds\n",
      "Step 12700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 712.6162066459656 seconds\n",
      "Step 12800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 719.0308451652527 seconds\n",
      "Step 12900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 724.555389881134 seconds\n",
      "Step 13000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 730.2425138950348 seconds\n",
      "Step 13100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 735.7752661705017 seconds\n",
      "Step 13200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 741.3007352352142 seconds\n",
      "Step 13300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 746.9991593360901 seconds\n",
      "Step 13400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 752.5670766830444 seconds\n",
      "Step 13500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 758.2496519088745 seconds\n",
      "Step 13600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 763.7871608734131 seconds\n",
      "Step 13700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 769.3311684131622 seconds\n",
      "Step 13800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 775.030237197876 seconds\n",
      "Step 13900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 780.6046378612518 seconds\n",
      "Step 14000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 786.2844824790955 seconds\n",
      "Step 14100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 791.813827753067 seconds\n",
      "Step 14200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 797.5106618404388 seconds\n",
      "Step 14300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 803.0260837078094 seconds\n",
      "Step 14400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 808.5793871879578 seconds\n",
      "Step 14500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 814.3032820224762 seconds\n",
      "Step 14600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 819.8707013130188 seconds\n",
      "Step 14700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 825.5951533317566 seconds\n",
      "Step 14800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 831.191930770874 seconds\n",
      "Step 14900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 836.7441592216492 seconds\n",
      "Step 15000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 842.506498336792 seconds\n",
      "Step 15100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 848.0676157474518 seconds\n",
      "Step 15200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 853.8258175849915 seconds\n",
      "Step 15300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 859.3778991699219 seconds\n",
      "Step 15400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 865.0970797538757 seconds\n",
      "Step 15500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 870.6274604797363 seconds\n",
      "Step 15600, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 876.2205767631531 seconds\n",
      "Step 15700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 881.9020705223083 seconds\n",
      "Step 15800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 887.4580714702606 seconds\n",
      "Step 15900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 893.1509921550751 seconds\n",
      "Step 16000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 898.6840958595276 seconds\n",
      "Step 16100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 904.2485272884369 seconds\n",
      "Step 16200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 909.9509944915771 seconds\n",
      "Step 16300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 916.0656545162201 seconds\n",
      "Step 16400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 921.7693846225739 seconds\n",
      "Step 16500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 927.2957646846771 seconds\n",
      "Step 16600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 932.8441982269287 seconds\n",
      "Step 16700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 938.5368502140045 seconds\n",
      "Step 16800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 944.0903992652893 seconds\n",
      "Step 16900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 949.7737166881561 seconds\n",
      "Step 17000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 955.3089420795441 seconds\n",
      "Step 17100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 961.0048727989197 seconds\n",
      "Step 17200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 966.5636699199677 seconds\n",
      "Step 17300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 972.1028883457184 seconds\n",
      "Step 17400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 977.860426902771 seconds\n",
      "Step 17500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 983.4076285362244 seconds\n",
      "Step 17600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 989.098210811615 seconds\n",
      "Step 17700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 994.7981579303741 seconds\n",
      "Step 17800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1000.3149869441986 seconds\n",
      "Step 17900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1006.0373125076294 seconds\n",
      "Step 18000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1011.5985879898071 seconds\n",
      "Step 18100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1017.3086831569672 seconds\n",
      "Step 18200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1022.8541932106018 seconds\n",
      "Step 18300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1028.6345341205597 seconds\n",
      "Step 18400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1034.1845893859863 seconds\n",
      "Step 18500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1040.3074204921722 seconds\n",
      "Step 18600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1045.995600938797 seconds\n",
      "Step 18700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1051.5416555404663 seconds\n",
      "Step 18800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1057.2655091285706 seconds\n",
      "Step 18900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1062.766709804535 seconds\n",
      "Step 19000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1068.2993099689484 seconds\n",
      "Step 19100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1073.9825901985168 seconds\n",
      "Step 19200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1079.527071237564 seconds\n",
      "Step 19300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1085.2407500743866 seconds\n",
      "Step 19400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1090.7718136310577 seconds\n",
      "Step 19500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1096.7864243984222 seconds\n",
      "Step 19600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1102.5511274337769 seconds\n",
      "Step 19700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1108.0683076381683 seconds\n",
      "Step 19800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1113.7643067836761 seconds\n",
      "Step 19900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1119.3246443271637 seconds\n",
      "Step 20000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1125.023999929428 seconds\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 0.06233978271484375 seconds\n",
      "Step 100, loss: tensor(0.0232, grad_fn=<SubBackward0>), time elapsed: 5.723978042602539 seconds\n",
      "Step 200, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 11.291936874389648 seconds\n",
      "Step 300, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 16.9344699382782 seconds\n",
      "Step 400, loss: tensor(0.0145, grad_fn=<SubBackward0>), time elapsed: 22.43358039855957 seconds\n",
      "Step 500, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 28.103668689727783 seconds\n",
      "Step 600, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 33.59884023666382 seconds\n",
      "Step 700, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 39.10199451446533 seconds\n",
      "Step 800, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 44.75360727310181 seconds\n",
      "Step 900, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 50.25390625 seconds\n",
      "Step 1000, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 55.90637683868408 seconds\n",
      "Step 1100, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 61.40460538864136 seconds\n",
      "Step 1200, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 67.05175185203552 seconds\n",
      "Step 1300, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 72.55066657066345 seconds\n",
      "Step 1400, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 78.06082701683044 seconds\n",
      "Step 1500, loss: tensor(0.0110, grad_fn=<SubBackward0>), time elapsed: 83.68422245979309 seconds\n",
      "Step 1600, loss: tensor(0.0105, grad_fn=<SubBackward0>), time elapsed: 89.22890996932983 seconds\n",
      "Step 1700, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 94.86425876617432 seconds\n",
      "Step 1800, loss: tensor(0.0108, grad_fn=<SubBackward0>), time elapsed: 100.3726851940155 seconds\n",
      "Step 1900, loss: tensor(0.0104, grad_fn=<SubBackward0>), time elapsed: 106.01401472091675 seconds\n",
      "Step 2000, loss: tensor(0.0101, grad_fn=<SubBackward0>), time elapsed: 111.52348184585571 seconds\n",
      "Step 2100, loss: tensor(0.0101, grad_fn=<SubBackward0>), time elapsed: 117.05768299102783 seconds\n",
      "Step 2200, loss: tensor(0.0102, grad_fn=<SubBackward0>), time elapsed: 122.70073056221008 seconds\n",
      "Step 2300, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 128.21632599830627 seconds\n",
      "Step 2400, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 133.8643536567688 seconds\n",
      "Step 2500, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 139.36913681030273 seconds\n",
      "Step 2600, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 145.02396202087402 seconds\n",
      "Step 2700, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 150.5463466644287 seconds\n",
      "Step 2800, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 156.06081438064575 seconds\n",
      "Step 2900, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 161.70347929000854 seconds\n",
      "Step 3000, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 167.19700574874878 seconds\n",
      "Step 3100, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 172.84527134895325 seconds\n",
      "Step 3200, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 178.37531423568726 seconds\n",
      "Step 3300, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 183.89143061637878 seconds\n",
      "Step 3400, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 189.52969527244568 seconds\n",
      "Step 3500, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 195.0445737838745 seconds\n",
      "Step 3600, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 200.69220209121704 seconds\n",
      "Step 3700, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 206.20357704162598 seconds\n",
      "Step 3800, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 211.86895108222961 seconds\n",
      "Step 3900, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 217.37724208831787 seconds\n",
      "Step 4000, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 222.8924605846405 seconds\n",
      "Step 4100, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 228.54268336296082 seconds\n",
      "Step 4200, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 234.05487871170044 seconds\n",
      "Step 4300, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 239.73361682891846 seconds\n",
      "Step 4400, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 245.25606298446655 seconds\n",
      "Step 4500, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 250.9119234085083 seconds\n",
      "Step 4600, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 256.4081394672394 seconds\n",
      "Step 4700, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 262.0629394054413 seconds\n",
      "Step 4800, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 267.72090196609497 seconds\n",
      "Step 4900, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 273.7256109714508 seconds\n",
      "Step 5000, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 279.3996465206146 seconds\n",
      "Step 5100, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 285.54602694511414 seconds\n",
      "Step 5200, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 291.06154465675354 seconds\n",
      "Step 5300, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 296.70413970947266 seconds\n",
      "Step 5400, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 302.2535455226898 seconds\n",
      "Step 5500, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 307.92690348625183 seconds\n",
      "Step 5600, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 313.4642143249512 seconds\n",
      "Step 5700, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 319.197212934494 seconds\n",
      "Step 5800, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 324.8245303630829 seconds\n",
      "Step 5900, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 330.38206243515015 seconds\n",
      "Step 6000, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 336.04999828338623 seconds\n",
      "Step 6100, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 341.58787631988525 seconds\n",
      "Step 6200, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 347.25838255882263 seconds\n",
      "Step 6300, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 352.7960422039032 seconds\n",
      "Step 6400, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 358.6399314403534 seconds\n",
      "Step 6500, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 364.25202083587646 seconds\n",
      "Step 6600, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 369.7840898036957 seconds\n",
      "Step 6700, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 375.89074420928955 seconds\n",
      "Step 6800, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 381.51100182533264 seconds\n",
      "Step 6900, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 387.27424573898315 seconds\n",
      "Step 7000, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 392.81840777397156 seconds\n",
      "Step 7100, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 398.325875043869 seconds\n",
      "Step 7200, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 403.99767661094666 seconds\n",
      "Step 7300, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 409.5253632068634 seconds\n",
      "Step 7400, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 415.4555072784424 seconds\n",
      "Step 7500, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 421.66302371025085 seconds\n",
      "Step 7600, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 427.3311712741852 seconds\n",
      "Step 7700, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 432.8495693206787 seconds\n",
      "Step 7800, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 438.3645496368408 seconds\n",
      "Step 7900, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 444.00831604003906 seconds\n",
      "Step 8000, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 449.578262090683 seconds\n",
      "Step 8100, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 455.27104330062866 seconds\n",
      "Step 8200, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 460.7971031665802 seconds\n",
      "Step 8300, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 466.7464988231659 seconds\n",
      "Step 8400, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 472.25097918510437 seconds\n",
      "Step 8500, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 477.88806891441345 seconds\n",
      "Step 8600, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 483.57876920700073 seconds\n",
      "Step 8700, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 489.1113452911377 seconds\n",
      "Step 8800, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 494.8112382888794 seconds\n",
      "Step 8900, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 500.3266201019287 seconds\n",
      "Step 9000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 506.597225189209 seconds\n",
      "Step 9100, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 512.2588629722595 seconds\n",
      "Step 9200, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 517.7998020648956 seconds\n",
      "Step 9300, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 523.4406147003174 seconds\n",
      "Step 9400, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 528.9471147060394 seconds\n",
      "Step 9500, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 534.7023177146912 seconds\n",
      "Step 9600, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 540.3270432949066 seconds\n",
      "Step 9700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 545.8698856830597 seconds\n",
      "Step 9800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 551.81081199646 seconds\n",
      "Step 9900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 557.3365721702576 seconds\n",
      "Step 10000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 563.3430712223053 seconds\n",
      "Step 10100, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 569.7208981513977 seconds\n",
      "Step 10200, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 575.2739131450653 seconds\n",
      "Step 10300, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 580.9762489795685 seconds\n",
      "Step 10400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 586.5085785388947 seconds\n",
      "Step 10500, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 592.2192385196686 seconds\n",
      "Step 10600, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 597.767112493515 seconds\n",
      "Step 10700, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 604.2446157932281 seconds\n",
      "Step 10800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 609.7682318687439 seconds\n",
      "Step 10900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 615.2788240909576 seconds\n",
      "Step 11000, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 620.9516227245331 seconds\n",
      "Step 11100, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 626.4922964572906 seconds\n",
      "Step 11200, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 632.1776237487793 seconds\n",
      "Step 11300, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 637.742990732193 seconds\n",
      "Step 11400, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 643.2694666385651 seconds\n",
      "Step 11500, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 649.0515460968018 seconds\n",
      "Step 11600, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 654.5677449703217 seconds\n",
      "Step 11700, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 660.2502286434174 seconds\n",
      "Step 11800, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 666.0377202033997 seconds\n",
      "Step 11900, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 671.7269186973572 seconds\n",
      "Step 12000, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 677.2341940402985 seconds\n",
      "Step 12100, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 682.7891128063202 seconds\n",
      "Step 12200, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 688.6123144626617 seconds\n",
      "Step 12300, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 694.1348204612732 seconds\n",
      "Step 12400, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 699.8256435394287 seconds\n",
      "Step 12500, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 705.3614857196808 seconds\n",
      "Step 12600, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 711.7527163028717 seconds\n",
      "Step 12700, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 717.4527184963226 seconds\n",
      "Step 12800, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 722.9872674942017 seconds\n",
      "Step 12900, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 728.6871538162231 seconds\n",
      "Step 13000, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 734.2187469005585 seconds\n",
      "Step 13100, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 739.9043757915497 seconds\n",
      "Step 13200, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 745.4230892658234 seconds\n",
      "Step 13300, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 750.9631769657135 seconds\n",
      "Step 13400, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 756.6762316226959 seconds\n",
      "Step 13500, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 762.2563395500183 seconds\n",
      "Step 13600, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 767.9647417068481 seconds\n",
      "Step 13700, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 773.5234129428864 seconds\n",
      "Step 13800, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 779.0766017436981 seconds\n",
      "Step 13900, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 784.7675385475159 seconds\n",
      "Step 14000, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 790.3265745639801 seconds\n",
      "Step 14100, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 796.0145297050476 seconds\n",
      "Step 14200, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 801.5558896064758 seconds\n",
      "Step 14300, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 807.2471420764923 seconds\n",
      "Step 14400, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 813.3567261695862 seconds\n",
      "Step 14500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 818.9959998130798 seconds\n",
      "Step 14600, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 824.6800301074982 seconds\n",
      "Step 14700, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 830.2172901630402 seconds\n",
      "Step 14800, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 835.9200096130371 seconds\n",
      "Step 14900, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 841.5322279930115 seconds\n",
      "Step 15000, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 847.1124696731567 seconds\n",
      "Step 15100, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 852.8409266471863 seconds\n",
      "Step 15200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 858.4093344211578 seconds\n",
      "Step 15300, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 864.1285221576691 seconds\n",
      "Step 15400, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 869.869048833847 seconds\n",
      "Step 15500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 875.5591149330139 seconds\n",
      "Step 15600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 881.1263227462769 seconds\n",
      "Step 15700, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 886.6857192516327 seconds\n",
      "Step 15800, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 893.1289749145508 seconds\n",
      "Step 15900, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 898.6922428607941 seconds\n",
      "Step 16000, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 904.4506340026855 seconds\n",
      "Step 16100, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 910.1264073848724 seconds\n",
      "Step 16200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 915.7011206150055 seconds\n",
      "Step 16300, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 921.388402223587 seconds\n",
      "Step 16400, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 927.007621049881 seconds\n",
      "Step 16500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 932.8700768947601 seconds\n",
      "Step 16600, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 938.4455947875977 seconds\n",
      "Step 16700, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 944.2621092796326 seconds\n",
      "Step 16800, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 949.8038580417633 seconds\n",
      "Step 16900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 955.5364000797272 seconds\n",
      "Step 17000, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 961.2221078872681 seconds\n",
      "Step 17100, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 966.7851836681366 seconds\n",
      "Step 17200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 972.5156166553497 seconds\n",
      "Step 17300, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 978.05206823349 seconds\n",
      "Step 17400, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 984.0827808380127 seconds\n",
      "Step 17500, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 989.7854316234589 seconds\n",
      "Step 17600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 995.3320541381836 seconds\n",
      "Step 17700, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1001.035924911499 seconds\n",
      "Step 17800, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1006.5751619338989 seconds\n",
      "Step 17900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1012.3049705028534 seconds\n",
      "Step 18000, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1018.004804611206 seconds\n",
      "Step 18100, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1023.5471153259277 seconds\n",
      "Step 18200, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1029.2805697917938 seconds\n",
      "Step 18300, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1034.8929278850555 seconds\n",
      "Step 18400, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1041.231393814087 seconds\n",
      "Step 18500, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1046.7825932502747 seconds\n",
      "Step 18600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1052.319095134735 seconds\n",
      "Step 18700, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1058.0139565467834 seconds\n",
      "Step 18800, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1063.5568821430206 seconds\n",
      "Step 18900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1069.2605321407318 seconds\n",
      "Step 19000, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1074.7930104732513 seconds\n",
      "Step 19100, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1080.3536577224731 seconds\n",
      "Step 19200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1086.085501909256 seconds\n",
      "Step 19300, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1091.6239614486694 seconds\n",
      "Step 19400, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1098.9929072856903 seconds\n",
      "Step 19500, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1104.5197582244873 seconds\n",
      "Step 19600, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1110.0379328727722 seconds\n",
      "Step 19700, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1115.7503135204315 seconds\n",
      "Step 19800, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 1121.318619966507 seconds\n",
      "Step 19900, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1127.0641932487488 seconds\n",
      "Step 20000, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1132.618617773056 seconds\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 0.06322193145751953 seconds\n",
      "Step 100, loss: tensor(0.0689, grad_fn=<SubBackward0>), time elapsed: 5.83461856842041 seconds\n",
      "Step 200, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 11.412747859954834 seconds\n",
      "Step 300, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 16.91568922996521 seconds\n",
      "Step 400, loss: tensor(0.0408, grad_fn=<SubBackward0>), time elapsed: 22.545907735824585 seconds\n",
      "Step 500, loss: tensor(0.0369, grad_fn=<SubBackward0>), time elapsed: 28.069189310073853 seconds\n",
      "Step 600, loss: tensor(0.0338, grad_fn=<SubBackward0>), time elapsed: 33.705358028411865 seconds\n",
      "Step 700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 39.2135329246521 seconds\n",
      "Step 800, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 44.86344289779663 seconds\n",
      "Step 900, loss: tensor(0.0285, grad_fn=<SubBackward0>), time elapsed: 50.377275466918945 seconds\n",
      "Step 1000, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 55.92368268966675 seconds\n",
      "Step 1100, loss: tensor(0.0253, grad_fn=<SubBackward0>), time elapsed: 61.576396226882935 seconds\n",
      "Step 1200, loss: tensor(0.0252, grad_fn=<SubBackward0>), time elapsed: 67.10011005401611 seconds\n",
      "Step 1300, loss: tensor(0.0247, grad_fn=<SubBackward0>), time elapsed: 72.74263453483582 seconds\n",
      "Step 1400, loss: tensor(0.0240, grad_fn=<SubBackward0>), time elapsed: 78.24697232246399 seconds\n",
      "Step 1500, loss: tensor(0.0236, grad_fn=<SubBackward0>), time elapsed: 83.75945353507996 seconds\n",
      "Step 1600, loss: tensor(0.0237, grad_fn=<SubBackward0>), time elapsed: 89.40973711013794 seconds\n",
      "Step 1700, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 94.92094326019287 seconds\n",
      "Step 1800, loss: tensor(0.0225, grad_fn=<SubBackward0>), time elapsed: 100.57371044158936 seconds\n",
      "Step 1900, loss: tensor(0.0227, grad_fn=<SubBackward0>), time elapsed: 106.09973549842834 seconds\n",
      "Step 2000, loss: tensor(0.0228, grad_fn=<SubBackward0>), time elapsed: 111.7511990070343 seconds\n",
      "Step 2100, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 117.27934122085571 seconds\n",
      "Step 2200, loss: tensor(0.0217, grad_fn=<SubBackward0>), time elapsed: 122.79649782180786 seconds\n",
      "Step 2300, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 128.4202916622162 seconds\n",
      "Step 2400, loss: tensor(0.0217, grad_fn=<SubBackward0>), time elapsed: 133.92150163650513 seconds\n",
      "Step 2500, loss: tensor(0.0210, grad_fn=<SubBackward0>), time elapsed: 139.56496572494507 seconds\n",
      "Step 2600, loss: tensor(0.0207, grad_fn=<SubBackward0>), time elapsed: 145.07069373130798 seconds\n",
      "Step 2700, loss: tensor(0.0209, grad_fn=<SubBackward0>), time elapsed: 150.7434606552124 seconds\n",
      "Step 2800, loss: tensor(0.0206, grad_fn=<SubBackward0>), time elapsed: 156.2567753791809 seconds\n",
      "Step 2900, loss: tensor(0.0199, grad_fn=<SubBackward0>), time elapsed: 161.75443148612976 seconds\n",
      "Step 3000, loss: tensor(0.0200, grad_fn=<SubBackward0>), time elapsed: 167.39836502075195 seconds\n",
      "Step 3100, loss: tensor(0.0199, grad_fn=<SubBackward0>), time elapsed: 172.89619493484497 seconds\n",
      "Step 3200, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 178.5434534549713 seconds\n",
      "Step 3300, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 184.04618620872498 seconds\n",
      "Step 3400, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 189.705482006073 seconds\n",
      "Step 3500, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 195.21847891807556 seconds\n",
      "Step 3600, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 200.75083231925964 seconds\n",
      "Step 3700, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 206.37914180755615 seconds\n",
      "Step 3800, loss: tensor(0.0178, grad_fn=<SubBackward0>), time elapsed: 211.9063422679901 seconds\n",
      "Step 3900, loss: tensor(0.0176, grad_fn=<SubBackward0>), time elapsed: 217.54955053329468 seconds\n",
      "Step 4000, loss: tensor(0.0172, grad_fn=<SubBackward0>), time elapsed: 223.05497694015503 seconds\n",
      "Step 4100, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 228.54956603050232 seconds\n",
      "Step 4200, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 234.18584823608398 seconds\n",
      "Step 4300, loss: tensor(0.0174, grad_fn=<SubBackward0>), time elapsed: 239.73816084861755 seconds\n",
      "Step 4400, loss: tensor(0.0167, grad_fn=<SubBackward0>), time elapsed: 245.392560005188 seconds\n",
      "Step 4500, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 250.9028398990631 seconds\n",
      "Step 4600, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 256.55813479423523 seconds\n",
      "Step 4700, loss: tensor(0.0170, grad_fn=<SubBackward0>), time elapsed: 262.06742453575134 seconds\n",
      "Step 4800, loss: tensor(0.0170, grad_fn=<SubBackward0>), time elapsed: 267.5618414878845 seconds\n",
      "Step 4900, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 273.2138249874115 seconds\n",
      "Step 5000, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 278.72084856033325 seconds\n",
      "Step 5100, loss: tensor(0.0170, grad_fn=<SubBackward0>), time elapsed: 284.38018321990967 seconds\n",
      "Step 5200, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 289.9037330150604 seconds\n",
      "Step 5300, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 295.56076669692993 seconds\n",
      "Step 5400, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 301.0850987434387 seconds\n",
      "Step 5500, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 306.60190510749817 seconds\n",
      "Step 5600, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 312.2172601222992 seconds\n",
      "Step 5700, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 317.72573947906494 seconds\n",
      "Step 5800, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 323.3908944129944 seconds\n",
      "Step 5900, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 328.91293597221375 seconds\n",
      "Step 6000, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 334.4490351676941 seconds\n",
      "Step 6100, loss: tensor(0.0159, grad_fn=<SubBackward0>), time elapsed: 340.11591267585754 seconds\n",
      "Step 6200, loss: tensor(0.0159, grad_fn=<SubBackward0>), time elapsed: 345.6530306339264 seconds\n",
      "Step 6300, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 351.3206310272217 seconds\n",
      "Step 6400, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 356.8043203353882 seconds\n",
      "Step 6500, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 362.48076820373535 seconds\n",
      "Step 6600, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 367.9849989414215 seconds\n",
      "Step 6700, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 373.5193500518799 seconds\n",
      "Step 6800, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 379.1690740585327 seconds\n",
      "Step 6900, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 384.6955668926239 seconds\n",
      "Step 7000, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 390.39300060272217 seconds\n",
      "Step 7100, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 395.91093015670776 seconds\n",
      "Step 7200, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 401.56850957870483 seconds\n",
      "Step 7300, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 407.06886315345764 seconds\n",
      "Step 7400, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 412.5798943042755 seconds\n",
      "Step 7500, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 418.2148959636688 seconds\n",
      "Step 7600, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 423.76298546791077 seconds\n",
      "Step 7700, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 429.4297094345093 seconds\n",
      "Step 7800, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 434.955441236496 seconds\n",
      "Step 7900, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 440.4715487957001 seconds\n",
      "Step 8000, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 446.14420795440674 seconds\n",
      "Step 8100, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 451.6760342121124 seconds\n",
      "Step 8200, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 457.3647093772888 seconds\n",
      "Step 8300, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 462.85499572753906 seconds\n",
      "Step 8400, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 468.5393283367157 seconds\n",
      "Step 8500, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 474.06827878952026 seconds\n",
      "Step 8600, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 479.65496802330017 seconds\n",
      "Step 8700, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 485.52892804145813 seconds\n",
      "Step 8800, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 491.0392019748688 seconds\n",
      "Step 8900, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 496.78982186317444 seconds\n",
      "Step 9000, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 502.3131630420685 seconds\n",
      "Step 9100, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 507.8075785636902 seconds\n",
      "Step 9200, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 514.1696376800537 seconds\n",
      "Step 9300, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 519.6949594020844 seconds\n",
      "Step 9400, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 525.3654274940491 seconds\n",
      "Step 9500, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 530.8940539360046 seconds\n",
      "Step 9600, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 536.5531287193298 seconds\n",
      "Step 9700, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 542.0600581169128 seconds\n",
      "Step 9800, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 547.5935516357422 seconds\n",
      "Step 9900, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 553.2464897632599 seconds\n",
      "Step 10000, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 558.7731363773346 seconds\n",
      "Step 10100, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 564.4267761707306 seconds\n",
      "Step 10200, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 569.9404149055481 seconds\n",
      "Step 10300, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 575.4875888824463 seconds\n",
      "Step 10400, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 581.1637029647827 seconds\n",
      "Step 10500, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 586.7012557983398 seconds\n",
      "Step 10600, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 592.3531670570374 seconds\n",
      "Step 10700, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 597.8717601299286 seconds\n",
      "Step 10800, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 603.5743863582611 seconds\n",
      "Step 10900, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 609.6531407833099 seconds\n",
      "Step 11000, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 615.1632578372955 seconds\n",
      "Step 11100, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 620.819319486618 seconds\n",
      "Step 11200, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 626.3386220932007 seconds\n",
      "Step 11300, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 631.9864356517792 seconds\n",
      "Step 11400, loss: tensor(0.0146, grad_fn=<SubBackward0>), time elapsed: 637.55304646492 seconds\n",
      "Step 11500, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 643.0971143245697 seconds\n",
      "Step 11600, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 648.7832217216492 seconds\n",
      "Step 11700, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 654.366052865982 seconds\n",
      "Step 11800, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 660.1416389942169 seconds\n",
      "Step 11900, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 665.7342166900635 seconds\n",
      "Step 12000, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 671.5316574573517 seconds\n",
      "Step 12100, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 677.1423602104187 seconds\n",
      "Step 12200, loss: tensor(0.0145, grad_fn=<SubBackward0>), time elapsed: 682.6961607933044 seconds\n",
      "Step 12300, loss: tensor(0.0146, grad_fn=<SubBackward0>), time elapsed: 688.6000134944916 seconds\n",
      "Step 12400, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 694.1332025527954 seconds\n",
      "Step 12500, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 699.8304252624512 seconds\n",
      "Step 12600, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 705.3661167621613 seconds\n",
      "Step 12700, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 712.2589218616486 seconds\n",
      "Step 12800, loss: tensor(0.0145, grad_fn=<SubBackward0>), time elapsed: 717.9504818916321 seconds\n",
      "Step 12900, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 723.4808104038239 seconds\n",
      "Step 13000, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 729.1604189872742 seconds\n",
      "Step 13100, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 734.681806564331 seconds\n",
      "Step 13200, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 740.3595407009125 seconds\n",
      "Step 13300, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 745.8751895427704 seconds\n",
      "Step 13400, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 751.4045221805573 seconds\n",
      "Step 13500, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 757.1050667762756 seconds\n",
      "Step 13600, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 762.6351418495178 seconds\n",
      "Step 13700, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 768.2979371547699 seconds\n",
      "Step 13800, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 773.8227667808533 seconds\n",
      "Step 13900, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 779.3503980636597 seconds\n",
      "Step 14000, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 785.0508601665497 seconds\n",
      "Step 14100, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 790.609979391098 seconds\n",
      "Step 14200, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 796.3252012729645 seconds\n",
      "Step 14300, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 801.8861904144287 seconds\n",
      "Step 14400, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 807.6142647266388 seconds\n",
      "Step 14500, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 813.1602501869202 seconds\n",
      "Step 14600, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 818.734500169754 seconds\n",
      "Step 14700, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 824.4499049186707 seconds\n",
      "Step 14800, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 830.0058379173279 seconds\n",
      "Step 14900, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 835.7266659736633 seconds\n",
      "Step 15000, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 841.2882122993469 seconds\n",
      "Step 15100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 846.8453500270844 seconds\n",
      "Step 15200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 852.5889163017273 seconds\n",
      "Step 15300, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 858.1241493225098 seconds\n",
      "Step 15400, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 863.8294382095337 seconds\n",
      "Step 15500, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 869.3628761768341 seconds\n",
      "Step 15600, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 875.0767500400543 seconds\n",
      "Step 15700, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 880.6597876548767 seconds\n",
      "Step 15800, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 886.266224861145 seconds\n",
      "Step 15900, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 892.097069978714 seconds\n",
      "Step 16000, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 897.6483228206635 seconds\n",
      "Step 16100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 903.9805343151093 seconds\n",
      "Step 16200, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 909.5568645000458 seconds\n",
      "Step 16300, loss: tensor(0.0133, grad_fn=<SubBackward0>), time elapsed: 915.1116745471954 seconds\n",
      "Step 16400, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 920.8176999092102 seconds\n",
      "Step 16500, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 926.4150812625885 seconds\n",
      "Step 16600, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 932.183146238327 seconds\n",
      "Step 16700, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 937.7109007835388 seconds\n",
      "Step 16800, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 943.2594132423401 seconds\n",
      "Step 16900, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 948.9404628276825 seconds\n",
      "Step 17000, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 954.4753234386444 seconds\n",
      "Step 17100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 960.1712663173676 seconds\n",
      "Step 17200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 965.7073264122009 seconds\n",
      "Step 17300, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 971.4736466407776 seconds\n",
      "Step 17400, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 977.0224945545197 seconds\n",
      "Step 17500, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 982.5719709396362 seconds\n",
      "Step 17600, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 988.3179149627686 seconds\n",
      "Step 17700, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 996.4139888286591 seconds\n",
      "Step 17800, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1002.131195306778 seconds\n",
      "Step 17900, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1007.690274477005 seconds\n",
      "Step 18000, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 1013.2291300296783 seconds\n",
      "Step 18100, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1018.9218950271606 seconds\n",
      "Step 18200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1024.470463514328 seconds\n",
      "Step 18300, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1030.168726682663 seconds\n",
      "Step 18400, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1035.703349351883 seconds\n",
      "Step 18500, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 1041.3901181221008 seconds\n",
      "Step 18600, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 1046.8937797546387 seconds\n",
      "Step 18700, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 1052.4473781585693 seconds\n",
      "Step 18800, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1058.1331470012665 seconds\n",
      "Step 18900, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1063.6907050609589 seconds\n",
      "Step 19000, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1069.4047546386719 seconds\n",
      "Step 19100, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 1074.9407842159271 seconds\n",
      "Step 19200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1080.771758556366 seconds\n",
      "Step 19300, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 1086.502452135086 seconds\n",
      "Step 19400, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1092.1167259216309 seconds\n",
      "Step 19500, loss: tensor(0.0128, grad_fn=<SubBackward0>), time elapsed: 1097.8725972175598 seconds\n",
      "Step 19600, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 1103.4740679264069 seconds\n",
      "Step 19700, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 1109.0879645347595 seconds\n",
      "Step 19800, loss: tensor(0.0133, grad_fn=<SubBackward0>), time elapsed: 1114.8645322322845 seconds\n",
      "Step 19900, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1120.5005927085876 seconds\n",
      "Step 20000, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1126.3305914402008 seconds\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1059, grad_fn=<SubBackward0>), time elapsed: 0.06616783142089844 seconds\n",
      "Step 100, loss: tensor(0.0958, grad_fn=<SubBackward0>), time elapsed: 5.824069261550903 seconds\n",
      "Step 200, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 11.636232852935791 seconds\n",
      "Step 300, loss: tensor(0.0730, grad_fn=<SubBackward0>), time elapsed: 17.208541870117188 seconds\n",
      "Step 400, loss: tensor(0.0632, grad_fn=<SubBackward0>), time elapsed: 22.782327890396118 seconds\n",
      "Step 500, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 28.49082112312317 seconds\n",
      "Step 600, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 34.10900259017944 seconds\n",
      "Step 700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 39.83146905899048 seconds\n",
      "Step 800, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 45.426713943481445 seconds\n",
      "Step 900, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 51.175193786621094 seconds\n",
      "Step 1000, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 56.74862003326416 seconds\n",
      "Step 1100, loss: tensor(0.0456, grad_fn=<SubBackward0>), time elapsed: 62.387656688690186 seconds\n",
      "Step 1200, loss: tensor(0.0440, grad_fn=<SubBackward0>), time elapsed: 68.11637139320374 seconds\n",
      "Step 1300, loss: tensor(0.0458, grad_fn=<SubBackward0>), time elapsed: 73.72607755661011 seconds\n",
      "Step 1400, loss: tensor(0.0420, grad_fn=<SubBackward0>), time elapsed: 79.46331071853638 seconds\n",
      "Step 1500, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 85.06915163993835 seconds\n",
      "Step 1600, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 90.72067260742188 seconds\n",
      "Step 1700, loss: tensor(0.0432, grad_fn=<SubBackward0>), time elapsed: 96.44718384742737 seconds\n",
      "Step 1800, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 102.06744599342346 seconds\n",
      "Step 1900, loss: tensor(0.0425, grad_fn=<SubBackward0>), time elapsed: 107.82430076599121 seconds\n",
      "Step 2000, loss: tensor(0.0415, grad_fn=<SubBackward0>), time elapsed: 113.40604209899902 seconds\n",
      "Step 2100, loss: tensor(0.0415, grad_fn=<SubBackward0>), time elapsed: 119.14943146705627 seconds\n",
      "Step 2200, loss: tensor(0.0419, grad_fn=<SubBackward0>), time elapsed: 124.76355218887329 seconds\n",
      "Step 2300, loss: tensor(0.0416, grad_fn=<SubBackward0>), time elapsed: 130.34821605682373 seconds\n",
      "Step 2400, loss: tensor(0.0397, grad_fn=<SubBackward0>), time elapsed: 136.0873568058014 seconds\n",
      "Step 2500, loss: tensor(0.0410, grad_fn=<SubBackward0>), time elapsed: 141.6681718826294 seconds\n",
      "Step 2600, loss: tensor(0.0401, grad_fn=<SubBackward0>), time elapsed: 147.3904881477356 seconds\n",
      "Step 2700, loss: tensor(0.0395, grad_fn=<SubBackward0>), time elapsed: 153.02842211723328 seconds\n",
      "Step 2800, loss: tensor(0.0413, grad_fn=<SubBackward0>), time elapsed: 158.764568567276 seconds\n",
      "Step 2900, loss: tensor(0.0398, grad_fn=<SubBackward0>), time elapsed: 164.35350489616394 seconds\n",
      "Step 3000, loss: tensor(0.0398, grad_fn=<SubBackward0>), time elapsed: 169.94674110412598 seconds\n",
      "Step 3100, loss: tensor(0.0388, grad_fn=<SubBackward0>), time elapsed: 175.68269157409668 seconds\n",
      "Step 3200, loss: tensor(0.0408, grad_fn=<SubBackward0>), time elapsed: 181.29150938987732 seconds\n",
      "Step 3300, loss: tensor(0.0396, grad_fn=<SubBackward0>), time elapsed: 187.0442521572113 seconds\n",
      "Step 3400, loss: tensor(0.0389, grad_fn=<SubBackward0>), time elapsed: 192.66985297203064 seconds\n",
      "Step 3500, loss: tensor(0.0388, grad_fn=<SubBackward0>), time elapsed: 198.42760467529297 seconds\n",
      "Step 3600, loss: tensor(0.0385, grad_fn=<SubBackward0>), time elapsed: 204.025945186615 seconds\n",
      "Step 3700, loss: tensor(0.0385, grad_fn=<SubBackward0>), time elapsed: 209.6463177204132 seconds\n",
      "Step 3800, loss: tensor(0.0383, grad_fn=<SubBackward0>), time elapsed: 215.38855743408203 seconds\n",
      "Step 3900, loss: tensor(0.0387, grad_fn=<SubBackward0>), time elapsed: 220.9858911037445 seconds\n",
      "Step 4000, loss: tensor(0.0380, grad_fn=<SubBackward0>), time elapsed: 226.7168788909912 seconds\n",
      "Step 4100, loss: tensor(0.0378, grad_fn=<SubBackward0>), time elapsed: 232.41851997375488 seconds\n",
      "Step 4200, loss: tensor(0.0389, grad_fn=<SubBackward0>), time elapsed: 238.44736790657043 seconds\n",
      "Step 4300, loss: tensor(0.0379, grad_fn=<SubBackward0>), time elapsed: 244.4294035434723 seconds\n",
      "Step 4400, loss: tensor(0.0375, grad_fn=<SubBackward0>), time elapsed: 250.17819380760193 seconds\n",
      "Step 4500, loss: tensor(0.0378, grad_fn=<SubBackward0>), time elapsed: 255.92971086502075 seconds\n",
      "Step 4600, loss: tensor(0.0367, grad_fn=<SubBackward0>), time elapsed: 261.51554703712463 seconds\n",
      "Step 4700, loss: tensor(0.0381, grad_fn=<SubBackward0>), time elapsed: 267.24836468696594 seconds\n",
      "Step 4800, loss: tensor(0.0366, grad_fn=<SubBackward0>), time elapsed: 272.86780047416687 seconds\n",
      "Step 4900, loss: tensor(0.0365, grad_fn=<SubBackward0>), time elapsed: 278.46096420288086 seconds\n",
      "Step 5000, loss: tensor(0.0367, grad_fn=<SubBackward0>), time elapsed: 284.1875593662262 seconds\n",
      "Step 5100, loss: tensor(0.0359, grad_fn=<SubBackward0>), time elapsed: 289.78953981399536 seconds\n",
      "Step 5200, loss: tensor(0.0365, grad_fn=<SubBackward0>), time elapsed: 295.50725746154785 seconds\n",
      "Step 5300, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 301.0879113674164 seconds\n",
      "Step 5400, loss: tensor(0.0366, grad_fn=<SubBackward0>), time elapsed: 306.86059641838074 seconds\n",
      "Step 5500, loss: tensor(0.0359, grad_fn=<SubBackward0>), time elapsed: 312.4495904445648 seconds\n",
      "Step 5600, loss: tensor(0.0376, grad_fn=<SubBackward0>), time elapsed: 318.0509395599365 seconds\n",
      "Step 5700, loss: tensor(0.0357, grad_fn=<SubBackward0>), time elapsed: 323.7900860309601 seconds\n",
      "Step 5800, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 329.4051899909973 seconds\n",
      "Step 5900, loss: tensor(0.0356, grad_fn=<SubBackward0>), time elapsed: 335.1671905517578 seconds\n",
      "Step 6000, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 340.77831864356995 seconds\n",
      "Step 6100, loss: tensor(0.0372, grad_fn=<SubBackward0>), time elapsed: 346.3879904747009 seconds\n",
      "Step 6200, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 352.1354808807373 seconds\n",
      "Step 6300, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 357.7510552406311 seconds\n",
      "Step 6400, loss: tensor(0.0361, grad_fn=<SubBackward0>), time elapsed: 363.4886496067047 seconds\n",
      "Step 6500, loss: tensor(0.0347, grad_fn=<SubBackward0>), time elapsed: 369.13188123703003 seconds\n",
      "Step 6600, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 374.8877184391022 seconds\n",
      "Step 6700, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 380.48923349380493 seconds\n",
      "Step 6800, loss: tensor(0.0346, grad_fn=<SubBackward0>), time elapsed: 386.10632848739624 seconds\n",
      "Step 6900, loss: tensor(0.0360, grad_fn=<SubBackward0>), time elapsed: 391.87999534606934 seconds\n",
      "Step 7000, loss: tensor(0.0342, grad_fn=<SubBackward0>), time elapsed: 397.5149748325348 seconds\n",
      "Step 7100, loss: tensor(0.0360, grad_fn=<SubBackward0>), time elapsed: 403.2708339691162 seconds\n",
      "Step 7200, loss: tensor(0.0346, grad_fn=<SubBackward0>), time elapsed: 408.8512876033783 seconds\n",
      "Step 7300, loss: tensor(0.0347, grad_fn=<SubBackward0>), time elapsed: 414.5831563472748 seconds\n",
      "Step 7400, loss: tensor(0.0346, grad_fn=<SubBackward0>), time elapsed: 420.16295337677 seconds\n",
      "Step 7500, loss: tensor(0.0344, grad_fn=<SubBackward0>), time elapsed: 425.7872052192688 seconds\n",
      "Step 7600, loss: tensor(0.0352, grad_fn=<SubBackward0>), time elapsed: 431.49931502342224 seconds\n",
      "Step 7700, loss: tensor(0.0341, grad_fn=<SubBackward0>), time elapsed: 437.08839750289917 seconds\n",
      "Step 7800, loss: tensor(0.0349, grad_fn=<SubBackward0>), time elapsed: 442.83047103881836 seconds\n",
      "Step 7900, loss: tensor(0.0342, grad_fn=<SubBackward0>), time elapsed: 448.43738055229187 seconds\n",
      "Step 8000, loss: tensor(0.0354, grad_fn=<SubBackward0>), time elapsed: 454.030410528183 seconds\n",
      "Step 8100, loss: tensor(0.0345, grad_fn=<SubBackward0>), time elapsed: 459.80301094055176 seconds\n",
      "Step 8200, loss: tensor(0.0338, grad_fn=<SubBackward0>), time elapsed: 465.40860772132874 seconds\n",
      "Step 8300, loss: tensor(0.0336, grad_fn=<SubBackward0>), time elapsed: 471.1604859828949 seconds\n",
      "Step 8400, loss: tensor(0.0343, grad_fn=<SubBackward0>), time elapsed: 476.7702889442444 seconds\n",
      "Step 8500, loss: tensor(0.0341, grad_fn=<SubBackward0>), time elapsed: 482.5177619457245 seconds\n",
      "Step 8600, loss: tensor(0.0333, grad_fn=<SubBackward0>), time elapsed: 488.125364780426 seconds\n",
      "Step 8700, loss: tensor(0.0337, grad_fn=<SubBackward0>), time elapsed: 493.7207808494568 seconds\n",
      "Step 8800, loss: tensor(0.0335, grad_fn=<SubBackward0>), time elapsed: 499.44727063179016 seconds\n",
      "Step 8900, loss: tensor(0.0334, grad_fn=<SubBackward0>), time elapsed: 505.0487976074219 seconds\n",
      "Step 9000, loss: tensor(0.0326, grad_fn=<SubBackward0>), time elapsed: 510.8087956905365 seconds\n",
      "Step 9100, loss: tensor(0.0325, grad_fn=<SubBackward0>), time elapsed: 516.4437403678894 seconds\n",
      "Step 9200, loss: tensor(0.0326, grad_fn=<SubBackward0>), time elapsed: 522.0744252204895 seconds\n",
      "Step 9300, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 527.8428001403809 seconds\n",
      "Step 9400, loss: tensor(0.0323, grad_fn=<SubBackward0>), time elapsed: 533.448403596878 seconds\n",
      "Step 9500, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 539.2084066867828 seconds\n",
      "Step 9600, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 544.8234820365906 seconds\n",
      "Step 9700, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 550.6012051105499 seconds\n",
      "Step 9800, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 556.2175161838531 seconds\n",
      "Step 9900, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 561.8105053901672 seconds\n",
      "Step 10000, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 567.5789973735809 seconds\n",
      "Step 10100, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 573.1820876598358 seconds\n",
      "Step 10200, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 578.9475665092468 seconds\n",
      "Step 10300, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 584.5335021018982 seconds\n",
      "Step 10400, loss: tensor(0.0285, grad_fn=<SubBackward0>), time elapsed: 590.1267306804657 seconds\n",
      "Step 10500, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 595.8492512702942 seconds\n",
      "Step 10600, loss: tensor(0.0288, grad_fn=<SubBackward0>), time elapsed: 601.4506447315216 seconds\n",
      "Step 10700, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 607.1751706600189 seconds\n",
      "Step 10800, loss: tensor(0.0275, grad_fn=<SubBackward0>), time elapsed: 612.8165068626404 seconds\n",
      "Step 10900, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 618.5638966560364 seconds\n",
      "Step 11000, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 624.148665189743 seconds\n",
      "Step 11100, loss: tensor(0.0287, grad_fn=<SubBackward0>), time elapsed: 629.7835993766785 seconds\n",
      "Step 11200, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 635.5604162216187 seconds\n",
      "Step 11300, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 641.1909282207489 seconds\n",
      "Step 11400, loss: tensor(0.0288, grad_fn=<SubBackward0>), time elapsed: 646.965115070343 seconds\n",
      "Step 11500, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 652.5692844390869 seconds\n",
      "Step 11600, loss: tensor(0.0266, grad_fn=<SubBackward0>), time elapsed: 658.2081608772278 seconds\n",
      "Step 11700, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 663.9683468341827 seconds\n",
      "Step 11800, loss: tensor(0.0292, grad_fn=<SubBackward0>), time elapsed: 669.6072957515717 seconds\n",
      "Step 11900, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 675.3505568504333 seconds\n",
      "Step 12000, loss: tensor(0.0270, grad_fn=<SubBackward0>), time elapsed: 680.9812934398651 seconds\n",
      "Step 12100, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 686.7163825035095 seconds\n",
      "Step 12200, loss: tensor(0.0279, grad_fn=<SubBackward0>), time elapsed: 692.3448166847229 seconds\n",
      "Step 12300, loss: tensor(0.0287, grad_fn=<SubBackward0>), time elapsed: 697.9359226226807 seconds\n",
      "Step 12400, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 703.7169232368469 seconds\n",
      "Step 12500, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 709.3245310783386 seconds\n",
      "Step 12600, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 715.1262345314026 seconds\n",
      "Step 12700, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 720.7340157032013 seconds\n",
      "Step 12800, loss: tensor(0.0284, grad_fn=<SubBackward0>), time elapsed: 726.3482277393341 seconds\n",
      "Step 12900, loss: tensor(0.0269, grad_fn=<SubBackward0>), time elapsed: 732.1165990829468 seconds\n",
      "Step 13000, loss: tensor(0.0279, grad_fn=<SubBackward0>), time elapsed: 737.7251334190369 seconds\n",
      "Step 13100, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 743.4768006801605 seconds\n",
      "Step 13200, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 749.10036444664 seconds\n",
      "Step 13300, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 754.8687863349915 seconds\n",
      "Step 13400, loss: tensor(0.0278, grad_fn=<SubBackward0>), time elapsed: 760.4909715652466 seconds\n",
      "Step 13500, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 766.0884618759155 seconds\n",
      "Step 13600, loss: tensor(0.0278, grad_fn=<SubBackward0>), time elapsed: 771.865076303482 seconds\n",
      "Step 13700, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 777.4498534202576 seconds\n",
      "Step 13800, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 783.2098331451416 seconds\n",
      "Step 13900, loss: tensor(0.0273, grad_fn=<SubBackward0>), time elapsed: 788.8198375701904 seconds\n",
      "Step 14000, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 794.4630641937256 seconds\n",
      "Step 14100, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 800.2312560081482 seconds\n",
      "Step 14200, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 805.8621368408203 seconds\n",
      "Step 14300, loss: tensor(0.0280, grad_fn=<SubBackward0>), time elapsed: 811.6220273971558 seconds\n",
      "Step 14400, loss: tensor(0.0278, grad_fn=<SubBackward0>), time elapsed: 817.2432281970978 seconds\n",
      "Step 14500, loss: tensor(0.0272, grad_fn=<SubBackward0>), time elapsed: 822.9845175743103 seconds\n",
      "Step 14600, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 828.5151083469391 seconds\n",
      "Step 14700, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 834.0600528717041 seconds\n",
      "Step 14800, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 839.7112228870392 seconds\n",
      "Step 14900, loss: tensor(0.0259, grad_fn=<SubBackward0>), time elapsed: 845.247392654419 seconds\n",
      "Step 15000, loss: tensor(0.0259, grad_fn=<SubBackward0>), time elapsed: 850.9296207427979 seconds\n",
      "Step 15100, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 856.4918527603149 seconds\n",
      "Step 15200, loss: tensor(0.0260, grad_fn=<SubBackward0>), time elapsed: 862.0103023052216 seconds\n",
      "Step 15300, loss: tensor(0.0268, grad_fn=<SubBackward0>), time elapsed: 867.6603178977966 seconds\n",
      "Step 15400, loss: tensor(0.0265, grad_fn=<SubBackward0>), time elapsed: 873.1879260540009 seconds\n",
      "Step 15500, loss: tensor(0.0249, grad_fn=<SubBackward0>), time elapsed: 878.8714692592621 seconds\n",
      "Step 15600, loss: tensor(0.0247, grad_fn=<SubBackward0>), time elapsed: 884.4213447570801 seconds\n",
      "Step 15700, loss: tensor(0.0243, grad_fn=<SubBackward0>), time elapsed: 890.0947844982147 seconds\n",
      "Step 15800, loss: tensor(0.0234, grad_fn=<SubBackward0>), time elapsed: 895.6176810264587 seconds\n",
      "Step 15900, loss: tensor(0.0241, grad_fn=<SubBackward0>), time elapsed: 901.1578793525696 seconds\n",
      "Step 16000, loss: tensor(0.0236, grad_fn=<SubBackward0>), time elapsed: 906.8347179889679 seconds\n",
      "Step 16100, loss: tensor(0.0238, grad_fn=<SubBackward0>), time elapsed: 912.3777613639832 seconds\n",
      "Step 16200, loss: tensor(0.0245, grad_fn=<SubBackward0>), time elapsed: 918.0590560436249 seconds\n",
      "Step 16300, loss: tensor(0.0233, grad_fn=<SubBackward0>), time elapsed: 923.5902149677277 seconds\n",
      "Step 16400, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 929.1205244064331 seconds\n",
      "Step 16500, loss: tensor(0.0231, grad_fn=<SubBackward0>), time elapsed: 934.8303906917572 seconds\n",
      "Step 16600, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 940.3862192630768 seconds\n",
      "Step 16700, loss: tensor(0.0224, grad_fn=<SubBackward0>), time elapsed: 946.1100995540619 seconds\n",
      "Step 16800, loss: tensor(0.0234, grad_fn=<SubBackward0>), time elapsed: 951.6924631595612 seconds\n",
      "Step 16900, loss: tensor(0.0233, grad_fn=<SubBackward0>), time elapsed: 957.2688591480255 seconds\n",
      "Step 17000, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 962.9609189033508 seconds\n",
      "Step 17100, loss: tensor(0.0215, grad_fn=<SubBackward0>), time elapsed: 968.5173273086548 seconds\n",
      "Step 17200, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 974.2397768497467 seconds\n",
      "Step 17300, loss: tensor(0.0225, grad_fn=<SubBackward0>), time elapsed: 979.8187167644501 seconds\n",
      "Step 17400, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 985.5411322116852 seconds\n",
      "Step 17500, loss: tensor(0.0234, grad_fn=<SubBackward0>), time elapsed: 991.1137363910675 seconds\n",
      "Step 17600, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 996.6851723194122 seconds\n",
      "Step 17700, loss: tensor(0.0224, grad_fn=<SubBackward0>), time elapsed: 1002.4325168132782 seconds\n",
      "Step 17800, loss: tensor(0.0222, grad_fn=<SubBackward0>), time elapsed: 1007.994250535965 seconds\n",
      "Step 17900, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 1013.676043510437 seconds\n",
      "Step 18000, loss: tensor(0.0222, grad_fn=<SubBackward0>), time elapsed: 1019.1957190036774 seconds\n",
      "Step 18100, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 1024.7476246356964 seconds\n",
      "Step 18200, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 1030.4712381362915 seconds\n",
      "Step 18300, loss: tensor(0.0224, grad_fn=<SubBackward0>), time elapsed: 1036.0457317829132 seconds\n",
      "Step 18400, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 1041.7859308719635 seconds\n",
      "Step 18500, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 1047.3135526180267 seconds\n",
      "Step 18600, loss: tensor(0.0208, grad_fn=<SubBackward0>), time elapsed: 1053.0455577373505 seconds\n",
      "Step 18700, loss: tensor(0.0213, grad_fn=<SubBackward0>), time elapsed: 1058.5802955627441 seconds\n",
      "Step 18800, loss: tensor(0.0221, grad_fn=<SubBackward0>), time elapsed: 1064.1203138828278 seconds\n",
      "Step 18900, loss: tensor(0.0225, grad_fn=<SubBackward0>), time elapsed: 1069.8612401485443 seconds\n",
      "Step 19000, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 1075.4463152885437 seconds\n",
      "Step 19100, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 1081.168471813202 seconds\n",
      "Step 19200, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 1086.7281725406647 seconds\n",
      "Step 19300, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 1092.2716686725616 seconds\n",
      "Step 19400, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 1098.0013637542725 seconds\n",
      "Step 19500, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 1103.5487480163574 seconds\n",
      "Step 19600, loss: tensor(0.0229, grad_fn=<SubBackward0>), time elapsed: 1109.264904975891 seconds\n",
      "Step 19700, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 1114.833734512329 seconds\n",
      "Step 19800, loss: tensor(0.0207, grad_fn=<SubBackward0>), time elapsed: 1120.4274525642395 seconds\n",
      "Step 19900, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 1126.199602842331 seconds\n",
      "Step 20000, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 1131.801593542099 seconds\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.2090, grad_fn=<SubBackward0>), time elapsed: 0.062062740325927734 seconds\n",
      "Step 100, loss: tensor(0.1864, grad_fn=<SubBackward0>), time elapsed: 5.914261102676392 seconds\n",
      "Step 200, loss: tensor(0.1532, grad_fn=<SubBackward0>), time elapsed: 11.539004802703857 seconds\n",
      "Step 300, loss: tensor(0.1291, grad_fn=<SubBackward0>), time elapsed: 17.18545126914978 seconds\n",
      "Step 400, loss: tensor(0.1158, grad_fn=<SubBackward0>), time elapsed: 22.699230670928955 seconds\n",
      "Step 500, loss: tensor(0.1083, grad_fn=<SubBackward0>), time elapsed: 28.213378429412842 seconds\n",
      "Step 600, loss: tensor(0.1061, grad_fn=<SubBackward0>), time elapsed: 33.86864686012268 seconds\n",
      "Step 700, loss: tensor(0.0982, grad_fn=<SubBackward0>), time elapsed: 39.36850070953369 seconds\n",
      "Step 800, loss: tensor(0.0967, grad_fn=<SubBackward0>), time elapsed: 45.010740995407104 seconds\n",
      "Step 900, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 50.51896405220032 seconds\n",
      "Step 1000, loss: tensor(0.0895, grad_fn=<SubBackward0>), time elapsed: 56.18190121650696 seconds\n",
      "Step 1100, loss: tensor(0.0857, grad_fn=<SubBackward0>), time elapsed: 61.68891644477844 seconds\n",
      "Step 1200, loss: tensor(0.0828, grad_fn=<SubBackward0>), time elapsed: 67.23157668113708 seconds\n",
      "Step 1300, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 72.8789496421814 seconds\n",
      "Step 1400, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 78.40281128883362 seconds\n",
      "Step 1500, loss: tensor(0.0802, grad_fn=<SubBackward0>), time elapsed: 84.04602575302124 seconds\n",
      "Step 1600, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 89.56595087051392 seconds\n",
      "Step 1700, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 95.25049614906311 seconds\n",
      "Step 1800, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 100.75752830505371 seconds\n",
      "Step 1900, loss: tensor(0.0696, grad_fn=<SubBackward0>), time elapsed: 106.28764605522156 seconds\n",
      "Step 2000, loss: tensor(0.0709, grad_fn=<SubBackward0>), time elapsed: 111.93590235710144 seconds\n",
      "Step 2100, loss: tensor(0.0679, grad_fn=<SubBackward0>), time elapsed: 117.4632499217987 seconds\n",
      "Step 2200, loss: tensor(0.0669, grad_fn=<SubBackward0>), time elapsed: 123.10934591293335 seconds\n",
      "Step 2300, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 128.63653898239136 seconds\n",
      "Step 2400, loss: tensor(0.0664, grad_fn=<SubBackward0>), time elapsed: 134.15347528457642 seconds\n",
      "Step 2500, loss: tensor(0.0659, grad_fn=<SubBackward0>), time elapsed: 139.7987334728241 seconds\n",
      "Step 2600, loss: tensor(0.0657, grad_fn=<SubBackward0>), time elapsed: 145.32171392440796 seconds\n",
      "Step 2700, loss: tensor(0.0654, grad_fn=<SubBackward0>), time elapsed: 150.96953892707825 seconds\n",
      "Step 2800, loss: tensor(0.0665, grad_fn=<SubBackward0>), time elapsed: 156.52113819122314 seconds\n",
      "Step 2900, loss: tensor(0.0646, grad_fn=<SubBackward0>), time elapsed: 162.16967916488647 seconds\n",
      "Step 3000, loss: tensor(0.0647, grad_fn=<SubBackward0>), time elapsed: 167.6913652420044 seconds\n",
      "Step 3100, loss: tensor(0.0633, grad_fn=<SubBackward0>), time elapsed: 173.22821307182312 seconds\n",
      "Step 3200, loss: tensor(0.0637, grad_fn=<SubBackward0>), time elapsed: 178.8937427997589 seconds\n",
      "Step 3300, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 184.40462470054626 seconds\n",
      "Step 3400, loss: tensor(0.0609, grad_fn=<SubBackward0>), time elapsed: 190.06577563285828 seconds\n",
      "Step 3500, loss: tensor(0.0629, grad_fn=<SubBackward0>), time elapsed: 195.57733416557312 seconds\n",
      "Step 3600, loss: tensor(0.0620, grad_fn=<SubBackward0>), time elapsed: 201.23616933822632 seconds\n",
      "Step 3700, loss: tensor(0.0621, grad_fn=<SubBackward0>), time elapsed: 206.74548983573914 seconds\n",
      "Step 3800, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 212.27950763702393 seconds\n",
      "Step 3900, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 217.9434633255005 seconds\n",
      "Step 4000, loss: tensor(0.0576, grad_fn=<SubBackward0>), time elapsed: 223.47261452674866 seconds\n",
      "Step 4100, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 229.13537549972534 seconds\n",
      "Step 4200, loss: tensor(0.0557, grad_fn=<SubBackward0>), time elapsed: 234.64012360572815 seconds\n",
      "Step 4300, loss: tensor(0.0549, grad_fn=<SubBackward0>), time elapsed: 240.31853723526 seconds\n",
      "Step 4400, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 245.85525012016296 seconds\n",
      "Step 4500, loss: tensor(0.0535, grad_fn=<SubBackward0>), time elapsed: 251.4261782169342 seconds\n",
      "Step 4600, loss: tensor(0.0552, grad_fn=<SubBackward0>), time elapsed: 257.1024417877197 seconds\n",
      "Step 4700, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 262.64177656173706 seconds\n",
      "Step 4800, loss: tensor(0.0547, grad_fn=<SubBackward0>), time elapsed: 268.3113284111023 seconds\n",
      "Step 4900, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 273.82122588157654 seconds\n",
      "Step 5000, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 279.385142326355 seconds\n",
      "Step 5100, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 285.16333174705505 seconds\n",
      "Step 5200, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 290.68963384628296 seconds\n",
      "Step 5300, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 296.35582208633423 seconds\n",
      "Step 5400, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 301.911159992218 seconds\n",
      "Step 5500, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 307.6025302410126 seconds\n",
      "Step 5600, loss: tensor(0.0465, grad_fn=<SubBackward0>), time elapsed: 313.1151611804962 seconds\n",
      "Step 5700, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 319.11694526672363 seconds\n",
      "Step 5800, loss: tensor(0.0465, grad_fn=<SubBackward0>), time elapsed: 324.76825070381165 seconds\n",
      "Step 5900, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 330.26574420928955 seconds\n",
      "Step 6000, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 335.9507210254669 seconds\n",
      "Step 6100, loss: tensor(0.0449, grad_fn=<SubBackward0>), time elapsed: 341.48739290237427 seconds\n",
      "Step 6200, loss: tensor(0.0450, grad_fn=<SubBackward0>), time elapsed: 347.16762590408325 seconds\n",
      "Step 6300, loss: tensor(0.0439, grad_fn=<SubBackward0>), time elapsed: 352.67083287239075 seconds\n",
      "Step 6400, loss: tensor(0.0462, grad_fn=<SubBackward0>), time elapsed: 358.1870036125183 seconds\n",
      "Step 6500, loss: tensor(0.0450, grad_fn=<SubBackward0>), time elapsed: 363.8343918323517 seconds\n",
      "Step 6600, loss: tensor(0.0446, grad_fn=<SubBackward0>), time elapsed: 369.3621950149536 seconds\n",
      "Step 6700, loss: tensor(0.0448, grad_fn=<SubBackward0>), time elapsed: 374.99418210983276 seconds\n",
      "Step 6800, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 380.5062017440796 seconds\n",
      "Step 6900, loss: tensor(0.0439, grad_fn=<SubBackward0>), time elapsed: 386.0210180282593 seconds\n",
      "Step 7000, loss: tensor(0.0433, grad_fn=<SubBackward0>), time elapsed: 391.66776943206787 seconds\n",
      "Step 7100, loss: tensor(0.0434, grad_fn=<SubBackward0>), time elapsed: 397.18723583221436 seconds\n",
      "Step 7200, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 402.85593938827515 seconds\n",
      "Step 7300, loss: tensor(0.0409, grad_fn=<SubBackward0>), time elapsed: 408.3912341594696 seconds\n",
      "Step 7400, loss: tensor(0.0421, grad_fn=<SubBackward0>), time elapsed: 414.07877564430237 seconds\n",
      "Step 7500, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 419.5971019268036 seconds\n",
      "Step 7600, loss: tensor(0.0420, grad_fn=<SubBackward0>), time elapsed: 425.14655685424805 seconds\n",
      "Step 7700, loss: tensor(0.0417, grad_fn=<SubBackward0>), time elapsed: 430.87453508377075 seconds\n",
      "Step 7800, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 436.41236639022827 seconds\n",
      "Step 7900, loss: tensor(0.0422, grad_fn=<SubBackward0>), time elapsed: 442.12187337875366 seconds\n",
      "Step 8000, loss: tensor(0.0410, grad_fn=<SubBackward0>), time elapsed: 448.12255334854126 seconds\n",
      "Step 8100, loss: tensor(0.0414, grad_fn=<SubBackward0>), time elapsed: 453.64374113082886 seconds\n",
      "Step 8200, loss: tensor(0.0421, grad_fn=<SubBackward0>), time elapsed: 459.2941517829895 seconds\n",
      "Step 8300, loss: tensor(0.0417, grad_fn=<SubBackward0>), time elapsed: 464.8198552131653 seconds\n",
      "Step 8400, loss: tensor(0.0408, grad_fn=<SubBackward0>), time elapsed: 470.47235012054443 seconds\n",
      "Step 8500, loss: tensor(0.0397, grad_fn=<SubBackward0>), time elapsed: 475.9995937347412 seconds\n",
      "Step 8600, loss: tensor(0.0399, grad_fn=<SubBackward0>), time elapsed: 481.67139315605164 seconds\n",
      "Step 8700, loss: tensor(0.0377, grad_fn=<SubBackward0>), time elapsed: 487.20130038261414 seconds\n",
      "Step 8800, loss: tensor(0.0386, grad_fn=<SubBackward0>), time elapsed: 492.74923872947693 seconds\n",
      "Step 8900, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 498.4276397228241 seconds\n",
      "Step 9000, loss: tensor(0.0373, grad_fn=<SubBackward0>), time elapsed: 503.94417238235474 seconds\n",
      "Step 9100, loss: tensor(0.0372, grad_fn=<SubBackward0>), time elapsed: 509.6053774356842 seconds\n",
      "Step 9200, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 515.119019985199 seconds\n",
      "Step 9300, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 520.6624953746796 seconds\n",
      "Step 9400, loss: tensor(0.0354, grad_fn=<SubBackward0>), time elapsed: 526.8654954433441 seconds\n",
      "Step 9500, loss: tensor(0.0348, grad_fn=<SubBackward0>), time elapsed: 532.3984415531158 seconds\n",
      "Step 9600, loss: tensor(0.0350, grad_fn=<SubBackward0>), time elapsed: 538.051675081253 seconds\n",
      "Step 9700, loss: tensor(0.0340, grad_fn=<SubBackward0>), time elapsed: 543.5662083625793 seconds\n",
      "Step 9800, loss: tensor(0.0341, grad_fn=<SubBackward0>), time elapsed: 549.2285931110382 seconds\n",
      "Step 9900, loss: tensor(0.0335, grad_fn=<SubBackward0>), time elapsed: 554.750789642334 seconds\n",
      "Step 10000, loss: tensor(0.0321, grad_fn=<SubBackward0>), time elapsed: 560.2829451560974 seconds\n",
      "Step 10100, loss: tensor(0.0333, grad_fn=<SubBackward0>), time elapsed: 565.9349687099457 seconds\n",
      "Step 10200, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 571.4602138996124 seconds\n",
      "Step 10300, loss: tensor(0.0322, grad_fn=<SubBackward0>), time elapsed: 577.1338860988617 seconds\n",
      "Step 10400, loss: tensor(0.0323, grad_fn=<SubBackward0>), time elapsed: 582.669810295105 seconds\n",
      "Step 10500, loss: tensor(0.0327, grad_fn=<SubBackward0>), time elapsed: 588.1829915046692 seconds\n",
      "Step 10600, loss: tensor(0.0320, grad_fn=<SubBackward0>), time elapsed: 593.828771352768 seconds\n",
      "Step 10700, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 599.3431322574615 seconds\n",
      "Step 10800, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 605.0036957263947 seconds\n",
      "Step 10900, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 610.5558376312256 seconds\n",
      "Step 11000, loss: tensor(0.0327, grad_fn=<SubBackward0>), time elapsed: 616.2699506282806 seconds\n",
      "Step 11100, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 621.8092494010925 seconds\n",
      "Step 11200, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 627.3503448963165 seconds\n",
      "Step 11300, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 633.0298895835876 seconds\n",
      "Step 11400, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 638.5622799396515 seconds\n",
      "Step 11500, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 644.2561225891113 seconds\n",
      "Step 11600, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 649.7775931358337 seconds\n",
      "Step 11700, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 655.2888798713684 seconds\n",
      "Step 11800, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 660.997768163681 seconds\n",
      "Step 11900, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 666.5424988269806 seconds\n",
      "Step 12000, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 672.2548711299896 seconds\n",
      "Step 12100, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 677.842808008194 seconds\n",
      "Step 12200, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 683.568124294281 seconds\n",
      "Step 12300, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 689.0793750286102 seconds\n",
      "Step 12400, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 694.6087803840637 seconds\n",
      "Step 12500, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 700.3285026550293 seconds\n",
      "Step 12600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 705.9144427776337 seconds\n",
      "Step 12700, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 711.7207527160645 seconds\n",
      "Step 12800, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 717.495046377182 seconds\n",
      "Step 12900, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 723.0211520195007 seconds\n",
      "Step 13000, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 728.715469121933 seconds\n",
      "Step 13100, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 734.2805716991425 seconds\n",
      "Step 13200, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 740.1306819915771 seconds\n",
      "Step 13300, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 745.6607446670532 seconds\n",
      "Step 13400, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 751.3369817733765 seconds\n",
      "Step 13500, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 756.8519911766052 seconds\n",
      "Step 13600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 762.3826441764832 seconds\n",
      "Step 13700, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 768.0949206352234 seconds\n",
      "Step 13800, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 773.8802154064178 seconds\n",
      "Step 13900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 779.546767950058 seconds\n",
      "Step 14000, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 785.1143264770508 seconds\n",
      "Step 14100, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 790.6495192050934 seconds\n",
      "Step 14200, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 796.3689484596252 seconds\n",
      "Step 14300, loss: tensor(0.0322, grad_fn=<SubBackward0>), time elapsed: 801.9386873245239 seconds\n",
      "Step 14400, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 807.628674030304 seconds\n",
      "Step 14500, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 813.1860072612762 seconds\n",
      "Step 14600, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 818.7615053653717 seconds\n",
      "Step 14700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 824.6137399673462 seconds\n",
      "Step 14800, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 830.1577208042145 seconds\n",
      "Step 14900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 835.9692018032074 seconds\n",
      "Step 15000, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 841.5116305351257 seconds\n",
      "Step 15100, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 847.2027840614319 seconds\n",
      "Step 15200, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 852.8893115520477 seconds\n",
      "Step 15300, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 858.4654037952423 seconds\n",
      "Step 15400, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 864.1783652305603 seconds\n",
      "Step 15500, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 869.7130138874054 seconds\n",
      "Step 15600, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 875.4324028491974 seconds\n",
      "Step 15700, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 881.1670429706573 seconds\n",
      "Step 15800, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 886.7130236625671 seconds\n",
      "Step 15900, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 892.4312806129456 seconds\n",
      "Step 16000, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 897.9793190956116 seconds\n",
      "Step 16100, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 903.6672358512878 seconds\n",
      "Step 16200, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 909.2066395282745 seconds\n",
      "Step 16300, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 915.2378239631653 seconds\n",
      "Step 16400, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 920.7973339557648 seconds\n",
      "Step 16500, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 926.3321685791016 seconds\n",
      "Step 16600, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 932.0225365161896 seconds\n",
      "Step 16700, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 937.5666637420654 seconds\n",
      "Step 16800, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 943.2707257270813 seconds\n",
      "Step 16900, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 948.8427860736847 seconds\n",
      "Step 17000, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 954.3976554870605 seconds\n",
      "Step 17100, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 960.1059110164642 seconds\n",
      "Step 17200, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 965.7735812664032 seconds\n",
      "Step 17300, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 971.4908924102783 seconds\n",
      "Step 17400, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 977.1084272861481 seconds\n",
      "Step 17500, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 982.9147565364838 seconds\n",
      "Step 17600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 988.5093867778778 seconds\n",
      "Step 17700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 994.6740581989288 seconds\n",
      "Step 17800, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1000.3606269359589 seconds\n",
      "Step 17900, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 1005.8871023654938 seconds\n",
      "Step 18000, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 1011.6653780937195 seconds\n",
      "Step 18100, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 1017.2448973655701 seconds\n",
      "Step 18200, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 1022.8391525745392 seconds\n",
      "Step 18300, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 1028.5856583118439 seconds\n",
      "Step 18400, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 1034.2089929580688 seconds\n",
      "Step 18500, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 1039.9846034049988 seconds\n",
      "Step 18600, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 1045.57359457016 seconds\n",
      "Step 18700, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1051.1667740345001 seconds\n",
      "Step 18800, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 1056.9257068634033 seconds\n",
      "Step 18900, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 1062.5178349018097 seconds\n",
      "Step 19000, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 1068.3019132614136 seconds\n",
      "Step 19100, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1073.9440214633942 seconds\n",
      "Step 19200, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1079.7394556999207 seconds\n",
      "Step 19300, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 1085.360764503479 seconds\n",
      "Step 19400, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 1090.9862899780273 seconds\n",
      "Step 19500, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 1096.7807505130768 seconds\n",
      "Step 19600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1102.4147124290466 seconds\n",
      "Step 19700, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 1108.2215247154236 seconds\n",
      "Step 19800, loss: tensor(0.0321, grad_fn=<SubBackward0>), time elapsed: 1113.8512780666351 seconds\n",
      "Step 19900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 1119.5235431194305 seconds\n",
      "Step 20000, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 1125.352337360382 seconds\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.3176, grad_fn=<SubBackward0>), time elapsed: 0.05955243110656738 seconds\n",
      "Step 100, loss: tensor(0.2851, grad_fn=<SubBackward0>), time elapsed: 5.748473882675171 seconds\n",
      "Step 200, loss: tensor(0.2483, grad_fn=<SubBackward0>), time elapsed: 11.529330015182495 seconds\n",
      "Step 300, loss: tensor(0.2187, grad_fn=<SubBackward0>), time elapsed: 17.10581946372986 seconds\n",
      "Step 400, loss: tensor(0.2048, grad_fn=<SubBackward0>), time elapsed: 22.82937526702881 seconds\n",
      "Step 500, loss: tensor(0.1950, grad_fn=<SubBackward0>), time elapsed: 28.3880832195282 seconds\n",
      "Step 600, loss: tensor(0.1810, grad_fn=<SubBackward0>), time elapsed: 33.97398138046265 seconds\n",
      "Step 700, loss: tensor(0.1685, grad_fn=<SubBackward0>), time elapsed: 39.658910512924194 seconds\n",
      "Step 800, loss: tensor(0.1570, grad_fn=<SubBackward0>), time elapsed: 45.217639684677124 seconds\n",
      "Step 900, loss: tensor(0.1476, grad_fn=<SubBackward0>), time elapsed: 50.869389057159424 seconds\n",
      "Step 1000, loss: tensor(0.1388, grad_fn=<SubBackward0>), time elapsed: 56.43752861022949 seconds\n",
      "Step 1100, loss: tensor(0.1299, grad_fn=<SubBackward0>), time elapsed: 62.13061165809631 seconds\n",
      "Step 1200, loss: tensor(0.1263, grad_fn=<SubBackward0>), time elapsed: 67.71975088119507 seconds\n",
      "Step 1300, loss: tensor(0.1211, grad_fn=<SubBackward0>), time elapsed: 73.27947115898132 seconds\n",
      "Step 1400, loss: tensor(0.1207, grad_fn=<SubBackward0>), time elapsed: 78.97702932357788 seconds\n",
      "Step 1500, loss: tensor(0.1123, grad_fn=<SubBackward0>), time elapsed: 84.54190921783447 seconds\n",
      "Step 1600, loss: tensor(0.1115, grad_fn=<SubBackward0>), time elapsed: 90.22992014884949 seconds\n",
      "Step 1700, loss: tensor(0.1096, grad_fn=<SubBackward0>), time elapsed: 95.80231833457947 seconds\n",
      "Step 1800, loss: tensor(0.1050, grad_fn=<SubBackward0>), time elapsed: 101.53613567352295 seconds\n",
      "Step 1900, loss: tensor(0.1055, grad_fn=<SubBackward0>), time elapsed: 107.10846018791199 seconds\n",
      "Step 2000, loss: tensor(0.0995, grad_fn=<SubBackward0>), time elapsed: 112.7068784236908 seconds\n",
      "Step 2100, loss: tensor(0.0972, grad_fn=<SubBackward0>), time elapsed: 118.42626333236694 seconds\n",
      "Step 2200, loss: tensor(0.0975, grad_fn=<SubBackward0>), time elapsed: 124.01848006248474 seconds\n",
      "Step 2300, loss: tensor(0.0962, grad_fn=<SubBackward0>), time elapsed: 129.7324583530426 seconds\n",
      "Step 2400, loss: tensor(0.0922, grad_fn=<SubBackward0>), time elapsed: 135.30587649345398 seconds\n",
      "Step 2500, loss: tensor(0.0938, grad_fn=<SubBackward0>), time elapsed: 140.88127517700195 seconds\n",
      "Step 2600, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 146.58695912361145 seconds\n",
      "Step 2700, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 152.18027138710022 seconds\n",
      "Step 2800, loss: tensor(0.0886, grad_fn=<SubBackward0>), time elapsed: 157.92360711097717 seconds\n",
      "Step 2900, loss: tensor(0.0888, grad_fn=<SubBackward0>), time elapsed: 163.5072774887085 seconds\n",
      "Step 3000, loss: tensor(0.0882, grad_fn=<SubBackward0>), time elapsed: 169.2235562801361 seconds\n",
      "Step 3100, loss: tensor(0.0860, grad_fn=<SubBackward0>), time elapsed: 174.78645300865173 seconds\n",
      "Step 3200, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 180.34625053405762 seconds\n",
      "Step 3300, loss: tensor(0.0847, grad_fn=<SubBackward0>), time elapsed: 186.04778909683228 seconds\n",
      "Step 3400, loss: tensor(0.0844, grad_fn=<SubBackward0>), time elapsed: 191.60766863822937 seconds\n",
      "Step 3500, loss: tensor(0.0844, grad_fn=<SubBackward0>), time elapsed: 197.3248791694641 seconds\n",
      "Step 3600, loss: tensor(0.0846, grad_fn=<SubBackward0>), time elapsed: 202.87760734558105 seconds\n",
      "Step 3700, loss: tensor(0.0835, grad_fn=<SubBackward0>), time elapsed: 208.5832815170288 seconds\n",
      "Step 3800, loss: tensor(0.0837, grad_fn=<SubBackward0>), time elapsed: 214.13459300994873 seconds\n",
      "Step 3900, loss: tensor(0.0824, grad_fn=<SubBackward0>), time elapsed: 219.7100899219513 seconds\n",
      "Step 4000, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 225.413831949234 seconds\n",
      "Step 4100, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 230.96858596801758 seconds\n",
      "Step 4200, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 236.6866934299469 seconds\n",
      "Step 4300, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 242.24649930000305 seconds\n",
      "Step 4400, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 247.84800148010254 seconds\n",
      "Step 4500, loss: tensor(0.0784, grad_fn=<SubBackward0>), time elapsed: 253.56836581230164 seconds\n",
      "Step 4600, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 259.1647198200226 seconds\n",
      "Step 4700, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 264.88627195358276 seconds\n",
      "Step 4800, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 270.5042061805725 seconds\n",
      "Step 4900, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 277.21961522102356 seconds\n",
      "Step 5000, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 282.85013365745544 seconds\n",
      "Step 5100, loss: tensor(0.0695, grad_fn=<SubBackward0>), time elapsed: 288.43079805374146 seconds\n",
      "Step 5200, loss: tensor(0.0663, grad_fn=<SubBackward0>), time elapsed: 294.14910769462585 seconds\n",
      "Step 5300, loss: tensor(0.0638, grad_fn=<SubBackward0>), time elapsed: 299.7464406490326 seconds\n",
      "Step 5400, loss: tensor(0.0623, grad_fn=<SubBackward0>), time elapsed: 305.47313833236694 seconds\n",
      "Step 5500, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 311.0808160305023 seconds\n",
      "Step 5600, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 316.9085958003998 seconds\n",
      "Step 5700, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 322.5841727256775 seconds\n",
      "Step 5800, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 328.24615025520325 seconds\n",
      "Step 5900, loss: tensor(0.0583, grad_fn=<SubBackward0>), time elapsed: 333.9604423046112 seconds\n",
      "Step 6000, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 339.5507962703705 seconds\n",
      "Step 6100, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 345.2863390445709 seconds\n",
      "Step 6200, loss: tensor(0.0553, grad_fn=<SubBackward0>), time elapsed: 350.86925745010376 seconds\n",
      "Step 6300, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 356.44347739219666 seconds\n",
      "Step 6400, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 362.14718532562256 seconds\n",
      "Step 6500, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 367.7257409095764 seconds\n",
      "Step 6600, loss: tensor(0.0549, grad_fn=<SubBackward0>), time elapsed: 373.42944526672363 seconds\n",
      "Step 6700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 378.99349999427795 seconds\n",
      "Step 6800, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 384.7160475254059 seconds\n",
      "Step 6900, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 390.32161808013916 seconds\n",
      "Step 7000, loss: tensor(0.0554, grad_fn=<SubBackward0>), time elapsed: 395.931414604187 seconds\n",
      "Step 7100, loss: tensor(0.0536, grad_fn=<SubBackward0>), time elapsed: 401.68725204467773 seconds\n",
      "Step 7200, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 407.3096160888672 seconds\n",
      "Step 7300, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 413.08639192581177 seconds\n",
      "Step 7400, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 418.98705101013184 seconds\n",
      "Step 7500, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 424.7481164932251 seconds\n",
      "Step 7600, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 430.3663799762726 seconds\n",
      "Step 7700, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 436.347375869751 seconds\n",
      "Step 7800, loss: tensor(0.0547, grad_fn=<SubBackward0>), time elapsed: 442.07501316070557 seconds\n",
      "Step 7900, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 447.64105772972107 seconds\n",
      "Step 8000, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 453.37301683425903 seconds\n",
      "Step 8100, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 459.05995512008667 seconds\n",
      "Step 8200, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 464.7887306213379 seconds\n",
      "Step 8300, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 470.3860397338867 seconds\n",
      "Step 8400, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 477.26818799972534 seconds\n",
      "Step 8500, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 483.0259165763855 seconds\n",
      "Step 8600, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 488.60759115219116 seconds\n",
      "Step 8700, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 494.3688757419586 seconds\n",
      "Step 8800, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 499.93288135528564 seconds\n",
      "Step 8900, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 505.5008466243744 seconds\n",
      "Step 9000, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 511.20658707618713 seconds\n",
      "Step 9100, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 516.7726154327393 seconds\n",
      "Step 9200, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 522.5096955299377 seconds\n",
      "Step 9300, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 528.1070251464844 seconds\n",
      "Step 9400, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 533.8837847709656 seconds\n",
      "Step 9500, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 539.4603731632233 seconds\n",
      "Step 9600, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 545.0252559185028 seconds\n",
      "Step 9700, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 550.7676687240601 seconds\n",
      "Step 9800, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 556.3483068943024 seconds\n",
      "Step 9900, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 562.0873031616211 seconds\n",
      "Step 10000, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 567.6681394577026 seconds\n",
      "Step 10100, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 573.248613357544 seconds\n",
      "Step 10200, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 578.9878339767456 seconds\n",
      "Step 10300, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 584.6423873901367 seconds\n",
      "Step 10400, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 590.3868091106415 seconds\n",
      "Step 10500, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 596.0164141654968 seconds\n",
      "Step 10600, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 601.7693402767181 seconds\n",
      "Step 10700, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 607.3832886219025 seconds\n",
      "Step 10800, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 613.0182299613953 seconds\n",
      "Step 10900, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 618.8042025566101 seconds\n",
      "Step 11000, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 624.4296867847443 seconds\n",
      "Step 11100, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 630.2479376792908 seconds\n",
      "Step 11200, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 635.8705625534058 seconds\n",
      "Step 11300, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 641.5083866119385 seconds\n",
      "Step 11400, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 647.3099083900452 seconds\n",
      "Step 11500, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 652.9460439682007 seconds\n",
      "Step 11600, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 658.7352364063263 seconds\n",
      "Step 11700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 664.3619492053986 seconds\n",
      "Step 11800, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 670.7601130008698 seconds\n",
      "Step 11900, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 676.3865756988525 seconds\n",
      "Step 12000, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 682.0075442790985 seconds\n",
      "Step 12100, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 687.7606558799744 seconds\n",
      "Step 12200, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 693.3817830085754 seconds\n",
      "Step 12300, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 699.1449429988861 seconds\n",
      "Step 12400, loss: tensor(0.0533, grad_fn=<SubBackward0>), time elapsed: 704.8046991825104 seconds\n",
      "Step 12500, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 710.4418804645538 seconds\n",
      "Step 12600, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 716.2603561878204 seconds\n",
      "Step 12700, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 721.9607198238373 seconds\n",
      "Step 12800, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 727.7968726158142 seconds\n",
      "Step 12900, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 733.7320804595947 seconds\n",
      "Step 13000, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 739.630802154541 seconds\n",
      "Step 13100, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 745.3313539028168 seconds\n",
      "Step 13200, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 751.6402590274811 seconds\n",
      "Step 13300, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 757.4940614700317 seconds\n",
      "Step 13400, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 763.1889896392822 seconds\n",
      "Step 13500, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 769.0601093769073 seconds\n",
      "Step 13600, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 774.7170853614807 seconds\n",
      "Step 13700, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 780.3799502849579 seconds\n",
      "Step 13800, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 786.2182383537292 seconds\n",
      "Step 13900, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 791.9325187206268 seconds\n",
      "Step 14000, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 798.0648708343506 seconds\n",
      "Step 14100, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 803.8085134029388 seconds\n",
      "Step 14200, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 809.5701491832733 seconds\n",
      "Step 14300, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 815.2806940078735 seconds\n",
      "Step 14400, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 820.8605124950409 seconds\n",
      "Step 14500, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 826.596892118454 seconds\n",
      "Step 14600, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 832.5196936130524 seconds\n",
      "Step 14700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 838.2242531776428 seconds\n",
      "Step 14800, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 843.88321352005 seconds\n",
      "Step 14900, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 849.4241764545441 seconds\n",
      "Step 15000, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 855.1978225708008 seconds\n",
      "Step 15100, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 860.8924238681793 seconds\n",
      "Step 15200, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 866.610265493393 seconds\n",
      "Step 15300, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 872.3215978145599 seconds\n",
      "Step 15400, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 877.9235453605652 seconds\n",
      "Step 15500, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 883.6518213748932 seconds\n",
      "Step 15600, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 889.365291595459 seconds\n",
      "Step 15700, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 895.1727449893951 seconds\n",
      "Step 15800, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 900.726633310318 seconds\n",
      "Step 15900, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 906.5364263057709 seconds\n",
      "Step 16000, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 912.113046169281 seconds\n",
      "Step 16100, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 918.3234853744507 seconds\n",
      "Step 16200, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 924.0252883434296 seconds\n",
      "Step 16300, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 929.5860455036163 seconds\n",
      "Step 16400, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 935.2930870056152 seconds\n",
      "Step 16500, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 940.861466884613 seconds\n",
      "Step 16600, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 946.5521433353424 seconds\n",
      "Step 16700, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 952.3761126995087 seconds\n",
      "Step 16800, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 957.9234426021576 seconds\n",
      "Step 16900, loss: tensor(0.0538, grad_fn=<SubBackward0>), time elapsed: 963.9816210269928 seconds\n",
      "Step 17000, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 969.5239350795746 seconds\n",
      "Step 17100, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 975.2326884269714 seconds\n",
      "Step 17200, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 980.8031227588654 seconds\n",
      "Step 17300, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 986.3434529304504 seconds\n",
      "Step 17400, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 992.0465984344482 seconds\n",
      "Step 17500, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 997.6012053489685 seconds\n",
      "Step 17600, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 1003.3417098522186 seconds\n",
      "Step 17700, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 1008.9661452770233 seconds\n",
      "Step 17800, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 1015.2822494506836 seconds\n",
      "Step 17900, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 1021.0028312206268 seconds\n",
      "Step 18000, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1026.5667371749878 seconds\n",
      "Step 18100, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 1032.2854351997375 seconds\n",
      "Step 18200, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 1037.8567569255829 seconds\n",
      "Step 18300, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 1043.421864748001 seconds\n",
      "Step 18400, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 1049.1612088680267 seconds\n",
      "Step 18500, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 1054.7826571464539 seconds\n",
      "Step 18600, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 1060.4891011714935 seconds\n",
      "Step 18700, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 1066.0472767353058 seconds\n",
      "Step 18800, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 1071.7778539657593 seconds\n",
      "Step 18900, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 1077.310611963272 seconds\n",
      "Step 19000, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 1082.8800299167633 seconds\n",
      "Step 19100, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 1088.5931570529938 seconds\n",
      "Step 19200, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 1094.1426129341125 seconds\n",
      "Step 19300, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1099.8860943317413 seconds\n",
      "Step 19400, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 1105.4398539066315 seconds\n",
      "Step 19500, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 1111.002469778061 seconds\n",
      "Step 19600, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1116.7968497276306 seconds\n",
      "Step 19700, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 1123.875884771347 seconds\n",
      "Step 19800, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 1129.6128253936768 seconds\n",
      "Step 19900, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 1135.1553122997284 seconds\n",
      "Step 20000, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 1140.7229056358337 seconds\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.4988, grad_fn=<SubBackward0>), time elapsed: 0.0599675178527832 seconds\n",
      "Step 100, loss: tensor(0.4451, grad_fn=<SubBackward0>), time elapsed: 5.673086404800415 seconds\n",
      "Step 200, loss: tensor(0.3738, grad_fn=<SubBackward0>), time elapsed: 11.24161696434021 seconds\n",
      "Step 300, loss: tensor(0.2956, grad_fn=<SubBackward0>), time elapsed: 16.864001750946045 seconds\n",
      "Step 400, loss: tensor(0.2498, grad_fn=<SubBackward0>), time elapsed: 22.365018844604492 seconds\n",
      "Step 500, loss: tensor(0.2220, grad_fn=<SubBackward0>), time elapsed: 28.001725912094116 seconds\n",
      "Step 600, loss: tensor(0.2042, grad_fn=<SubBackward0>), time elapsed: 33.48745512962341 seconds\n",
      "Step 700, loss: tensor(0.1913, grad_fn=<SubBackward0>), time elapsed: 39.01809763908386 seconds\n",
      "Step 800, loss: tensor(0.1802, grad_fn=<SubBackward0>), time elapsed: 44.65255284309387 seconds\n",
      "Step 900, loss: tensor(0.1721, grad_fn=<SubBackward0>), time elapsed: 50.17551612854004 seconds\n",
      "Step 1000, loss: tensor(0.1664, grad_fn=<SubBackward0>), time elapsed: 55.83338451385498 seconds\n",
      "Step 1100, loss: tensor(0.1608, grad_fn=<SubBackward0>), time elapsed: 61.375001192092896 seconds\n",
      "Step 1200, loss: tensor(0.1562, grad_fn=<SubBackward0>), time elapsed: 67.01808547973633 seconds\n",
      "Step 1300, loss: tensor(0.1519, grad_fn=<SubBackward0>), time elapsed: 72.506676197052 seconds\n",
      "Step 1400, loss: tensor(0.1496, grad_fn=<SubBackward0>), time elapsed: 78.00006413459778 seconds\n",
      "Step 1500, loss: tensor(0.1481, grad_fn=<SubBackward0>), time elapsed: 83.63940644264221 seconds\n",
      "Step 1600, loss: tensor(0.1474, grad_fn=<SubBackward0>), time elapsed: 89.16807675361633 seconds\n",
      "Step 1700, loss: tensor(0.1444, grad_fn=<SubBackward0>), time elapsed: 94.81337904930115 seconds\n",
      "Step 1800, loss: tensor(0.1396, grad_fn=<SubBackward0>), time elapsed: 100.33804321289062 seconds\n",
      "Step 1900, loss: tensor(0.1367, grad_fn=<SubBackward0>), time elapsed: 105.97847986221313 seconds\n",
      "Step 2000, loss: tensor(0.1340, grad_fn=<SubBackward0>), time elapsed: 111.47847127914429 seconds\n",
      "Step 2100, loss: tensor(0.1355, grad_fn=<SubBackward0>), time elapsed: 116.9933431148529 seconds\n",
      "Step 2200, loss: tensor(0.1292, grad_fn=<SubBackward0>), time elapsed: 122.63233184814453 seconds\n",
      "Step 2300, loss: tensor(0.1278, grad_fn=<SubBackward0>), time elapsed: 128.14141654968262 seconds\n",
      "Step 2400, loss: tensor(0.1271, grad_fn=<SubBackward0>), time elapsed: 133.7721812725067 seconds\n",
      "Step 2500, loss: tensor(0.1267, grad_fn=<SubBackward0>), time elapsed: 139.28489136695862 seconds\n",
      "Step 2600, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 144.9409384727478 seconds\n",
      "Step 2700, loss: tensor(0.1192, grad_fn=<SubBackward0>), time elapsed: 150.46457171440125 seconds\n",
      "Step 2800, loss: tensor(0.1165, grad_fn=<SubBackward0>), time elapsed: 155.99446320533752 seconds\n",
      "Step 2900, loss: tensor(0.1190, grad_fn=<SubBackward0>), time elapsed: 161.61800456047058 seconds\n",
      "Step 3000, loss: tensor(0.1171, grad_fn=<SubBackward0>), time elapsed: 167.12133979797363 seconds\n",
      "Step 3100, loss: tensor(0.1149, grad_fn=<SubBackward0>), time elapsed: 172.74762296676636 seconds\n",
      "Step 3200, loss: tensor(0.1155, grad_fn=<SubBackward0>), time elapsed: 178.2496473789215 seconds\n",
      "Step 3300, loss: tensor(0.1132, grad_fn=<SubBackward0>), time elapsed: 183.78439331054688 seconds\n",
      "Step 3400, loss: tensor(0.1144, grad_fn=<SubBackward0>), time elapsed: 189.4445445537567 seconds\n",
      "Step 3500, loss: tensor(0.1141, grad_fn=<SubBackward0>), time elapsed: 194.9724669456482 seconds\n",
      "Step 3600, loss: tensor(0.1120, grad_fn=<SubBackward0>), time elapsed: 200.60442185401917 seconds\n",
      "Step 3700, loss: tensor(0.1114, grad_fn=<SubBackward0>), time elapsed: 206.11015701293945 seconds\n",
      "Step 3800, loss: tensor(0.1094, grad_fn=<SubBackward0>), time elapsed: 211.7900938987732 seconds\n",
      "Step 3900, loss: tensor(0.1118, grad_fn=<SubBackward0>), time elapsed: 217.28892970085144 seconds\n",
      "Step 4000, loss: tensor(0.1089, grad_fn=<SubBackward0>), time elapsed: 222.7831666469574 seconds\n",
      "Step 4100, loss: tensor(0.1087, grad_fn=<SubBackward0>), time elapsed: 228.40738797187805 seconds\n",
      "Step 4200, loss: tensor(0.1052, grad_fn=<SubBackward0>), time elapsed: 233.92565393447876 seconds\n",
      "Step 4300, loss: tensor(0.1025, grad_fn=<SubBackward0>), time elapsed: 239.6027045249939 seconds\n",
      "Step 4400, loss: tensor(0.1042, grad_fn=<SubBackward0>), time elapsed: 245.10628294944763 seconds\n",
      "Step 4500, loss: tensor(0.1023, grad_fn=<SubBackward0>), time elapsed: 250.7570915222168 seconds\n",
      "Step 4600, loss: tensor(0.1019, grad_fn=<SubBackward0>), time elapsed: 256.25228929519653 seconds\n",
      "Step 4700, loss: tensor(0.0986, grad_fn=<SubBackward0>), time elapsed: 261.7512278556824 seconds\n",
      "Step 4800, loss: tensor(0.0974, grad_fn=<SubBackward0>), time elapsed: 267.3765971660614 seconds\n",
      "Step 4900, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 272.9295725822449 seconds\n",
      "Step 5000, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 278.5631742477417 seconds\n",
      "Step 5100, loss: tensor(0.0956, grad_fn=<SubBackward0>), time elapsed: 284.0665936470032 seconds\n",
      "Step 5200, loss: tensor(0.0911, grad_fn=<SubBackward0>), time elapsed: 289.5810661315918 seconds\n",
      "Step 5300, loss: tensor(0.0850, grad_fn=<SubBackward0>), time elapsed: 295.242205619812 seconds\n",
      "Step 5400, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 300.7599060535431 seconds\n",
      "Step 5500, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 306.3815643787384 seconds\n",
      "Step 5600, loss: tensor(0.0856, grad_fn=<SubBackward0>), time elapsed: 311.87810349464417 seconds\n",
      "Step 5700, loss: tensor(0.0859, grad_fn=<SubBackward0>), time elapsed: 317.54471015930176 seconds\n",
      "Step 5800, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 323.0485098361969 seconds\n",
      "Step 5900, loss: tensor(0.0834, grad_fn=<SubBackward0>), time elapsed: 328.5654036998749 seconds\n",
      "Step 6000, loss: tensor(0.0827, grad_fn=<SubBackward0>), time elapsed: 334.244802236557 seconds\n",
      "Step 6100, loss: tensor(0.0815, grad_fn=<SubBackward0>), time elapsed: 339.77567434310913 seconds\n",
      "Step 6200, loss: tensor(0.0796, grad_fn=<SubBackward0>), time elapsed: 345.4236104488373 seconds\n",
      "Step 6300, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 350.9368839263916 seconds\n",
      "Step 6400, loss: tensor(0.0786, grad_fn=<SubBackward0>), time elapsed: 356.57995772361755 seconds\n",
      "Step 6500, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 362.0937993526459 seconds\n",
      "Step 6600, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 367.5864565372467 seconds\n",
      "Step 6700, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 373.2098114490509 seconds\n",
      "Step 6800, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 378.7232904434204 seconds\n",
      "Step 6900, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 384.3595414161682 seconds\n",
      "Step 7000, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 389.8657877445221 seconds\n",
      "Step 7100, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 395.40313720703125 seconds\n",
      "Step 7200, loss: tensor(0.0692, grad_fn=<SubBackward0>), time elapsed: 401.03588032722473 seconds\n",
      "Step 7300, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 406.54494428634644 seconds\n",
      "Step 7400, loss: tensor(0.0683, grad_fn=<SubBackward0>), time elapsed: 412.19260907173157 seconds\n",
      "Step 7500, loss: tensor(0.0698, grad_fn=<SubBackward0>), time elapsed: 417.67816519737244 seconds\n",
      "Step 7600, loss: tensor(0.0708, grad_fn=<SubBackward0>), time elapsed: 423.3643014431 seconds\n",
      "Step 7700, loss: tensor(0.0667, grad_fn=<SubBackward0>), time elapsed: 428.8981611728668 seconds\n",
      "Step 7800, loss: tensor(0.0668, grad_fn=<SubBackward0>), time elapsed: 434.4219686985016 seconds\n",
      "Step 7900, loss: tensor(0.0664, grad_fn=<SubBackward0>), time elapsed: 440.07550382614136 seconds\n",
      "Step 8000, loss: tensor(0.0687, grad_fn=<SubBackward0>), time elapsed: 445.5874009132385 seconds\n",
      "Step 8100, loss: tensor(0.0641, grad_fn=<SubBackward0>), time elapsed: 451.2412705421448 seconds\n",
      "Step 8200, loss: tensor(0.0663, grad_fn=<SubBackward0>), time elapsed: 456.7759494781494 seconds\n",
      "Step 8300, loss: tensor(0.0611, grad_fn=<SubBackward0>), time elapsed: 462.4183371067047 seconds\n",
      "Step 8400, loss: tensor(0.0625, grad_fn=<SubBackward0>), time elapsed: 467.9539477825165 seconds\n",
      "Step 8500, loss: tensor(0.0630, grad_fn=<SubBackward0>), time elapsed: 473.5014202594757 seconds\n",
      "Step 8600, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 479.1730053424835 seconds\n",
      "Step 8700, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 484.7386963367462 seconds\n",
      "Step 8800, loss: tensor(0.0602, grad_fn=<SubBackward0>), time elapsed: 490.38742303848267 seconds\n",
      "Step 8900, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 495.890825510025 seconds\n",
      "Step 9000, loss: tensor(0.0603, grad_fn=<SubBackward0>), time elapsed: 501.3968241214752 seconds\n",
      "Step 9100, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 507.034375667572 seconds\n",
      "Step 9200, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 512.5718734264374 seconds\n",
      "Step 9300, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 518.2810888290405 seconds\n",
      "Step 9400, loss: tensor(0.0550, grad_fn=<SubBackward0>), time elapsed: 523.8460538387299 seconds\n",
      "Step 9500, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 529.6781101226807 seconds\n",
      "Step 9600, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 535.1822938919067 seconds\n",
      "Step 9700, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 540.8322486877441 seconds\n",
      "Step 9800, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 546.6477773189545 seconds\n",
      "Step 9900, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 552.2231426239014 seconds\n",
      "Step 10000, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 557.9065403938293 seconds\n",
      "Step 10100, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 563.439530134201 seconds\n",
      "Step 10200, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 568.9841668605804 seconds\n",
      "Step 10300, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 574.7065677642822 seconds\n",
      "Step 10400, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 580.243301153183 seconds\n",
      "Step 10500, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 586.1811199188232 seconds\n",
      "Step 10600, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 591.7654511928558 seconds\n",
      "Step 10700, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 597.4948587417603 seconds\n",
      "Step 10800, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 603.0189030170441 seconds\n",
      "Step 10900, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 608.5819706916809 seconds\n",
      "Step 11000, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 614.2862317562103 seconds\n",
      "Step 11100, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 619.8410928249359 seconds\n",
      "Step 11200, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 625.5774357318878 seconds\n",
      "Step 11300, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 631.1902642250061 seconds\n",
      "Step 11400, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 637.24422955513 seconds\n",
      "Step 11500, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 642.9186019897461 seconds\n",
      "Step 11600, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 648.453046798706 seconds\n",
      "Step 11700, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 654.1252748966217 seconds\n",
      "Step 11800, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 659.661972284317 seconds\n",
      "Step 11900, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 665.3366413116455 seconds\n",
      "Step 12000, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 670.8865487575531 seconds\n",
      "Step 12100, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 676.3966162204742 seconds\n",
      "Step 12200, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 683.5611908435822 seconds\n",
      "Step 12300, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 689.0823228359222 seconds\n",
      "Step 12400, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 694.7885458469391 seconds\n",
      "Step 12500, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 700.3312077522278 seconds\n",
      "Step 12600, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 705.8883099555969 seconds\n",
      "Step 12700, loss: tensor(0.0490, grad_fn=<SubBackward0>), time elapsed: 711.5646901130676 seconds\n",
      "Step 12800, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 717.0920596122742 seconds\n",
      "Step 12900, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 723.0514805316925 seconds\n",
      "Step 13000, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 728.6466352939606 seconds\n",
      "Step 13100, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 734.3439521789551 seconds\n",
      "Step 13200, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 739.8784718513489 seconds\n",
      "Step 13300, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 745.4277634620667 seconds\n",
      "Step 13400, loss: tensor(0.0490, grad_fn=<SubBackward0>), time elapsed: 751.129613161087 seconds\n",
      "Step 13500, loss: tensor(0.0470, grad_fn=<SubBackward0>), time elapsed: 756.6651866436005 seconds\n",
      "Step 13600, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 762.350359916687 seconds\n",
      "Step 13700, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 768.2872657775879 seconds\n",
      "Step 13800, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 773.8382461071014 seconds\n",
      "Step 13900, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 779.5263504981995 seconds\n",
      "Step 14000, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 785.0602910518646 seconds\n",
      "Step 14100, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 790.748804807663 seconds\n",
      "Step 14200, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 796.2898495197296 seconds\n",
      "Step 14300, loss: tensor(0.0465, grad_fn=<SubBackward0>), time elapsed: 801.9644291400909 seconds\n",
      "Step 14400, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 807.4875628948212 seconds\n",
      "Step 14500, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 813.0062470436096 seconds\n",
      "Step 14600, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 818.7102348804474 seconds\n",
      "Step 14700, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 824.2460877895355 seconds\n",
      "Step 14800, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 829.9279897212982 seconds\n",
      "Step 14900, loss: tensor(0.0476, grad_fn=<SubBackward0>), time elapsed: 835.482593536377 seconds\n",
      "Step 15000, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 841.028082370758 seconds\n",
      "Step 15100, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 846.7138230800629 seconds\n",
      "Step 15200, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 852.2825059890747 seconds\n",
      "Step 15300, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 857.9814918041229 seconds\n",
      "Step 15400, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 863.5412068367004 seconds\n",
      "Step 15500, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 869.2450213432312 seconds\n",
      "Step 15600, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 874.8102378845215 seconds\n",
      "Step 15700, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 880.3904340267181 seconds\n",
      "Step 15800, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 886.1103668212891 seconds\n",
      "Step 15900, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 891.640810251236 seconds\n",
      "Step 16000, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 897.3554458618164 seconds\n",
      "Step 16100, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 902.890917301178 seconds\n",
      "Step 16200, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 908.4556124210358 seconds\n",
      "Step 16300, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 914.1698575019836 seconds\n",
      "Step 16400, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 919.7221763134003 seconds\n",
      "Step 16500, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 925.4267356395721 seconds\n",
      "Step 16600, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 930.9922246932983 seconds\n",
      "Step 16700, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 936.6995394229889 seconds\n",
      "Step 16800, loss: tensor(0.0474, grad_fn=<SubBackward0>), time elapsed: 942.26478099823 seconds\n",
      "Step 16900, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 948.070100069046 seconds\n",
      "Step 17000, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 953.7785315513611 seconds\n",
      "Step 17100, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 959.3301241397858 seconds\n",
      "Step 17200, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 965.0182387828827 seconds\n",
      "Step 17300, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 970.585462808609 seconds\n",
      "Step 17400, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 976.1398568153381 seconds\n",
      "Step 17500, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 981.8573267459869 seconds\n",
      "Step 17600, loss: tensor(0.0463, grad_fn=<SubBackward0>), time elapsed: 987.4308831691742 seconds\n",
      "Step 17700, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 993.1442363262177 seconds\n",
      "Step 17800, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 998.7782990932465 seconds\n",
      "Step 17900, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 1004.3922004699707 seconds\n",
      "Step 18000, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 1010.1226665973663 seconds\n",
      "Step 18100, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1015.7214493751526 seconds\n",
      "Step 18200, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1021.5352745056152 seconds\n",
      "Step 18300, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 1027.6863079071045 seconds\n",
      "Step 18400, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 1033.4458253383636 seconds\n",
      "Step 18500, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 1038.9813511371613 seconds\n",
      "Step 18600, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 1044.5038740634918 seconds\n",
      "Step 18700, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 1050.1986043453217 seconds\n",
      "Step 18800, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 1055.7481174468994 seconds\n",
      "Step 18900, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 1061.4542796611786 seconds\n",
      "Step 19000, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 1067.0076925754547 seconds\n",
      "Step 19100, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 1072.5519406795502 seconds\n",
      "Step 19200, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1078.256949186325 seconds\n",
      "Step 19300, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 1083.808467388153 seconds\n",
      "Step 19400, loss: tensor(0.0468, grad_fn=<SubBackward0>), time elapsed: 1089.5544850826263 seconds\n",
      "Step 19500, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 1095.1537899971008 seconds\n",
      "Step 19600, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 1101.854486465454 seconds\n",
      "Step 19700, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1107.5655677318573 seconds\n",
      "Step 19800, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1113.1126036643982 seconds\n",
      "Step 19900, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 1118.8268251419067 seconds\n",
      "Step 20000, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 1124.401051044464 seconds\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.7608, grad_fn=<SubBackward0>), time elapsed: 0.0603337287902832 seconds\n",
      "Step 100, loss: tensor(0.7097, grad_fn=<SubBackward0>), time elapsed: 5.879121780395508 seconds\n",
      "Step 200, loss: tensor(0.6380, grad_fn=<SubBackward0>), time elapsed: 11.447932243347168 seconds\n",
      "Step 300, loss: tensor(0.5466, grad_fn=<SubBackward0>), time elapsed: 16.93709635734558 seconds\n",
      "Step 400, loss: tensor(0.4489, grad_fn=<SubBackward0>), time elapsed: 22.574985027313232 seconds\n",
      "Step 500, loss: tensor(0.3625, grad_fn=<SubBackward0>), time elapsed: 28.082396745681763 seconds\n",
      "Step 600, loss: tensor(0.3036, grad_fn=<SubBackward0>), time elapsed: 33.736506938934326 seconds\n",
      "Step 700, loss: tensor(0.2621, grad_fn=<SubBackward0>), time elapsed: 39.241549491882324 seconds\n",
      "Step 800, loss: tensor(0.2312, grad_fn=<SubBackward0>), time elapsed: 44.88120126724243 seconds\n",
      "Step 900, loss: tensor(0.2048, grad_fn=<SubBackward0>), time elapsed: 50.38529968261719 seconds\n",
      "Step 1000, loss: tensor(0.1855, grad_fn=<SubBackward0>), time elapsed: 55.91467046737671 seconds\n",
      "Step 1100, loss: tensor(0.1687, grad_fn=<SubBackward0>), time elapsed: 61.550291776657104 seconds\n",
      "Step 1200, loss: tensor(0.1561, grad_fn=<SubBackward0>), time elapsed: 67.0875494480133 seconds\n",
      "Step 1300, loss: tensor(0.1446, grad_fn=<SubBackward0>), time elapsed: 72.74131965637207 seconds\n",
      "Step 1400, loss: tensor(0.1363, grad_fn=<SubBackward0>), time elapsed: 78.23391461372375 seconds\n",
      "Step 1500, loss: tensor(0.1338, grad_fn=<SubBackward0>), time elapsed: 83.74945855140686 seconds\n",
      "Step 1600, loss: tensor(0.1265, grad_fn=<SubBackward0>), time elapsed: 89.38594770431519 seconds\n",
      "Step 1700, loss: tensor(0.1239, grad_fn=<SubBackward0>), time elapsed: 94.94654273986816 seconds\n",
      "Step 1800, loss: tensor(0.1237, grad_fn=<SubBackward0>), time elapsed: 100.59437608718872 seconds\n",
      "Step 1900, loss: tensor(0.1207, grad_fn=<SubBackward0>), time elapsed: 106.10434412956238 seconds\n",
      "Step 2000, loss: tensor(0.1196, grad_fn=<SubBackward0>), time elapsed: 111.72513341903687 seconds\n",
      "Step 2100, loss: tensor(0.1191, grad_fn=<SubBackward0>), time elapsed: 117.22833752632141 seconds\n",
      "Step 2200, loss: tensor(0.1160, grad_fn=<SubBackward0>), time elapsed: 122.70956897735596 seconds\n",
      "Step 2300, loss: tensor(0.1175, grad_fn=<SubBackward0>), time elapsed: 128.3465437889099 seconds\n",
      "Step 2400, loss: tensor(0.1154, grad_fn=<SubBackward0>), time elapsed: 133.8225281238556 seconds\n",
      "Step 2500, loss: tensor(0.1156, grad_fn=<SubBackward0>), time elapsed: 139.45855569839478 seconds\n",
      "Step 2600, loss: tensor(0.1144, grad_fn=<SubBackward0>), time elapsed: 144.97835874557495 seconds\n",
      "Step 2700, loss: tensor(0.1159, grad_fn=<SubBackward0>), time elapsed: 150.63480591773987 seconds\n",
      "Step 2800, loss: tensor(0.1162, grad_fn=<SubBackward0>), time elapsed: 156.15611219406128 seconds\n",
      "Step 2900, loss: tensor(0.1149, grad_fn=<SubBackward0>), time elapsed: 161.65440964698792 seconds\n",
      "Step 3000, loss: tensor(0.1150, grad_fn=<SubBackward0>), time elapsed: 167.26637768745422 seconds\n",
      "Step 3100, loss: tensor(0.1139, grad_fn=<SubBackward0>), time elapsed: 172.77274107933044 seconds\n",
      "Step 3200, loss: tensor(0.1120, grad_fn=<SubBackward0>), time elapsed: 178.39787793159485 seconds\n",
      "Step 3300, loss: tensor(0.1135, grad_fn=<SubBackward0>), time elapsed: 183.91302037239075 seconds\n",
      "Step 3400, loss: tensor(0.1121, grad_fn=<SubBackward0>), time elapsed: 189.58109402656555 seconds\n",
      "Step 3500, loss: tensor(0.1138, grad_fn=<SubBackward0>), time elapsed: 195.10535264015198 seconds\n",
      "Step 3600, loss: tensor(0.1119, grad_fn=<SubBackward0>), time elapsed: 200.63884592056274 seconds\n",
      "Step 3700, loss: tensor(0.1123, grad_fn=<SubBackward0>), time elapsed: 206.3046178817749 seconds\n",
      "Step 3800, loss: tensor(0.1122, grad_fn=<SubBackward0>), time elapsed: 211.81570529937744 seconds\n",
      "Step 3900, loss: tensor(0.1120, grad_fn=<SubBackward0>), time elapsed: 217.4867389202118 seconds\n",
      "Step 4000, loss: tensor(0.1082, grad_fn=<SubBackward0>), time elapsed: 222.99141097068787 seconds\n",
      "Step 4100, loss: tensor(0.1100, grad_fn=<SubBackward0>), time elapsed: 228.51919984817505 seconds\n",
      "Step 4200, loss: tensor(0.1081, grad_fn=<SubBackward0>), time elapsed: 234.17058038711548 seconds\n",
      "Step 4300, loss: tensor(0.1074, grad_fn=<SubBackward0>), time elapsed: 239.69841647148132 seconds\n",
      "Step 4400, loss: tensor(0.1075, grad_fn=<SubBackward0>), time elapsed: 245.3645932674408 seconds\n",
      "Step 4500, loss: tensor(0.1080, grad_fn=<SubBackward0>), time elapsed: 250.9122474193573 seconds\n",
      "Step 4600, loss: tensor(0.1052, grad_fn=<SubBackward0>), time elapsed: 256.6622009277344 seconds\n",
      "Step 4700, loss: tensor(0.1060, grad_fn=<SubBackward0>), time elapsed: 262.16317343711853 seconds\n",
      "Step 4800, loss: tensor(0.1060, grad_fn=<SubBackward0>), time elapsed: 267.67206740379333 seconds\n",
      "Step 4900, loss: tensor(0.1040, grad_fn=<SubBackward0>), time elapsed: 273.32578134536743 seconds\n",
      "Step 5000, loss: tensor(0.1050, grad_fn=<SubBackward0>), time elapsed: 278.98210096359253 seconds\n",
      "Step 5100, loss: tensor(0.1009, grad_fn=<SubBackward0>), time elapsed: 284.689058303833 seconds\n",
      "Step 5200, loss: tensor(0.1028, grad_fn=<SubBackward0>), time elapsed: 290.24828815460205 seconds\n",
      "Step 5300, loss: tensor(0.1020, grad_fn=<SubBackward0>), time elapsed: 296.00524830818176 seconds\n",
      "Step 5400, loss: tensor(0.1004, grad_fn=<SubBackward0>), time elapsed: 301.8977732658386 seconds\n",
      "Step 5500, loss: tensor(0.1010, grad_fn=<SubBackward0>), time elapsed: 307.4168395996094 seconds\n",
      "Step 5600, loss: tensor(0.0986, grad_fn=<SubBackward0>), time elapsed: 313.0764813423157 seconds\n",
      "Step 5700, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 318.5865454673767 seconds\n",
      "Step 5800, loss: tensor(0.0965, grad_fn=<SubBackward0>), time elapsed: 324.3381977081299 seconds\n",
      "Step 5900, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 329.92324805259705 seconds\n",
      "Step 6000, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 335.46077966690063 seconds\n",
      "Step 6100, loss: tensor(0.0953, grad_fn=<SubBackward0>), time elapsed: 341.1923863887787 seconds\n",
      "Step 6200, loss: tensor(0.0956, grad_fn=<SubBackward0>), time elapsed: 346.74488592147827 seconds\n",
      "Step 6300, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 352.4346971511841 seconds\n",
      "Step 6400, loss: tensor(0.0947, grad_fn=<SubBackward0>), time elapsed: 358.008757352829 seconds\n",
      "Step 6500, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 363.67988109588623 seconds\n",
      "Step 6600, loss: tensor(0.0926, grad_fn=<SubBackward0>), time elapsed: 369.6839122772217 seconds\n",
      "Step 6700, loss: tensor(0.0881, grad_fn=<SubBackward0>), time elapsed: 375.2170674800873 seconds\n",
      "Step 6800, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 380.8889539241791 seconds\n",
      "Step 6900, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 386.43874168395996 seconds\n",
      "Step 7000, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 392.1860282421112 seconds\n",
      "Step 7100, loss: tensor(0.0903, grad_fn=<SubBackward0>), time elapsed: 397.7644066810608 seconds\n",
      "Step 7200, loss: tensor(0.0897, grad_fn=<SubBackward0>), time elapsed: 403.7826421260834 seconds\n",
      "Step 7300, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 409.29382252693176 seconds\n",
      "Step 7400, loss: tensor(0.0887, grad_fn=<SubBackward0>), time elapsed: 414.8438663482666 seconds\n",
      "Step 7500, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 420.53237438201904 seconds\n",
      "Step 7600, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 426.04705142974854 seconds\n",
      "Step 7700, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 432.03768968582153 seconds\n",
      "Step 7800, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 437.54259490966797 seconds\n",
      "Step 7900, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 443.1467490196228 seconds\n",
      "Step 8000, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 448.84343671798706 seconds\n",
      "Step 8100, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 454.3517143726349 seconds\n",
      "Step 8200, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 461.65694856643677 seconds\n",
      "Step 8300, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 467.16131830215454 seconds\n",
      "Step 8400, loss: tensor(0.0858, grad_fn=<SubBackward0>), time elapsed: 472.804869890213 seconds\n",
      "Step 8500, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 478.3243441581726 seconds\n",
      "Step 8600, loss: tensor(0.0856, grad_fn=<SubBackward0>), time elapsed: 483.83592915534973 seconds\n",
      "Step 8700, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 489.47475481033325 seconds\n",
      "Step 8800, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 495.0138421058655 seconds\n",
      "Step 8900, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 500.67922163009644 seconds\n",
      "Step 9000, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 506.224782705307 seconds\n",
      "Step 9100, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 511.7584102153778 seconds\n",
      "Step 9200, loss: tensor(0.0858, grad_fn=<SubBackward0>), time elapsed: 517.4158658981323 seconds\n",
      "Step 9300, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 522.9623482227325 seconds\n",
      "Step 9400, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 528.6061420440674 seconds\n",
      "Step 9500, loss: tensor(0.0856, grad_fn=<SubBackward0>), time elapsed: 534.1157503128052 seconds\n",
      "Step 9600, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 539.7808523178101 seconds\n",
      "Step 9700, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 545.2888009548187 seconds\n",
      "Step 9800, loss: tensor(0.0834, grad_fn=<SubBackward0>), time elapsed: 550.8068788051605 seconds\n",
      "Step 9900, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 556.490473985672 seconds\n",
      "Step 10000, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 562.0432133674622 seconds\n",
      "Step 10100, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 567.7386200428009 seconds\n",
      "Step 10200, loss: tensor(0.0829, grad_fn=<SubBackward0>), time elapsed: 573.2413337230682 seconds\n",
      "Step 10300, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 578.735963344574 seconds\n",
      "Step 10400, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 584.4220261573792 seconds\n",
      "Step 10500, loss: tensor(0.0838, grad_fn=<SubBackward0>), time elapsed: 589.9419734477997 seconds\n",
      "Step 10600, loss: tensor(0.0840, grad_fn=<SubBackward0>), time elapsed: 595.6065537929535 seconds\n",
      "Step 10700, loss: tensor(0.0854, grad_fn=<SubBackward0>), time elapsed: 601.1274037361145 seconds\n",
      "Step 10800, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 606.8149135112762 seconds\n",
      "Step 10900, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 612.3506281375885 seconds\n",
      "Step 11000, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 617.8863000869751 seconds\n",
      "Step 11100, loss: tensor(0.0852, grad_fn=<SubBackward0>), time elapsed: 623.5642166137695 seconds\n",
      "Step 11200, loss: tensor(0.0833, grad_fn=<SubBackward0>), time elapsed: 629.0907504558563 seconds\n",
      "Step 11300, loss: tensor(0.0839, grad_fn=<SubBackward0>), time elapsed: 634.7890930175781 seconds\n",
      "Step 11400, loss: tensor(0.0809, grad_fn=<SubBackward0>), time elapsed: 640.3459007740021 seconds\n",
      "Step 11500, loss: tensor(0.0845, grad_fn=<SubBackward0>), time elapsed: 646.2340710163116 seconds\n",
      "Step 11600, loss: tensor(0.0848, grad_fn=<SubBackward0>), time elapsed: 651.9247403144836 seconds\n",
      "Step 11700, loss: tensor(0.0837, grad_fn=<SubBackward0>), time elapsed: 657.4716284275055 seconds\n",
      "Step 11800, loss: tensor(0.0846, grad_fn=<SubBackward0>), time elapsed: 663.1652472019196 seconds\n",
      "Step 11900, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 668.6839876174927 seconds\n",
      "Step 12000, loss: tensor(0.0813, grad_fn=<SubBackward0>), time elapsed: 674.4261243343353 seconds\n",
      "Step 12100, loss: tensor(0.0826, grad_fn=<SubBackward0>), time elapsed: 679.942088842392 seconds\n",
      "Step 12200, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 686.2031238079071 seconds\n",
      "Step 12300, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 691.892386674881 seconds\n",
      "Step 12400, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 697.4202644824982 seconds\n",
      "Step 12500, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 703.091105222702 seconds\n",
      "Step 12600, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 708.6271376609802 seconds\n",
      "Step 12700, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 714.2588460445404 seconds\n",
      "Step 12800, loss: tensor(0.0819, grad_fn=<SubBackward0>), time elapsed: 720.7527046203613 seconds\n",
      "Step 12900, loss: tensor(0.0802, grad_fn=<SubBackward0>), time elapsed: 726.264258146286 seconds\n",
      "Step 13000, loss: tensor(0.0805, grad_fn=<SubBackward0>), time elapsed: 731.9313812255859 seconds\n",
      "Step 13100, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 737.490659236908 seconds\n",
      "Step 13200, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 743.1541695594788 seconds\n",
      "Step 13300, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 748.6570460796356 seconds\n",
      "Step 13400, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 754.1950271129608 seconds\n",
      "Step 13500, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 759.9047477245331 seconds\n",
      "Step 13600, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 765.4645259380341 seconds\n",
      "Step 13700, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 771.1920506954193 seconds\n",
      "Step 13800, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 776.7399156093597 seconds\n",
      "Step 13900, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 782.2929570674896 seconds\n",
      "Step 14000, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 788.0089118480682 seconds\n",
      "Step 14100, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 793.5523526668549 seconds\n",
      "Step 14200, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 799.4713568687439 seconds\n",
      "Step 14300, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 805.0211291313171 seconds\n",
      "Step 14400, loss: tensor(0.0720, grad_fn=<SubBackward0>), time elapsed: 810.8129241466522 seconds\n",
      "Step 14500, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 816.4064371585846 seconds\n",
      "Step 14600, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 822.0110766887665 seconds\n",
      "Step 14700, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 829.4356262683868 seconds\n",
      "Step 14800, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 834.9770126342773 seconds\n",
      "Step 14900, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 840.6798319816589 seconds\n",
      "Step 15000, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 846.2099134922028 seconds\n",
      "Step 15100, loss: tensor(0.0728, grad_fn=<SubBackward0>), time elapsed: 851.7436838150024 seconds\n",
      "Step 15200, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 857.438560962677 seconds\n",
      "Step 15300, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 862.9740941524506 seconds\n",
      "Step 15400, loss: tensor(0.0701, grad_fn=<SubBackward0>), time elapsed: 868.6587691307068 seconds\n",
      "Step 15500, loss: tensor(0.0721, grad_fn=<SubBackward0>), time elapsed: 874.1878924369812 seconds\n",
      "Step 15600, loss: tensor(0.0728, grad_fn=<SubBackward0>), time elapsed: 879.8690364360809 seconds\n",
      "Step 15700, loss: tensor(0.0700, grad_fn=<SubBackward0>), time elapsed: 885.392781496048 seconds\n",
      "Step 15800, loss: tensor(0.0719, grad_fn=<SubBackward0>), time elapsed: 890.940096616745 seconds\n",
      "Step 15900, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 896.6258842945099 seconds\n",
      "Step 16000, loss: tensor(0.0715, grad_fn=<SubBackward0>), time elapsed: 902.146240234375 seconds\n",
      "Step 16100, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 907.9746460914612 seconds\n",
      "Step 16200, loss: tensor(0.0719, grad_fn=<SubBackward0>), time elapsed: 913.5090169906616 seconds\n",
      "Step 16300, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 919.0538387298584 seconds\n",
      "Step 16400, loss: tensor(0.0701, grad_fn=<SubBackward0>), time elapsed: 924.7681548595428 seconds\n",
      "Step 16500, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 930.2952003479004 seconds\n",
      "Step 16600, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 936.3065390586853 seconds\n",
      "Step 16700, loss: tensor(0.0703, grad_fn=<SubBackward0>), time elapsed: 941.843498468399 seconds\n",
      "Step 16800, loss: tensor(0.0708, grad_fn=<SubBackward0>), time elapsed: 947.3911111354828 seconds\n",
      "Step 16900, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 953.111980676651 seconds\n",
      "Step 17000, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 958.6562175750732 seconds\n",
      "Step 17100, loss: tensor(0.0682, grad_fn=<SubBackward0>), time elapsed: 964.35626745224 seconds\n",
      "Step 17200, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 969.9033029079437 seconds\n",
      "Step 17300, loss: tensor(0.0701, grad_fn=<SubBackward0>), time elapsed: 975.5986108779907 seconds\n",
      "Step 17400, loss: tensor(0.0712, grad_fn=<SubBackward0>), time elapsed: 981.1729204654694 seconds\n",
      "Step 17500, loss: tensor(0.0676, grad_fn=<SubBackward0>), time elapsed: 986.7095324993134 seconds\n",
      "Step 17600, loss: tensor(0.0673, grad_fn=<SubBackward0>), time elapsed: 992.4306647777557 seconds\n",
      "Step 17700, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 997.9745721817017 seconds\n",
      "Step 17800, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 1004.3386538028717 seconds\n",
      "Step 17900, loss: tensor(0.0678, grad_fn=<SubBackward0>), time elapsed: 1009.9074931144714 seconds\n",
      "Step 18000, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 1015.4733119010925 seconds\n",
      "Step 18100, loss: tensor(0.0671, grad_fn=<SubBackward0>), time elapsed: 1021.1605265140533 seconds\n",
      "Step 18200, loss: tensor(0.0721, grad_fn=<SubBackward0>), time elapsed: 1026.6853835582733 seconds\n",
      "Step 18300, loss: tensor(0.0676, grad_fn=<SubBackward0>), time elapsed: 1032.3889744281769 seconds\n",
      "Step 18400, loss: tensor(0.0677, grad_fn=<SubBackward0>), time elapsed: 1037.9504313468933 seconds\n",
      "Step 18500, loss: tensor(0.0689, grad_fn=<SubBackward0>), time elapsed: 1043.6925644874573 seconds\n",
      "Step 18600, loss: tensor(0.0678, grad_fn=<SubBackward0>), time elapsed: 1049.242736339569 seconds\n",
      "Step 18700, loss: tensor(0.0673, grad_fn=<SubBackward0>), time elapsed: 1054.8013832569122 seconds\n",
      "Step 18800, loss: tensor(0.0666, grad_fn=<SubBackward0>), time elapsed: 1060.5497028827667 seconds\n",
      "Step 18900, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 1066.1200411319733 seconds\n",
      "Step 19000, loss: tensor(0.0651, grad_fn=<SubBackward0>), time elapsed: 1071.9036588668823 seconds\n",
      "Step 19100, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 1077.4908678531647 seconds\n",
      "Step 19200, loss: tensor(0.0624, grad_fn=<SubBackward0>), time elapsed: 1083.0265316963196 seconds\n",
      "Step 19300, loss: tensor(0.0677, grad_fn=<SubBackward0>), time elapsed: 1088.764208316803 seconds\n",
      "Step 19400, loss: tensor(0.0648, grad_fn=<SubBackward0>), time elapsed: 1094.319179058075 seconds\n",
      "Step 19500, loss: tensor(0.0657, grad_fn=<SubBackward0>), time elapsed: 1100.0799469947815 seconds\n",
      "Step 19600, loss: tensor(0.0656, grad_fn=<SubBackward0>), time elapsed: 1105.6524164676666 seconds\n",
      "Step 19700, loss: tensor(0.0643, grad_fn=<SubBackward0>), time elapsed: 1111.1856527328491 seconds\n",
      "Step 19800, loss: tensor(0.0649, grad_fn=<SubBackward0>), time elapsed: 1116.9231343269348 seconds\n",
      "Step 19900, loss: tensor(0.0681, grad_fn=<SubBackward0>), time elapsed: 1122.4857308864594 seconds\n",
      "Step 20000, loss: tensor(0.0674, grad_fn=<SubBackward0>), time elapsed: 1128.1913888454437 seconds\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.9006, grad_fn=<SubBackward0>), time elapsed: 0.0589601993560791 seconds\n",
      "Step 100, loss: tensor(0.8347, grad_fn=<SubBackward0>), time elapsed: 5.685437202453613 seconds\n",
      "Step 200, loss: tensor(0.7449, grad_fn=<SubBackward0>), time elapsed: 11.38449478149414 seconds\n",
      "Step 300, loss: tensor(0.6671, grad_fn=<SubBackward0>), time elapsed: 16.866544723510742 seconds\n",
      "Step 400, loss: tensor(0.6072, grad_fn=<SubBackward0>), time elapsed: 22.361700296401978 seconds\n",
      "Step 500, loss: tensor(0.5596, grad_fn=<SubBackward0>), time elapsed: 27.980072259902954 seconds\n",
      "Step 600, loss: tensor(0.5040, grad_fn=<SubBackward0>), time elapsed: 33.497756242752075 seconds\n",
      "Step 700, loss: tensor(0.4467, grad_fn=<SubBackward0>), time elapsed: 39.14647674560547 seconds\n",
      "Step 800, loss: tensor(0.3918, grad_fn=<SubBackward0>), time elapsed: 44.674232006073 seconds\n",
      "Step 900, loss: tensor(0.3541, grad_fn=<SubBackward0>), time elapsed: 50.32330775260925 seconds\n",
      "Step 1000, loss: tensor(0.3122, grad_fn=<SubBackward0>), time elapsed: 55.838579177856445 seconds\n",
      "Step 1100, loss: tensor(0.2707, grad_fn=<SubBackward0>), time elapsed: 61.36057424545288 seconds\n",
      "Step 1200, loss: tensor(0.2358, grad_fn=<SubBackward0>), time elapsed: 66.9978895187378 seconds\n",
      "Step 1300, loss: tensor(0.1987, grad_fn=<SubBackward0>), time elapsed: 72.52506995201111 seconds\n",
      "Step 1400, loss: tensor(0.1686, grad_fn=<SubBackward0>), time elapsed: 78.16832375526428 seconds\n",
      "Step 1500, loss: tensor(0.1536, grad_fn=<SubBackward0>), time elapsed: 83.67292618751526 seconds\n",
      "Step 1600, loss: tensor(0.1420, grad_fn=<SubBackward0>), time elapsed: 89.31725788116455 seconds\n",
      "Step 1700, loss: tensor(0.1375, grad_fn=<SubBackward0>), time elapsed: 94.81732845306396 seconds\n",
      "Step 1800, loss: tensor(0.1332, grad_fn=<SubBackward0>), time elapsed: 100.335458278656 seconds\n",
      "Step 1900, loss: tensor(0.1317, grad_fn=<SubBackward0>), time elapsed: 105.996906042099 seconds\n",
      "Step 2000, loss: tensor(0.1309, grad_fn=<SubBackward0>), time elapsed: 111.50765299797058 seconds\n",
      "Step 2100, loss: tensor(0.1293, grad_fn=<SubBackward0>), time elapsed: 117.1560845375061 seconds\n",
      "Step 2200, loss: tensor(0.1289, grad_fn=<SubBackward0>), time elapsed: 122.65601682662964 seconds\n",
      "Step 2300, loss: tensor(0.1282, grad_fn=<SubBackward0>), time elapsed: 128.15339517593384 seconds\n",
      "Step 2400, loss: tensor(0.1289, grad_fn=<SubBackward0>), time elapsed: 133.81958031654358 seconds\n",
      "Step 2500, loss: tensor(0.1280, grad_fn=<SubBackward0>), time elapsed: 139.32093024253845 seconds\n",
      "Step 2600, loss: tensor(0.1274, grad_fn=<SubBackward0>), time elapsed: 144.96576595306396 seconds\n",
      "Step 2700, loss: tensor(0.1275, grad_fn=<SubBackward0>), time elapsed: 150.4640429019928 seconds\n",
      "Step 2800, loss: tensor(0.1273, grad_fn=<SubBackward0>), time elapsed: 156.1130726337433 seconds\n",
      "Step 2900, loss: tensor(0.1274, grad_fn=<SubBackward0>), time elapsed: 161.59652972221375 seconds\n",
      "Step 3000, loss: tensor(0.1271, grad_fn=<SubBackward0>), time elapsed: 167.133873462677 seconds\n",
      "Step 3100, loss: tensor(0.1265, grad_fn=<SubBackward0>), time elapsed: 172.77470302581787 seconds\n",
      "Step 3200, loss: tensor(0.1265, grad_fn=<SubBackward0>), time elapsed: 178.2645947933197 seconds\n",
      "Step 3300, loss: tensor(0.1267, grad_fn=<SubBackward0>), time elapsed: 183.89729237556458 seconds\n",
      "Step 3400, loss: tensor(0.1261, grad_fn=<SubBackward0>), time elapsed: 189.41304278373718 seconds\n",
      "Step 3500, loss: tensor(0.1261, grad_fn=<SubBackward0>), time elapsed: 195.11370730400085 seconds\n",
      "Step 3600, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 200.6266894340515 seconds\n",
      "Step 3700, loss: tensor(0.1263, grad_fn=<SubBackward0>), time elapsed: 206.1461329460144 seconds\n",
      "Step 3800, loss: tensor(0.1263, grad_fn=<SubBackward0>), time elapsed: 211.77829670906067 seconds\n",
      "Step 3900, loss: tensor(0.1261, grad_fn=<SubBackward0>), time elapsed: 217.29269361495972 seconds\n",
      "Step 4000, loss: tensor(0.1260, grad_fn=<SubBackward0>), time elapsed: 222.93043160438538 seconds\n",
      "Step 4100, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 228.47283720970154 seconds\n",
      "Step 4200, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 233.98174214363098 seconds\n",
      "Step 4300, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 239.63052678108215 seconds\n",
      "Step 4400, loss: tensor(0.1255, grad_fn=<SubBackward0>), time elapsed: 245.14523363113403 seconds\n",
      "Step 4500, loss: tensor(0.1255, grad_fn=<SubBackward0>), time elapsed: 250.79987907409668 seconds\n",
      "Step 4600, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 256.33619022369385 seconds\n",
      "Step 4700, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 261.9691514968872 seconds\n",
      "Step 4800, loss: tensor(0.1254, grad_fn=<SubBackward0>), time elapsed: 267.46748304367065 seconds\n",
      "Step 4900, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 272.9828522205353 seconds\n",
      "Step 5000, loss: tensor(0.1255, grad_fn=<SubBackward0>), time elapsed: 278.62629985809326 seconds\n",
      "Step 5100, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 284.1362340450287 seconds\n",
      "Step 5200, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 289.79220962524414 seconds\n",
      "Step 5300, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 295.30193758010864 seconds\n",
      "Step 5400, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 300.9367835521698 seconds\n",
      "Step 5500, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 306.42805552482605 seconds\n",
      "Step 5600, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 311.9183497428894 seconds\n",
      "Step 5700, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 317.6050179004669 seconds\n",
      "Step 5800, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 323.11590027809143 seconds\n",
      "Step 5900, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 328.7794120311737 seconds\n",
      "Step 6000, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 334.2915029525757 seconds\n",
      "Step 6100, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 339.8208341598511 seconds\n",
      "Step 6200, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 345.4687931537628 seconds\n",
      "Step 6300, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 351.0031487941742 seconds\n",
      "Step 6400, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 356.61629819869995 seconds\n",
      "Step 6500, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 362.1283931732178 seconds\n",
      "Step 6600, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 367.79641008377075 seconds\n",
      "Step 6700, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 373.30381083488464 seconds\n",
      "Step 6800, loss: tensor(0.1246, grad_fn=<SubBackward0>), time elapsed: 378.8469820022583 seconds\n",
      "Step 6900, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 384.4952335357666 seconds\n",
      "Step 7000, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 390.01288890838623 seconds\n",
      "Step 7100, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 395.65164589881897 seconds\n",
      "Step 7200, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 401.136745929718 seconds\n",
      "Step 7300, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 406.7719361782074 seconds\n",
      "Step 7400, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 412.2852351665497 seconds\n",
      "Step 7500, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 417.79533195495605 seconds\n",
      "Step 7600, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 423.46334505081177 seconds\n",
      "Step 7700, loss: tensor(0.1242, grad_fn=<SubBackward0>), time elapsed: 428.9919717311859 seconds\n",
      "Step 7800, loss: tensor(0.1240, grad_fn=<SubBackward0>), time elapsed: 434.6622791290283 seconds\n",
      "Step 7900, loss: tensor(0.1240, grad_fn=<SubBackward0>), time elapsed: 440.21053767204285 seconds\n",
      "Step 8000, loss: tensor(0.1237, grad_fn=<SubBackward0>), time elapsed: 445.7121877670288 seconds\n",
      "Step 8100, loss: tensor(0.1236, grad_fn=<SubBackward0>), time elapsed: 451.38631534576416 seconds\n",
      "Step 8200, loss: tensor(0.1234, grad_fn=<SubBackward0>), time elapsed: 457.09366035461426 seconds\n",
      "Step 8300, loss: tensor(0.1233, grad_fn=<SubBackward0>), time elapsed: 462.72738432884216 seconds\n",
      "Step 8400, loss: tensor(0.1232, grad_fn=<SubBackward0>), time elapsed: 468.70948791503906 seconds\n",
      "Step 8500, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 474.36397981643677 seconds\n",
      "Step 8600, loss: tensor(0.1230, grad_fn=<SubBackward0>), time elapsed: 479.8702247142792 seconds\n",
      "Step 8700, loss: tensor(0.1228, grad_fn=<SubBackward0>), time elapsed: 485.38734459877014 seconds\n",
      "Step 8800, loss: tensor(0.1228, grad_fn=<SubBackward0>), time elapsed: 491.03122305870056 seconds\n",
      "Step 8900, loss: tensor(0.1226, grad_fn=<SubBackward0>), time elapsed: 496.57352924346924 seconds\n",
      "Step 9000, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 502.2739214897156 seconds\n",
      "Step 9100, loss: tensor(0.1224, grad_fn=<SubBackward0>), time elapsed: 507.83039140701294 seconds\n",
      "Step 9200, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 513.4098234176636 seconds\n",
      "Step 9300, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 519.2541375160217 seconds\n",
      "Step 9400, loss: tensor(0.1224, grad_fn=<SubBackward0>), time elapsed: 524.8406705856323 seconds\n",
      "Step 9500, loss: tensor(0.1223, grad_fn=<SubBackward0>), time elapsed: 530.5392289161682 seconds\n",
      "Step 9600, loss: tensor(0.1222, grad_fn=<SubBackward0>), time elapsed: 536.095682144165 seconds\n",
      "Step 9700, loss: tensor(0.1221, grad_fn=<SubBackward0>), time elapsed: 541.8161988258362 seconds\n",
      "Step 9800, loss: tensor(0.1220, grad_fn=<SubBackward0>), time elapsed: 548.6547391414642 seconds\n",
      "Step 9900, loss: tensor(0.1216, grad_fn=<SubBackward0>), time elapsed: 554.169615983963 seconds\n",
      "Step 10000, loss: tensor(0.1215, grad_fn=<SubBackward0>), time elapsed: 559.8807461261749 seconds\n",
      "Step 10100, loss: tensor(0.1214, grad_fn=<SubBackward0>), time elapsed: 565.400110244751 seconds\n",
      "Step 10200, loss: tensor(0.1210, grad_fn=<SubBackward0>), time elapsed: 571.0764985084534 seconds\n",
      "Step 10300, loss: tensor(0.1210, grad_fn=<SubBackward0>), time elapsed: 576.6062614917755 seconds\n",
      "Step 10400, loss: tensor(0.1200, grad_fn=<SubBackward0>), time elapsed: 582.1375465393066 seconds\n",
      "Step 10500, loss: tensor(0.1184, grad_fn=<SubBackward0>), time elapsed: 588.3636810779572 seconds\n",
      "Step 10600, loss: tensor(0.1122, grad_fn=<SubBackward0>), time elapsed: 593.9109020233154 seconds\n",
      "Step 10700, loss: tensor(0.1035, grad_fn=<SubBackward0>), time elapsed: 599.5703639984131 seconds\n",
      "Step 10800, loss: tensor(0.0983, grad_fn=<SubBackward0>), time elapsed: 605.0879509449005 seconds\n",
      "Step 10900, loss: tensor(0.0971, grad_fn=<SubBackward0>), time elapsed: 610.7476110458374 seconds\n",
      "Step 11000, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 616.2652924060822 seconds\n",
      "Step 11100, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 621.8000500202179 seconds\n",
      "Step 11200, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 627.4642395973206 seconds\n",
      "Step 11300, loss: tensor(0.0864, grad_fn=<SubBackward0>), time elapsed: 632.9707005023956 seconds\n",
      "Step 11400, loss: tensor(0.0834, grad_fn=<SubBackward0>), time elapsed: 638.6702828407288 seconds\n",
      "Step 11500, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 644.1937870979309 seconds\n",
      "Step 11600, loss: tensor(0.0815, grad_fn=<SubBackward0>), time elapsed: 649.7442600727081 seconds\n",
      "Step 11700, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 655.4151754379272 seconds\n",
      "Step 11800, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 660.9543333053589 seconds\n",
      "Step 11900, loss: tensor(0.0793, grad_fn=<SubBackward0>), time elapsed: 666.6457672119141 seconds\n",
      "Step 12000, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 672.1971890926361 seconds\n",
      "Step 12100, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 677.8685023784637 seconds\n",
      "Step 12200, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 683.4238965511322 seconds\n",
      "Step 12300, loss: tensor(0.0785, grad_fn=<SubBackward0>), time elapsed: 688.9389963150024 seconds\n",
      "Step 12400, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 694.6184952259064 seconds\n",
      "Step 12500, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 700.1529939174652 seconds\n",
      "Step 12600, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 705.8291256427765 seconds\n",
      "Step 12700, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 711.3538262844086 seconds\n",
      "Step 12800, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 716.8952696323395 seconds\n",
      "Step 12900, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 723.9145405292511 seconds\n",
      "Step 13000, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 729.4602553844452 seconds\n",
      "Step 13100, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 735.1461985111237 seconds\n",
      "Step 13200, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 740.658447265625 seconds\n",
      "Step 13300, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 746.3378534317017 seconds\n",
      "Step 13400, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 751.9048082828522 seconds\n",
      "Step 13500, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 757.4377663135529 seconds\n",
      "Step 13600, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 763.1254546642303 seconds\n",
      "Step 13700, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 768.6440889835358 seconds\n",
      "Step 13800, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 774.3513581752777 seconds\n",
      "Step 13900, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 779.8813421726227 seconds\n",
      "Step 14000, loss: tensor(0.0730, grad_fn=<SubBackward0>), time elapsed: 785.4156968593597 seconds\n",
      "Step 14100, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 791.0927317142487 seconds\n",
      "Step 14200, loss: tensor(0.0725, grad_fn=<SubBackward0>), time elapsed: 796.6431844234467 seconds\n",
      "Step 14300, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 802.3137023448944 seconds\n",
      "Step 14400, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 807.8585379123688 seconds\n",
      "Step 14500, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 813.5299725532532 seconds\n",
      "Step 14600, loss: tensor(0.0724, grad_fn=<SubBackward0>), time elapsed: 819.0455651283264 seconds\n",
      "Step 14700, loss: tensor(0.0724, grad_fn=<SubBackward0>), time elapsed: 824.5815489292145 seconds\n",
      "Step 14800, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 830.2527358531952 seconds\n",
      "Step 14900, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 835.831887960434 seconds\n",
      "Step 15000, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 841.5136342048645 seconds\n",
      "Step 15100, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 847.0333671569824 seconds\n",
      "Step 15200, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 852.5594940185547 seconds\n",
      "Step 15300, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 858.2485692501068 seconds\n",
      "Step 15400, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 863.7578725814819 seconds\n",
      "Step 15500, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 869.442877292633 seconds\n",
      "Step 15600, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 874.9418082237244 seconds\n",
      "Step 15700, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 880.6845374107361 seconds\n",
      "Step 15800, loss: tensor(0.0714, grad_fn=<SubBackward0>), time elapsed: 886.2071025371552 seconds\n",
      "Step 15900, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 891.7472791671753 seconds\n",
      "Step 16000, loss: tensor(0.0726, grad_fn=<SubBackward0>), time elapsed: 897.4452726840973 seconds\n",
      "Step 16100, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 902.9996609687805 seconds\n",
      "Step 16200, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 908.6851408481598 seconds\n",
      "Step 16300, loss: tensor(0.0717, grad_fn=<SubBackward0>), time elapsed: 914.2064793109894 seconds\n",
      "Step 16400, loss: tensor(0.0709, grad_fn=<SubBackward0>), time elapsed: 919.7341768741608 seconds\n",
      "Step 16500, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 925.4613420963287 seconds\n",
      "Step 16600, loss: tensor(0.0717, grad_fn=<SubBackward0>), time elapsed: 931.0260698795319 seconds\n",
      "Step 16700, loss: tensor(0.0723, grad_fn=<SubBackward0>), time elapsed: 936.7593252658844 seconds\n",
      "Step 16800, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 942.3120663166046 seconds\n",
      "Step 16900, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 947.8682975769043 seconds\n",
      "Step 17000, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 953.5881226062775 seconds\n",
      "Step 17100, loss: tensor(0.0674, grad_fn=<SubBackward0>), time elapsed: 959.3082129955292 seconds\n",
      "Step 17200, loss: tensor(0.0696, grad_fn=<SubBackward0>), time elapsed: 965.0532808303833 seconds\n",
      "Step 17300, loss: tensor(0.0690, grad_fn=<SubBackward0>), time elapsed: 970.6271409988403 seconds\n",
      "Step 17400, loss: tensor(0.0703, grad_fn=<SubBackward0>), time elapsed: 977.4132900238037 seconds\n",
      "Step 17500, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 982.9929776191711 seconds\n",
      "Step 17600, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 988.5647337436676 seconds\n",
      "Step 17700, loss: tensor(0.0710, grad_fn=<SubBackward0>), time elapsed: 994.2817220687866 seconds\n",
      "Step 17800, loss: tensor(0.0702, grad_fn=<SubBackward0>), time elapsed: 999.8199117183685 seconds\n",
      "Step 17900, loss: tensor(0.0711, grad_fn=<SubBackward0>), time elapsed: 1005.5386884212494 seconds\n",
      "Step 18000, loss: tensor(0.0705, grad_fn=<SubBackward0>), time elapsed: 1011.085953950882 seconds\n",
      "Step 18100, loss: tensor(0.0704, grad_fn=<SubBackward0>), time elapsed: 1016.6277616024017 seconds\n",
      "Step 18200, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 1022.3486104011536 seconds\n",
      "Step 18300, loss: tensor(0.0702, grad_fn=<SubBackward0>), time elapsed: 1027.9026205539703 seconds\n",
      "Step 18400, loss: tensor(0.0679, grad_fn=<SubBackward0>), time elapsed: 1033.6269624233246 seconds\n",
      "Step 18500, loss: tensor(0.0694, grad_fn=<SubBackward0>), time elapsed: 1039.1865587234497 seconds\n",
      "Step 18600, loss: tensor(0.0698, grad_fn=<SubBackward0>), time elapsed: 1044.8931646347046 seconds\n",
      "Step 18700, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 1050.4910371303558 seconds\n",
      "Step 18800, loss: tensor(0.0704, grad_fn=<SubBackward0>), time elapsed: 1056.0720779895782 seconds\n",
      "Step 18900, loss: tensor(0.0703, grad_fn=<SubBackward0>), time elapsed: 1061.8197619915009 seconds\n",
      "Step 19000, loss: tensor(0.0698, grad_fn=<SubBackward0>), time elapsed: 1067.42076253891 seconds\n",
      "Step 19100, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 1073.1322212219238 seconds\n",
      "Step 19200, loss: tensor(0.0685, grad_fn=<SubBackward0>), time elapsed: 1078.6815793514252 seconds\n",
      "Step 19300, loss: tensor(0.0684, grad_fn=<SubBackward0>), time elapsed: 1084.2178654670715 seconds\n",
      "Step 19400, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 1089.947408914566 seconds\n",
      "Step 19500, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 1095.5418405532837 seconds\n",
      "Step 19600, loss: tensor(0.0702, grad_fn=<SubBackward0>), time elapsed: 1101.639924287796 seconds\n",
      "Step 19700, loss: tensor(0.0711, grad_fn=<SubBackward0>), time elapsed: 1107.1912875175476 seconds\n",
      "Step 19800, loss: tensor(0.0693, grad_fn=<SubBackward0>), time elapsed: 1112.7883338928223 seconds\n",
      "Step 19900, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 1118.4788446426392 seconds\n",
      "Step 20000, loss: tensor(0.0684, grad_fn=<SubBackward0>), time elapsed: 1124.0350768566132 seconds\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(1.0626, grad_fn=<SubBackward0>), time elapsed: 0.06366205215454102 seconds\n",
      "Step 100, loss: tensor(0.9619, grad_fn=<SubBackward0>), time elapsed: 5.856937646865845 seconds\n",
      "Step 200, loss: tensor(0.8342, grad_fn=<SubBackward0>), time elapsed: 11.411599397659302 seconds\n",
      "Step 300, loss: tensor(0.7113, grad_fn=<SubBackward0>), time elapsed: 17.04464101791382 seconds\n",
      "Step 400, loss: tensor(0.6038, grad_fn=<SubBackward0>), time elapsed: 22.575085639953613 seconds\n",
      "Step 500, loss: tensor(0.5203, grad_fn=<SubBackward0>), time elapsed: 28.08791184425354 seconds\n",
      "Step 600, loss: tensor(0.4396, grad_fn=<SubBackward0>), time elapsed: 33.69665718078613 seconds\n",
      "Step 700, loss: tensor(0.3817, grad_fn=<SubBackward0>), time elapsed: 39.20098900794983 seconds\n",
      "Step 800, loss: tensor(0.3364, grad_fn=<SubBackward0>), time elapsed: 44.82703495025635 seconds\n",
      "Step 900, loss: tensor(0.2882, grad_fn=<SubBackward0>), time elapsed: 50.357731103897095 seconds\n",
      "Step 1000, loss: tensor(0.2341, grad_fn=<SubBackward0>), time elapsed: 55.98743271827698 seconds\n",
      "Step 1100, loss: tensor(0.2076, grad_fn=<SubBackward0>), time elapsed: 61.47940421104431 seconds\n",
      "Step 1200, loss: tensor(0.1865, grad_fn=<SubBackward0>), time elapsed: 66.97989964485168 seconds\n",
      "Step 1300, loss: tensor(0.1693, grad_fn=<SubBackward0>), time elapsed: 72.60607314109802 seconds\n",
      "Step 1400, loss: tensor(0.1535, grad_fn=<SubBackward0>), time elapsed: 78.10436916351318 seconds\n",
      "Step 1500, loss: tensor(0.1438, grad_fn=<SubBackward0>), time elapsed: 83.75465273857117 seconds\n",
      "Step 1600, loss: tensor(0.1310, grad_fn=<SubBackward0>), time elapsed: 89.24928283691406 seconds\n",
      "Step 1700, loss: tensor(0.1279, grad_fn=<SubBackward0>), time elapsed: 94.87662768363953 seconds\n",
      "Step 1800, loss: tensor(0.1153, grad_fn=<SubBackward0>), time elapsed: 100.38036894798279 seconds\n",
      "Step 1900, loss: tensor(0.1126, grad_fn=<SubBackward0>), time elapsed: 105.88838601112366 seconds\n",
      "Step 2000, loss: tensor(0.1119, grad_fn=<SubBackward0>), time elapsed: 111.56647634506226 seconds\n",
      "Step 2100, loss: tensor(0.1085, grad_fn=<SubBackward0>), time elapsed: 117.06910133361816 seconds\n",
      "Step 2200, loss: tensor(0.1026, grad_fn=<SubBackward0>), time elapsed: 122.71047902107239 seconds\n",
      "Step 2300, loss: tensor(0.1028, grad_fn=<SubBackward0>), time elapsed: 128.21481943130493 seconds\n",
      "Step 2400, loss: tensor(0.0948, grad_fn=<SubBackward0>), time elapsed: 133.8704056739807 seconds\n",
      "Step 2500, loss: tensor(0.0955, grad_fn=<SubBackward0>), time elapsed: 139.3709762096405 seconds\n",
      "Step 2600, loss: tensor(0.0928, grad_fn=<SubBackward0>), time elapsed: 144.91666746139526 seconds\n",
      "Step 2700, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 150.60518884658813 seconds\n",
      "Step 2800, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 156.13659811019897 seconds\n",
      "Step 2900, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 161.77836179733276 seconds\n",
      "Step 3000, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 167.29023385047913 seconds\n",
      "Step 3100, loss: tensor(0.0854, grad_fn=<SubBackward0>), time elapsed: 172.81529355049133 seconds\n",
      "Step 3200, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 178.4465720653534 seconds\n",
      "Step 3300, loss: tensor(0.0853, grad_fn=<SubBackward0>), time elapsed: 183.94220399856567 seconds\n",
      "Step 3400, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 189.59465646743774 seconds\n",
      "Step 3500, loss: tensor(0.0828, grad_fn=<SubBackward0>), time elapsed: 195.1121735572815 seconds\n",
      "Step 3600, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 200.76309466362 seconds\n",
      "Step 3700, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 206.274995803833 seconds\n",
      "Step 3800, loss: tensor(0.0796, grad_fn=<SubBackward0>), time elapsed: 211.77875804901123 seconds\n",
      "Step 3900, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 217.4238748550415 seconds\n",
      "Step 4000, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 222.9165494441986 seconds\n",
      "Step 4100, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 228.56310725212097 seconds\n",
      "Step 4200, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 234.08530378341675 seconds\n",
      "Step 4300, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 239.74097442626953 seconds\n",
      "Step 4400, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 245.24835348129272 seconds\n",
      "Step 4500, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 250.7830512523651 seconds\n",
      "Step 4600, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 256.4469828605652 seconds\n",
      "Step 4700, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 261.9753084182739 seconds\n",
      "Step 4800, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 267.62809896469116 seconds\n",
      "Step 4900, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 274.2188847064972 seconds\n",
      "Step 5000, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 280.2834663391113 seconds\n",
      "Step 5100, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 285.96069383621216 seconds\n",
      "Step 5200, loss: tensor(0.0716, grad_fn=<SubBackward0>), time elapsed: 291.5083785057068 seconds\n",
      "Step 5300, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 297.1924498081207 seconds\n",
      "Step 5400, loss: tensor(0.0705, grad_fn=<SubBackward0>), time elapsed: 302.708922624588 seconds\n",
      "Step 5500, loss: tensor(0.0686, grad_fn=<SubBackward0>), time elapsed: 308.37460803985596 seconds\n",
      "Step 5600, loss: tensor(0.0691, grad_fn=<SubBackward0>), time elapsed: 314.07137966156006 seconds\n",
      "Step 5700, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 319.5717968940735 seconds\n",
      "Step 5800, loss: tensor(0.0674, grad_fn=<SubBackward0>), time elapsed: 325.2573926448822 seconds\n",
      "Step 5900, loss: tensor(0.0694, grad_fn=<SubBackward0>), time elapsed: 330.7734658718109 seconds\n",
      "Step 6000, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 336.4769811630249 seconds\n",
      "Step 6100, loss: tensor(0.0656, grad_fn=<SubBackward0>), time elapsed: 342.21183943748474 seconds\n",
      "Step 6200, loss: tensor(0.0646, grad_fn=<SubBackward0>), time elapsed: 347.86743330955505 seconds\n",
      "Step 6300, loss: tensor(0.0646, grad_fn=<SubBackward0>), time elapsed: 353.395299911499 seconds\n",
      "Step 6400, loss: tensor(0.0657, grad_fn=<SubBackward0>), time elapsed: 358.9256327152252 seconds\n",
      "Step 6500, loss: tensor(0.0642, grad_fn=<SubBackward0>), time elapsed: 364.60861325263977 seconds\n",
      "Step 6600, loss: tensor(0.0645, grad_fn=<SubBackward0>), time elapsed: 370.14488339424133 seconds\n",
      "Step 6700, loss: tensor(0.0651, grad_fn=<SubBackward0>), time elapsed: 375.83851528167725 seconds\n",
      "Step 6800, loss: tensor(0.0622, grad_fn=<SubBackward0>), time elapsed: 381.4244980812073 seconds\n",
      "Step 6900, loss: tensor(0.0619, grad_fn=<SubBackward0>), time elapsed: 387.1333546638489 seconds\n",
      "Step 7000, loss: tensor(0.0641, grad_fn=<SubBackward0>), time elapsed: 393.29481649398804 seconds\n",
      "Step 7100, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 398.809024810791 seconds\n",
      "Step 7200, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 404.46404457092285 seconds\n",
      "Step 7300, loss: tensor(0.0610, grad_fn=<SubBackward0>), time elapsed: 409.9781596660614 seconds\n",
      "Step 7400, loss: tensor(0.0622, grad_fn=<SubBackward0>), time elapsed: 415.69148349761963 seconds\n",
      "Step 7500, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 421.2323360443115 seconds\n",
      "Step 7600, loss: tensor(0.0627, grad_fn=<SubBackward0>), time elapsed: 426.77100563049316 seconds\n",
      "Step 7700, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 432.56025195121765 seconds\n",
      "Step 7800, loss: tensor(0.0637, grad_fn=<SubBackward0>), time elapsed: 438.29197359085083 seconds\n",
      "Step 7900, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 443.95631980895996 seconds\n",
      "Step 8000, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 449.4931983947754 seconds\n",
      "Step 8100, loss: tensor(0.0627, grad_fn=<SubBackward0>), time elapsed: 455.19588804244995 seconds\n",
      "Step 8200, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 460.74261808395386 seconds\n",
      "Step 8300, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 466.25044417381287 seconds\n",
      "Step 8400, loss: tensor(0.0621, grad_fn=<SubBackward0>), time elapsed: 471.9205038547516 seconds\n",
      "Step 8500, loss: tensor(0.0615, grad_fn=<SubBackward0>), time elapsed: 477.75867533683777 seconds\n",
      "Step 8600, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 483.4395709037781 seconds\n",
      "Step 8700, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 489.0100202560425 seconds\n",
      "Step 8800, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 494.53993225097656 seconds\n",
      "Step 8900, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 500.6031105518341 seconds\n",
      "Step 9000, loss: tensor(0.0602, grad_fn=<SubBackward0>), time elapsed: 506.1539583206177 seconds\n",
      "Step 9100, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 511.8027799129486 seconds\n",
      "Step 9200, loss: tensor(0.0592, grad_fn=<SubBackward0>), time elapsed: 517.3128833770752 seconds\n",
      "Step 9300, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 523.0170311927795 seconds\n",
      "Step 9400, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 528.5270364284515 seconds\n",
      "Step 9500, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 534.0412495136261 seconds\n",
      "Step 9600, loss: tensor(0.0600, grad_fn=<SubBackward0>), time elapsed: 539.7478048801422 seconds\n",
      "Step 9700, loss: tensor(0.0606, grad_fn=<SubBackward0>), time elapsed: 546.0374310016632 seconds\n",
      "Step 9800, loss: tensor(0.0612, grad_fn=<SubBackward0>), time elapsed: 551.7143807411194 seconds\n",
      "Step 9900, loss: tensor(0.0609, grad_fn=<SubBackward0>), time elapsed: 557.2383823394775 seconds\n",
      "Step 10000, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 562.759290933609 seconds\n",
      "Step 10100, loss: tensor(0.0576, grad_fn=<SubBackward0>), time elapsed: 568.4417607784271 seconds\n",
      "Step 10200, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 573.9622147083282 seconds\n",
      "Step 10300, loss: tensor(0.0620, grad_fn=<SubBackward0>), time elapsed: 579.6428940296173 seconds\n",
      "Step 10400, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 585.1805417537689 seconds\n",
      "Step 10500, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 590.870744228363 seconds\n",
      "Step 10600, loss: tensor(0.0600, grad_fn=<SubBackward0>), time elapsed: 596.4013757705688 seconds\n",
      "Step 10700, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 601.9948146343231 seconds\n",
      "Step 10800, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 607.7336113452911 seconds\n",
      "Step 10900, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 613.6002259254456 seconds\n",
      "Step 11000, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 619.340677022934 seconds\n",
      "Step 11100, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 624.9154813289642 seconds\n",
      "Step 11200, loss: tensor(0.0603, grad_fn=<SubBackward0>), time elapsed: 630.4546291828156 seconds\n",
      "Step 11300, loss: tensor(0.0604, grad_fn=<SubBackward0>), time elapsed: 636.1885275840759 seconds\n",
      "Step 11400, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 642.4418745040894 seconds\n",
      "Step 11500, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 648.1489112377167 seconds\n",
      "Step 11600, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 653.654951095581 seconds\n",
      "Step 11700, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 659.3500738143921 seconds\n",
      "Step 11800, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 664.8673875331879 seconds\n",
      "Step 11900, loss: tensor(0.0591, grad_fn=<SubBackward0>), time elapsed: 670.3685851097107 seconds\n",
      "Step 12000, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 676.0357239246368 seconds\n",
      "Step 12100, loss: tensor(0.0583, grad_fn=<SubBackward0>), time elapsed: 681.5463383197784 seconds\n",
      "Step 12200, loss: tensor(0.0575, grad_fn=<SubBackward0>), time elapsed: 687.2151391506195 seconds\n",
      "Step 12300, loss: tensor(0.0572, grad_fn=<SubBackward0>), time elapsed: 692.783043384552 seconds\n",
      "Step 12400, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 698.3612751960754 seconds\n",
      "Step 12500, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 704.74485206604 seconds\n",
      "Step 12600, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 710.275310754776 seconds\n",
      "Step 12700, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 715.9433703422546 seconds\n",
      "Step 12800, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 721.4931735992432 seconds\n",
      "Step 12900, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 727.1709191799164 seconds\n",
      "Step 13000, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 732.6950812339783 seconds\n",
      "Step 13100, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 738.2257797718048 seconds\n",
      "Step 13200, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 743.9028792381287 seconds\n",
      "Step 13300, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 749.4643921852112 seconds\n",
      "Step 13400, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 755.2758159637451 seconds\n",
      "Step 13500, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 760.7962436676025 seconds\n",
      "Step 13600, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 766.9473214149475 seconds\n",
      "Step 13700, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 772.6497004032135 seconds\n",
      "Step 13800, loss: tensor(0.0582, grad_fn=<SubBackward0>), time elapsed: 778.1859471797943 seconds\n",
      "Step 13900, loss: tensor(0.0593, grad_fn=<SubBackward0>), time elapsed: 783.8866443634033 seconds\n",
      "Step 14000, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 789.4202265739441 seconds\n",
      "Step 14100, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 795.0998258590698 seconds\n",
      "Step 14200, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 800.6420660018921 seconds\n",
      "Step 14300, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 806.1751415729523 seconds\n",
      "Step 14400, loss: tensor(0.0568, grad_fn=<SubBackward0>), time elapsed: 811.8887858390808 seconds\n",
      "Step 14500, loss: tensor(0.0606, grad_fn=<SubBackward0>), time elapsed: 817.433794260025 seconds\n",
      "Step 14600, loss: tensor(0.0593, grad_fn=<SubBackward0>), time elapsed: 823.135577917099 seconds\n",
      "Step 14700, loss: tensor(0.0592, grad_fn=<SubBackward0>), time elapsed: 828.6737494468689 seconds\n",
      "Step 14800, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 834.1995713710785 seconds\n",
      "Step 14900, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 839.882609128952 seconds\n",
      "Step 15000, loss: tensor(0.0572, grad_fn=<SubBackward0>), time elapsed: 845.4597582817078 seconds\n",
      "Step 15100, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 851.1474885940552 seconds\n",
      "Step 15200, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 856.6972243785858 seconds\n",
      "Step 15300, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 862.4240198135376 seconds\n",
      "Step 15400, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 867.9753406047821 seconds\n",
      "Step 15500, loss: tensor(0.0581, grad_fn=<SubBackward0>), time elapsed: 873.5395841598511 seconds\n",
      "Step 15600, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 879.2479798793793 seconds\n",
      "Step 15700, loss: tensor(0.0592, grad_fn=<SubBackward0>), time elapsed: 885.6220850944519 seconds\n",
      "Step 15800, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 891.329377412796 seconds\n",
      "Step 15900, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 896.8506269454956 seconds\n",
      "Step 16000, loss: tensor(0.0593, grad_fn=<SubBackward0>), time elapsed: 902.4009482860565 seconds\n",
      "Step 16100, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 908.0831904411316 seconds\n",
      "Step 16200, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 913.6168220043182 seconds\n",
      "Step 16300, loss: tensor(0.0590, grad_fn=<SubBackward0>), time elapsed: 919.3100171089172 seconds\n",
      "Step 16400, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 924.8614883422852 seconds\n",
      "Step 16500, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 931.3418016433716 seconds\n",
      "Step 16600, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 936.8931241035461 seconds\n",
      "Step 16700, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 942.4242446422577 seconds\n",
      "Step 16800, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 948.0972962379456 seconds\n",
      "Step 16900, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 953.6799824237823 seconds\n",
      "Step 17000, loss: tensor(0.0563, grad_fn=<SubBackward0>), time elapsed: 959.3921272754669 seconds\n",
      "Step 17100, loss: tensor(0.0557, grad_fn=<SubBackward0>), time elapsed: 964.9774451255798 seconds\n",
      "Step 17200, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 970.5394623279572 seconds\n",
      "Step 17300, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 976.2372596263885 seconds\n",
      "Step 17400, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 981.7772860527039 seconds\n",
      "Step 17500, loss: tensor(0.0573, grad_fn=<SubBackward0>), time elapsed: 987.517288684845 seconds\n",
      "Step 17600, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 993.8112921714783 seconds\n",
      "Step 17700, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 999.3656537532806 seconds\n",
      "Step 17800, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 1005.0421707630157 seconds\n",
      "Step 17900, loss: tensor(0.0582, grad_fn=<SubBackward0>), time elapsed: 1010.5778560638428 seconds\n",
      "Step 18000, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 1016.2797529697418 seconds\n",
      "Step 18100, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 1021.8152439594269 seconds\n",
      "Step 18200, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 1027.5337193012238 seconds\n",
      "Step 18300, loss: tensor(0.0583, grad_fn=<SubBackward0>), time elapsed: 1033.0899305343628 seconds\n",
      "Step 18400, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 1038.6342799663544 seconds\n",
      "Step 18500, loss: tensor(0.0591, grad_fn=<SubBackward0>), time elapsed: 1044.3804759979248 seconds\n",
      "Step 18600, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 1049.9277963638306 seconds\n",
      "Step 18700, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 1055.8949303627014 seconds\n",
      "Step 18800, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 1061.4664688110352 seconds\n",
      "Step 18900, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 1067.0357646942139 seconds\n",
      "Step 19000, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 1072.9355251789093 seconds\n",
      "Step 19100, loss: tensor(0.0565, grad_fn=<SubBackward0>), time elapsed: 1079.3397300243378 seconds\n",
      "Step 19200, loss: tensor(0.0559, grad_fn=<SubBackward0>), time elapsed: 1085.1037938594818 seconds\n",
      "Step 19300, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 1090.6914727687836 seconds\n",
      "Step 19400, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 1096.2862575054169 seconds\n",
      "Step 19500, loss: tensor(0.0573, grad_fn=<SubBackward0>), time elapsed: 1101.9996011257172 seconds\n",
      "Step 19600, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 1107.5420277118683 seconds\n",
      "Step 19700, loss: tensor(0.0576, grad_fn=<SubBackward0>), time elapsed: 1113.2342340946198 seconds\n",
      "Step 19800, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 1118.8107323646545 seconds\n",
      "Step 19900, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 1124.5297067165375 seconds\n",
      "Step 20000, loss: tensor(0.0585, grad_fn=<SubBackward0>), time elapsed: 1130.118370771408 seconds\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(1.1024, grad_fn=<SubBackward0>), time elapsed: 0.06127309799194336 seconds\n",
      "Step 100, loss: tensor(0.9126, grad_fn=<SubBackward0>), time elapsed: 5.6715171337127686 seconds\n",
      "Step 200, loss: tensor(0.7467, grad_fn=<SubBackward0>), time elapsed: 11.347835779190063 seconds\n",
      "Step 300, loss: tensor(0.6280, grad_fn=<SubBackward0>), time elapsed: 16.84639048576355 seconds\n",
      "Step 400, loss: tensor(0.5410, grad_fn=<SubBackward0>), time elapsed: 22.477572679519653 seconds\n",
      "Step 500, loss: tensor(0.4670, grad_fn=<SubBackward0>), time elapsed: 27.987683057785034 seconds\n",
      "Step 600, loss: tensor(0.4107, grad_fn=<SubBackward0>), time elapsed: 33.626829385757446 seconds\n",
      "Step 700, loss: tensor(0.3521, grad_fn=<SubBackward0>), time elapsed: 39.13586783409119 seconds\n",
      "Step 800, loss: tensor(0.3067, grad_fn=<SubBackward0>), time elapsed: 44.66277599334717 seconds\n",
      "Step 900, loss: tensor(0.2615, grad_fn=<SubBackward0>), time elapsed: 50.305768728256226 seconds\n",
      "Step 1000, loss: tensor(0.2271, grad_fn=<SubBackward0>), time elapsed: 55.83565640449524 seconds\n",
      "Step 1100, loss: tensor(0.1972, grad_fn=<SubBackward0>), time elapsed: 61.46279859542847 seconds\n",
      "Step 1200, loss: tensor(0.1750, grad_fn=<SubBackward0>), time elapsed: 66.94369196891785 seconds\n",
      "Step 1300, loss: tensor(0.1598, grad_fn=<SubBackward0>), time elapsed: 72.5898232460022 seconds\n",
      "Step 1400, loss: tensor(0.1461, grad_fn=<SubBackward0>), time elapsed: 78.0912024974823 seconds\n",
      "Step 1500, loss: tensor(0.1366, grad_fn=<SubBackward0>), time elapsed: 83.63745903968811 seconds\n",
      "Step 1600, loss: tensor(0.1291, grad_fn=<SubBackward0>), time elapsed: 89.30206680297852 seconds\n",
      "Step 1700, loss: tensor(0.1229, grad_fn=<SubBackward0>), time elapsed: 94.81974005699158 seconds\n",
      "Step 1800, loss: tensor(0.1180, grad_fn=<SubBackward0>), time elapsed: 100.48179030418396 seconds\n",
      "Step 1900, loss: tensor(0.1143, grad_fn=<SubBackward0>), time elapsed: 105.9915189743042 seconds\n",
      "Step 2000, loss: tensor(0.1073, grad_fn=<SubBackward0>), time elapsed: 111.54001212120056 seconds\n",
      "Step 2100, loss: tensor(0.1048, grad_fn=<SubBackward0>), time elapsed: 117.1845166683197 seconds\n",
      "Step 2200, loss: tensor(0.1032, grad_fn=<SubBackward0>), time elapsed: 122.70169949531555 seconds\n",
      "Step 2300, loss: tensor(0.0996, grad_fn=<SubBackward0>), time elapsed: 128.340900182724 seconds\n",
      "Step 2400, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 133.87253427505493 seconds\n",
      "Step 2500, loss: tensor(0.0942, grad_fn=<SubBackward0>), time elapsed: 139.52904963493347 seconds\n",
      "Step 2600, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 145.0523386001587 seconds\n",
      "Step 2700, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 150.55467414855957 seconds\n",
      "Step 2800, loss: tensor(0.0886, grad_fn=<SubBackward0>), time elapsed: 156.1949806213379 seconds\n",
      "Step 2900, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 161.69279026985168 seconds\n",
      "Step 3000, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 167.33424973487854 seconds\n",
      "Step 3100, loss: tensor(0.0854, grad_fn=<SubBackward0>), time elapsed: 172.84259819984436 seconds\n",
      "Step 3200, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 178.50521302223206 seconds\n",
      "Step 3300, loss: tensor(0.0829, grad_fn=<SubBackward0>), time elapsed: 184.0066740512848 seconds\n",
      "Step 3400, loss: tensor(0.0816, grad_fn=<SubBackward0>), time elapsed: 189.51614022254944 seconds\n",
      "Step 3500, loss: tensor(0.0813, grad_fn=<SubBackward0>), time elapsed: 195.14990997314453 seconds\n",
      "Step 3600, loss: tensor(0.0817, grad_fn=<SubBackward0>), time elapsed: 200.6685700416565 seconds\n",
      "Step 3700, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 206.32434606552124 seconds\n",
      "Step 3800, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 211.8214201927185 seconds\n",
      "Step 3900, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 217.45604133605957 seconds\n",
      "Step 4000, loss: tensor(0.0777, grad_fn=<SubBackward0>), time elapsed: 222.9657747745514 seconds\n",
      "Step 4100, loss: tensor(0.0788, grad_fn=<SubBackward0>), time elapsed: 228.48394179344177 seconds\n",
      "Step 4200, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 234.14177942276 seconds\n",
      "Step 4300, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 239.64850735664368 seconds\n",
      "Step 4400, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 245.29300618171692 seconds\n",
      "Step 4500, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 250.8073811531067 seconds\n",
      "Step 4600, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 256.3065378665924 seconds\n",
      "Step 4700, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 262.434285402298 seconds\n",
      "Step 4800, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 268.062415599823 seconds\n",
      "Step 4900, loss: tensor(0.0726, grad_fn=<SubBackward0>), time elapsed: 274.22010350227356 seconds\n",
      "Step 5000, loss: tensor(0.0717, grad_fn=<SubBackward0>), time elapsed: 279.7360954284668 seconds\n",
      "Step 5100, loss: tensor(0.0714, grad_fn=<SubBackward0>), time elapsed: 285.4201512336731 seconds\n",
      "Step 5200, loss: tensor(0.0716, grad_fn=<SubBackward0>), time elapsed: 290.9356656074524 seconds\n",
      "Step 5300, loss: tensor(0.0707, grad_fn=<SubBackward0>), time elapsed: 296.4886209964752 seconds\n",
      "Step 5400, loss: tensor(0.0686, grad_fn=<SubBackward0>), time elapsed: 303.0358622074127 seconds\n",
      "Step 5500, loss: tensor(0.0689, grad_fn=<SubBackward0>), time elapsed: 308.5718150138855 seconds\n",
      "Step 5600, loss: tensor(0.0671, grad_fn=<SubBackward0>), time elapsed: 314.27351546287537 seconds\n",
      "Step 5700, loss: tensor(0.0676, grad_fn=<SubBackward0>), time elapsed: 319.78784441947937 seconds\n",
      "Step 5800, loss: tensor(0.0658, grad_fn=<SubBackward0>), time elapsed: 325.44705033302307 seconds\n",
      "Step 5900, loss: tensor(0.0655, grad_fn=<SubBackward0>), time elapsed: 330.9605493545532 seconds\n",
      "Step 6000, loss: tensor(0.0668, grad_fn=<SubBackward0>), time elapsed: 336.4807574748993 seconds\n",
      "Step 6100, loss: tensor(0.0656, grad_fn=<SubBackward0>), time elapsed: 342.1211955547333 seconds\n",
      "Step 6200, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 347.6421332359314 seconds\n",
      "Step 6300, loss: tensor(0.0634, grad_fn=<SubBackward0>), time elapsed: 353.3016428947449 seconds\n",
      "Step 6400, loss: tensor(0.0636, grad_fn=<SubBackward0>), time elapsed: 358.83563590049744 seconds\n",
      "Step 6500, loss: tensor(0.0617, grad_fn=<SubBackward0>), time elapsed: 364.34699964523315 seconds\n",
      "Step 6600, loss: tensor(0.0620, grad_fn=<SubBackward0>), time elapsed: 369.99903297424316 seconds\n",
      "Step 6700, loss: tensor(0.0624, grad_fn=<SubBackward0>), time elapsed: 375.51307559013367 seconds\n",
      "Step 6800, loss: tensor(0.0625, grad_fn=<SubBackward0>), time elapsed: 381.15696573257446 seconds\n",
      "Step 6900, loss: tensor(0.0622, grad_fn=<SubBackward0>), time elapsed: 386.69809913635254 seconds\n",
      "Step 7000, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 392.34743213653564 seconds\n",
      "Step 7100, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 397.8651988506317 seconds\n",
      "Step 7200, loss: tensor(0.0609, grad_fn=<SubBackward0>), time elapsed: 403.3893039226532 seconds\n",
      "Step 7300, loss: tensor(0.0599, grad_fn=<SubBackward0>), time elapsed: 409.057213306427 seconds\n",
      "Step 7400, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 414.5716829299927 seconds\n",
      "Step 7500, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 420.2739887237549 seconds\n",
      "Step 7600, loss: tensor(0.0581, grad_fn=<SubBackward0>), time elapsed: 425.7739963531494 seconds\n",
      "Step 7700, loss: tensor(0.0585, grad_fn=<SubBackward0>), time elapsed: 431.4463906288147 seconds\n",
      "Step 7800, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 436.9662232398987 seconds\n",
      "Step 7900, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 442.50547456741333 seconds\n",
      "Step 8000, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 448.2317056655884 seconds\n",
      "Step 8100, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 453.76620507240295 seconds\n",
      "Step 8200, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 459.47711181640625 seconds\n",
      "Step 8300, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 465.02250123023987 seconds\n",
      "Step 8400, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 470.55354714393616 seconds\n",
      "Step 8500, loss: tensor(0.0575, grad_fn=<SubBackward0>), time elapsed: 476.2201509475708 seconds\n",
      "Step 8600, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 481.766459941864 seconds\n",
      "Step 8700, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 487.4225935935974 seconds\n",
      "Step 8800, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 492.94405603408813 seconds\n",
      "Step 8900, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 498.74072337150574 seconds\n",
      "Step 9000, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 504.276424407959 seconds\n",
      "Step 9100, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 509.8367202281952 seconds\n",
      "Step 9200, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 515.4841959476471 seconds\n",
      "Step 9300, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 521.5908606052399 seconds\n",
      "Step 9400, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 527.2744948863983 seconds\n",
      "Step 9500, loss: tensor(0.0572, grad_fn=<SubBackward0>), time elapsed: 532.7897870540619 seconds\n",
      "Step 9600, loss: tensor(0.0575, grad_fn=<SubBackward0>), time elapsed: 538.3292503356934 seconds\n",
      "Step 9700, loss: tensor(0.0551, grad_fn=<SubBackward0>), time elapsed: 544.0246722698212 seconds\n",
      "Step 9800, loss: tensor(0.0564, grad_fn=<SubBackward0>), time elapsed: 549.6206266880035 seconds\n",
      "Step 9900, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 555.288565158844 seconds\n",
      "Step 10000, loss: tensor(0.0564, grad_fn=<SubBackward0>), time elapsed: 560.8042471408844 seconds\n",
      "Step 10100, loss: tensor(0.0549, grad_fn=<SubBackward0>), time elapsed: 566.4587273597717 seconds\n",
      "Step 10200, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 571.9870393276215 seconds\n",
      "Step 10300, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 577.504052400589 seconds\n",
      "Step 10400, loss: tensor(0.0560, grad_fn=<SubBackward0>), time elapsed: 583.1558156013489 seconds\n",
      "Step 10500, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 588.6835012435913 seconds\n",
      "Step 10600, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 594.3674554824829 seconds\n",
      "Step 10700, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 599.9322688579559 seconds\n",
      "Step 10800, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 605.4943957328796 seconds\n",
      "Step 10900, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 611.1824054718018 seconds\n",
      "Step 11000, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 616.7371206283569 seconds\n",
      "Step 11100, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 622.4104290008545 seconds\n",
      "Step 11200, loss: tensor(0.0540, grad_fn=<SubBackward0>), time elapsed: 627.9580652713776 seconds\n",
      "Step 11300, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 634.0456011295319 seconds\n",
      "Step 11400, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 639.5622549057007 seconds\n",
      "Step 11500, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 645.0807678699493 seconds\n",
      "Step 11600, loss: tensor(0.0550, grad_fn=<SubBackward0>), time elapsed: 650.7491993904114 seconds\n",
      "Step 11700, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 656.2686066627502 seconds\n",
      "Step 11800, loss: tensor(0.0543, grad_fn=<SubBackward0>), time elapsed: 662.051059961319 seconds\n",
      "Step 11900, loss: tensor(0.0553, grad_fn=<SubBackward0>), time elapsed: 667.6526017189026 seconds\n",
      "Step 12000, loss: tensor(0.0542, grad_fn=<SubBackward0>), time elapsed: 673.2235827445984 seconds\n",
      "Step 12100, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 678.8814959526062 seconds\n",
      "Step 12200, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 685.0388987064362 seconds\n",
      "Step 12300, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 690.731103181839 seconds\n",
      "Step 12400, loss: tensor(0.0535, grad_fn=<SubBackward0>), time elapsed: 696.2600948810577 seconds\n",
      "Step 12500, loss: tensor(0.0533, grad_fn=<SubBackward0>), time elapsed: 701.9402132034302 seconds\n",
      "Step 12600, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 707.472647190094 seconds\n",
      "Step 12700, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 712.9872524738312 seconds\n",
      "Step 12800, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 718.6666285991669 seconds\n",
      "Step 12900, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 724.2233757972717 seconds\n",
      "Step 13000, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 729.8973054885864 seconds\n",
      "Step 13100, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 735.4387547969818 seconds\n",
      "Step 13200, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 740.9814183712006 seconds\n",
      "Step 13300, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 747.0603196620941 seconds\n",
      "Step 13400, loss: tensor(0.0540, grad_fn=<SubBackward0>), time elapsed: 752.6677746772766 seconds\n",
      "Step 13500, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 758.4015424251556 seconds\n",
      "Step 13600, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 763.960177898407 seconds\n",
      "Step 13700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 770.199179649353 seconds\n",
      "Step 13800, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 775.753497838974 seconds\n",
      "Step 13900, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 781.8729887008667 seconds\n",
      "Step 14000, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 787.5797619819641 seconds\n",
      "Step 14100, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 793.0921356678009 seconds\n",
      "Step 14200, loss: tensor(0.0536, grad_fn=<SubBackward0>), time elapsed: 798.7618281841278 seconds\n",
      "Step 14300, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 804.266491651535 seconds\n",
      "Step 14400, loss: tensor(0.0535, grad_fn=<SubBackward0>), time elapsed: 809.7927303314209 seconds\n",
      "Step 14500, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 815.4841501712799 seconds\n",
      "Step 14600, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 821.0158307552338 seconds\n",
      "Step 14700, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 826.6965725421906 seconds\n",
      "Step 14800, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 832.2199156284332 seconds\n",
      "Step 14900, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 837.8842566013336 seconds\n",
      "Step 15000, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 843.4446849822998 seconds\n",
      "Step 15100, loss: tensor(0.0538, grad_fn=<SubBackward0>), time elapsed: 848.989381313324 seconds\n",
      "Step 15200, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 854.6723608970642 seconds\n",
      "Step 15300, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 860.1939985752106 seconds\n",
      "Step 15400, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 865.9038319587708 seconds\n",
      "Step 15500, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 871.4453122615814 seconds\n",
      "Step 15600, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 877.0386221408844 seconds\n",
      "Step 15700, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 882.7336685657501 seconds\n",
      "Step 15800, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 888.2780206203461 seconds\n",
      "Step 15900, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 894.0019116401672 seconds\n",
      "Step 16000, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 899.5530455112457 seconds\n",
      "Step 16100, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 905.2950778007507 seconds\n",
      "Step 16200, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 910.8432850837708 seconds\n",
      "Step 16300, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 916.7256052494049 seconds\n",
      "Step 16400, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 922.4151880741119 seconds\n",
      "Step 16500, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 927.952260017395 seconds\n",
      "Step 16600, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 933.6293203830719 seconds\n",
      "Step 16700, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 939.1675901412964 seconds\n",
      "Step 16800, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 944.6946909427643 seconds\n",
      "Step 16900, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 950.3833408355713 seconds\n",
      "Step 17000, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 955.9246695041656 seconds\n",
      "Step 17100, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 961.6282420158386 seconds\n",
      "Step 17200, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 967.2000916004181 seconds\n",
      "Step 17300, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 972.8307156562805 seconds\n",
      "Step 17400, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 978.6204776763916 seconds\n",
      "Step 17500, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 984.1652226448059 seconds\n",
      "Step 17600, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 990.2793476581573 seconds\n",
      "Step 17700, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 995.8477613925934 seconds\n",
      "Step 17800, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1001.5437209606171 seconds\n",
      "Step 17900, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 1007.0915729999542 seconds\n",
      "Step 18000, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 1012.6376776695251 seconds\n",
      "Step 18100, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 1018.3344006538391 seconds\n",
      "Step 18200, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1023.8804130554199 seconds\n",
      "Step 18300, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 1029.6142644882202 seconds\n",
      "Step 18400, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 1035.16548037529 seconds\n",
      "Step 18500, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 1040.836682319641 seconds\n",
      "Step 18600, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 1046.6226258277893 seconds\n",
      "Step 18700, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1052.1882600784302 seconds\n",
      "Step 18800, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 1058.910239458084 seconds\n",
      "Step 18900, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 1064.4784977436066 seconds\n",
      "Step 19000, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 1070.0485138893127 seconds\n",
      "Step 19100, loss: tensor(0.0490, grad_fn=<SubBackward0>), time elapsed: 1075.7678608894348 seconds\n",
      "Step 19200, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 1081.3343198299408 seconds\n",
      "Step 19300, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 1087.0792722702026 seconds\n",
      "Step 19400, loss: tensor(0.0468, grad_fn=<SubBackward0>), time elapsed: 1092.6563303470612 seconds\n",
      "Step 19500, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 1098.3458564281464 seconds\n",
      "Step 19600, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1103.8634746074677 seconds\n",
      "Step 19700, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1109.3799471855164 seconds\n",
      "Step 19800, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 1115.063797712326 seconds\n",
      "Step 19900, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 1120.667457818985 seconds\n",
      "Step 20000, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 1126.3942999839783 seconds\n"
     ]
    }
   ],
   "source": [
    "#train on diffusion data\n",
    "n, na = 4, 1 # number of data and ancilla qubits\n",
    "T = 20 # number of diffusion steps\n",
    "L = 6 # layer of backward PQC\n",
    "Ndata = 2000 # number of data in the training data set\n",
    "epochs = 20001\n",
    "\n",
    " # number of training epochs\n",
    "\n",
    "#gen random samples at t = T\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_T = diffModel.HaarSampleGeneration(Ndata, seed = 22)\n",
    "\n",
    "#load diffusion process\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "model.set_diffusionSet(states_diff)\n",
    "\n",
    "params_total = []\n",
    "loss_hist_total = []\n",
    "\n",
    "for t in range(T - 1, -1, -1):\n",
    "    params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "    for tt in range(t+1, T):\n",
    "        print(tt)\n",
    "        params_tot[tt] = np.load('paramsandloss_squaremodel/params_t%d.npy'%tt)\n",
    "    \n",
    "    params, loss_hist = Training_t(model, t, inputs_T, params_tot, Ndata, epochs)\n",
    "\n",
    "    np.save('paramsandloss_squaremodel/params_t%d'%t, params.detach().numpy())\n",
    "    np.save('paramsandloss_squaremodel/loss_t%d'%t, loss_hist.detach().numpy())\n",
    "    \n",
    "    #params_total = np.append(params_total, params, axis = 1)\n",
    "    #loss_hist_total = np.append(loss_hist_total, loss_hist, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: tensor(0.0262, grad_fn=<SubBackward0>), time elapsed: 0.03679513931274414 seconds\n",
      "Step 100, loss: tensor(0.0215, grad_fn=<SubBackward0>), time elapsed: 3.812459945678711 seconds\n",
      "Step 200, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 7.346958875656128 seconds\n",
      "Step 300, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 10.86698293685913 seconds\n",
      "Step 400, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 14.589691400527954 seconds\n",
      "Step 500, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 18.12534761428833 seconds\n",
      "Step 600, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 21.64742136001587 seconds\n",
      "Step 700, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 25.405336618423462 seconds\n",
      "Step 800, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 28.938697338104248 seconds\n",
      "Step 900, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 32.47132849693298 seconds\n",
      "Step 1000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 35.988669872283936 seconds\n",
      "Step 1100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 39.686453104019165 seconds\n",
      "Step 1200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 43.203102588653564 seconds\n",
      "Step 1300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 46.73646020889282 seconds\n",
      "Step 1400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 50.47656583786011 seconds\n",
      "Step 1500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 54.00294852256775 seconds\n",
      "Step 1600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 57.51102352142334 seconds\n",
      "Step 1700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 61.21854066848755 seconds\n",
      "Step 1800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 64.76162242889404 seconds\n",
      "Step 1900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 68.28668737411499 seconds\n",
      "Step 2000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 72.01335859298706 seconds\n",
      "Step 2100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 75.51773715019226 seconds\n",
      "Step 2200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 79.0456862449646 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 82.79448890686035 seconds\n",
      "Step 2400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 86.31014776229858 seconds\n",
      "Step 2500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 89.8660409450531 seconds\n",
      "Step 2600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 93.61159992218018 seconds\n",
      "Step 2700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 97.14932513237 seconds\n",
      "Step 2800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 100.71432590484619 seconds\n",
      "Step 2900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 104.46139621734619 seconds\n",
      "Step 3000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 107.97855138778687 seconds\n",
      "Step 3100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 111.50855016708374 seconds\n",
      "Step 3200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 115.21787619590759 seconds\n",
      "Step 3300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 118.70307350158691 seconds\n",
      "Step 3400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 122.2102267742157 seconds\n",
      "Step 3500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 125.70753312110901 seconds\n",
      "Step 3600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 129.43459296226501 seconds\n",
      "Step 3700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 133.02361845970154 seconds\n",
      "Step 3800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 136.51178407669067 seconds\n",
      "Step 3900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 140.20446968078613 seconds\n",
      "Step 4000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 143.67996048927307 seconds\n",
      "Step 4100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 147.148770570755 seconds\n",
      "Step 4200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 150.86054611206055 seconds\n",
      "Step 4300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 154.38039898872375 seconds\n",
      "Step 4400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 157.87466549873352 seconds\n",
      "Step 4500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 161.551096200943 seconds\n",
      "Step 4600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 165.0590181350708 seconds\n",
      "Step 4700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 168.52648782730103 seconds\n",
      "Step 4800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 172.20336294174194 seconds\n",
      "Step 4900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 175.70459961891174 seconds\n",
      "Step 5000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 179.17498922348022 seconds\n",
      "Step 5100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 182.88284993171692 seconds\n",
      "Step 5200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 186.38074707984924 seconds\n",
      "Step 5300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 189.89519262313843 seconds\n",
      "Step 5400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 193.63137197494507 seconds\n",
      "Step 5500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 197.1236035823822 seconds\n",
      "Step 5600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 200.61403346061707 seconds\n",
      "Step 5700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 204.30229449272156 seconds\n",
      "Step 5800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 207.80475497245789 seconds\n",
      "Step 5900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 211.289142370224 seconds\n",
      "Step 6000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 214.78245854377747 seconds\n",
      "Step 6100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 218.47379970550537 seconds\n",
      "Step 6200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 221.97430300712585 seconds\n",
      "Step 6300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 225.51269912719727 seconds\n",
      "Step 6400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 229.20926642417908 seconds\n",
      "Step 6500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 232.7987859249115 seconds\n",
      "Step 6600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 236.38131189346313 seconds\n",
      "Step 6700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 240.15682339668274 seconds\n",
      "Step 6800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 243.6726098060608 seconds\n",
      "Step 6900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 247.15685415267944 seconds\n",
      "Step 7000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 250.87683582305908 seconds\n",
      "Step 7100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 254.38344478607178 seconds\n",
      "Step 7200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 257.86183738708496 seconds\n",
      "Step 7300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 261.5839285850525 seconds\n",
      "Step 7400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 265.10242319107056 seconds\n",
      "Step 7500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 268.60680079460144 seconds\n",
      "Step 7600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 272.3317949771881 seconds\n",
      "Step 7700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 275.88242983818054 seconds\n",
      "Step 7800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 279.38866114616394 seconds\n",
      "Step 7900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 283.10926723480225 seconds\n",
      "Step 8000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 286.6014792919159 seconds\n",
      "Step 8100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 290.0814414024353 seconds\n",
      "Step 8200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 293.86214876174927 seconds\n",
      "Step 8300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 297.3714089393616 seconds\n",
      "Step 8400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 300.8989017009735 seconds\n",
      "Step 8500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 304.63336157798767 seconds\n",
      "Step 8600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 308.1456115245819 seconds\n",
      "Step 8700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 311.63061141967773 seconds\n",
      "Step 8800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 315.10849237442017 seconds\n",
      "Step 8900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 318.8288514614105 seconds\n",
      "Step 9000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 322.31965231895447 seconds\n",
      "Step 9100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 325.85157537460327 seconds\n",
      "Step 9200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 329.5807297229767 seconds\n",
      "Step 9300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 333.08884477615356 seconds\n",
      "Step 9400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 336.5797040462494 seconds\n",
      "Step 9500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 340.305233001709 seconds\n",
      "Step 9600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 343.7918574810028 seconds\n",
      "Step 9700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 347.2755615711212 seconds\n",
      "Step 9800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 350.9917049407959 seconds\n",
      "Step 9900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 354.4831929206848 seconds\n",
      "Step 10000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 357.9678883552551 seconds\n",
      "Step 10100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 361.6658296585083 seconds\n",
      "Step 10200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 365.16768884658813 seconds\n",
      "Step 10300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 368.66404390335083 seconds\n",
      "Step 10400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 372.3762700557709 seconds\n",
      "Step 10500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 375.87364506721497 seconds\n",
      "Step 10600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 379.3839011192322 seconds\n",
      "Step 10700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 383.08806347846985 seconds\n",
      "Step 10800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 386.59162044525146 seconds\n",
      "Step 10900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 390.088809967041 seconds\n",
      "Step 11000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 393.8002691268921 seconds\n",
      "Step 11100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 397.30385541915894 seconds\n",
      "Step 11200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 400.823942899704 seconds\n",
      "Step 11300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 404.33376455307007 seconds\n",
      "Step 11400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 408.0637905597687 seconds\n",
      "Step 11500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 411.57568359375 seconds\n",
      "Step 11600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 415.1388282775879 seconds\n",
      "Step 11700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 418.88720440864563 seconds\n",
      "Step 11800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 422.4255106449127 seconds\n",
      "Step 11900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 425.9762899875641 seconds\n",
      "Step 12000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 429.68371868133545 seconds\n",
      "19\n",
      "Step 0, loss: tensor(0.0245, grad_fn=<SubBackward0>), time elapsed: 0.03488302230834961 seconds\n",
      "Step 100, loss: tensor(0.0172, grad_fn=<SubBackward0>), time elapsed: 3.5310065746307373 seconds\n",
      "Step 200, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 7.0189549922943115 seconds\n",
      "Step 300, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 10.673619985580444 seconds\n",
      "Step 400, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 14.12725281715393 seconds\n",
      "Step 500, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 17.59183955192566 seconds\n",
      "Step 600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 21.25606918334961 seconds\n",
      "Step 700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 24.720569610595703 seconds\n",
      "Step 800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 28.222763538360596 seconds\n",
      "Step 900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 31.890825986862183 seconds\n",
      "Step 1000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 35.392661809921265 seconds\n",
      "Step 1100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 38.8666410446167 seconds\n",
      "Step 1200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 42.531219482421875 seconds\n",
      "Step 1300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 46.02445077896118 seconds\n",
      "Step 1400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 49.555790424346924 seconds\n",
      "Step 1500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 53.238577127456665 seconds\n",
      "Step 1600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 56.75557041168213 seconds\n",
      "Step 1700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 60.24803376197815 seconds\n",
      "Step 1800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 63.745383501052856 seconds\n",
      "Step 1900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 67.45482349395752 seconds\n",
      "Step 2000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 70.98024654388428 seconds\n",
      "Step 2100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 74.51008605957031 seconds\n",
      "Step 2200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 78.22211408615112 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 81.75372219085693 seconds\n",
      "Step 2400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 85.27566123008728 seconds\n",
      "Step 2500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 88.9729745388031 seconds\n",
      "Step 2600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 92.46184015274048 seconds\n",
      "Step 2700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 95.94553422927856 seconds\n",
      "Step 2800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 99.65808486938477 seconds\n",
      "Step 2900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 103.13169026374817 seconds\n",
      "Step 3000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 106.61108064651489 seconds\n",
      "Step 3100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 110.31293535232544 seconds\n",
      "Step 3200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 113.81450128555298 seconds\n",
      "Step 3300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 117.31038308143616 seconds\n",
      "Step 3400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 121.01347351074219 seconds\n",
      "Step 3500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 124.52533054351807 seconds\n",
      "Step 3600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 128.0009410381317 seconds\n",
      "Step 3700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 131.686527967453 seconds\n",
      "Step 3800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 135.1774206161499 seconds\n",
      "Step 3900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 138.69658374786377 seconds\n",
      "Step 4000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 142.39603066444397 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 145.8731653690338 seconds\n",
      "Step 4200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 149.36203813552856 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 153.0473062992096 seconds\n",
      "Step 4400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 156.54013800621033 seconds\n",
      "Step 4500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 160.0452561378479 seconds\n",
      "Step 4600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 163.52741527557373 seconds\n",
      "Step 4700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 167.23118686676025 seconds\n",
      "Step 4800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 170.7407693862915 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 174.24020099639893 seconds\n",
      "Step 5000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 177.90924263000488 seconds\n",
      "Step 5100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 181.3956310749054 seconds\n",
      "Step 5200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 184.8762149810791 seconds\n",
      "Step 5300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 188.56989908218384 seconds\n",
      "Step 5400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 192.0699908733368 seconds\n",
      "Step 5500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 195.54671025276184 seconds\n",
      "Step 5600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 199.23208808898926 seconds\n",
      "Step 5700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 202.71798586845398 seconds\n",
      "Step 5800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 206.19906640052795 seconds\n",
      "Step 5900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 209.9211814403534 seconds\n",
      "Step 6000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 213.39332628250122 seconds\n",
      "Step 6100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 216.86800694465637 seconds\n",
      "Step 6200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 220.5455677509308 seconds\n",
      "Step 6300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 224.02194666862488 seconds\n",
      "Step 6400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 227.53821802139282 seconds\n",
      "Step 6500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 231.2397496700287 seconds\n",
      "Step 6600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 234.7215497493744 seconds\n",
      "Step 6700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 238.20257258415222 seconds\n",
      "Step 6800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 241.89184093475342 seconds\n",
      "Step 6900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 245.40555143356323 seconds\n",
      "Step 7000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 248.89368295669556 seconds\n",
      "Step 7100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 252.3927240371704 seconds\n",
      "Step 7200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 256.1203553676605 seconds\n",
      "Step 7300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 259.6163420677185 seconds\n",
      "Step 7400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 263.1326858997345 seconds\n",
      "Step 7500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 266.8261625766754 seconds\n",
      "Step 7600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 270.30289101600647 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 273.79168581962585 seconds\n",
      "Step 7800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 277.49391412734985 seconds\n",
      "Step 7900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 281.0081820487976 seconds\n",
      "Step 8000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 284.4779734611511 seconds\n",
      "Step 8100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 288.1997983455658 seconds\n",
      "Step 8200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 291.72645139694214 seconds\n",
      "Step 8300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 295.2130217552185 seconds\n",
      "Step 8400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 298.9165873527527 seconds\n",
      "Step 8500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 302.4240288734436 seconds\n",
      "Step 8600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 305.9258990287781 seconds\n",
      "Step 8700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 309.6145353317261 seconds\n",
      "Step 8800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 313.13879776000977 seconds\n",
      "Step 8900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 316.6535427570343 seconds\n",
      "Step 9000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 320.34922885894775 seconds\n",
      "Step 9100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 323.8556833267212 seconds\n",
      "Step 9200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 327.37302684783936 seconds\n",
      "Step 9300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 331.0858449935913 seconds\n",
      "Step 9400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 334.61444091796875 seconds\n",
      "Step 9500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 338.1163604259491 seconds\n",
      "Step 9600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 341.61471009254456 seconds\n",
      "Step 9700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 345.3584599494934 seconds\n",
      "Step 9800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 348.93306612968445 seconds\n",
      "Step 9900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 352.4431154727936 seconds\n",
      "Step 10000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 356.1926066875458 seconds\n",
      "Step 10100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 359.7437963485718 seconds\n",
      "Step 10200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 363.276850938797 seconds\n",
      "Step 10300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 367.03989124298096 seconds\n",
      "Step 10400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 370.6094899177551 seconds\n",
      "Step 10500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 374.1506378650665 seconds\n",
      "Step 10600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 377.9711515903473 seconds\n",
      "Step 10700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 381.530873298645 seconds\n",
      "Step 10800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 385.1050834655762 seconds\n",
      "Step 10900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 388.890864610672 seconds\n",
      "Step 11000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 392.45503306388855 seconds\n",
      "Step 11100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 396.0293505191803 seconds\n",
      "Step 11200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 399.80914187431335 seconds\n",
      "Step 11300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 403.3934428691864 seconds\n",
      "Step 11400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 406.9586253166199 seconds\n",
      "Step 11500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 410.7473108768463 seconds\n",
      "Step 11600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 414.3049387931824 seconds\n",
      "Step 11700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 417.86874437332153 seconds\n",
      "Step 11800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 421.6602339744568 seconds\n",
      "Step 11900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 425.23009991645813 seconds\n",
      "Step 12000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 428.8350439071655 seconds\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0126, grad_fn=<SubBackward0>), time elapsed: 0.03887009620666504 seconds\n",
      "Step 100, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 3.5966341495513916 seconds\n",
      "Step 200, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 7.336001634597778 seconds\n",
      "Step 300, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 10.872233629226685 seconds\n",
      "Step 400, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 14.347691535949707 seconds\n",
      "Step 500, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 18.01476812362671 seconds\n",
      "Step 600, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 21.55010437965393 seconds\n",
      "Step 700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 25.063281536102295 seconds\n",
      "Step 800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 28.772546768188477 seconds\n",
      "Step 900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 32.282888889312744 seconds\n",
      "Step 1000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 35.76998043060303 seconds\n",
      "Step 1100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 39.48460674285889 seconds\n",
      "Step 1200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 42.985870361328125 seconds\n",
      "Step 1300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 46.476977586746216 seconds\n",
      "Step 1400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 50.17363476753235 seconds\n",
      "Step 1500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 53.68339133262634 seconds\n",
      "Step 1600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 57.18711853027344 seconds\n",
      "Step 1700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 60.85516095161438 seconds\n",
      "Step 1800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 64.3321943283081 seconds\n",
      "Step 1900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 67.81748533248901 seconds\n",
      "Step 2000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 71.54082155227661 seconds\n",
      "Step 2100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 75.02481341362 seconds\n",
      "Step 2200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 78.49166107177734 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 82.19707083702087 seconds\n",
      "Step 2400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 85.69721245765686 seconds\n",
      "Step 2500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 89.21373391151428 seconds\n",
      "Step 2600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 92.88231253623962 seconds\n",
      "Step 2700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 96.38782405853271 seconds\n",
      "Step 2800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 99.88821744918823 seconds\n",
      "Step 2900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 103.5798556804657 seconds\n",
      "Step 3000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 107.05190372467041 seconds\n",
      "Step 3100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 110.54090309143066 seconds\n",
      "Step 3200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 114.26495218276978 seconds\n",
      "Step 3300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 117.74004673957825 seconds\n",
      "Step 3400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 121.20593595504761 seconds\n",
      "Step 3500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 124.73811173439026 seconds\n",
      "Step 3600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 128.43732833862305 seconds\n",
      "Step 3700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 131.96342754364014 seconds\n",
      "Step 3800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 135.44082689285278 seconds\n",
      "Step 3900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 139.10559487342834 seconds\n",
      "Step 4000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 142.6070363521576 seconds\n",
      "Step 4100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 146.09333491325378 seconds\n",
      "Step 4200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 149.76048469543457 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 153.26914238929749 seconds\n",
      "Step 4400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 156.79507541656494 seconds\n",
      "Step 4500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 160.5224747657776 seconds\n",
      "Step 4600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 164.00454187393188 seconds\n",
      "Step 4700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 167.5293834209442 seconds\n",
      "Step 4800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 171.21391105651855 seconds\n",
      "Step 4900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 174.69645643234253 seconds\n",
      "Step 5000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 178.23523569107056 seconds\n",
      "Step 5100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 181.94371485710144 seconds\n",
      "Step 5200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 185.43360257148743 seconds\n",
      "Step 5300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 188.92535662651062 seconds\n",
      "Step 5400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 192.62492775917053 seconds\n",
      "Step 5500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 196.12158465385437 seconds\n",
      "Step 5600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 199.6380898952484 seconds\n",
      "Step 5700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 203.35961294174194 seconds\n",
      "Step 5800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 206.83819484710693 seconds\n",
      "Step 5900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 210.33377480506897 seconds\n",
      "Step 6000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 214.0683159828186 seconds\n",
      "Step 6100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 217.5854320526123 seconds\n",
      "Step 6200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 221.06867241859436 seconds\n",
      "Step 6300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 224.57340288162231 seconds\n",
      "Step 6400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 228.29679250717163 seconds\n",
      "Step 6500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 231.79785346984863 seconds\n",
      "Step 6600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 235.27392387390137 seconds\n",
      "Step 6700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 238.95345997810364 seconds\n",
      "Step 6800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 242.44801783561707 seconds\n",
      "Step 6900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 245.92605137825012 seconds\n",
      "Step 7000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 249.64599752426147 seconds\n",
      "Step 7100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 253.179851770401 seconds\n",
      "Step 7200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 256.66158962249756 seconds\n",
      "Step 7300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 260.37419509887695 seconds\n",
      "Step 7400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 263.8676187992096 seconds\n",
      "Step 7500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 267.34859323501587 seconds\n",
      "Step 7600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 271.07102704048157 seconds\n",
      "Step 7700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 274.5712559223175 seconds\n",
      "Step 7800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 278.09176325798035 seconds\n",
      "Step 7900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 281.7769401073456 seconds\n",
      "Step 8000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 285.28987288475037 seconds\n",
      "Step 8100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 288.783242225647 seconds\n",
      "Step 8200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 292.48922896385193 seconds\n",
      "Step 8300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 295.976487159729 seconds\n",
      "Step 8400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 299.49860191345215 seconds\n",
      "Step 8500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 303.2049677371979 seconds\n",
      "Step 8600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 306.7165651321411 seconds\n",
      "Step 8700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 310.31606554985046 seconds\n",
      "Step 8800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 313.85578775405884 seconds\n",
      "Step 8900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 317.6322400569916 seconds\n",
      "Step 9000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 321.2184307575226 seconds\n",
      "Step 9100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 324.7091588973999 seconds\n",
      "Step 9200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 328.39755177497864 seconds\n",
      "Step 9300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 331.9066801071167 seconds\n",
      "Step 9400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 335.4171988964081 seconds\n",
      "Step 9500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 339.1541500091553 seconds\n",
      "Step 9600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 342.6752943992615 seconds\n",
      "Step 9700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 346.18666553497314 seconds\n",
      "Step 9800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 349.89393877983093 seconds\n",
      "Step 9900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 353.38569736480713 seconds\n",
      "Step 10000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 356.9080719947815 seconds\n",
      "Step 10100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 360.602276802063 seconds\n",
      "Step 10200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 364.1006097793579 seconds\n",
      "Step 10300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 367.6101973056793 seconds\n",
      "Step 10400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 371.3389790058136 seconds\n",
      "Step 10500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 374.86303329467773 seconds\n",
      "Step 10600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 378.3756408691406 seconds\n",
      "Step 10700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 382.1010482311249 seconds\n",
      "Step 10800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 385.62064480781555 seconds\n",
      "Step 10900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 389.1707968711853 seconds\n",
      "Step 11000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 392.9074184894562 seconds\n",
      "Step 11100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 396.4308681488037 seconds\n",
      "Step 11200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 399.967577457428 seconds\n",
      "Step 11300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 403.47141003608704 seconds\n",
      "Step 11400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 407.2342941761017 seconds\n",
      "Step 11500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 410.77860379219055 seconds\n",
      "Step 11600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 414.3002769947052 seconds\n",
      "Step 11700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 418.0458221435547 seconds\n",
      "Step 11800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 421.59889340400696 seconds\n",
      "Step 11900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 425.1600890159607 seconds\n",
      "Step 12000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 428.9286870956421 seconds\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 3.5828912258148193 seconds\n",
      "Step 200, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 7.101088047027588 seconds\n",
      "Step 300, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 10.806631326675415 seconds\n",
      "Step 400, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 14.280253887176514 seconds\n",
      "Step 500, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 17.76872706413269 seconds\n",
      "Step 600, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 21.449604988098145 seconds\n",
      "Step 700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 24.94672656059265 seconds\n",
      "Step 800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 28.42120385169983 seconds\n",
      "Step 900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 32.13471531867981 seconds\n",
      "Step 1000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 35.63890719413757 seconds\n",
      "Step 1100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 39.12725472450256 seconds\n",
      "Step 1200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 42.854259967803955 seconds\n",
      "Step 1300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 46.38786292076111 seconds\n",
      "Step 1400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 49.86634397506714 seconds\n",
      "Step 1500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 53.53940415382385 seconds\n",
      "Step 1600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 57.01451230049133 seconds\n",
      "Step 1700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 60.53337383270264 seconds\n",
      "Step 1800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 64.23229813575745 seconds\n",
      "Step 1900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 67.75779247283936 seconds\n",
      "Step 2000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 71.23668384552002 seconds\n",
      "Step 2100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 74.9349377155304 seconds\n",
      "Step 2200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 78.44613766670227 seconds\n",
      "Step 2300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 81.91955661773682 seconds\n",
      "Step 2400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 85.37648391723633 seconds\n",
      "Step 2500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 89.04923033714294 seconds\n",
      "Step 2600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 92.52856731414795 seconds\n",
      "Step 2700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 96.0150077342987 seconds\n",
      "Step 2800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 99.70433211326599 seconds\n",
      "Step 2900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 103.18247270584106 seconds\n",
      "Step 3000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 106.68990969657898 seconds\n",
      "Step 3100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 110.4082977771759 seconds\n",
      "Step 3200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 113.88683581352234 seconds\n",
      "Step 3300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 117.35542607307434 seconds\n",
      "Step 3400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 121.05355453491211 seconds\n",
      "Step 3500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 124.55997681617737 seconds\n",
      "Step 3600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 128.03498792648315 seconds\n",
      "Step 3700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 131.7379777431488 seconds\n",
      "Step 3800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 135.21812343597412 seconds\n",
      "Step 3900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 138.69520354270935 seconds\n",
      "Step 4000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 142.39796209335327 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 145.88252806663513 seconds\n",
      "Step 4200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 149.35961651802063 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 153.02886581420898 seconds\n",
      "Step 4400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 156.5124020576477 seconds\n",
      "Step 4500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 159.99069738388062 seconds\n",
      "Step 4600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 163.6717655658722 seconds\n",
      "Step 4700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 167.1960415840149 seconds\n",
      "Step 4800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 170.68892002105713 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 174.18794441223145 seconds\n",
      "Step 5000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 177.91159391403198 seconds\n",
      "Step 5100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 181.40313172340393 seconds\n",
      "Step 5200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 184.8865213394165 seconds\n",
      "Step 5300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 188.56273484230042 seconds\n",
      "Step 5400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 192.05538725852966 seconds\n",
      "Step 5500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 195.57790613174438 seconds\n",
      "Step 5600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 199.2465386390686 seconds\n",
      "Step 5700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 202.72906064987183 seconds\n",
      "Step 5800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 206.20551538467407 seconds\n",
      "Step 5900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 209.89231181144714 seconds\n",
      "Step 6000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 213.39301419258118 seconds\n",
      "Step 6100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 216.88196635246277 seconds\n",
      "Step 6200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 220.58012700080872 seconds\n",
      "Step 6300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 224.0480535030365 seconds\n",
      "Step 6400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 227.51917433738708 seconds\n",
      "Step 6500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 231.20948696136475 seconds\n",
      "Step 6600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 234.68044686317444 seconds\n",
      "Step 6700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 238.1844072341919 seconds\n",
      "Step 6800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 241.8693289756775 seconds\n",
      "Step 6900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 245.3403902053833 seconds\n",
      "Step 7000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 248.84957075119019 seconds\n",
      "Step 7100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 252.51744723320007 seconds\n",
      "Step 7200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 255.98451495170593 seconds\n",
      "Step 7300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 259.47327065467834 seconds\n",
      "Step 7400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 263.1630914211273 seconds\n",
      "Step 7500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 266.6622405052185 seconds\n",
      "Step 7600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 270.14360332489014 seconds\n",
      "Step 7700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 273.61048412323 seconds\n",
      "Step 7800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 277.3190665245056 seconds\n",
      "Step 7900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 280.8510971069336 seconds\n",
      "Step 8000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 284.3398861885071 seconds\n",
      "Step 8100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 288.0162444114685 seconds\n",
      "Step 8200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 291.5513770580292 seconds\n",
      "Step 8300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 295.0347390174866 seconds\n",
      "Step 8400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 298.7858657836914 seconds\n",
      "Step 8500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 302.34705781936646 seconds\n",
      "Step 8600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 305.8864920139313 seconds\n",
      "Step 8700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 309.602326631546 seconds\n",
      "Step 8800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 313.0930063724518 seconds\n",
      "Step 8900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 316.5737462043762 seconds\n",
      "Step 9000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 320.288010597229 seconds\n",
      "Step 9100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 323.77028012275696 seconds\n",
      "Step 9200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 327.28544521331787 seconds\n",
      "Step 9300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 330.9736523628235 seconds\n",
      "Step 9400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 334.46143412590027 seconds\n",
      "Step 9500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 337.97680616378784 seconds\n",
      "Step 9600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 341.6651644706726 seconds\n",
      "Step 9700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 345.1372480392456 seconds\n",
      "Step 9800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 348.62248945236206 seconds\n",
      "Step 9900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 352.1002826690674 seconds\n",
      "Step 10000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 355.84514260292053 seconds\n",
      "Step 10100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 359.34317278862 seconds\n",
      "Step 10200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 362.83207631111145 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 366.5292868614197 seconds\n",
      "Step 10400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 370.0389220714569 seconds\n",
      "Step 10500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 373.5825560092926 seconds\n",
      "Step 10600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 377.30735445022583 seconds\n",
      "Step 10700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 380.8136022090912 seconds\n",
      "Step 10800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 384.30314803123474 seconds\n",
      "Step 10900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 388.04067039489746 seconds\n",
      "Step 11000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 391.63128876686096 seconds\n",
      "Step 11100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 395.2643105983734 seconds\n",
      "Step 11200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 399.0286431312561 seconds\n",
      "Step 11300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 402.6330609321594 seconds\n",
      "Step 11400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 406.19357919692993 seconds\n",
      "Step 11500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 409.99416851997375 seconds\n",
      "Step 11600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 413.6083753108978 seconds\n",
      "Step 11700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 417.2461392879486 seconds\n",
      "Step 11800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 421.04839849472046 seconds\n",
      "Step 11900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 424.66152715682983 seconds\n",
      "Step 12000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 428.29623341560364 seconds\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0227, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 3.768280029296875 seconds\n",
      "Step 200, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 7.261916637420654 seconds\n",
      "Step 300, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 10.797386646270752 seconds\n",
      "Step 400, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 14.506479501724243 seconds\n",
      "Step 500, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 18.061668634414673 seconds\n",
      "Step 600, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 21.604176998138428 seconds\n",
      "Step 700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 25.141831159591675 seconds\n",
      "Step 800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 28.829481601715088 seconds\n",
      "Step 900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 32.29324960708618 seconds\n",
      "Step 1000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 35.849563121795654 seconds\n",
      "Step 1100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 39.60171818733215 seconds\n",
      "Step 1200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 43.10779571533203 seconds\n",
      "Step 1300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 46.602736711502075 seconds\n",
      "Step 1400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 50.28721046447754 seconds\n",
      "Step 1500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 53.88081622123718 seconds\n",
      "Step 1600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 57.43643856048584 seconds\n",
      "Step 1700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 61.1841561794281 seconds\n",
      "Step 1800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 64.75919675827026 seconds\n",
      "Step 1900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 68.31012463569641 seconds\n",
      "Step 2000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 72.06109499931335 seconds\n",
      "Step 2100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 75.59466981887817 seconds\n",
      "Step 2200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 79.14653849601746 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 82.87652039527893 seconds\n",
      "Step 2400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 86.4088830947876 seconds\n",
      "Step 2500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 89.98018550872803 seconds\n",
      "Step 2600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 93.71250939369202 seconds\n",
      "Step 2700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 97.21357560157776 seconds\n",
      "Step 2800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 100.73585629463196 seconds\n",
      "Step 2900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 104.424556016922 seconds\n",
      "Step 3000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 107.90023159980774 seconds\n",
      "Step 3100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 111.42199349403381 seconds\n",
      "Step 3200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 114.91553354263306 seconds\n",
      "Step 3300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 118.59072494506836 seconds\n",
      "Step 3400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 122.08503746986389 seconds\n",
      "Step 3500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 125.55822920799255 seconds\n",
      "Step 3600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 129.2750949859619 seconds\n",
      "Step 3700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 132.75958490371704 seconds\n",
      "Step 3800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 136.23875164985657 seconds\n",
      "Step 3900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 139.899427652359 seconds\n",
      "Step 4000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 143.3748321533203 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 146.88464188575745 seconds\n",
      "Step 4200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 150.57867074012756 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 154.05976557731628 seconds\n",
      "Step 4400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 157.53787851333618 seconds\n",
      "Step 4500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 161.22277808189392 seconds\n",
      "Step 4600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 164.75369596481323 seconds\n",
      "Step 4700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 168.24239897727966 seconds\n",
      "Step 4800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 171.92261338233948 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 175.41477155685425 seconds\n",
      "Step 5000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 178.90637469291687 seconds\n",
      "Step 5100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 182.6069791316986 seconds\n",
      "Step 5200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 186.13480424880981 seconds\n",
      "Step 5300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 189.62737369537354 seconds\n",
      "Step 5400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 193.30372405052185 seconds\n",
      "Step 5500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 196.7900152206421 seconds\n",
      "Step 5600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 200.28793954849243 seconds\n",
      "Step 5700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 203.7586555480957 seconds\n",
      "Step 5800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 207.47601771354675 seconds\n",
      "Step 5900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 210.97498536109924 seconds\n",
      "Step 6000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 214.44708609580994 seconds\n",
      "Step 6100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 218.12318420410156 seconds\n",
      "Step 6200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 221.6098222732544 seconds\n",
      "Step 6300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 225.08321380615234 seconds\n",
      "Step 6400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 228.7642593383789 seconds\n",
      "Step 6500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 232.24109864234924 seconds\n",
      "Step 6600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 235.7089822292328 seconds\n",
      "Step 6700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 239.4320375919342 seconds\n",
      "Step 6800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 242.9465777873993 seconds\n",
      "Step 6900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 246.42489743232727 seconds\n",
      "Step 7000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 250.11041235923767 seconds\n",
      "Step 7100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 253.599102973938 seconds\n",
      "Step 7200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 257.07333159446716 seconds\n",
      "Step 7300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 260.76748037338257 seconds\n",
      "Step 7400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 264.24909257888794 seconds\n",
      "Step 7500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 267.72628688812256 seconds\n",
      "Step 7600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 271.42585802078247 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 274.91040110588074 seconds\n",
      "Step 7800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 278.39082646369934 seconds\n",
      "Step 7900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 282.0730028152466 seconds\n",
      "Step 8000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 285.5537133216858 seconds\n",
      "Step 8100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 289.0769021511078 seconds\n",
      "Step 8200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 292.79039764404297 seconds\n",
      "Step 8300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 296.28205609321594 seconds\n",
      "Step 8400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 299.79942655563354 seconds\n",
      "Step 8500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 303.27728748321533 seconds\n",
      "Step 8600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 306.9927623271942 seconds\n",
      "Step 8700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 310.53595447540283 seconds\n",
      "Step 8800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 314.0765883922577 seconds\n",
      "Step 8900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 317.7537417411804 seconds\n",
      "Step 9000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 321.24251437187195 seconds\n",
      "Step 9100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 324.7840688228607 seconds\n",
      "Step 9200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 328.51466178894043 seconds\n",
      "Step 9300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 332.0647966861725 seconds\n",
      "Step 9400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 335.5691361427307 seconds\n",
      "Step 9500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 339.2514748573303 seconds\n",
      "Step 9600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 342.8040096759796 seconds\n",
      "Step 9700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 346.33389687538147 seconds\n",
      "Step 9800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 350.03610038757324 seconds\n",
      "Step 9900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 353.51544880867004 seconds\n",
      "Step 10000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 357.01829838752747 seconds\n",
      "Step 10100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 360.7385959625244 seconds\n",
      "Step 10200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 364.22424054145813 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 367.7113516330719 seconds\n",
      "Step 10400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 371.41684794425964 seconds\n",
      "Step 10500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 374.94196105003357 seconds\n",
      "Step 10600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 378.46108841896057 seconds\n",
      "Step 10700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 381.94921255111694 seconds\n",
      "Step 10800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 385.69376945495605 seconds\n",
      "Step 10900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 389.256028175354 seconds\n",
      "Step 11000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 392.77650809288025 seconds\n",
      "Step 11100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 396.52445244789124 seconds\n",
      "Step 11200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 400.0359625816345 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 403.54897475242615 seconds\n",
      "Step 11400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 407.2780156135559 seconds\n",
      "Step 11500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 410.7981584072113 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 414.34382796287537 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 418.1316258907318 seconds\n",
      "Step 11800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 421.72098898887634 seconds\n",
      "Step 11900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 425.2804973125458 seconds\n",
      "Step 12000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 429.1292338371277 seconds\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(0.0102, grad_fn=<SubBackward0>), time elapsed: 3.570087194442749 seconds\n",
      "Step 200, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 7.105337619781494 seconds\n",
      "Step 300, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 10.833814144134521 seconds\n",
      "Step 400, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 14.366061687469482 seconds\n",
      "Step 500, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 17.884968280792236 seconds\n",
      "Step 600, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 21.565211057662964 seconds\n",
      "Step 700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 25.03697109222412 seconds\n",
      "Step 800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 28.517565488815308 seconds\n",
      "Step 900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 32.201135873794556 seconds\n",
      "Step 1000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 35.73167300224304 seconds\n",
      "Step 1100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 39.214903354644775 seconds\n",
      "Step 1200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 42.94651198387146 seconds\n",
      "Step 1300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 46.41737365722656 seconds\n",
      "Step 1400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 49.90142250061035 seconds\n",
      "Step 1500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 53.61031675338745 seconds\n",
      "Step 1600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 57.12578296661377 seconds\n",
      "Step 1700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 60.63590621948242 seconds\n",
      "Step 1800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 64.12310886383057 seconds\n",
      "Step 1900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 67.80297446250916 seconds\n",
      "Step 2000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 71.29753732681274 seconds\n",
      "Step 2100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 74.77433633804321 seconds\n",
      "Step 2200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 78.48276877403259 seconds\n",
      "Step 2300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 81.97031426429749 seconds\n",
      "Step 2400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 85.44509673118591 seconds\n",
      "Step 2500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 89.15024161338806 seconds\n",
      "Step 2600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 92.64381623268127 seconds\n",
      "Step 2700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 96.12448263168335 seconds\n",
      "Step 2800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 99.78758907318115 seconds\n",
      "Step 2900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 103.27006316184998 seconds\n",
      "Step 3000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 106.78135967254639 seconds\n",
      "Step 3100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 110.49269795417786 seconds\n",
      "Step 3200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 114.02465271949768 seconds\n",
      "Step 3300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 117.54749989509583 seconds\n",
      "Step 3400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 121.24989914894104 seconds\n",
      "Step 3500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 124.7374861240387 seconds\n",
      "Step 3600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 128.21822929382324 seconds\n",
      "Step 3700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 131.9138216972351 seconds\n",
      "Step 3800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 135.42324948310852 seconds\n",
      "Step 3900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 138.94194388389587 seconds\n",
      "Step 4000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 142.64611387252808 seconds\n",
      "Step 4100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 146.1769495010376 seconds\n",
      "Step 4200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 149.6957335472107 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 153.40411901474 seconds\n",
      "Step 4400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 156.8784646987915 seconds\n",
      "Step 4500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 160.3520860671997 seconds\n",
      "Step 4600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 163.82387518882751 seconds\n",
      "Step 4700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 167.48607540130615 seconds\n",
      "Step 4800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 170.99113178253174 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 174.50746822357178 seconds\n",
      "Step 5000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 178.1965627670288 seconds\n",
      "Step 5100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 181.6811592578888 seconds\n",
      "Step 5200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 185.1535885334015 seconds\n",
      "Step 5300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 188.87121963500977 seconds\n",
      "Step 5400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 192.37535667419434 seconds\n",
      "Step 5500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 195.8493914604187 seconds\n",
      "Step 5600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 199.55961108207703 seconds\n",
      "Step 5700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 203.0447359085083 seconds\n",
      "Step 5800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 206.52229690551758 seconds\n",
      "Step 5900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 210.21659564971924 seconds\n",
      "Step 6000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 213.69851875305176 seconds\n",
      "Step 6100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 217.17484974861145 seconds\n",
      "Step 6200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 220.86200833320618 seconds\n",
      "Step 6300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 224.33918404579163 seconds\n",
      "Step 6400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 227.8549449443817 seconds\n",
      "Step 6500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 231.53483510017395 seconds\n",
      "Step 6600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 235.0034658908844 seconds\n",
      "Step 6700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 238.47701334953308 seconds\n",
      "Step 6800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 242.1501100063324 seconds\n",
      "Step 6900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 245.64878273010254 seconds\n",
      "Step 7000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 249.13256239891052 seconds\n",
      "Step 7100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 252.80561876296997 seconds\n",
      "Step 7200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 256.33499932289124 seconds\n",
      "Step 7300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 259.8562924861908 seconds\n",
      "Step 7400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 263.34254264831543 seconds\n",
      "Step 7500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 267.03253078460693 seconds\n",
      "Step 7600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 270.52477264404297 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 274.07050371170044 seconds\n",
      "Step 7800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 277.75606632232666 seconds\n",
      "Step 7900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 281.28066992759705 seconds\n",
      "Step 8000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 284.80420088768005 seconds\n",
      "Step 8100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 288.51011753082275 seconds\n",
      "Step 8200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 292.01785349845886 seconds\n",
      "Step 8300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 295.516964673996 seconds\n",
      "Step 8400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 299.2258059978485 seconds\n",
      "Step 8500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 302.74250316619873 seconds\n",
      "Step 8600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 306.2290382385254 seconds\n",
      "Step 8700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 309.9097170829773 seconds\n",
      "Step 8800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 313.3971598148346 seconds\n",
      "Step 8900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 316.92419052124023 seconds\n",
      "Step 9000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 320.6219639778137 seconds\n",
      "Step 9100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 324.12095284461975 seconds\n",
      "Step 9200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 327.6162974834442 seconds\n",
      "Step 9300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 331.3355300426483 seconds\n",
      "Step 9400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 334.832852602005 seconds\n",
      "Step 9500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 338.32998061180115 seconds\n",
      "Step 9600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 342.0512297153473 seconds\n",
      "Step 9700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 345.5424304008484 seconds\n",
      "Step 9800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 349.0370225906372 seconds\n",
      "Step 9900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 352.58215737342834 seconds\n",
      "Step 10000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 356.2838432788849 seconds\n",
      "Step 10100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 359.78408908843994 seconds\n",
      "Step 10200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 363.33452892303467 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 367.07805371284485 seconds\n",
      "Step 10400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 370.65494751930237 seconds\n",
      "Step 10500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 374.1502721309662 seconds\n",
      "Step 10600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 377.89498472213745 seconds\n",
      "Step 10700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 381.3972523212433 seconds\n",
      "Step 10800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 384.9035632610321 seconds\n",
      "Step 10900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 388.6467926502228 seconds\n",
      "Step 11000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 392.1636781692505 seconds\n",
      "Step 11100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 395.72261667251587 seconds\n",
      "Step 11200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 399.49206137657166 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 403.05749106407166 seconds\n",
      "Step 11400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 406.61570167541504 seconds\n",
      "Step 11500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 410.3454191684723 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 413.8804864883423 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 417.41991686820984 seconds\n",
      "Step 11800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 421.18396830558777 seconds\n",
      "Step 11900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 424.72741055488586 seconds\n",
      "Step 12000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 428.3392126560211 seconds\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0174, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(0.0108, grad_fn=<SubBackward0>), time elapsed: 3.7658262252807617 seconds\n",
      "Step 200, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 7.26974081993103 seconds\n",
      "Step 300, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 10.758131265640259 seconds\n",
      "Step 400, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 14.223711252212524 seconds\n",
      "Step 500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 17.88692855834961 seconds\n",
      "Step 600, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 21.37123441696167 seconds\n",
      "Step 700, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 24.894464015960693 seconds\n",
      "Step 800, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 28.576881885528564 seconds\n",
      "Step 900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 32.066569805145264 seconds\n",
      "Step 1000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 35.53994607925415 seconds\n",
      "Step 1100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 39.20879340171814 seconds\n",
      "Step 1200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 42.723153829574585 seconds\n",
      "Step 1300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 46.24322032928467 seconds\n",
      "Step 1400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 49.94549036026001 seconds\n",
      "Step 1500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 53.48367357254028 seconds\n",
      "Step 1600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 56.96287250518799 seconds\n",
      "Step 1700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 60.65591835975647 seconds\n",
      "Step 1800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 64.13478326797485 seconds\n",
      "Step 1900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 67.61118364334106 seconds\n",
      "Step 2000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 71.31025385856628 seconds\n",
      "Step 2100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 74.84526586532593 seconds\n",
      "Step 2200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 78.36668229103088 seconds\n",
      "Step 2300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 82.06448554992676 seconds\n",
      "Step 2400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 85.54982709884644 seconds\n",
      "Step 2500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 89.02849245071411 seconds\n",
      "Step 2600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 92.6963267326355 seconds\n",
      "Step 2700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 96.21251344680786 seconds\n",
      "Step 2800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 99.69396376609802 seconds\n",
      "Step 2900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 103.37020611763 seconds\n",
      "Step 3000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 106.84600782394409 seconds\n",
      "Step 3100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 110.32592678070068 seconds\n",
      "Step 3200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 113.83543038368225 seconds\n",
      "Step 3300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 117.52647018432617 seconds\n",
      "Step 3400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 121.0083339214325 seconds\n",
      "Step 3500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 124.49630737304688 seconds\n",
      "Step 3600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 128.1858229637146 seconds\n",
      "Step 3700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 131.691974401474 seconds\n",
      "Step 3800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 135.17071843147278 seconds\n",
      "Step 3900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 138.86776781082153 seconds\n",
      "Step 4000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 142.32508158683777 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 145.805095911026 seconds\n",
      "Step 4200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 149.46205830574036 seconds\n",
      "Step 4300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 152.9131200313568 seconds\n",
      "Step 4400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 156.35984992980957 seconds\n",
      "Step 4500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 160.00719285011292 seconds\n",
      "Step 4600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 163.4741289615631 seconds\n",
      "Step 4700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 166.95789194107056 seconds\n",
      "Step 4800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 170.6223587989807 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 174.06028270721436 seconds\n",
      "Step 5000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 177.521559715271 seconds\n",
      "Step 5100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 181.16618657112122 seconds\n",
      "Step 5200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 184.65771460533142 seconds\n",
      "Step 5300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 188.16372179985046 seconds\n",
      "Step 5400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 191.81391644477844 seconds\n",
      "Step 5500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 195.27509093284607 seconds\n",
      "Step 5600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 198.82693815231323 seconds\n",
      "Step 5700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 202.33618569374084 seconds\n",
      "Step 5800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 206.00950288772583 seconds\n",
      "Step 5900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 209.48788857460022 seconds\n",
      "Step 6000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 212.96773052215576 seconds\n",
      "Step 6100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 216.65066695213318 seconds\n",
      "Step 6200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 220.1749758720398 seconds\n",
      "Step 6300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 223.71390414237976 seconds\n",
      "Step 6400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 227.41419410705566 seconds\n",
      "Step 6500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 230.9035565853119 seconds\n",
      "Step 6600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 234.37838745117188 seconds\n",
      "Step 6700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 238.07898712158203 seconds\n",
      "Step 6800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 241.57175302505493 seconds\n",
      "Step 6900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 245.04390740394592 seconds\n",
      "Step 7000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 248.73473358154297 seconds\n",
      "Step 7100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 252.25454092025757 seconds\n",
      "Step 7200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 255.74414229393005 seconds\n",
      "Step 7300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 259.4503710269928 seconds\n",
      "Step 7400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 262.9333896636963 seconds\n",
      "Step 7500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 266.4178431034088 seconds\n",
      "Step 7600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 270.09928798675537 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 273.6306300163269 seconds\n",
      "Step 7800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 277.1186845302582 seconds\n",
      "Step 7900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 280.800418138504 seconds\n",
      "Step 8000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 284.33088541030884 seconds\n",
      "Step 8100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 287.86992931365967 seconds\n",
      "Step 8200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 291.4185104370117 seconds\n",
      "Step 8300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 295.09989500045776 seconds\n",
      "Step 8400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 298.6085648536682 seconds\n",
      "Step 8500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 302.0995047092438 seconds\n",
      "Step 8600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 305.8463044166565 seconds\n",
      "Step 8700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 309.3663535118103 seconds\n",
      "Step 8800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 312.85364031791687 seconds\n",
      "Step 8900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 316.5551257133484 seconds\n",
      "Step 9000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 320.08800292015076 seconds\n",
      "Step 9100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 323.6290693283081 seconds\n",
      "Step 9200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 327.3644313812256 seconds\n",
      "Step 9300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 330.904887676239 seconds\n",
      "Step 9400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 334.41829228401184 seconds\n",
      "Step 9500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 338.1361472606659 seconds\n",
      "Step 9600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 341.67314171791077 seconds\n",
      "Step 9700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 345.19550013542175 seconds\n",
      "Step 9800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 348.9121551513672 seconds\n",
      "Step 9900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 352.41315031051636 seconds\n",
      "Step 10000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 355.9139211177826 seconds\n",
      "Step 10100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 359.6572983264923 seconds\n",
      "Step 10200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 363.1560106277466 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 366.65707445144653 seconds\n",
      "Step 10400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 370.365690946579 seconds\n",
      "Step 10500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 373.87670040130615 seconds\n",
      "Step 10600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 377.40275382995605 seconds\n",
      "Step 10700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 380.90455198287964 seconds\n",
      "Step 10800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 384.60561442375183 seconds\n",
      "Step 10900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 388.10788583755493 seconds\n",
      "Step 11000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 391.60197138786316 seconds\n",
      "Step 11100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 395.3529634475708 seconds\n",
      "Step 11200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 398.86226081848145 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 402.3705863952637 seconds\n",
      "Step 11400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 406.12247133255005 seconds\n",
      "Step 11500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 409.63849568367004 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 413.1485016345978 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 416.90062522888184 seconds\n",
      "Step 11800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 420.4361777305603 seconds\n",
      "Step 11900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 423.98126673698425 seconds\n",
      "Step 12000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 427.7879946231842 seconds\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 0.03917956352233887 seconds\n",
      "Step 100, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 3.599332809448242 seconds\n",
      "Step 200, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 7.087504863739014 seconds\n",
      "Step 300, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 10.772325038909912 seconds\n",
      "Step 400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 14.28405499458313 seconds\n",
      "Step 500, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 17.759498596191406 seconds\n",
      "Step 600, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 21.455859899520874 seconds\n",
      "Step 700, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 24.954052209854126 seconds\n",
      "Step 800, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 28.428738117218018 seconds\n",
      "Step 900, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 32.11094522476196 seconds\n",
      "Step 1000, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 35.61266732215881 seconds\n",
      "Step 1100, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 39.09427833557129 seconds\n",
      "Step 1200, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 42.76606774330139 seconds\n",
      "Step 1300, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 46.256988525390625 seconds\n",
      "Step 1400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 49.741068601608276 seconds\n",
      "Step 1500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 53.25114941596985 seconds\n",
      "Step 1600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 56.929227352142334 seconds\n",
      "Step 1700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 60.411842584609985 seconds\n",
      "Step 1800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 63.87916278839111 seconds\n",
      "Step 1900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 67.5567855834961 seconds\n",
      "Step 2000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 71.09097814559937 seconds\n",
      "Step 2100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 74.58026909828186 seconds\n",
      "Step 2200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 78.25717306137085 seconds\n",
      "Step 2300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 81.73761200904846 seconds\n",
      "Step 2400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 85.21355652809143 seconds\n",
      "Step 2500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 88.91281867027283 seconds\n",
      "Step 2600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 92.43589472770691 seconds\n",
      "Step 2700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 95.91253304481506 seconds\n",
      "Step 2800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 99.61626601219177 seconds\n",
      "Step 2900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 103.12596988677979 seconds\n",
      "Step 3000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 106.63895201683044 seconds\n",
      "Step 3100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 110.3128035068512 seconds\n",
      "Step 3200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 113.82288908958435 seconds\n",
      "Step 3300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 117.29971623420715 seconds\n",
      "Step 3400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 121.00194787979126 seconds\n",
      "Step 3500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 124.5243411064148 seconds\n",
      "Step 3600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 128.02576994895935 seconds\n",
      "Step 3700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 131.70156574249268 seconds\n",
      "Step 3800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 135.20853996276855 seconds\n",
      "Step 3900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 138.69164776802063 seconds\n",
      "Step 4000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 142.20641613006592 seconds\n",
      "Step 4100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 145.8674156665802 seconds\n",
      "Step 4200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 149.34751677513123 seconds\n",
      "Step 4300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 152.8327353000641 seconds\n",
      "Step 4400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 156.54815125465393 seconds\n",
      "Step 4500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 160.03087186813354 seconds\n",
      "Step 4600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 163.50415134429932 seconds\n",
      "Step 4700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 167.16532254219055 seconds\n",
      "Step 4800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 170.64391994476318 seconds\n",
      "Step 4900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 174.13819789886475 seconds\n",
      "Step 5000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 177.85611486434937 seconds\n",
      "Step 5100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 181.35542917251587 seconds\n",
      "Step 5200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 184.83235025405884 seconds\n",
      "Step 5300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 188.51766705513 seconds\n",
      "Step 5400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 192.01822209358215 seconds\n",
      "Step 5500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 195.49570751190186 seconds\n",
      "Step 5600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 199.17640900611877 seconds\n",
      "Step 5700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 202.7092800140381 seconds\n",
      "Step 5800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 206.20538353919983 seconds\n",
      "Step 5900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 209.89053606987 seconds\n",
      "Step 6000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 213.41349697113037 seconds\n",
      "Step 6100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 216.961035490036 seconds\n",
      "Step 6200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 220.65654611587524 seconds\n",
      "Step 6300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 224.13643622398376 seconds\n",
      "Step 6400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 227.6291148662567 seconds\n",
      "Step 6500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 231.0987193584442 seconds\n",
      "Step 6600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 234.76510667800903 seconds\n",
      "Step 6700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 238.248188495636 seconds\n",
      "Step 6800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 241.7164182662964 seconds\n",
      "Step 6900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 245.40504097938538 seconds\n",
      "Step 7000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 248.90748286247253 seconds\n",
      "Step 7100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 252.37894010543823 seconds\n",
      "Step 7200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 256.0722391605377 seconds\n",
      "Step 7300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 259.53809118270874 seconds\n",
      "Step 7400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 263.0753390789032 seconds\n",
      "Step 7500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 266.7724268436432 seconds\n",
      "Step 7600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 270.24550223350525 seconds\n",
      "Step 7700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 273.7232744693756 seconds\n",
      "Step 7800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 277.40252017974854 seconds\n",
      "Step 7900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 280.9028594493866 seconds\n",
      "Step 8000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 284.38020944595337 seconds\n",
      "Step 8100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 288.06337237358093 seconds\n",
      "Step 8200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 291.6187138557434 seconds\n",
      "Step 8300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 295.12346601486206 seconds\n",
      "Step 8400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 298.854558467865 seconds\n",
      "Step 8500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 302.35926699638367 seconds\n",
      "Step 8600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 305.852751493454 seconds\n",
      "Step 8700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 309.5617365837097 seconds\n",
      "Step 8800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 313.077840089798 seconds\n",
      "Step 8900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 316.60533356666565 seconds\n",
      "Step 9000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 320.3114140033722 seconds\n",
      "Step 9100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 323.79191398620605 seconds\n",
      "Step 9200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 327.28159642219543 seconds\n",
      "Step 9300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 330.7678966522217 seconds\n",
      "Step 9400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 334.51737332344055 seconds\n",
      "Step 9500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 338.03827476501465 seconds\n",
      "Step 9600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 341.55886602401733 seconds\n",
      "Step 9700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 345.28090143203735 seconds\n",
      "Step 9800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 348.77295660972595 seconds\n",
      "Step 9900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 352.2863314151764 seconds\n",
      "Step 10000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 356.0142230987549 seconds\n",
      "Step 10100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 359.5756871700287 seconds\n",
      "Step 10200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 363.07304406166077 seconds\n",
      "Step 10300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 366.810742855072 seconds\n",
      "Step 10400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 370.36564564704895 seconds\n",
      "Step 10500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 373.8835310935974 seconds\n",
      "Step 10600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 377.60820388793945 seconds\n",
      "Step 10700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 381.16180658340454 seconds\n",
      "Step 10800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 384.67920994758606 seconds\n",
      "Step 10900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 388.42960596084595 seconds\n",
      "Step 11000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 391.9573516845703 seconds\n",
      "Step 11100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 395.4700562953949 seconds\n",
      "Step 11200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 399.1932055950165 seconds\n",
      "Step 11300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 402.7307450771332 seconds\n",
      "Step 11400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 406.25516629219055 seconds\n",
      "Step 11500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 409.99602150917053 seconds\n",
      "Step 11600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 413.51861476898193 seconds\n",
      "Step 11700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 417.061723947525 seconds\n",
      "Step 11800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 420.6460952758789 seconds\n",
      "Step 11900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 424.41165804862976 seconds\n",
      "Step 12000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 428.00989294052124 seconds\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 0.036876678466796875 seconds\n",
      "Step 100, loss: tensor(0.0146, grad_fn=<SubBackward0>), time elapsed: 3.578139066696167 seconds\n",
      "Step 200, loss: tensor(0.0106, grad_fn=<SubBackward0>), time elapsed: 7.293212413787842 seconds\n",
      "Step 300, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 10.788696050643921 seconds\n",
      "Step 400, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 14.270237922668457 seconds\n",
      "Step 500, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 17.929084539413452 seconds\n",
      "Step 600, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 21.43719458580017 seconds\n",
      "Step 700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 24.964884042739868 seconds\n",
      "Step 800, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 28.666215419769287 seconds\n",
      "Step 900, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 32.14519762992859 seconds\n",
      "Step 1000, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 35.617488622665405 seconds\n",
      "Step 1100, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 39.29738807678223 seconds\n",
      "Step 1200, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 42.76779222488403 seconds\n",
      "Step 1300, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 46.244362354278564 seconds\n",
      "Step 1400, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 49.92976474761963 seconds\n",
      "Step 1500, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 53.409828424453735 seconds\n",
      "Step 1600, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 56.93528079986572 seconds\n",
      "Step 1700, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 60.6352813243866 seconds\n",
      "Step 1800, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 64.1452169418335 seconds\n",
      "Step 1900, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 67.63343214988708 seconds\n",
      "Step 2000, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 71.30151224136353 seconds\n",
      "Step 2100, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 74.78969669342041 seconds\n",
      "Step 2200, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 78.2669563293457 seconds\n",
      "Step 2300, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 81.93727469444275 seconds\n",
      "Step 2400, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 85.46178102493286 seconds\n",
      "Step 2500, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 88.96152806282043 seconds\n",
      "Step 2600, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 92.66434621810913 seconds\n",
      "Step 2700, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 96.15922331809998 seconds\n",
      "Step 2800, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 99.64685773849487 seconds\n",
      "Step 2900, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 103.12386441230774 seconds\n",
      "Step 3000, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 106.82316780090332 seconds\n",
      "Step 3100, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 110.35331702232361 seconds\n",
      "Step 3200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 113.83439254760742 seconds\n",
      "Step 3300, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 117.50583100318909 seconds\n",
      "Step 3400, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 120.99796152114868 seconds\n",
      "Step 3500, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 124.47062253952026 seconds\n",
      "Step 3600, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 128.19066667556763 seconds\n",
      "Step 3700, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 131.69181442260742 seconds\n",
      "Step 3800, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 135.1873815059662 seconds\n",
      "Step 3900, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 138.9289972782135 seconds\n",
      "Step 4000, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 142.4419584274292 seconds\n",
      "Step 4100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 145.95418906211853 seconds\n",
      "Step 4200, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 149.67857670783997 seconds\n",
      "Step 4300, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 153.15377950668335 seconds\n",
      "Step 4400, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 156.6281394958496 seconds\n",
      "Step 4500, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 160.3224720954895 seconds\n",
      "Step 4600, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 163.87061738967896 seconds\n",
      "Step 4700, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 167.35420036315918 seconds\n",
      "Step 4800, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 171.04531288146973 seconds\n",
      "Step 4900, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 174.53654074668884 seconds\n",
      "Step 5000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 178.0226378440857 seconds\n",
      "Step 5100, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 181.71537828445435 seconds\n",
      "Step 5200, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 185.24979877471924 seconds\n",
      "Step 5300, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 188.79158329963684 seconds\n",
      "Step 5400, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 192.51925683021545 seconds\n",
      "Step 5500, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 196.00161600112915 seconds\n",
      "Step 5600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 199.5236427783966 seconds\n",
      "Step 5700, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 203.02057147026062 seconds\n",
      "Step 5800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 206.75327920913696 seconds\n",
      "Step 5900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 210.29786467552185 seconds\n",
      "Step 6000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 213.80951356887817 seconds\n",
      "Step 6100, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 217.50425958633423 seconds\n",
      "Step 6200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 220.98590993881226 seconds\n",
      "Step 6300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 224.46088671684265 seconds\n",
      "Step 6400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 228.13195490837097 seconds\n",
      "Step 6500, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 231.61255645751953 seconds\n",
      "Step 6600, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 235.1098394393921 seconds\n",
      "Step 6700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 238.81602954864502 seconds\n",
      "Step 6800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 242.3309292793274 seconds\n",
      "Step 6900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 245.80485701560974 seconds\n",
      "Step 7000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 249.50083684921265 seconds\n",
      "Step 7100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 253.0283133983612 seconds\n",
      "Step 7200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 256.5319743156433 seconds\n",
      "Step 7300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 260.2168381214142 seconds\n",
      "Step 7400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 263.6988000869751 seconds\n",
      "Step 7500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 267.1793677806854 seconds\n",
      "Step 7600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 270.8858222961426 seconds\n",
      "Step 7700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 274.3678810596466 seconds\n",
      "Step 7800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 277.85642194747925 seconds\n",
      "Step 7900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 281.5379891395569 seconds\n",
      "Step 8000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 284.9989204406738 seconds\n",
      "Step 8100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 288.52697014808655 seconds\n",
      "Step 8200, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 292.29019832611084 seconds\n",
      "Step 8300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 295.7577383518219 seconds\n",
      "Step 8400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 299.27239322662354 seconds\n",
      "Step 8500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 302.7698004245758 seconds\n",
      "Step 8600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 306.5093460083008 seconds\n",
      "Step 8700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 310.00808334350586 seconds\n",
      "Step 8800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 313.49172258377075 seconds\n",
      "Step 8900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 317.1749424934387 seconds\n",
      "Step 9000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 320.70380187034607 seconds\n",
      "Step 9100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 324.2511658668518 seconds\n",
      "Step 9200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 327.9529218673706 seconds\n",
      "Step 9300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 331.4487714767456 seconds\n",
      "Step 9400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 334.9569218158722 seconds\n",
      "Step 9500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 338.70392179489136 seconds\n",
      "Step 9600, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 342.19791746139526 seconds\n",
      "Step 9700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 345.67876648902893 seconds\n",
      "Step 9800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 349.3750789165497 seconds\n",
      "Step 9900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 352.9124686717987 seconds\n",
      "Step 10000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 356.43481969833374 seconds\n",
      "Step 10100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 360.1549451351166 seconds\n",
      "Step 10200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 363.6946294307709 seconds\n",
      "Step 10300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 367.1805832386017 seconds\n",
      "Step 10400, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 370.9260494709015 seconds\n",
      "Step 10500, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 374.42461371421814 seconds\n",
      "Step 10600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 377.92901635169983 seconds\n",
      "Step 10700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 381.4247224330902 seconds\n",
      "Step 10800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 385.12650442123413 seconds\n",
      "Step 10900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 388.66642594337463 seconds\n",
      "Step 11000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 392.1893892288208 seconds\n",
      "Step 11100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 395.9190180301666 seconds\n",
      "Step 11200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 399.4336178302765 seconds\n",
      "Step 11300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 402.9537556171417 seconds\n",
      "Step 11400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 406.7313768863678 seconds\n",
      "Step 11500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 410.25585103034973 seconds\n",
      "Step 11600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 413.7783524990082 seconds\n",
      "Step 11700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 417.5316517353058 seconds\n",
      "Step 11800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 421.0902187824249 seconds\n",
      "Step 11900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 424.6736149787903 seconds\n",
      "Step 12000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 428.475305557251 seconds\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 0.035880088806152344 seconds\n",
      "Step 100, loss: tensor(0.0219, grad_fn=<SubBackward0>), time elapsed: 3.544881820678711 seconds\n",
      "Step 200, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 7.061788558959961 seconds\n",
      "Step 300, loss: tensor(0.0127, grad_fn=<SubBackward0>), time elapsed: 10.75009822845459 seconds\n",
      "Step 400, loss: tensor(0.0100, grad_fn=<SubBackward0>), time elapsed: 14.277324914932251 seconds\n",
      "Step 500, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 17.76131296157837 seconds\n",
      "Step 600, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 21.445863962173462 seconds\n",
      "Step 700, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 24.93892192840576 seconds\n",
      "Step 800, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 28.42077088356018 seconds\n",
      "Step 900, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 32.11305499076843 seconds\n",
      "Step 1000, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 35.59862208366394 seconds\n",
      "Step 1100, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 39.08476495742798 seconds\n",
      "Step 1200, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 42.805760860443115 seconds\n",
      "Step 1300, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 46.279381275177 seconds\n",
      "Step 1400, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 49.76122522354126 seconds\n",
      "Step 1500, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 53.45899224281311 seconds\n",
      "Step 1600, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 56.92413926124573 seconds\n",
      "Step 1700, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 60.42526030540466 seconds\n",
      "Step 1800, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 64.09862303733826 seconds\n",
      "Step 1900, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 67.61400151252747 seconds\n",
      "Step 2000, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 71.10084629058838 seconds\n",
      "Step 2100, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 74.5757417678833 seconds\n",
      "Step 2200, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 78.26342177391052 seconds\n",
      "Step 2300, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 81.7567024230957 seconds\n",
      "Step 2400, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 85.23084354400635 seconds\n",
      "Step 2500, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 88.9207808971405 seconds\n",
      "Step 2600, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 92.4027464389801 seconds\n",
      "Step 2700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 95.91419458389282 seconds\n",
      "Step 2800, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 99.61274886131287 seconds\n",
      "Step 2900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 103.0887393951416 seconds\n",
      "Step 3000, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 106.5663251876831 seconds\n",
      "Step 3100, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 110.23340225219727 seconds\n",
      "Step 3200, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 113.75357794761658 seconds\n",
      "Step 3300, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 117.23327875137329 seconds\n",
      "Step 3400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 120.92390727996826 seconds\n",
      "Step 3500, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 124.4546332359314 seconds\n",
      "Step 3600, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 127.96142029762268 seconds\n",
      "Step 3700, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 131.63555598258972 seconds\n",
      "Step 3800, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 135.1076946258545 seconds\n",
      "Step 3900, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 138.62842774391174 seconds\n",
      "Step 4000, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 142.31243705749512 seconds\n",
      "Step 4100, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 145.8264865875244 seconds\n",
      "Step 4200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 149.33257412910461 seconds\n",
      "Step 4300, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 153.00715899467468 seconds\n",
      "Step 4400, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 156.52899384498596 seconds\n",
      "Step 4500, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 160.0209140777588 seconds\n",
      "Step 4600, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 163.69831252098083 seconds\n",
      "Step 4700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 167.16700172424316 seconds\n",
      "Step 4800, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 170.65276265144348 seconds\n",
      "Step 4900, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 174.135027885437 seconds\n",
      "Step 5000, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 177.85633063316345 seconds\n",
      "Step 5100, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 181.40297722816467 seconds\n",
      "Step 5200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 184.90971493721008 seconds\n",
      "Step 5300, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 188.60620498657227 seconds\n",
      "Step 5400, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 192.141371011734 seconds\n",
      "Step 5500, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 195.641371011734 seconds\n",
      "Step 5600, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 199.32057452201843 seconds\n",
      "Step 5700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 202.81336736679077 seconds\n",
      "Step 5800, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 206.30880284309387 seconds\n",
      "Step 5900, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 210.00969195365906 seconds\n",
      "Step 6000, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 213.4905068874359 seconds\n",
      "Step 6100, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 216.97782111167908 seconds\n",
      "Step 6200, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 220.68634390830994 seconds\n",
      "Step 6300, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 224.22298169136047 seconds\n",
      "Step 6400, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 227.7078766822815 seconds\n",
      "Step 6500, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 231.38998007774353 seconds\n",
      "Step 6600, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 234.88321781158447 seconds\n",
      "Step 6700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 238.4005048274994 seconds\n",
      "Step 6800, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 242.09485268592834 seconds\n",
      "Step 6900, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 245.57412576675415 seconds\n",
      "Step 7000, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 249.0622160434723 seconds\n",
      "Step 7100, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 252.75325918197632 seconds\n",
      "Step 7200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 256.2719774246216 seconds\n",
      "Step 7300, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 259.7578561306 seconds\n",
      "Step 7400, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 263.45013761520386 seconds\n",
      "Step 7500, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 266.97536396980286 seconds\n",
      "Step 7600, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 270.538024187088 seconds\n",
      "Step 7700, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 274.0121908187866 seconds\n",
      "Step 7800, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 277.76273703575134 seconds\n",
      "Step 7900, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 281.2411410808563 seconds\n",
      "Step 8000, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 284.7599811553955 seconds\n",
      "Step 8100, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 288.48124074935913 seconds\n",
      "Step 8200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 291.99667620658875 seconds\n",
      "Step 8300, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 295.51038670539856 seconds\n",
      "Step 8400, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 299.21618843078613 seconds\n",
      "Step 8500, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 302.72650599479675 seconds\n",
      "Step 8600, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 306.2608788013458 seconds\n",
      "Step 8700, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 310.0105049610138 seconds\n",
      "Step 8800, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 313.52177691459656 seconds\n",
      "Step 8900, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 317.0073959827423 seconds\n",
      "Step 9000, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 320.70951104164124 seconds\n",
      "Step 9100, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 324.23851323127747 seconds\n",
      "Step 9200, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 327.7843277454376 seconds\n",
      "Step 9300, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 331.477276802063 seconds\n",
      "Step 9400, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 334.9679284095764 seconds\n",
      "Step 9500, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 338.5118598937988 seconds\n",
      "Step 9600, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 342.23075246810913 seconds\n",
      "Step 9700, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 345.71827721595764 seconds\n",
      "Step 9800, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 349.23117685317993 seconds\n",
      "Step 9900, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 352.72050857543945 seconds\n",
      "Step 10000, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 356.4438650608063 seconds\n",
      "Step 10100, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 359.94087958335876 seconds\n",
      "Step 10200, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 363.43914794921875 seconds\n",
      "Step 10300, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 367.1791880130768 seconds\n",
      "Step 10400, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 370.7300386428833 seconds\n",
      "Step 10500, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 374.22950887680054 seconds\n",
      "Step 10600, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 377.9291205406189 seconds\n",
      "Step 10700, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 381.4470522403717 seconds\n",
      "Step 10800, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 384.9894697666168 seconds\n",
      "Step 10900, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 388.7405743598938 seconds\n",
      "Step 11000, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 392.25657773017883 seconds\n",
      "Step 11100, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 395.7651903629303 seconds\n",
      "Step 11200, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 399.50942397117615 seconds\n",
      "Step 11300, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 403.09749126434326 seconds\n",
      "Step 11400, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 406.6389241218567 seconds\n",
      "Step 11500, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 410.379034280777 seconds\n",
      "Step 11600, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 413.94560980796814 seconds\n",
      "Step 11700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 417.48716735839844 seconds\n",
      "Step 11800, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 421.2579917907715 seconds\n",
      "Step 11900, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 424.810364484787 seconds\n",
      "Step 12000, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 428.39433789253235 seconds\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 0.036876678466796875 seconds\n",
      "Step 100, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 3.7562942504882812 seconds\n",
      "Step 200, loss: tensor(0.0250, grad_fn=<SubBackward0>), time elapsed: 7.234121322631836 seconds\n",
      "Step 300, loss: tensor(0.0215, grad_fn=<SubBackward0>), time elapsed: 10.712956190109253 seconds\n",
      "Step 400, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 14.410688161849976 seconds\n",
      "Step 500, loss: tensor(0.0179, grad_fn=<SubBackward0>), time elapsed: 17.879313230514526 seconds\n",
      "Step 600, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 21.362444400787354 seconds\n",
      "Step 700, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 24.84304141998291 seconds\n",
      "Step 800, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 28.56386399269104 seconds\n",
      "Step 900, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 32.10641169548035 seconds\n",
      "Step 1000, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 35.58451843261719 seconds\n",
      "Step 1100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 39.27843880653381 seconds\n",
      "Step 1200, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 42.76937913894653 seconds\n",
      "Step 1300, loss: tensor(0.0128, grad_fn=<SubBackward0>), time elapsed: 46.26774072647095 seconds\n",
      "Step 1400, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 49.98433232307434 seconds\n",
      "Step 1500, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 53.4708411693573 seconds\n",
      "Step 1600, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 56.99096703529358 seconds\n",
      "Step 1700, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 60.66257834434509 seconds\n",
      "Step 1800, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 64.18679404258728 seconds\n",
      "Step 1900, loss: tensor(0.0110, grad_fn=<SubBackward0>), time elapsed: 67.66914057731628 seconds\n",
      "Step 2000, loss: tensor(0.0099, grad_fn=<SubBackward0>), time elapsed: 71.37172603607178 seconds\n",
      "Step 2100, loss: tensor(0.0105, grad_fn=<SubBackward0>), time elapsed: 74.91281771659851 seconds\n",
      "Step 2200, loss: tensor(0.0101, grad_fn=<SubBackward0>), time elapsed: 78.40727543830872 seconds\n",
      "Step 2300, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 82.09154105186462 seconds\n",
      "Step 2400, loss: tensor(0.0102, grad_fn=<SubBackward0>), time elapsed: 85.56916308403015 seconds\n",
      "Step 2500, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 89.0871171951294 seconds\n",
      "Step 2600, loss: tensor(0.0098, grad_fn=<SubBackward0>), time elapsed: 92.78710055351257 seconds\n",
      "Step 2700, loss: tensor(0.0099, grad_fn=<SubBackward0>), time elapsed: 96.2677686214447 seconds\n",
      "Step 2800, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 99.74602794647217 seconds\n",
      "Step 2900, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 103.45114827156067 seconds\n",
      "Step 3000, loss: tensor(0.0098, grad_fn=<SubBackward0>), time elapsed: 106.98959493637085 seconds\n",
      "Step 3100, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 110.47324347496033 seconds\n",
      "Step 3200, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 114.14586329460144 seconds\n",
      "Step 3300, loss: tensor(0.0090, grad_fn=<SubBackward0>), time elapsed: 117.65300798416138 seconds\n",
      "Step 3400, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 121.19792771339417 seconds\n",
      "Step 3500, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 124.67976140975952 seconds\n",
      "Step 3600, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 128.4036226272583 seconds\n",
      "Step 3700, loss: tensor(0.0093, grad_fn=<SubBackward0>), time elapsed: 131.9153790473938 seconds\n",
      "Step 3800, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 135.39797973632812 seconds\n",
      "Step 3900, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 139.10719656944275 seconds\n",
      "Step 4000, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 142.58580541610718 seconds\n",
      "Step 4100, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 146.06317329406738 seconds\n",
      "Step 4200, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 149.76493310928345 seconds\n",
      "Step 4300, loss: tensor(0.0090, grad_fn=<SubBackward0>), time elapsed: 153.27955746650696 seconds\n",
      "Step 4400, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 156.75462293624878 seconds\n",
      "Step 4500, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 160.43878173828125 seconds\n",
      "Step 4600, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 163.92898750305176 seconds\n",
      "Step 4700, loss: tensor(0.0100, grad_fn=<SubBackward0>), time elapsed: 167.42037558555603 seconds\n",
      "Step 4800, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 171.10749697685242 seconds\n",
      "Step 4900, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 174.61925053596497 seconds\n",
      "Step 5000, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 178.1019995212555 seconds\n",
      "Step 5100, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 181.8131091594696 seconds\n",
      "Step 5200, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 185.34053945541382 seconds\n",
      "Step 5300, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 188.8424825668335 seconds\n",
      "Step 5400, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 192.52649211883545 seconds\n",
      "Step 5500, loss: tensor(0.0093, grad_fn=<SubBackward0>), time elapsed: 196.03286361694336 seconds\n",
      "Step 5600, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 199.57060313224792 seconds\n",
      "Step 5700, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 203.30846619606018 seconds\n",
      "Step 5800, loss: tensor(0.0093, grad_fn=<SubBackward0>), time elapsed: 206.81026220321655 seconds\n",
      "Step 5900, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 210.30780625343323 seconds\n",
      "Step 6000, loss: tensor(0.0089, grad_fn=<SubBackward0>), time elapsed: 213.8139729499817 seconds\n",
      "Step 6100, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 217.48631286621094 seconds\n",
      "Step 6200, loss: tensor(0.0089, grad_fn=<SubBackward0>), time elapsed: 220.9695281982422 seconds\n",
      "Step 6300, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 224.43848085403442 seconds\n",
      "Step 6400, loss: tensor(0.0090, grad_fn=<SubBackward0>), time elapsed: 228.1215944290161 seconds\n",
      "Step 6500, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 231.6161985397339 seconds\n",
      "Step 6600, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 235.083087682724 seconds\n",
      "Step 6700, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 238.75120902061462 seconds\n",
      "Step 6800, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 242.25359630584717 seconds\n",
      "Step 6900, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 245.74258303642273 seconds\n",
      "Step 7000, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 249.4300720691681 seconds\n",
      "Step 7100, loss: tensor(0.0090, grad_fn=<SubBackward0>), time elapsed: 252.9420144557953 seconds\n",
      "Step 7200, loss: tensor(0.0086, grad_fn=<SubBackward0>), time elapsed: 256.43004989624023 seconds\n",
      "Step 7300, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 260.14992809295654 seconds\n",
      "Step 7400, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 263.63397574424744 seconds\n",
      "Step 7500, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 267.157265663147 seconds\n",
      "Step 7600, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 270.85480856895447 seconds\n",
      "Step 7700, loss: tensor(0.0080, grad_fn=<SubBackward0>), time elapsed: 274.3366093635559 seconds\n",
      "Step 7800, loss: tensor(0.0083, grad_fn=<SubBackward0>), time elapsed: 277.8149735927582 seconds\n",
      "Step 7900, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 281.49912095069885 seconds\n",
      "Step 8000, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 285.01142024993896 seconds\n",
      "Step 8100, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 288.5292410850525 seconds\n",
      "Step 8200, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 292.270222902298 seconds\n",
      "Step 8300, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 295.743417263031 seconds\n",
      "Step 8400, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 299.24788427352905 seconds\n",
      "Step 8500, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 302.7657778263092 seconds\n",
      "Step 8600, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 306.4524738788605 seconds\n",
      "Step 8700, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 309.940958738327 seconds\n",
      "Step 8800, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 313.41868925094604 seconds\n",
      "Step 8900, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 317.14944219589233 seconds\n",
      "Step 9000, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 320.6353406906128 seconds\n",
      "Step 9100, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 324.12022709846497 seconds\n",
      "Step 9200, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 327.80299735069275 seconds\n",
      "Step 9300, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 331.342955827713 seconds\n",
      "Step 9400, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 334.8331618309021 seconds\n",
      "Step 9500, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 338.5621237754822 seconds\n",
      "Step 9600, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 342.0675950050354 seconds\n",
      "Step 9700, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 345.59729743003845 seconds\n",
      "Step 9800, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 349.3553764820099 seconds\n",
      "Step 9900, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 352.8498604297638 seconds\n",
      "Step 10000, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 356.376473903656 seconds\n",
      "Step 10100, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 360.0841248035431 seconds\n",
      "Step 10200, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 363.6264064311981 seconds\n",
      "Step 10300, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 367.1418390274048 seconds\n",
      "Step 10400, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 370.8674566745758 seconds\n",
      "Step 10500, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 374.4147684574127 seconds\n",
      "Step 10600, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 377.9286127090454 seconds\n",
      "Step 10700, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 381.6529076099396 seconds\n",
      "Step 10800, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 385.1508433818817 seconds\n",
      "Step 10900, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 388.6590187549591 seconds\n",
      "Step 11000, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 392.15799045562744 seconds\n",
      "Step 11100, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 395.9163682460785 seconds\n",
      "Step 11200, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 399.46640181541443 seconds\n",
      "Step 11300, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 403.0253505706787 seconds\n",
      "Step 11400, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 406.7714755535126 seconds\n",
      "Step 11500, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 410.3345685005188 seconds\n",
      "Step 11600, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 413.8530650138855 seconds\n",
      "Step 11700, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 417.57959723472595 seconds\n",
      "Step 11800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 421.13101172447205 seconds\n",
      "Step 11900, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 424.6786332130432 seconds\n",
      "Step 12000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 428.49564838409424 seconds\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0869, grad_fn=<SubBackward0>), time elapsed: 0.03587961196899414 seconds\n",
      "Step 100, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 3.5619943141937256 seconds\n",
      "Step 200, loss: tensor(0.0653, grad_fn=<SubBackward0>), time elapsed: 7.047213792800903 seconds\n",
      "Step 300, loss: tensor(0.0560, grad_fn=<SubBackward0>), time elapsed: 10.756914377212524 seconds\n",
      "Step 400, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 14.281321048736572 seconds\n",
      "Step 500, loss: tensor(0.0454, grad_fn=<SubBackward0>), time elapsed: 17.801019191741943 seconds\n",
      "Step 600, loss: tensor(0.0421, grad_fn=<SubBackward0>), time elapsed: 21.472999334335327 seconds\n",
      "Step 700, loss: tensor(0.0400, grad_fn=<SubBackward0>), time elapsed: 24.943591356277466 seconds\n",
      "Step 800, loss: tensor(0.0393, grad_fn=<SubBackward0>), time elapsed: 28.416939735412598 seconds\n",
      "Step 900, loss: tensor(0.0383, grad_fn=<SubBackward0>), time elapsed: 32.1218159198761 seconds\n",
      "Step 1000, loss: tensor(0.0368, grad_fn=<SubBackward0>), time elapsed: 35.610368967056274 seconds\n",
      "Step 1100, loss: tensor(0.0349, grad_fn=<SubBackward0>), time elapsed: 39.0992476940155 seconds\n",
      "Step 1200, loss: tensor(0.0345, grad_fn=<SubBackward0>), time elapsed: 42.78773903846741 seconds\n",
      "Step 1300, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 46.28389286994934 seconds\n",
      "Step 1400, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 49.76965641975403 seconds\n",
      "Step 1500, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 53.48999905586243 seconds\n",
      "Step 1600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 56.96112060546875 seconds\n",
      "Step 1700, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 60.4416925907135 seconds\n",
      "Step 1800, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 64.1320629119873 seconds\n",
      "Step 1900, loss: tensor(0.0292, grad_fn=<SubBackward0>), time elapsed: 67.63599348068237 seconds\n",
      "Step 2000, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 71.11680459976196 seconds\n",
      "Step 2100, loss: tensor(0.0279, grad_fn=<SubBackward0>), time elapsed: 74.78723526000977 seconds\n",
      "Step 2200, loss: tensor(0.0273, grad_fn=<SubBackward0>), time elapsed: 78.24840807914734 seconds\n",
      "Step 2300, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 81.75924301147461 seconds\n",
      "Step 2400, loss: tensor(0.0277, grad_fn=<SubBackward0>), time elapsed: 85.28568744659424 seconds\n",
      "Step 2500, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 88.94399404525757 seconds\n",
      "Step 2600, loss: tensor(0.0270, grad_fn=<SubBackward0>), time elapsed: 92.41198635101318 seconds\n",
      "Step 2700, loss: tensor(0.0264, grad_fn=<SubBackward0>), time elapsed: 95.93747663497925 seconds\n",
      "Step 2800, loss: tensor(0.0262, grad_fn=<SubBackward0>), time elapsed: 99.65372657775879 seconds\n",
      "Step 2900, loss: tensor(0.0260, grad_fn=<SubBackward0>), time elapsed: 103.14068031311035 seconds\n",
      "Step 3000, loss: tensor(0.0257, grad_fn=<SubBackward0>), time elapsed: 106.60276222229004 seconds\n",
      "Step 3100, loss: tensor(0.0252, grad_fn=<SubBackward0>), time elapsed: 110.2868983745575 seconds\n",
      "Step 3200, loss: tensor(0.0240, grad_fn=<SubBackward0>), time elapsed: 113.76836562156677 seconds\n",
      "Step 3300, loss: tensor(0.0247, grad_fn=<SubBackward0>), time elapsed: 117.23619794845581 seconds\n",
      "Step 3400, loss: tensor(0.0268, grad_fn=<SubBackward0>), time elapsed: 120.93909311294556 seconds\n",
      "Step 3500, loss: tensor(0.0257, grad_fn=<SubBackward0>), time elapsed: 124.46607685089111 seconds\n",
      "Step 3600, loss: tensor(0.0255, grad_fn=<SubBackward0>), time elapsed: 127.99234509468079 seconds\n",
      "Step 3700, loss: tensor(0.0250, grad_fn=<SubBackward0>), time elapsed: 131.6951515674591 seconds\n",
      "Step 3800, loss: tensor(0.0239, grad_fn=<SubBackward0>), time elapsed: 135.17788577079773 seconds\n",
      "Step 3900, loss: tensor(0.0258, grad_fn=<SubBackward0>), time elapsed: 138.67256546020508 seconds\n",
      "Step 4000, loss: tensor(0.0246, grad_fn=<SubBackward0>), time elapsed: 142.40166306495667 seconds\n",
      "Step 4100, loss: tensor(0.0236, grad_fn=<SubBackward0>), time elapsed: 145.93763065338135 seconds\n",
      "Step 4200, loss: tensor(0.0244, grad_fn=<SubBackward0>), time elapsed: 149.4249062538147 seconds\n",
      "Step 4300, loss: tensor(0.0237, grad_fn=<SubBackward0>), time elapsed: 153.14404678344727 seconds\n",
      "Step 4400, loss: tensor(0.0236, grad_fn=<SubBackward0>), time elapsed: 156.62157702445984 seconds\n",
      "Step 4500, loss: tensor(0.0231, grad_fn=<SubBackward0>), time elapsed: 160.13539290428162 seconds\n",
      "Step 4600, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 163.8155643939972 seconds\n",
      "Step 4700, loss: tensor(0.0221, grad_fn=<SubBackward0>), time elapsed: 167.29721879959106 seconds\n",
      "Step 4800, loss: tensor(0.0221, grad_fn=<SubBackward0>), time elapsed: 170.78158330917358 seconds\n",
      "Step 4900, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 174.48805952072144 seconds\n",
      "Step 5000, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 177.96966433525085 seconds\n",
      "Step 5100, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 181.44794869422913 seconds\n",
      "Step 5200, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 185.12463855743408 seconds\n",
      "Step 5300, loss: tensor(0.0195, grad_fn=<SubBackward0>), time elapsed: 188.61186909675598 seconds\n",
      "Step 5400, loss: tensor(0.0201, grad_fn=<SubBackward0>), time elapsed: 192.13136625289917 seconds\n",
      "Step 5500, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 195.6023736000061 seconds\n",
      "Step 5600, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 199.3238639831543 seconds\n",
      "Step 5700, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 202.81017351150513 seconds\n",
      "Step 5800, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 206.2893087863922 seconds\n",
      "Step 5900, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 209.98669242858887 seconds\n",
      "Step 6000, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 213.46344304084778 seconds\n",
      "Step 6100, loss: tensor(0.0175, grad_fn=<SubBackward0>), time elapsed: 216.9360475540161 seconds\n",
      "Step 6200, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 220.62975239753723 seconds\n",
      "Step 6300, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 224.1304247379303 seconds\n",
      "Step 6400, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 227.5947506427765 seconds\n",
      "Step 6500, loss: tensor(0.0176, grad_fn=<SubBackward0>), time elapsed: 231.26902079582214 seconds\n",
      "Step 6600, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 234.74860429763794 seconds\n",
      "Step 6700, loss: tensor(0.0167, grad_fn=<SubBackward0>), time elapsed: 238.2573812007904 seconds\n",
      "Step 6800, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 241.9798285961151 seconds\n",
      "Step 6900, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 245.49833059310913 seconds\n",
      "Step 7000, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 248.9836871623993 seconds\n",
      "Step 7100, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 252.6989302635193 seconds\n",
      "Step 7200, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 256.1828989982605 seconds\n",
      "Step 7300, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 259.66963481903076 seconds\n",
      "Step 7400, loss: tensor(0.0165, grad_fn=<SubBackward0>), time elapsed: 263.3664677143097 seconds\n",
      "Step 7500, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 266.8865044116974 seconds\n",
      "Step 7600, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 270.3719003200531 seconds\n",
      "Step 7700, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 274.05845069885254 seconds\n",
      "Step 7800, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 277.5301423072815 seconds\n",
      "Step 7900, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 281.01897954940796 seconds\n",
      "Step 8000, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 284.69284200668335 seconds\n",
      "Step 8100, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 288.1599373817444 seconds\n",
      "Step 8200, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 291.6561212539673 seconds\n",
      "Step 8300, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 295.12809562683105 seconds\n",
      "Step 8400, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 298.83751225471497 seconds\n",
      "Step 8500, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 302.33132338523865 seconds\n",
      "Step 8600, loss: tensor(0.0168, grad_fn=<SubBackward0>), time elapsed: 305.8208293914795 seconds\n",
      "Step 8700, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 309.55286049842834 seconds\n",
      "Step 8800, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 313.08905506134033 seconds\n",
      "Step 8900, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 316.56944727897644 seconds\n",
      "Step 9000, loss: tensor(0.0159, grad_fn=<SubBackward0>), time elapsed: 320.28811955451965 seconds\n",
      "Step 9100, loss: tensor(0.0167, grad_fn=<SubBackward0>), time elapsed: 323.81486558914185 seconds\n",
      "Step 9200, loss: tensor(0.0165, grad_fn=<SubBackward0>), time elapsed: 327.2962279319763 seconds\n",
      "Step 9300, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 330.98135566711426 seconds\n",
      "Step 9400, loss: tensor(0.0171, grad_fn=<SubBackward0>), time elapsed: 334.4798047542572 seconds\n",
      "Step 9500, loss: tensor(0.0165, grad_fn=<SubBackward0>), time elapsed: 337.95165157318115 seconds\n",
      "Step 9600, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 341.63402247428894 seconds\n",
      "Step 9700, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 345.1311173439026 seconds\n",
      "Step 9800, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 348.636572599411 seconds\n",
      "Step 9900, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 352.3437166213989 seconds\n",
      "Step 10000, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 355.8548672199249 seconds\n",
      "Step 10100, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 359.3752827644348 seconds\n",
      "Step 10200, loss: tensor(0.0165, grad_fn=<SubBackward0>), time elapsed: 363.1113073825836 seconds\n",
      "Step 10300, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 366.60025000572205 seconds\n",
      "Step 10400, loss: tensor(0.0159, grad_fn=<SubBackward0>), time elapsed: 370.1017451286316 seconds\n",
      "Step 10500, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 373.6633071899414 seconds\n",
      "Step 10600, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 377.36343717575073 seconds\n",
      "Step 10700, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 380.92270851135254 seconds\n",
      "Step 10800, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 384.48552942276 seconds\n",
      "Step 10900, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 388.2102997303009 seconds\n",
      "Step 11000, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 391.7348008155823 seconds\n",
      "Step 11100, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 395.23378443717957 seconds\n",
      "Step 11200, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 398.9414253234863 seconds\n",
      "Step 11300, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 402.472252368927 seconds\n",
      "Step 11400, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 406.02549052238464 seconds\n",
      "Step 11500, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 409.7915287017822 seconds\n",
      "Step 11600, loss: tensor(0.0165, grad_fn=<SubBackward0>), time elapsed: 413.3260715007782 seconds\n",
      "Step 11700, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 416.8548274040222 seconds\n",
      "Step 11800, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 420.6242253780365 seconds\n",
      "Step 11900, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 424.1651291847229 seconds\n",
      "Step 12000, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 427.7440309524536 seconds\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1088, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(0.0993, grad_fn=<SubBackward0>), time elapsed: 3.773041248321533 seconds\n",
      "Step 200, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 7.272053480148315 seconds\n",
      "Step 300, loss: tensor(0.0824, grad_fn=<SubBackward0>), time elapsed: 10.760518550872803 seconds\n",
      "Step 400, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 14.493759155273438 seconds\n",
      "Step 500, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 18.014156818389893 seconds\n",
      "Step 600, loss: tensor(0.0681, grad_fn=<SubBackward0>), time elapsed: 21.512830018997192 seconds\n",
      "Step 700, loss: tensor(0.0652, grad_fn=<SubBackward0>), time elapsed: 25.19141411781311 seconds\n",
      "Step 800, loss: tensor(0.0611, grad_fn=<SubBackward0>), time elapsed: 28.66476559638977 seconds\n",
      "Step 900, loss: tensor(0.0581, grad_fn=<SubBackward0>), time elapsed: 32.158485889434814 seconds\n",
      "Step 1000, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 35.861074686050415 seconds\n",
      "Step 1100, loss: tensor(0.0551, grad_fn=<SubBackward0>), time elapsed: 39.322752475738525 seconds\n",
      "Step 1200, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 42.8102445602417 seconds\n",
      "Step 1300, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 46.48662972450256 seconds\n",
      "Step 1400, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 50.00839686393738 seconds\n",
      "Step 1500, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 53.50218224525452 seconds\n",
      "Step 1600, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 57.21256375312805 seconds\n",
      "Step 1700, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 60.69242858886719 seconds\n",
      "Step 1800, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 64.19478464126587 seconds\n",
      "Step 1900, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 67.69536924362183 seconds\n",
      "Step 2000, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 71.3549747467041 seconds\n",
      "Step 2100, loss: tensor(0.0475, grad_fn=<SubBackward0>), time elapsed: 74.83860778808594 seconds\n",
      "Step 2200, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 78.30432224273682 seconds\n",
      "Step 2300, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 82.01018381118774 seconds\n",
      "Step 2400, loss: tensor(0.0454, grad_fn=<SubBackward0>), time elapsed: 85.51931715011597 seconds\n",
      "Step 2500, loss: tensor(0.0437, grad_fn=<SubBackward0>), time elapsed: 88.99313068389893 seconds\n",
      "Step 2600, loss: tensor(0.0439, grad_fn=<SubBackward0>), time elapsed: 92.66580700874329 seconds\n",
      "Step 2700, loss: tensor(0.0424, grad_fn=<SubBackward0>), time elapsed: 96.14313197135925 seconds\n",
      "Step 2800, loss: tensor(0.0404, grad_fn=<SubBackward0>), time elapsed: 99.61622095108032 seconds\n",
      "Step 2900, loss: tensor(0.0398, grad_fn=<SubBackward0>), time elapsed: 103.31478071212769 seconds\n",
      "Step 3000, loss: tensor(0.0394, grad_fn=<SubBackward0>), time elapsed: 106.83007335662842 seconds\n",
      "Step 3100, loss: tensor(0.0368, grad_fn=<SubBackward0>), time elapsed: 110.35733485221863 seconds\n",
      "Step 3200, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 114.07304835319519 seconds\n",
      "Step 3300, loss: tensor(0.0352, grad_fn=<SubBackward0>), time elapsed: 117.55591201782227 seconds\n",
      "Step 3400, loss: tensor(0.0347, grad_fn=<SubBackward0>), time elapsed: 121.09296345710754 seconds\n",
      "Step 3500, loss: tensor(0.0339, grad_fn=<SubBackward0>), time elapsed: 124.82125902175903 seconds\n",
      "Step 3600, loss: tensor(0.0332, grad_fn=<SubBackward0>), time elapsed: 128.32372879981995 seconds\n",
      "Step 3700, loss: tensor(0.0333, grad_fn=<SubBackward0>), time elapsed: 131.8020534515381 seconds\n",
      "Step 3800, loss: tensor(0.0337, grad_fn=<SubBackward0>), time elapsed: 135.50144219398499 seconds\n",
      "Step 3900, loss: tensor(0.0329, grad_fn=<SubBackward0>), time elapsed: 139.0081377029419 seconds\n",
      "Step 4000, loss: tensor(0.0345, grad_fn=<SubBackward0>), time elapsed: 142.54437971115112 seconds\n",
      "Step 4100, loss: tensor(0.0339, grad_fn=<SubBackward0>), time elapsed: 146.22708249092102 seconds\n",
      "Step 4200, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 149.72786021232605 seconds\n",
      "Step 4300, loss: tensor(0.0329, grad_fn=<SubBackward0>), time elapsed: 153.220370054245 seconds\n",
      "Step 4400, loss: tensor(0.0327, grad_fn=<SubBackward0>), time elapsed: 156.89669609069824 seconds\n",
      "Step 4500, loss: tensor(0.0320, grad_fn=<SubBackward0>), time elapsed: 160.372549533844 seconds\n",
      "Step 4600, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 163.8719823360443 seconds\n",
      "Step 4700, loss: tensor(0.0322, grad_fn=<SubBackward0>), time elapsed: 167.54474234580994 seconds\n",
      "Step 4800, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 171.01617622375488 seconds\n",
      "Step 4900, loss: tensor(0.0328, grad_fn=<SubBackward0>), time elapsed: 174.50050616264343 seconds\n",
      "Step 5000, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 178.01382851600647 seconds\n",
      "Step 5100, loss: tensor(0.0325, grad_fn=<SubBackward0>), time elapsed: 181.7206254005432 seconds\n",
      "Step 5200, loss: tensor(0.0327, grad_fn=<SubBackward0>), time elapsed: 185.2118034362793 seconds\n",
      "Step 5300, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 188.68750858306885 seconds\n",
      "Step 5400, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 192.3830578327179 seconds\n",
      "Step 5500, loss: tensor(0.0318, grad_fn=<SubBackward0>), time elapsed: 195.89139080047607 seconds\n",
      "Step 5600, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 199.35596323013306 seconds\n",
      "Step 5700, loss: tensor(0.0323, grad_fn=<SubBackward0>), time elapsed: 203.028639793396 seconds\n",
      "Step 5800, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 206.51033234596252 seconds\n",
      "Step 5900, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 209.9809365272522 seconds\n",
      "Step 6000, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 213.70024752616882 seconds\n",
      "Step 6100, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 217.2056028842926 seconds\n",
      "Step 6200, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 220.68537950515747 seconds\n",
      "Step 6300, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 224.38117957115173 seconds\n",
      "Step 6400, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 227.85906171798706 seconds\n",
      "Step 6500, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 231.38165521621704 seconds\n",
      "Step 6600, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 235.10309767723083 seconds\n",
      "Step 6700, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 238.5872356891632 seconds\n",
      "Step 6800, loss: tensor(0.0325, grad_fn=<SubBackward0>), time elapsed: 242.0793161392212 seconds\n",
      "Step 6900, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 245.77963376045227 seconds\n",
      "Step 7000, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 249.2880094051361 seconds\n",
      "Step 7100, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 252.7620346546173 seconds\n",
      "Step 7200, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 256.4642930030823 seconds\n",
      "Step 7300, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 260.00336599349976 seconds\n",
      "Step 7400, loss: tensor(0.0320, grad_fn=<SubBackward0>), time elapsed: 263.54652857780457 seconds\n",
      "Step 7500, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 267.0195951461792 seconds\n",
      "Step 7600, loss: tensor(0.0321, grad_fn=<SubBackward0>), time elapsed: 270.7460799217224 seconds\n",
      "Step 7700, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 274.2777271270752 seconds\n",
      "Step 7800, loss: tensor(0.0321, grad_fn=<SubBackward0>), time elapsed: 277.8040306568146 seconds\n",
      "Step 7900, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 281.4881374835968 seconds\n",
      "Step 8000, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 284.9659993648529 seconds\n",
      "Step 8100, loss: tensor(0.0320, grad_fn=<SubBackward0>), time elapsed: 288.43185687065125 seconds\n",
      "Step 8200, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 292.15809202194214 seconds\n",
      "Step 8300, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 295.64917850494385 seconds\n",
      "Step 8400, loss: tensor(0.0328, grad_fn=<SubBackward0>), time elapsed: 299.15302085876465 seconds\n",
      "Step 8500, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 302.88206362724304 seconds\n",
      "Step 8600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 306.41516041755676 seconds\n",
      "Step 8700, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 309.9308431148529 seconds\n",
      "Step 8800, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 313.627032995224 seconds\n",
      "Step 8900, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 317.1392357349396 seconds\n",
      "Step 9000, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 320.665150642395 seconds\n",
      "Step 9100, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 324.3741464614868 seconds\n",
      "Step 9200, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 327.86284613609314 seconds\n",
      "Step 9300, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 331.353458404541 seconds\n",
      "Step 9400, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 335.0823817253113 seconds\n",
      "Step 9500, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 338.5740201473236 seconds\n",
      "Step 9600, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 342.0701620578766 seconds\n",
      "Step 9700, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 345.7544732093811 seconds\n",
      "Step 9800, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 349.2625288963318 seconds\n",
      "Step 9900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 352.7343809604645 seconds\n",
      "Step 10000, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 356.28045749664307 seconds\n",
      "Step 10100, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 360.0290904045105 seconds\n",
      "Step 10200, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 363.60711765289307 seconds\n",
      "Step 10300, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 367.12613558769226 seconds\n",
      "Step 10400, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 370.8789219856262 seconds\n",
      "Step 10500, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 374.4204988479614 seconds\n",
      "Step 10600, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 377.93706369400024 seconds\n",
      "Step 10700, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 381.6679012775421 seconds\n",
      "Step 10800, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 385.1734848022461 seconds\n",
      "Step 10900, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 388.68385791778564 seconds\n",
      "Step 11000, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 392.41220116615295 seconds\n",
      "Step 11100, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 395.9383544921875 seconds\n",
      "Step 11200, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 399.4465811252594 seconds\n",
      "Step 11300, loss: tensor(0.0324, grad_fn=<SubBackward0>), time elapsed: 403.20428013801575 seconds\n",
      "Step 11400, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 406.72747111320496 seconds\n",
      "Step 11500, loss: tensor(0.0287, grad_fn=<SubBackward0>), time elapsed: 410.24432015419006 seconds\n",
      "Step 11600, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 413.99574518203735 seconds\n",
      "Step 11700, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 417.57763600349426 seconds\n",
      "Step 11800, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 421.17374777793884 seconds\n",
      "Step 11900, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 424.94711780548096 seconds\n",
      "Step 12000, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 428.52447056770325 seconds\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.2015, grad_fn=<SubBackward0>), time elapsed: 0.03687691688537598 seconds\n",
      "Step 100, loss: tensor(0.1894, grad_fn=<SubBackward0>), time elapsed: 3.6003315448760986 seconds\n",
      "Step 200, loss: tensor(0.1803, grad_fn=<SubBackward0>), time elapsed: 7.292380332946777 seconds\n",
      "Step 300, loss: tensor(0.1633, grad_fn=<SubBackward0>), time elapsed: 10.794636487960815 seconds\n",
      "Step 400, loss: tensor(0.1461, grad_fn=<SubBackward0>), time elapsed: 14.320456981658936 seconds\n",
      "Step 500, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 17.99337077140808 seconds\n",
      "Step 600, loss: tensor(0.1134, grad_fn=<SubBackward0>), time elapsed: 21.47077751159668 seconds\n",
      "Step 700, loss: tensor(0.1089, grad_fn=<SubBackward0>), time elapsed: 24.951905965805054 seconds\n",
      "Step 800, loss: tensor(0.1048, grad_fn=<SubBackward0>), time elapsed: 28.435869693756104 seconds\n",
      "Step 900, loss: tensor(0.0957, grad_fn=<SubBackward0>), time elapsed: 32.15460467338562 seconds\n",
      "Step 1000, loss: tensor(0.0917, grad_fn=<SubBackward0>), time elapsed: 35.634787797927856 seconds\n",
      "Step 1100, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 39.125730752944946 seconds\n",
      "Step 1200, loss: tensor(0.0848, grad_fn=<SubBackward0>), time elapsed: 42.818875789642334 seconds\n",
      "Step 1300, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 46.307124853134155 seconds\n",
      "Step 1400, loss: tensor(0.0790, grad_fn=<SubBackward0>), time elapsed: 49.79448914527893 seconds\n",
      "Step 1500, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 53.47507381439209 seconds\n",
      "Step 1600, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 56.99120330810547 seconds\n",
      "Step 1700, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 60.468921422958374 seconds\n",
      "Step 1800, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 64.17106986045837 seconds\n",
      "Step 1900, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 67.66582179069519 seconds\n",
      "Step 2000, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 71.18515849113464 seconds\n",
      "Step 2100, loss: tensor(0.0721, grad_fn=<SubBackward0>), time elapsed: 74.88327097892761 seconds\n",
      "Step 2200, loss: tensor(0.0711, grad_fn=<SubBackward0>), time elapsed: 78.35632967948914 seconds\n",
      "Step 2300, loss: tensor(0.0715, grad_fn=<SubBackward0>), time elapsed: 81.82822513580322 seconds\n",
      "Step 2400, loss: tensor(0.0715, grad_fn=<SubBackward0>), time elapsed: 85.54828310012817 seconds\n",
      "Step 2500, loss: tensor(0.0709, grad_fn=<SubBackward0>), time elapsed: 89.03128433227539 seconds\n",
      "Step 2600, loss: tensor(0.0690, grad_fn=<SubBackward0>), time elapsed: 92.52457022666931 seconds\n",
      "Step 2700, loss: tensor(0.0685, grad_fn=<SubBackward0>), time elapsed: 96.23887872695923 seconds\n",
      "Step 2800, loss: tensor(0.0692, grad_fn=<SubBackward0>), time elapsed: 99.7676932811737 seconds\n",
      "Step 2900, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 103.25800585746765 seconds\n",
      "Step 3000, loss: tensor(0.0702, grad_fn=<SubBackward0>), time elapsed: 106.9433183670044 seconds\n",
      "Step 3100, loss: tensor(0.0670, grad_fn=<SubBackward0>), time elapsed: 110.45629715919495 seconds\n",
      "Step 3200, loss: tensor(0.0685, grad_fn=<SubBackward0>), time elapsed: 113.96771669387817 seconds\n",
      "Step 3300, loss: tensor(0.0679, grad_fn=<SubBackward0>), time elapsed: 117.43649077415466 seconds\n",
      "Step 3400, loss: tensor(0.0663, grad_fn=<SubBackward0>), time elapsed: 121.09642434120178 seconds\n",
      "Step 3500, loss: tensor(0.0669, grad_fn=<SubBackward0>), time elapsed: 124.57906436920166 seconds\n",
      "Step 3600, loss: tensor(0.0682, grad_fn=<SubBackward0>), time elapsed: 128.08984661102295 seconds\n",
      "Step 3700, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 131.77373027801514 seconds\n",
      "Step 3800, loss: tensor(0.0657, grad_fn=<SubBackward0>), time elapsed: 135.25247406959534 seconds\n",
      "Step 3900, loss: tensor(0.0666, grad_fn=<SubBackward0>), time elapsed: 138.73375248908997 seconds\n",
      "Step 4000, loss: tensor(0.0670, grad_fn=<SubBackward0>), time elapsed: 142.40063571929932 seconds\n",
      "Step 4100, loss: tensor(0.0664, grad_fn=<SubBackward0>), time elapsed: 145.9128520488739 seconds\n",
      "Step 4200, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 149.40066289901733 seconds\n",
      "Step 4300, loss: tensor(0.0637, grad_fn=<SubBackward0>), time elapsed: 153.10866570472717 seconds\n",
      "Step 4400, loss: tensor(0.0633, grad_fn=<SubBackward0>), time elapsed: 156.5950162410736 seconds\n",
      "Step 4500, loss: tensor(0.0637, grad_fn=<SubBackward0>), time elapsed: 160.10453391075134 seconds\n",
      "Step 4600, loss: tensor(0.0665, grad_fn=<SubBackward0>), time elapsed: 163.8174545764923 seconds\n",
      "Step 4700, loss: tensor(0.0649, grad_fn=<SubBackward0>), time elapsed: 167.3066327571869 seconds\n",
      "Step 4800, loss: tensor(0.0630, grad_fn=<SubBackward0>), time elapsed: 170.78230452537537 seconds\n",
      "Step 4900, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 174.48328924179077 seconds\n",
      "Step 5000, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 177.96110081672668 seconds\n",
      "Step 5100, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 181.4703586101532 seconds\n",
      "Step 5200, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 185.1857476234436 seconds\n",
      "Step 5300, loss: tensor(0.0618, grad_fn=<SubBackward0>), time elapsed: 188.7141501903534 seconds\n",
      "Step 5400, loss: tensor(0.0611, grad_fn=<SubBackward0>), time elapsed: 192.2051558494568 seconds\n",
      "Step 5500, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 195.89841485023499 seconds\n",
      "Step 5600, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 199.42774295806885 seconds\n",
      "Step 5700, loss: tensor(0.0568, grad_fn=<SubBackward0>), time elapsed: 202.95057678222656 seconds\n",
      "Step 5800, loss: tensor(0.0562, grad_fn=<SubBackward0>), time elapsed: 206.637104511261 seconds\n",
      "Step 5900, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 210.09954857826233 seconds\n",
      "Step 6000, loss: tensor(0.0560, grad_fn=<SubBackward0>), time elapsed: 213.6169774532318 seconds\n",
      "Step 6100, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 217.0904860496521 seconds\n",
      "Step 6200, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 220.80987548828125 seconds\n",
      "Step 6300, loss: tensor(0.0583, grad_fn=<SubBackward0>), time elapsed: 224.31483364105225 seconds\n",
      "Step 6400, loss: tensor(0.0573, grad_fn=<SubBackward0>), time elapsed: 227.80624175071716 seconds\n",
      "Step 6500, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 231.52043318748474 seconds\n",
      "Step 6600, loss: tensor(0.0559, grad_fn=<SubBackward0>), time elapsed: 235.053040266037 seconds\n",
      "Step 6700, loss: tensor(0.0552, grad_fn=<SubBackward0>), time elapsed: 238.54092478752136 seconds\n",
      "Step 6800, loss: tensor(0.0540, grad_fn=<SubBackward0>), time elapsed: 242.23649764060974 seconds\n",
      "Step 6900, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 245.71205306053162 seconds\n",
      "Step 7000, loss: tensor(0.0552, grad_fn=<SubBackward0>), time elapsed: 249.18530750274658 seconds\n",
      "Step 7100, loss: tensor(0.0552, grad_fn=<SubBackward0>), time elapsed: 252.89419269561768 seconds\n",
      "Step 7200, loss: tensor(0.0542, grad_fn=<SubBackward0>), time elapsed: 256.3761639595032 seconds\n",
      "Step 7300, loss: tensor(0.0563, grad_fn=<SubBackward0>), time elapsed: 259.9021520614624 seconds\n",
      "Step 7400, loss: tensor(0.0553, grad_fn=<SubBackward0>), time elapsed: 263.6087155342102 seconds\n",
      "Step 7500, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 267.1247069835663 seconds\n",
      "Step 7600, loss: tensor(0.0553, grad_fn=<SubBackward0>), time elapsed: 270.6600344181061 seconds\n",
      "Step 7700, loss: tensor(0.0546, grad_fn=<SubBackward0>), time elapsed: 274.3736970424652 seconds\n",
      "Step 7800, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 277.8525342941284 seconds\n",
      "Step 7900, loss: tensor(0.0547, grad_fn=<SubBackward0>), time elapsed: 281.33570432662964 seconds\n",
      "Step 8000, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 285.02207612991333 seconds\n",
      "Step 8100, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 288.55218029022217 seconds\n",
      "Step 8200, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 292.0375919342041 seconds\n",
      "Step 8300, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 295.7260994911194 seconds\n",
      "Step 8400, loss: tensor(0.0546, grad_fn=<SubBackward0>), time elapsed: 299.2528645992279 seconds\n",
      "Step 8500, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 302.75338435173035 seconds\n",
      "Step 8600, loss: tensor(0.0542, grad_fn=<SubBackward0>), time elapsed: 306.2585446834564 seconds\n",
      "Step 8700, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 310.0036609172821 seconds\n",
      "Step 8800, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 313.4901840686798 seconds\n",
      "Step 8900, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 316.96836733818054 seconds\n",
      "Step 9000, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 320.7115330696106 seconds\n",
      "Step 9100, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 324.2073919773102 seconds\n",
      "Step 9200, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 327.69666504859924 seconds\n",
      "Step 9300, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 331.3797948360443 seconds\n",
      "Step 9400, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 334.8755958080292 seconds\n",
      "Step 9500, loss: tensor(0.0441, grad_fn=<SubBackward0>), time elapsed: 338.3930025100708 seconds\n",
      "Step 9600, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 342.11319279670715 seconds\n",
      "Step 9700, loss: tensor(0.0448, grad_fn=<SubBackward0>), time elapsed: 345.665025472641 seconds\n",
      "Step 9800, loss: tensor(0.0474, grad_fn=<SubBackward0>), time elapsed: 349.17984342575073 seconds\n",
      "Step 9900, loss: tensor(0.0445, grad_fn=<SubBackward0>), time elapsed: 352.90472507476807 seconds\n",
      "Step 10000, loss: tensor(0.0436, grad_fn=<SubBackward0>), time elapsed: 356.4390835762024 seconds\n",
      "Step 10100, loss: tensor(0.0442, grad_fn=<SubBackward0>), time elapsed: 359.9258396625519 seconds\n",
      "Step 10200, loss: tensor(0.0453, grad_fn=<SubBackward0>), time elapsed: 363.6385610103607 seconds\n",
      "Step 10300, loss: tensor(0.0453, grad_fn=<SubBackward0>), time elapsed: 367.14230704307556 seconds\n",
      "Step 10400, loss: tensor(0.0409, grad_fn=<SubBackward0>), time elapsed: 370.6459484100342 seconds\n",
      "Step 10500, loss: tensor(0.0442, grad_fn=<SubBackward0>), time elapsed: 374.40724635124207 seconds\n",
      "Step 10600, loss: tensor(0.0432, grad_fn=<SubBackward0>), time elapsed: 377.91633701324463 seconds\n",
      "Step 10700, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 381.4641606807709 seconds\n",
      "Step 10800, loss: tensor(0.0422, grad_fn=<SubBackward0>), time elapsed: 385.1826841831207 seconds\n",
      "Step 10900, loss: tensor(0.0409, grad_fn=<SubBackward0>), time elapsed: 388.7037687301636 seconds\n",
      "Step 11000, loss: tensor(0.0424, grad_fn=<SubBackward0>), time elapsed: 392.22152733802795 seconds\n",
      "Step 11100, loss: tensor(0.0409, grad_fn=<SubBackward0>), time elapsed: 395.7448511123657 seconds\n",
      "Step 11200, loss: tensor(0.0408, grad_fn=<SubBackward0>), time elapsed: 399.4555869102478 seconds\n",
      "Step 11300, loss: tensor(0.0413, grad_fn=<SubBackward0>), time elapsed: 402.981689453125 seconds\n",
      "Step 11400, loss: tensor(0.0392, grad_fn=<SubBackward0>), time elapsed: 406.5026786327362 seconds\n",
      "Step 11500, loss: tensor(0.0397, grad_fn=<SubBackward0>), time elapsed: 410.2211377620697 seconds\n",
      "Step 11600, loss: tensor(0.0398, grad_fn=<SubBackward0>), time elapsed: 413.7435071468353 seconds\n",
      "Step 11700, loss: tensor(0.0414, grad_fn=<SubBackward0>), time elapsed: 417.3099410533905 seconds\n",
      "Step 11800, loss: tensor(0.0411, grad_fn=<SubBackward0>), time elapsed: 421.0946750640869 seconds\n",
      "Step 11900, loss: tensor(0.0424, grad_fn=<SubBackward0>), time elapsed: 424.6517195701599 seconds\n",
      "Step 12000, loss: tensor(0.0373, grad_fn=<SubBackward0>), time elapsed: 428.2661075592041 seconds\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.3271, grad_fn=<SubBackward0>), time elapsed: 0.03687644004821777 seconds\n",
      "Step 100, loss: tensor(0.2989, grad_fn=<SubBackward0>), time elapsed: 3.757495880126953 seconds\n",
      "Step 200, loss: tensor(0.2647, grad_fn=<SubBackward0>), time elapsed: 7.234053373336792 seconds\n",
      "Step 300, loss: tensor(0.2366, grad_fn=<SubBackward0>), time elapsed: 10.74651050567627 seconds\n",
      "Step 400, loss: tensor(0.2132, grad_fn=<SubBackward0>), time elapsed: 14.43767786026001 seconds\n",
      "Step 500, loss: tensor(0.1967, grad_fn=<SubBackward0>), time elapsed: 17.918778896331787 seconds\n",
      "Step 600, loss: tensor(0.1707, grad_fn=<SubBackward0>), time elapsed: 21.38995671272278 seconds\n",
      "Step 700, loss: tensor(0.1610, grad_fn=<SubBackward0>), time elapsed: 25.092495918273926 seconds\n",
      "Step 800, loss: tensor(0.1510, grad_fn=<SubBackward0>), time elapsed: 28.625974893569946 seconds\n",
      "Step 900, loss: tensor(0.1415, grad_fn=<SubBackward0>), time elapsed: 32.13244938850403 seconds\n",
      "Step 1000, loss: tensor(0.1342, grad_fn=<SubBackward0>), time elapsed: 35.82314896583557 seconds\n",
      "Step 1100, loss: tensor(0.1289, grad_fn=<SubBackward0>), time elapsed: 39.35225701332092 seconds\n",
      "Step 1200, loss: tensor(0.1271, grad_fn=<SubBackward0>), time elapsed: 42.86101055145264 seconds\n",
      "Step 1300, loss: tensor(0.1214, grad_fn=<SubBackward0>), time elapsed: 46.53383421897888 seconds\n",
      "Step 1400, loss: tensor(0.1186, grad_fn=<SubBackward0>), time elapsed: 50.01488733291626 seconds\n",
      "Step 1500, loss: tensor(0.1160, grad_fn=<SubBackward0>), time elapsed: 53.46460938453674 seconds\n",
      "Step 1600, loss: tensor(0.1157, grad_fn=<SubBackward0>), time elapsed: 57.1424994468689 seconds\n",
      "Step 1700, loss: tensor(0.1130, grad_fn=<SubBackward0>), time elapsed: 60.604055881500244 seconds\n",
      "Step 1800, loss: tensor(0.1106, grad_fn=<SubBackward0>), time elapsed: 64.09008455276489 seconds\n",
      "Step 1900, loss: tensor(0.1081, grad_fn=<SubBackward0>), time elapsed: 67.73874688148499 seconds\n",
      "Step 2000, loss: tensor(0.1072, grad_fn=<SubBackward0>), time elapsed: 71.22794365882874 seconds\n",
      "Step 2100, loss: tensor(0.1054, grad_fn=<SubBackward0>), time elapsed: 74.69085383415222 seconds\n",
      "Step 2200, loss: tensor(0.1056, grad_fn=<SubBackward0>), time elapsed: 78.34969425201416 seconds\n",
      "Step 2300, loss: tensor(0.1041, grad_fn=<SubBackward0>), time elapsed: 81.84709477424622 seconds\n",
      "Step 2400, loss: tensor(0.1029, grad_fn=<SubBackward0>), time elapsed: 85.30576419830322 seconds\n",
      "Step 2500, loss: tensor(0.1004, grad_fn=<SubBackward0>), time elapsed: 88.80630111694336 seconds\n",
      "Step 2600, loss: tensor(0.0999, grad_fn=<SubBackward0>), time elapsed: 92.44385147094727 seconds\n",
      "Step 2700, loss: tensor(0.0999, grad_fn=<SubBackward0>), time elapsed: 95.8963623046875 seconds\n",
      "Step 2800, loss: tensor(0.1005, grad_fn=<SubBackward0>), time elapsed: 99.3798828125 seconds\n",
      "Step 2900, loss: tensor(0.0978, grad_fn=<SubBackward0>), time elapsed: 103.0738046169281 seconds\n",
      "Step 3000, loss: tensor(0.0981, grad_fn=<SubBackward0>), time elapsed: 106.54976224899292 seconds\n",
      "Step 3100, loss: tensor(0.0986, grad_fn=<SubBackward0>), time elapsed: 109.99352693557739 seconds\n",
      "Step 3200, loss: tensor(0.0978, grad_fn=<SubBackward0>), time elapsed: 113.66174364089966 seconds\n",
      "Step 3300, loss: tensor(0.0993, grad_fn=<SubBackward0>), time elapsed: 117.17177033424377 seconds\n",
      "Step 3400, loss: tensor(0.0967, grad_fn=<SubBackward0>), time elapsed: 120.61980366706848 seconds\n",
      "Step 3500, loss: tensor(0.0970, grad_fn=<SubBackward0>), time elapsed: 124.29747295379639 seconds\n",
      "Step 3600, loss: tensor(0.0963, grad_fn=<SubBackward0>), time elapsed: 127.7514476776123 seconds\n",
      "Step 3700, loss: tensor(0.0973, grad_fn=<SubBackward0>), time elapsed: 131.23510694503784 seconds\n",
      "Step 3800, loss: tensor(0.0971, grad_fn=<SubBackward0>), time elapsed: 134.95332312583923 seconds\n",
      "Step 3900, loss: tensor(0.0956, grad_fn=<SubBackward0>), time elapsed: 138.49475955963135 seconds\n",
      "Step 4000, loss: tensor(0.0965, grad_fn=<SubBackward0>), time elapsed: 141.97409534454346 seconds\n",
      "Step 4100, loss: tensor(0.0968, grad_fn=<SubBackward0>), time elapsed: 145.69397377967834 seconds\n",
      "Step 4200, loss: tensor(0.0949, grad_fn=<SubBackward0>), time elapsed: 149.2082200050354 seconds\n",
      "Step 4300, loss: tensor(0.0938, grad_fn=<SubBackward0>), time elapsed: 152.73016262054443 seconds\n",
      "Step 4400, loss: tensor(0.0953, grad_fn=<SubBackward0>), time elapsed: 156.47632837295532 seconds\n",
      "Step 4500, loss: tensor(0.0952, grad_fn=<SubBackward0>), time elapsed: 160.01514196395874 seconds\n",
      "Step 4600, loss: tensor(0.0917, grad_fn=<SubBackward0>), time elapsed: 163.52149510383606 seconds\n",
      "Step 4700, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 167.22045803070068 seconds\n",
      "Step 4800, loss: tensor(0.0918, grad_fn=<SubBackward0>), time elapsed: 170.70065331459045 seconds\n",
      "Step 4900, loss: tensor(0.0910, grad_fn=<SubBackward0>), time elapsed: 174.22190380096436 seconds\n",
      "Step 5000, loss: tensor(0.0916, grad_fn=<SubBackward0>), time elapsed: 177.94413661956787 seconds\n",
      "Step 5100, loss: tensor(0.0885, grad_fn=<SubBackward0>), time elapsed: 181.4655110836029 seconds\n",
      "Step 5200, loss: tensor(0.0898, grad_fn=<SubBackward0>), time elapsed: 185.00257682800293 seconds\n",
      "Step 5300, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 188.71857142448425 seconds\n",
      "Step 5400, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 192.24916863441467 seconds\n",
      "Step 5500, loss: tensor(0.0899, grad_fn=<SubBackward0>), time elapsed: 195.75028133392334 seconds\n",
      "Step 5600, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 199.2463836669922 seconds\n",
      "Step 5700, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 202.96668410301208 seconds\n",
      "Step 5800, loss: tensor(0.0891, grad_fn=<SubBackward0>), time elapsed: 206.47585916519165 seconds\n",
      "Step 5900, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 209.99464058876038 seconds\n",
      "Step 6000, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 213.73365330696106 seconds\n",
      "Step 6100, loss: tensor(0.0848, grad_fn=<SubBackward0>), time elapsed: 217.22430896759033 seconds\n",
      "Step 6200, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 220.70340394973755 seconds\n",
      "Step 6300, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 224.42201018333435 seconds\n",
      "Step 6400, loss: tensor(0.0831, grad_fn=<SubBackward0>), time elapsed: 227.9211938381195 seconds\n",
      "Step 6500, loss: tensor(0.0850, grad_fn=<SubBackward0>), time elapsed: 231.4284040927887 seconds\n",
      "Step 6600, loss: tensor(0.0833, grad_fn=<SubBackward0>), time elapsed: 235.13138890266418 seconds\n",
      "Step 6700, loss: tensor(0.0838, grad_fn=<SubBackward0>), time elapsed: 238.65404057502747 seconds\n",
      "Step 6800, loss: tensor(0.0830, grad_fn=<SubBackward0>), time elapsed: 242.15638542175293 seconds\n",
      "Step 6900, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 245.8560380935669 seconds\n",
      "Step 7000, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 249.33628034591675 seconds\n",
      "Step 7100, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 252.8618655204773 seconds\n",
      "Step 7200, loss: tensor(0.0789, grad_fn=<SubBackward0>), time elapsed: 256.5558967590332 seconds\n",
      "Step 7300, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 260.0798909664154 seconds\n",
      "Step 7400, loss: tensor(0.0781, grad_fn=<SubBackward0>), time elapsed: 263.5621235370636 seconds\n",
      "Step 7500, loss: tensor(0.0785, grad_fn=<SubBackward0>), time elapsed: 267.25367617607117 seconds\n",
      "Step 7600, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 270.73366045951843 seconds\n",
      "Step 7700, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 274.2268753051758 seconds\n",
      "Step 7800, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 277.9123227596283 seconds\n",
      "Step 7900, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 281.4204123020172 seconds\n",
      "Step 8000, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 284.9055275917053 seconds\n",
      "Step 8100, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 288.39195680618286 seconds\n",
      "Step 8200, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 292.13750171661377 seconds\n",
      "Step 8300, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 295.6619005203247 seconds\n",
      "Step 8400, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 299.20035457611084 seconds\n",
      "Step 8500, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 302.8998193740845 seconds\n",
      "Step 8600, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 306.4498174190521 seconds\n",
      "Step 8700, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 309.9933400154114 seconds\n",
      "Step 8800, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 313.70597982406616 seconds\n",
      "Step 8900, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 317.24566626548767 seconds\n",
      "Step 9000, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 320.7453787326813 seconds\n",
      "Step 9100, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 324.4648151397705 seconds\n",
      "Step 9200, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 328.02026081085205 seconds\n",
      "Step 9300, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 331.51993894577026 seconds\n",
      "Step 9400, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 335.2359149456024 seconds\n",
      "Step 9500, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 338.7615125179291 seconds\n",
      "Step 9600, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 342.28998589515686 seconds\n",
      "Step 9700, loss: tensor(0.0779, grad_fn=<SubBackward0>), time elapsed: 346.0094299316406 seconds\n",
      "Step 9800, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 349.55273604393005 seconds\n",
      "Step 9900, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 353.02544236183167 seconds\n",
      "Step 10000, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 356.72311544418335 seconds\n",
      "Step 10100, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 360.2416055202484 seconds\n",
      "Step 10200, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 363.7447953224182 seconds\n",
      "Step 10300, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 367.4685513973236 seconds\n",
      "Step 10400, loss: tensor(0.0770, grad_fn=<SubBackward0>), time elapsed: 371.0113170146942 seconds\n",
      "Step 10500, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 374.51596784591675 seconds\n",
      "Step 10600, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 378.0410611629486 seconds\n",
      "Step 10700, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 381.79274106025696 seconds\n",
      "Step 10800, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 385.3477146625519 seconds\n",
      "Step 10900, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 388.8530304431915 seconds\n",
      "Step 11000, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 392.5878601074219 seconds\n",
      "Step 11100, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 396.10173535346985 seconds\n",
      "Step 11200, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 399.6084704399109 seconds\n",
      "Step 11300, loss: tensor(0.0720, grad_fn=<SubBackward0>), time elapsed: 403.3668038845062 seconds\n",
      "Step 11400, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 406.90205216407776 seconds\n",
      "Step 11500, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 410.44654846191406 seconds\n",
      "Step 11600, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 414.21003699302673 seconds\n",
      "Step 11700, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 417.82642245292664 seconds\n",
      "Step 11800, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 421.44743180274963 seconds\n",
      "Step 11900, loss: tensor(0.0712, grad_fn=<SubBackward0>), time elapsed: 425.28258442878723 seconds\n",
      "Step 12000, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 428.9039306640625 seconds\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.4868, grad_fn=<SubBackward0>), time elapsed: 0.03986668586730957 seconds\n",
      "Step 100, loss: tensor(0.4339, grad_fn=<SubBackward0>), time elapsed: 3.586085796356201 seconds\n",
      "Step 200, loss: tensor(0.3647, grad_fn=<SubBackward0>), time elapsed: 7.2689292430877686 seconds\n",
      "Step 300, loss: tensor(0.2991, grad_fn=<SubBackward0>), time elapsed: 10.735272884368896 seconds\n",
      "Step 400, loss: tensor(0.2551, grad_fn=<SubBackward0>), time elapsed: 14.209991455078125 seconds\n",
      "Step 500, loss: tensor(0.2274, grad_fn=<SubBackward0>), time elapsed: 17.911296367645264 seconds\n",
      "Step 600, loss: tensor(0.2095, grad_fn=<SubBackward0>), time elapsed: 21.424159288406372 seconds\n",
      "Step 700, loss: tensor(0.1922, grad_fn=<SubBackward0>), time elapsed: 24.943932056427002 seconds\n",
      "Step 800, loss: tensor(0.1832, grad_fn=<SubBackward0>), time elapsed: 28.69083595275879 seconds\n",
      "Step 900, loss: tensor(0.1815, grad_fn=<SubBackward0>), time elapsed: 32.21791887283325 seconds\n",
      "Step 1000, loss: tensor(0.1750, grad_fn=<SubBackward0>), time elapsed: 35.7135853767395 seconds\n",
      "Step 1100, loss: tensor(0.1729, grad_fn=<SubBackward0>), time elapsed: 39.4170618057251 seconds\n",
      "Step 1200, loss: tensor(0.1651, grad_fn=<SubBackward0>), time elapsed: 42.94035220146179 seconds\n",
      "Step 1300, loss: tensor(0.1635, grad_fn=<SubBackward0>), time elapsed: 46.4602792263031 seconds\n",
      "Step 1400, loss: tensor(0.1588, grad_fn=<SubBackward0>), time elapsed: 50.130321979522705 seconds\n",
      "Step 1500, loss: tensor(0.1573, grad_fn=<SubBackward0>), time elapsed: 53.60097146034241 seconds\n",
      "Step 1600, loss: tensor(0.1561, grad_fn=<SubBackward0>), time elapsed: 57.08697438240051 seconds\n",
      "Step 1700, loss: tensor(0.1553, grad_fn=<SubBackward0>), time elapsed: 60.595881938934326 seconds\n",
      "Step 1800, loss: tensor(0.1514, grad_fn=<SubBackward0>), time elapsed: 64.26115131378174 seconds\n",
      "Step 1900, loss: tensor(0.1554, grad_fn=<SubBackward0>), time elapsed: 67.759605884552 seconds\n",
      "Step 2000, loss: tensor(0.1528, grad_fn=<SubBackward0>), time elapsed: 71.23480606079102 seconds\n",
      "Step 2100, loss: tensor(0.1527, grad_fn=<SubBackward0>), time elapsed: 74.94386458396912 seconds\n",
      "Step 2200, loss: tensor(0.1532, grad_fn=<SubBackward0>), time elapsed: 78.49336385726929 seconds\n",
      "Step 2300, loss: tensor(0.1496, grad_fn=<SubBackward0>), time elapsed: 82.04436564445496 seconds\n",
      "Step 2400, loss: tensor(0.1514, grad_fn=<SubBackward0>), time elapsed: 85.72322869300842 seconds\n",
      "Step 2500, loss: tensor(0.1505, grad_fn=<SubBackward0>), time elapsed: 89.24671840667725 seconds\n",
      "Step 2600, loss: tensor(0.1489, grad_fn=<SubBackward0>), time elapsed: 92.77589583396912 seconds\n",
      "Step 2700, loss: tensor(0.1488, grad_fn=<SubBackward0>), time elapsed: 96.47841024398804 seconds\n",
      "Step 2800, loss: tensor(0.1483, grad_fn=<SubBackward0>), time elapsed: 100.00764060020447 seconds\n",
      "Step 2900, loss: tensor(0.1484, grad_fn=<SubBackward0>), time elapsed: 103.48688626289368 seconds\n",
      "Step 3000, loss: tensor(0.1472, grad_fn=<SubBackward0>), time elapsed: 107.19556307792664 seconds\n",
      "Step 3100, loss: tensor(0.1474, grad_fn=<SubBackward0>), time elapsed: 110.73387694358826 seconds\n",
      "Step 3200, loss: tensor(0.1452, grad_fn=<SubBackward0>), time elapsed: 114.24414825439453 seconds\n",
      "Step 3300, loss: tensor(0.1414, grad_fn=<SubBackward0>), time elapsed: 117.91267442703247 seconds\n",
      "Step 3400, loss: tensor(0.1424, grad_fn=<SubBackward0>), time elapsed: 121.40591716766357 seconds\n",
      "Step 3500, loss: tensor(0.1428, grad_fn=<SubBackward0>), time elapsed: 124.88213181495667 seconds\n",
      "Step 3600, loss: tensor(0.1409, grad_fn=<SubBackward0>), time elapsed: 128.5844156742096 seconds\n",
      "Step 3700, loss: tensor(0.1414, grad_fn=<SubBackward0>), time elapsed: 132.05582308769226 seconds\n",
      "Step 3800, loss: tensor(0.1398, grad_fn=<SubBackward0>), time elapsed: 135.56135392189026 seconds\n",
      "Step 3900, loss: tensor(0.1377, grad_fn=<SubBackward0>), time elapsed: 139.2904782295227 seconds\n",
      "Step 4000, loss: tensor(0.1387, grad_fn=<SubBackward0>), time elapsed: 142.82724165916443 seconds\n",
      "Step 4100, loss: tensor(0.1416, grad_fn=<SubBackward0>), time elapsed: 146.3148205280304 seconds\n",
      "Step 4200, loss: tensor(0.1351, grad_fn=<SubBackward0>), time elapsed: 150.01910638809204 seconds\n",
      "Step 4300, loss: tensor(0.1367, grad_fn=<SubBackward0>), time elapsed: 153.5464162826538 seconds\n",
      "Step 4400, loss: tensor(0.1360, grad_fn=<SubBackward0>), time elapsed: 157.09180641174316 seconds\n",
      "Step 4500, loss: tensor(0.1377, grad_fn=<SubBackward0>), time elapsed: 160.78406739234924 seconds\n",
      "Step 4600, loss: tensor(0.1375, grad_fn=<SubBackward0>), time elapsed: 164.24925327301025 seconds\n",
      "Step 4700, loss: tensor(0.1358, grad_fn=<SubBackward0>), time elapsed: 167.74472975730896 seconds\n",
      "Step 4800, loss: tensor(0.1365, grad_fn=<SubBackward0>), time elapsed: 171.4136130809784 seconds\n",
      "Step 4900, loss: tensor(0.1352, grad_fn=<SubBackward0>), time elapsed: 174.88464045524597 seconds\n",
      "Step 5000, loss: tensor(0.1354, grad_fn=<SubBackward0>), time elapsed: 178.37086153030396 seconds\n",
      "Step 5100, loss: tensor(0.1377, grad_fn=<SubBackward0>), time elapsed: 181.90947556495667 seconds\n",
      "Step 5200, loss: tensor(0.1320, grad_fn=<SubBackward0>), time elapsed: 185.63814854621887 seconds\n",
      "Step 5300, loss: tensor(0.1383, grad_fn=<SubBackward0>), time elapsed: 189.13494563102722 seconds\n",
      "Step 5400, loss: tensor(0.1351, grad_fn=<SubBackward0>), time elapsed: 192.6092176437378 seconds\n",
      "Step 5500, loss: tensor(0.1361, grad_fn=<SubBackward0>), time elapsed: 196.34551548957825 seconds\n",
      "Step 5600, loss: tensor(0.1321, grad_fn=<SubBackward0>), time elapsed: 199.86051440238953 seconds\n",
      "Step 5700, loss: tensor(0.1344, grad_fn=<SubBackward0>), time elapsed: 203.33268761634827 seconds\n",
      "Step 5800, loss: tensor(0.1369, grad_fn=<SubBackward0>), time elapsed: 207.00132155418396 seconds\n",
      "Step 5900, loss: tensor(0.1341, grad_fn=<SubBackward0>), time elapsed: 210.52291297912598 seconds\n",
      "Step 6000, loss: tensor(0.1313, grad_fn=<SubBackward0>), time elapsed: 214.0125653743744 seconds\n",
      "Step 6100, loss: tensor(0.1320, grad_fn=<SubBackward0>), time elapsed: 217.70781540870667 seconds\n",
      "Step 6200, loss: tensor(0.1300, grad_fn=<SubBackward0>), time elapsed: 221.2011022567749 seconds\n",
      "Step 6300, loss: tensor(0.1304, grad_fn=<SubBackward0>), time elapsed: 224.70887780189514 seconds\n",
      "Step 6400, loss: tensor(0.1266, grad_fn=<SubBackward0>), time elapsed: 228.39574027061462 seconds\n",
      "Step 6500, loss: tensor(0.1312, grad_fn=<SubBackward0>), time elapsed: 231.92381048202515 seconds\n",
      "Step 6600, loss: tensor(0.1306, grad_fn=<SubBackward0>), time elapsed: 235.41085577011108 seconds\n",
      "Step 6700, loss: tensor(0.1275, grad_fn=<SubBackward0>), time elapsed: 239.11551332473755 seconds\n",
      "Step 6800, loss: tensor(0.1312, grad_fn=<SubBackward0>), time elapsed: 242.62265491485596 seconds\n",
      "Step 6900, loss: tensor(0.1264, grad_fn=<SubBackward0>), time elapsed: 246.11731958389282 seconds\n",
      "Step 7000, loss: tensor(0.1260, grad_fn=<SubBackward0>), time elapsed: 249.8263018131256 seconds\n",
      "Step 7100, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 253.33829164505005 seconds\n",
      "Step 7200, loss: tensor(0.1271, grad_fn=<SubBackward0>), time elapsed: 256.8237009048462 seconds\n",
      "Step 7300, loss: tensor(0.1229, grad_fn=<SubBackward0>), time elapsed: 260.52025985717773 seconds\n",
      "Step 7400, loss: tensor(0.1273, grad_fn=<SubBackward0>), time elapsed: 264.01499342918396 seconds\n",
      "Step 7500, loss: tensor(0.1242, grad_fn=<SubBackward0>), time elapsed: 267.54870343208313 seconds\n",
      "Step 7600, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 271.02920389175415 seconds\n",
      "Step 7700, loss: tensor(0.1254, grad_fn=<SubBackward0>), time elapsed: 274.73249650001526 seconds\n",
      "Step 7800, loss: tensor(0.1203, grad_fn=<SubBackward0>), time elapsed: 278.24168586730957 seconds\n",
      "Step 7900, loss: tensor(0.1226, grad_fn=<SubBackward0>), time elapsed: 281.7585368156433 seconds\n",
      "Step 8000, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 285.44670724868774 seconds\n",
      "Step 8100, loss: tensor(0.1219, grad_fn=<SubBackward0>), time elapsed: 288.98086619377136 seconds\n",
      "Step 8200, loss: tensor(0.1199, grad_fn=<SubBackward0>), time elapsed: 292.49554109573364 seconds\n",
      "Step 8300, loss: tensor(0.1206, grad_fn=<SubBackward0>), time elapsed: 296.20776748657227 seconds\n",
      "Step 8400, loss: tensor(0.1216, grad_fn=<SubBackward0>), time elapsed: 299.7793996334076 seconds\n",
      "Step 8500, loss: tensor(0.1223, grad_fn=<SubBackward0>), time elapsed: 303.3226046562195 seconds\n",
      "Step 8600, loss: tensor(0.1207, grad_fn=<SubBackward0>), time elapsed: 307.0374231338501 seconds\n",
      "Step 8700, loss: tensor(0.1206, grad_fn=<SubBackward0>), time elapsed: 310.58647632598877 seconds\n",
      "Step 8800, loss: tensor(0.1164, grad_fn=<SubBackward0>), time elapsed: 314.07488536834717 seconds\n",
      "Step 8900, loss: tensor(0.1195, grad_fn=<SubBackward0>), time elapsed: 317.84125232696533 seconds\n",
      "Step 9000, loss: tensor(0.1152, grad_fn=<SubBackward0>), time elapsed: 321.3901000022888 seconds\n",
      "Step 9100, loss: tensor(0.1160, grad_fn=<SubBackward0>), time elapsed: 324.9050693511963 seconds\n",
      "Step 9200, loss: tensor(0.1238, grad_fn=<SubBackward0>), time elapsed: 328.62646079063416 seconds\n",
      "Step 9300, loss: tensor(0.1154, grad_fn=<SubBackward0>), time elapsed: 332.1379511356354 seconds\n",
      "Step 9400, loss: tensor(0.1188, grad_fn=<SubBackward0>), time elapsed: 335.62535643577576 seconds\n",
      "Step 9500, loss: tensor(0.1141, grad_fn=<SubBackward0>), time elapsed: 339.3285071849823 seconds\n",
      "Step 9600, loss: tensor(0.1144, grad_fn=<SubBackward0>), time elapsed: 342.8393352031708 seconds\n",
      "Step 9700, loss: tensor(0.1164, grad_fn=<SubBackward0>), time elapsed: 346.37410140037537 seconds\n",
      "Step 9800, loss: tensor(0.1107, grad_fn=<SubBackward0>), time elapsed: 350.0745403766632 seconds\n",
      "Step 9900, loss: tensor(0.1142, grad_fn=<SubBackward0>), time elapsed: 353.5964503288269 seconds\n",
      "Step 10000, loss: tensor(0.1166, grad_fn=<SubBackward0>), time elapsed: 357.08663964271545 seconds\n",
      "Step 10100, loss: tensor(0.1130, grad_fn=<SubBackward0>), time elapsed: 360.6180703639984 seconds\n",
      "Step 10200, loss: tensor(0.1138, grad_fn=<SubBackward0>), time elapsed: 364.34438920021057 seconds\n",
      "Step 10300, loss: tensor(0.1178, grad_fn=<SubBackward0>), time elapsed: 367.82724833488464 seconds\n",
      "Step 10400, loss: tensor(0.1149, grad_fn=<SubBackward0>), time elapsed: 371.32070422172546 seconds\n",
      "Step 10500, loss: tensor(0.1167, grad_fn=<SubBackward0>), time elapsed: 375.0774610042572 seconds\n",
      "Step 10600, loss: tensor(0.1111, grad_fn=<SubBackward0>), time elapsed: 378.58469796180725 seconds\n",
      "Step 10700, loss: tensor(0.1152, grad_fn=<SubBackward0>), time elapsed: 382.0820996761322 seconds\n",
      "Step 10800, loss: tensor(0.1193, grad_fn=<SubBackward0>), time elapsed: 385.8090546131134 seconds\n",
      "Step 10900, loss: tensor(0.1153, grad_fn=<SubBackward0>), time elapsed: 389.3706896305084 seconds\n",
      "Step 11000, loss: tensor(0.1134, grad_fn=<SubBackward0>), time elapsed: 392.8910303115845 seconds\n",
      "Step 11100, loss: tensor(0.1176, grad_fn=<SubBackward0>), time elapsed: 396.6231598854065 seconds\n",
      "Step 11200, loss: tensor(0.1155, grad_fn=<SubBackward0>), time elapsed: 400.16244411468506 seconds\n",
      "Step 11300, loss: tensor(0.1122, grad_fn=<SubBackward0>), time elapsed: 403.6933808326721 seconds\n",
      "Step 11400, loss: tensor(0.1162, grad_fn=<SubBackward0>), time elapsed: 407.46065521240234 seconds\n",
      "Step 11500, loss: tensor(0.1158, grad_fn=<SubBackward0>), time elapsed: 411.0295886993408 seconds\n",
      "Step 11600, loss: tensor(0.1136, grad_fn=<SubBackward0>), time elapsed: 414.55330204963684 seconds\n",
      "Step 11700, loss: tensor(0.1164, grad_fn=<SubBackward0>), time elapsed: 418.3542215824127 seconds\n",
      "Step 11800, loss: tensor(0.1090, grad_fn=<SubBackward0>), time elapsed: 421.91415548324585 seconds\n",
      "Step 11900, loss: tensor(0.1138, grad_fn=<SubBackward0>), time elapsed: 425.46621584892273 seconds\n",
      "Step 12000, loss: tensor(0.1159, grad_fn=<SubBackward0>), time elapsed: 429.2565882205963 seconds\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.6593, grad_fn=<SubBackward0>), time elapsed: 0.03687644004821777 seconds\n",
      "Step 100, loss: tensor(0.5984, grad_fn=<SubBackward0>), time elapsed: 3.567805290222168 seconds\n",
      "Step 200, loss: tensor(0.5332, grad_fn=<SubBackward0>), time elapsed: 7.049844741821289 seconds\n",
      "Step 300, loss: tensor(0.4926, grad_fn=<SubBackward0>), time elapsed: 10.73621654510498 seconds\n",
      "Step 400, loss: tensor(0.4432, grad_fn=<SubBackward0>), time elapsed: 14.272724866867065 seconds\n",
      "Step 500, loss: tensor(0.4078, grad_fn=<SubBackward0>), time elapsed: 17.812486171722412 seconds\n",
      "Step 600, loss: tensor(0.3682, grad_fn=<SubBackward0>), time elapsed: 21.540273189544678 seconds\n",
      "Step 700, loss: tensor(0.3365, grad_fn=<SubBackward0>), time elapsed: 25.027557849884033 seconds\n",
      "Step 800, loss: tensor(0.3140, grad_fn=<SubBackward0>), time elapsed: 28.526965379714966 seconds\n",
      "Step 900, loss: tensor(0.2981, grad_fn=<SubBackward0>), time elapsed: 32.01567029953003 seconds\n",
      "Step 1000, loss: tensor(0.2849, grad_fn=<SubBackward0>), time elapsed: 35.70251774787903 seconds\n",
      "Step 1100, loss: tensor(0.2739, grad_fn=<SubBackward0>), time elapsed: 39.247493743896484 seconds\n",
      "Step 1200, loss: tensor(0.2719, grad_fn=<SubBackward0>), time elapsed: 42.78779602050781 seconds\n",
      "Step 1300, loss: tensor(0.2594, grad_fn=<SubBackward0>), time elapsed: 46.515013694763184 seconds\n",
      "Step 1400, loss: tensor(0.2514, grad_fn=<SubBackward0>), time elapsed: 49.999606132507324 seconds\n",
      "Step 1500, loss: tensor(0.2476, grad_fn=<SubBackward0>), time elapsed: 53.50645899772644 seconds\n",
      "Step 1600, loss: tensor(0.2443, grad_fn=<SubBackward0>), time elapsed: 57.17971992492676 seconds\n",
      "Step 1700, loss: tensor(0.2340, grad_fn=<SubBackward0>), time elapsed: 60.684309244155884 seconds\n",
      "Step 1800, loss: tensor(0.2313, grad_fn=<SubBackward0>), time elapsed: 64.1839611530304 seconds\n",
      "Step 1900, loss: tensor(0.2233, grad_fn=<SubBackward0>), time elapsed: 67.87808322906494 seconds\n",
      "Step 2000, loss: tensor(0.2231, grad_fn=<SubBackward0>), time elapsed: 71.35493659973145 seconds\n",
      "Step 2100, loss: tensor(0.2213, grad_fn=<SubBackward0>), time elapsed: 74.82725286483765 seconds\n",
      "Step 2200, loss: tensor(0.2171, grad_fn=<SubBackward0>), time elapsed: 78.51684403419495 seconds\n",
      "Step 2300, loss: tensor(0.2071, grad_fn=<SubBackward0>), time elapsed: 82.0258378982544 seconds\n",
      "Step 2400, loss: tensor(0.2105, grad_fn=<SubBackward0>), time elapsed: 85.5078809261322 seconds\n",
      "Step 2500, loss: tensor(0.2065, grad_fn=<SubBackward0>), time elapsed: 89.2125563621521 seconds\n",
      "Step 2600, loss: tensor(0.2048, grad_fn=<SubBackward0>), time elapsed: 92.7253258228302 seconds\n",
      "Step 2700, loss: tensor(0.1961, grad_fn=<SubBackward0>), time elapsed: 96.21396517753601 seconds\n",
      "Step 2800, loss: tensor(0.1896, grad_fn=<SubBackward0>), time elapsed: 99.90121912956238 seconds\n",
      "Step 2900, loss: tensor(0.1922, grad_fn=<SubBackward0>), time elapsed: 103.42528009414673 seconds\n",
      "Step 3000, loss: tensor(0.1831, grad_fn=<SubBackward0>), time elapsed: 106.90771818161011 seconds\n",
      "Step 3100, loss: tensor(0.1752, grad_fn=<SubBackward0>), time elapsed: 110.61355972290039 seconds\n",
      "Step 3200, loss: tensor(0.1815, grad_fn=<SubBackward0>), time elapsed: 114.0924756526947 seconds\n",
      "Step 3300, loss: tensor(0.1676, grad_fn=<SubBackward0>), time elapsed: 117.56942677497864 seconds\n",
      "Step 3400, loss: tensor(0.1710, grad_fn=<SubBackward0>), time elapsed: 121.26389670372009 seconds\n",
      "Step 3500, loss: tensor(0.1625, grad_fn=<SubBackward0>), time elapsed: 124.78663349151611 seconds\n",
      "Step 3600, loss: tensor(0.1657, grad_fn=<SubBackward0>), time elapsed: 128.2637016773224 seconds\n",
      "Step 3700, loss: tensor(0.1580, grad_fn=<SubBackward0>), time elapsed: 131.98291778564453 seconds\n",
      "Step 3800, loss: tensor(0.1558, grad_fn=<SubBackward0>), time elapsed: 135.50314331054688 seconds\n",
      "Step 3900, loss: tensor(0.1547, grad_fn=<SubBackward0>), time elapsed: 139.02165269851685 seconds\n",
      "Step 4000, loss: tensor(0.1564, grad_fn=<SubBackward0>), time elapsed: 142.69469499588013 seconds\n",
      "Step 4100, loss: tensor(0.1529, grad_fn=<SubBackward0>), time elapsed: 146.1575801372528 seconds\n",
      "Step 4200, loss: tensor(0.1528, grad_fn=<SubBackward0>), time elapsed: 149.64132928848267 seconds\n",
      "Step 4300, loss: tensor(0.1487, grad_fn=<SubBackward0>), time elapsed: 153.13567805290222 seconds\n",
      "Step 4400, loss: tensor(0.1463, grad_fn=<SubBackward0>), time elapsed: 156.8926224708557 seconds\n",
      "Step 4500, loss: tensor(0.1475, grad_fn=<SubBackward0>), time elapsed: 160.44505214691162 seconds\n",
      "Step 4600, loss: tensor(0.1461, grad_fn=<SubBackward0>), time elapsed: 163.96697854995728 seconds\n",
      "Step 4700, loss: tensor(0.1486, grad_fn=<SubBackward0>), time elapsed: 167.68489742279053 seconds\n",
      "Step 4800, loss: tensor(0.1443, grad_fn=<SubBackward0>), time elapsed: 171.1899013519287 seconds\n",
      "Step 4900, loss: tensor(0.1362, grad_fn=<SubBackward0>), time elapsed: 174.70849323272705 seconds\n",
      "Step 5000, loss: tensor(0.1418, grad_fn=<SubBackward0>), time elapsed: 178.43807673454285 seconds\n",
      "Step 5100, loss: tensor(0.1489, grad_fn=<SubBackward0>), time elapsed: 181.9307336807251 seconds\n",
      "Step 5200, loss: tensor(0.1416, grad_fn=<SubBackward0>), time elapsed: 185.48310732841492 seconds\n",
      "Step 5300, loss: tensor(0.1385, grad_fn=<SubBackward0>), time elapsed: 189.21054530143738 seconds\n",
      "Step 5400, loss: tensor(0.1438, grad_fn=<SubBackward0>), time elapsed: 192.77966809272766 seconds\n",
      "Step 5500, loss: tensor(0.1415, grad_fn=<SubBackward0>), time elapsed: 196.3189342021942 seconds\n",
      "Step 5600, loss: tensor(0.1426, grad_fn=<SubBackward0>), time elapsed: 200.02811288833618 seconds\n",
      "Step 5700, loss: tensor(0.1406, grad_fn=<SubBackward0>), time elapsed: 203.53125047683716 seconds\n",
      "Step 5800, loss: tensor(0.1354, grad_fn=<SubBackward0>), time elapsed: 207.01937580108643 seconds\n",
      "Step 5900, loss: tensor(0.1377, grad_fn=<SubBackward0>), time elapsed: 210.73780512809753 seconds\n",
      "Step 6000, loss: tensor(0.1388, grad_fn=<SubBackward0>), time elapsed: 214.276837348938 seconds\n",
      "Step 6100, loss: tensor(0.1385, grad_fn=<SubBackward0>), time elapsed: 217.75604677200317 seconds\n",
      "Step 6200, loss: tensor(0.1354, grad_fn=<SubBackward0>), time elapsed: 221.45571660995483 seconds\n",
      "Step 6300, loss: tensor(0.1407, grad_fn=<SubBackward0>), time elapsed: 225.00504779815674 seconds\n",
      "Step 6400, loss: tensor(0.1367, grad_fn=<SubBackward0>), time elapsed: 228.5207724571228 seconds\n",
      "Step 6500, loss: tensor(0.1371, grad_fn=<SubBackward0>), time elapsed: 232.20662879943848 seconds\n",
      "Step 6600, loss: tensor(0.1383, grad_fn=<SubBackward0>), time elapsed: 235.702632188797 seconds\n",
      "Step 6700, loss: tensor(0.1349, grad_fn=<SubBackward0>), time elapsed: 239.21787929534912 seconds\n",
      "Step 6800, loss: tensor(0.1400, grad_fn=<SubBackward0>), time elapsed: 242.94325518608093 seconds\n",
      "Step 6900, loss: tensor(0.1351, grad_fn=<SubBackward0>), time elapsed: 246.44634199142456 seconds\n",
      "Step 7000, loss: tensor(0.1360, grad_fn=<SubBackward0>), time elapsed: 249.98569750785828 seconds\n",
      "Step 7100, loss: tensor(0.1415, grad_fn=<SubBackward0>), time elapsed: 253.47008895874023 seconds\n",
      "Step 7200, loss: tensor(0.1305, grad_fn=<SubBackward0>), time elapsed: 257.1942059993744 seconds\n",
      "Step 7300, loss: tensor(0.1413, grad_fn=<SubBackward0>), time elapsed: 260.73434615135193 seconds\n",
      "Step 7400, loss: tensor(0.1394, grad_fn=<SubBackward0>), time elapsed: 264.24916434288025 seconds\n",
      "Step 7500, loss: tensor(0.1383, grad_fn=<SubBackward0>), time elapsed: 267.97081565856934 seconds\n",
      "Step 7600, loss: tensor(0.1338, grad_fn=<SubBackward0>), time elapsed: 271.46873593330383 seconds\n",
      "Step 7700, loss: tensor(0.1376, grad_fn=<SubBackward0>), time elapsed: 275.00872898101807 seconds\n",
      "Step 7800, loss: tensor(0.1393, grad_fn=<SubBackward0>), time elapsed: 278.7160680294037 seconds\n",
      "Step 7900, loss: tensor(0.1380, grad_fn=<SubBackward0>), time elapsed: 282.23928713798523 seconds\n",
      "Step 8000, loss: tensor(0.1330, grad_fn=<SubBackward0>), time elapsed: 285.7201690673828 seconds\n",
      "Step 8100, loss: tensor(0.1344, grad_fn=<SubBackward0>), time elapsed: 289.4337589740753 seconds\n",
      "Step 8200, loss: tensor(0.1367, grad_fn=<SubBackward0>), time elapsed: 292.9292342662811 seconds\n",
      "Step 8300, loss: tensor(0.1401, grad_fn=<SubBackward0>), time elapsed: 296.45745182037354 seconds\n",
      "Step 8400, loss: tensor(0.1374, grad_fn=<SubBackward0>), time elapsed: 300.2210998535156 seconds\n",
      "Step 8500, loss: tensor(0.1344, grad_fn=<SubBackward0>), time elapsed: 303.7520594596863 seconds\n",
      "Step 8600, loss: tensor(0.1394, grad_fn=<SubBackward0>), time elapsed: 307.218962430954 seconds\n",
      "Step 8700, loss: tensor(0.1385, grad_fn=<SubBackward0>), time elapsed: 310.9470067024231 seconds\n",
      "Step 8800, loss: tensor(0.1380, grad_fn=<SubBackward0>), time elapsed: 314.43222308158875 seconds\n",
      "Step 8900, loss: tensor(0.1412, grad_fn=<SubBackward0>), time elapsed: 317.9691390991211 seconds\n",
      "Step 9000, loss: tensor(0.1351, grad_fn=<SubBackward0>), time elapsed: 321.6710386276245 seconds\n",
      "Step 9100, loss: tensor(0.1323, grad_fn=<SubBackward0>), time elapsed: 325.170330286026 seconds\n",
      "Step 9200, loss: tensor(0.1478, grad_fn=<SubBackward0>), time elapsed: 328.68022298812866 seconds\n",
      "Step 9300, loss: tensor(0.1343, grad_fn=<SubBackward0>), time elapsed: 332.44707703590393 seconds\n",
      "Step 9400, loss: tensor(0.1406, grad_fn=<SubBackward0>), time elapsed: 335.96059370040894 seconds\n",
      "Step 9500, loss: tensor(0.1328, grad_fn=<SubBackward0>), time elapsed: 339.4819598197937 seconds\n",
      "Step 9600, loss: tensor(0.1369, grad_fn=<SubBackward0>), time elapsed: 342.98193883895874 seconds\n",
      "Step 9700, loss: tensor(0.1366, grad_fn=<SubBackward0>), time elapsed: 346.73586773872375 seconds\n",
      "Step 9800, loss: tensor(0.1369, grad_fn=<SubBackward0>), time elapsed: 350.29072308540344 seconds\n",
      "Step 9900, loss: tensor(0.1345, grad_fn=<SubBackward0>), time elapsed: 353.7901999950409 seconds\n",
      "Step 10000, loss: tensor(0.1402, grad_fn=<SubBackward0>), time elapsed: 357.50518441200256 seconds\n",
      "Step 10100, loss: tensor(0.1373, grad_fn=<SubBackward0>), time elapsed: 361.02114367485046 seconds\n",
      "Step 10200, loss: tensor(0.1298, grad_fn=<SubBackward0>), time elapsed: 364.54587602615356 seconds\n",
      "Step 10300, loss: tensor(0.1328, grad_fn=<SubBackward0>), time elapsed: 368.3077712059021 seconds\n",
      "Step 10400, loss: tensor(0.1292, grad_fn=<SubBackward0>), time elapsed: 371.8778898715973 seconds\n",
      "Step 10500, loss: tensor(0.1404, grad_fn=<SubBackward0>), time elapsed: 375.4059703350067 seconds\n",
      "Step 10600, loss: tensor(0.1376, grad_fn=<SubBackward0>), time elapsed: 379.14476919174194 seconds\n",
      "Step 10700, loss: tensor(0.1356, grad_fn=<SubBackward0>), time elapsed: 382.66909170150757 seconds\n",
      "Step 10800, loss: tensor(0.1353, grad_fn=<SubBackward0>), time elapsed: 386.20182824134827 seconds\n",
      "Step 10900, loss: tensor(0.1337, grad_fn=<SubBackward0>), time elapsed: 389.929025888443 seconds\n",
      "Step 11000, loss: tensor(0.1359, grad_fn=<SubBackward0>), time elapsed: 393.44461035728455 seconds\n",
      "Step 11100, loss: tensor(0.1360, grad_fn=<SubBackward0>), time elapsed: 396.9776406288147 seconds\n",
      "Step 11200, loss: tensor(0.1317, grad_fn=<SubBackward0>), time elapsed: 400.72133803367615 seconds\n",
      "Step 11300, loss: tensor(0.1396, grad_fn=<SubBackward0>), time elapsed: 404.2368998527527 seconds\n",
      "Step 11400, loss: tensor(0.1340, grad_fn=<SubBackward0>), time elapsed: 407.7816393375397 seconds\n",
      "Step 11500, loss: tensor(0.1321, grad_fn=<SubBackward0>), time elapsed: 411.5545332431793 seconds\n",
      "Step 11600, loss: tensor(0.1380, grad_fn=<SubBackward0>), time elapsed: 415.15143299102783 seconds\n",
      "Step 11700, loss: tensor(0.1325, grad_fn=<SubBackward0>), time elapsed: 418.70069909095764 seconds\n",
      "Step 11800, loss: tensor(0.1286, grad_fn=<SubBackward0>), time elapsed: 422.3243021965027 seconds\n",
      "Step 11900, loss: tensor(0.1400, grad_fn=<SubBackward0>), time elapsed: 426.16963720321655 seconds\n",
      "Step 12000, loss: tensor(0.1335, grad_fn=<SubBackward0>), time elapsed: 429.8011577129364 seconds\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.8437, grad_fn=<SubBackward0>), time elapsed: 0.03488349914550781 seconds\n",
      "Step 100, loss: tensor(0.7659, grad_fn=<SubBackward0>), time elapsed: 3.767207384109497 seconds\n",
      "Step 200, loss: tensor(0.6739, grad_fn=<SubBackward0>), time elapsed: 7.281392574310303 seconds\n",
      "Step 300, loss: tensor(0.5759, grad_fn=<SubBackward0>), time elapsed: 10.817251920700073 seconds\n",
      "Step 400, loss: tensor(0.4964, grad_fn=<SubBackward0>), time elapsed: 14.329386949539185 seconds\n",
      "Step 500, loss: tensor(0.4406, grad_fn=<SubBackward0>), time elapsed: 18.08327031135559 seconds\n",
      "Step 600, loss: tensor(0.3905, grad_fn=<SubBackward0>), time elapsed: 21.618103504180908 seconds\n",
      "Step 700, loss: tensor(0.3482, grad_fn=<SubBackward0>), time elapsed: 25.102728366851807 seconds\n",
      "Step 800, loss: tensor(0.3058, grad_fn=<SubBackward0>), time elapsed: 28.831626415252686 seconds\n",
      "Step 900, loss: tensor(0.2805, grad_fn=<SubBackward0>), time elapsed: 32.33667850494385 seconds\n",
      "Step 1000, loss: tensor(0.2598, grad_fn=<SubBackward0>), time elapsed: 35.84820365905762 seconds\n",
      "Step 1100, loss: tensor(0.2463, grad_fn=<SubBackward0>), time elapsed: 39.55271363258362 seconds\n",
      "Step 1200, loss: tensor(0.2349, grad_fn=<SubBackward0>), time elapsed: 43.072235107421875 seconds\n",
      "Step 1300, loss: tensor(0.2280, grad_fn=<SubBackward0>), time elapsed: 46.56377291679382 seconds\n",
      "Step 1400, loss: tensor(0.2241, grad_fn=<SubBackward0>), time elapsed: 50.293779611587524 seconds\n",
      "Step 1500, loss: tensor(0.2199, grad_fn=<SubBackward0>), time elapsed: 53.78545832633972 seconds\n",
      "Step 1600, loss: tensor(0.2170, grad_fn=<SubBackward0>), time elapsed: 57.26928734779358 seconds\n",
      "Step 1700, loss: tensor(0.2156, grad_fn=<SubBackward0>), time elapsed: 60.98729610443115 seconds\n",
      "Step 1800, loss: tensor(0.2101, grad_fn=<SubBackward0>), time elapsed: 64.53767204284668 seconds\n",
      "Step 1900, loss: tensor(0.2109, grad_fn=<SubBackward0>), time elapsed: 68.05141282081604 seconds\n",
      "Step 2000, loss: tensor(0.2087, grad_fn=<SubBackward0>), time elapsed: 71.73980116844177 seconds\n",
      "Step 2100, loss: tensor(0.2080, grad_fn=<SubBackward0>), time elapsed: 75.23172497749329 seconds\n",
      "Step 2200, loss: tensor(0.2071, grad_fn=<SubBackward0>), time elapsed: 78.72690773010254 seconds\n",
      "Step 2300, loss: tensor(0.2053, grad_fn=<SubBackward0>), time elapsed: 82.43081831932068 seconds\n",
      "Step 2400, loss: tensor(0.2043, grad_fn=<SubBackward0>), time elapsed: 85.91355323791504 seconds\n",
      "Step 2500, loss: tensor(0.2051, grad_fn=<SubBackward0>), time elapsed: 89.41419291496277 seconds\n",
      "Step 2600, loss: tensor(0.2021, grad_fn=<SubBackward0>), time elapsed: 93.13174557685852 seconds\n",
      "Step 2700, loss: tensor(0.2022, grad_fn=<SubBackward0>), time elapsed: 96.6444149017334 seconds\n",
      "Step 2800, loss: tensor(0.2034, grad_fn=<SubBackward0>), time elapsed: 100.13746118545532 seconds\n",
      "Step 2900, loss: tensor(0.1988, grad_fn=<SubBackward0>), time elapsed: 103.82206916809082 seconds\n",
      "Step 3000, loss: tensor(0.2031, grad_fn=<SubBackward0>), time elapsed: 107.35184526443481 seconds\n",
      "Step 3100, loss: tensor(0.1991, grad_fn=<SubBackward0>), time elapsed: 110.86275815963745 seconds\n",
      "Step 3200, loss: tensor(0.2007, grad_fn=<SubBackward0>), time elapsed: 114.59797668457031 seconds\n",
      "Step 3300, loss: tensor(0.1970, grad_fn=<SubBackward0>), time elapsed: 118.09835839271545 seconds\n",
      "Step 3400, loss: tensor(0.1976, grad_fn=<SubBackward0>), time elapsed: 121.58106899261475 seconds\n",
      "Step 3500, loss: tensor(0.1977, grad_fn=<SubBackward0>), time elapsed: 125.31253981590271 seconds\n",
      "Step 3600, loss: tensor(0.1988, grad_fn=<SubBackward0>), time elapsed: 128.83867025375366 seconds\n",
      "Step 3700, loss: tensor(0.1987, grad_fn=<SubBackward0>), time elapsed: 132.3547990322113 seconds\n",
      "Step 3800, loss: tensor(0.1949, grad_fn=<SubBackward0>), time elapsed: 135.83586430549622 seconds\n",
      "Step 3900, loss: tensor(0.1952, grad_fn=<SubBackward0>), time elapsed: 139.5319573879242 seconds\n",
      "Step 4000, loss: tensor(0.1954, grad_fn=<SubBackward0>), time elapsed: 143.0390980243683 seconds\n",
      "Step 4100, loss: tensor(0.1939, grad_fn=<SubBackward0>), time elapsed: 146.56269788742065 seconds\n",
      "Step 4200, loss: tensor(0.1949, grad_fn=<SubBackward0>), time elapsed: 150.28274297714233 seconds\n",
      "Step 4300, loss: tensor(0.1873, grad_fn=<SubBackward0>), time elapsed: 153.7918839454651 seconds\n",
      "Step 4400, loss: tensor(0.1872, grad_fn=<SubBackward0>), time elapsed: 157.27411198616028 seconds\n",
      "Step 4500, loss: tensor(0.1851, grad_fn=<SubBackward0>), time elapsed: 161.01207852363586 seconds\n",
      "Step 4600, loss: tensor(0.1819, grad_fn=<SubBackward0>), time elapsed: 164.5403869152069 seconds\n",
      "Step 4700, loss: tensor(0.1789, grad_fn=<SubBackward0>), time elapsed: 168.03164768218994 seconds\n",
      "Step 4800, loss: tensor(0.1787, grad_fn=<SubBackward0>), time elapsed: 171.76936531066895 seconds\n",
      "Step 4900, loss: tensor(0.1798, grad_fn=<SubBackward0>), time elapsed: 175.3005712032318 seconds\n",
      "Step 5000, loss: tensor(0.1765, grad_fn=<SubBackward0>), time elapsed: 178.80356812477112 seconds\n",
      "Step 5100, loss: tensor(0.1735, grad_fn=<SubBackward0>), time elapsed: 182.5186643600464 seconds\n",
      "Step 5200, loss: tensor(0.1689, grad_fn=<SubBackward0>), time elapsed: 186.03491282463074 seconds\n",
      "Step 5300, loss: tensor(0.1654, grad_fn=<SubBackward0>), time elapsed: 189.54725670814514 seconds\n",
      "Step 5400, loss: tensor(0.1625, grad_fn=<SubBackward0>), time elapsed: 193.27010369300842 seconds\n",
      "Step 5500, loss: tensor(0.1593, grad_fn=<SubBackward0>), time elapsed: 196.81019854545593 seconds\n",
      "Step 5600, loss: tensor(0.1581, grad_fn=<SubBackward0>), time elapsed: 200.3016800880432 seconds\n",
      "Step 5700, loss: tensor(0.1575, grad_fn=<SubBackward0>), time elapsed: 203.99408721923828 seconds\n",
      "Step 5800, loss: tensor(0.1554, grad_fn=<SubBackward0>), time elapsed: 207.51627278327942 seconds\n",
      "Step 5900, loss: tensor(0.1552, grad_fn=<SubBackward0>), time elapsed: 211.03219604492188 seconds\n",
      "Step 6000, loss: tensor(0.1490, grad_fn=<SubBackward0>), time elapsed: 214.75379705429077 seconds\n",
      "Step 6100, loss: tensor(0.1514, grad_fn=<SubBackward0>), time elapsed: 218.2956190109253 seconds\n",
      "Step 6200, loss: tensor(0.1495, grad_fn=<SubBackward0>), time elapsed: 221.81679821014404 seconds\n",
      "Step 6300, loss: tensor(0.1513, grad_fn=<SubBackward0>), time elapsed: 225.5612554550171 seconds\n",
      "Step 6400, loss: tensor(0.1442, grad_fn=<SubBackward0>), time elapsed: 229.087087392807 seconds\n",
      "Step 6500, loss: tensor(0.1482, grad_fn=<SubBackward0>), time elapsed: 232.58561396598816 seconds\n",
      "Step 6600, loss: tensor(0.1482, grad_fn=<SubBackward0>), time elapsed: 236.10739374160767 seconds\n",
      "Step 6700, loss: tensor(0.1433, grad_fn=<SubBackward0>), time elapsed: 239.8465781211853 seconds\n",
      "Step 6800, loss: tensor(0.1513, grad_fn=<SubBackward0>), time elapsed: 243.38784909248352 seconds\n",
      "Step 6900, loss: tensor(0.1493, grad_fn=<SubBackward0>), time elapsed: 246.88524436950684 seconds\n",
      "Step 7000, loss: tensor(0.1542, grad_fn=<SubBackward0>), time elapsed: 250.5992305278778 seconds\n",
      "Step 7100, loss: tensor(0.1466, grad_fn=<SubBackward0>), time elapsed: 254.1381058692932 seconds\n",
      "Step 7200, loss: tensor(0.1476, grad_fn=<SubBackward0>), time elapsed: 257.6369984149933 seconds\n",
      "Step 7300, loss: tensor(0.1483, grad_fn=<SubBackward0>), time elapsed: 261.3806037902832 seconds\n",
      "Step 7400, loss: tensor(0.1462, grad_fn=<SubBackward0>), time elapsed: 264.90884590148926 seconds\n",
      "Step 7500, loss: tensor(0.1492, grad_fn=<SubBackward0>), time elapsed: 268.39642357826233 seconds\n",
      "Step 7600, loss: tensor(0.1513, grad_fn=<SubBackward0>), time elapsed: 272.146507024765 seconds\n",
      "Step 7700, loss: tensor(0.1512, grad_fn=<SubBackward0>), time elapsed: 275.68053579330444 seconds\n",
      "Step 7800, loss: tensor(0.1475, grad_fn=<SubBackward0>), time elapsed: 279.167533159256 seconds\n",
      "Step 7900, loss: tensor(0.1465, grad_fn=<SubBackward0>), time elapsed: 282.90228486061096 seconds\n",
      "Step 8000, loss: tensor(0.1475, grad_fn=<SubBackward0>), time elapsed: 286.4400808811188 seconds\n",
      "Step 8100, loss: tensor(0.1510, grad_fn=<SubBackward0>), time elapsed: 289.9744155406952 seconds\n",
      "Step 8200, loss: tensor(0.1508, grad_fn=<SubBackward0>), time elapsed: 293.7233521938324 seconds\n",
      "Step 8300, loss: tensor(0.1476, grad_fn=<SubBackward0>), time elapsed: 297.24151825904846 seconds\n",
      "Step 8400, loss: tensor(0.1466, grad_fn=<SubBackward0>), time elapsed: 300.7823281288147 seconds\n",
      "Step 8500, loss: tensor(0.1509, grad_fn=<SubBackward0>), time elapsed: 304.52408480644226 seconds\n",
      "Step 8600, loss: tensor(0.1551, grad_fn=<SubBackward0>), time elapsed: 308.0543622970581 seconds\n",
      "Step 8700, loss: tensor(0.1480, grad_fn=<SubBackward0>), time elapsed: 311.57527804374695 seconds\n",
      "Step 8800, loss: tensor(0.1418, grad_fn=<SubBackward0>), time elapsed: 315.28503823280334 seconds\n",
      "Step 8900, loss: tensor(0.1456, grad_fn=<SubBackward0>), time elapsed: 318.803076505661 seconds\n",
      "Step 9000, loss: tensor(0.1556, grad_fn=<SubBackward0>), time elapsed: 322.33108472824097 seconds\n",
      "Step 9100, loss: tensor(0.1450, grad_fn=<SubBackward0>), time elapsed: 325.8468849658966 seconds\n",
      "Step 9200, loss: tensor(0.1445, grad_fn=<SubBackward0>), time elapsed: 329.55722188949585 seconds\n",
      "Step 9300, loss: tensor(0.1432, grad_fn=<SubBackward0>), time elapsed: 333.06920981407166 seconds\n",
      "Step 9400, loss: tensor(0.1492, grad_fn=<SubBackward0>), time elapsed: 336.5897309780121 seconds\n",
      "Step 9500, loss: tensor(0.1459, grad_fn=<SubBackward0>), time elapsed: 340.34105229377747 seconds\n",
      "Step 9600, loss: tensor(0.1484, grad_fn=<SubBackward0>), time elapsed: 343.831622838974 seconds\n",
      "Step 9700, loss: tensor(0.1457, grad_fn=<SubBackward0>), time elapsed: 347.34989619255066 seconds\n",
      "Step 9800, loss: tensor(0.1485, grad_fn=<SubBackward0>), time elapsed: 351.04978489875793 seconds\n",
      "Step 9900, loss: tensor(0.1468, grad_fn=<SubBackward0>), time elapsed: 354.5763807296753 seconds\n",
      "Step 10000, loss: tensor(0.1465, grad_fn=<SubBackward0>), time elapsed: 358.10056257247925 seconds\n",
      "Step 10100, loss: tensor(0.1474, grad_fn=<SubBackward0>), time elapsed: 361.8148937225342 seconds\n",
      "Step 10200, loss: tensor(0.1447, grad_fn=<SubBackward0>), time elapsed: 365.32815623283386 seconds\n",
      "Step 10300, loss: tensor(0.1467, grad_fn=<SubBackward0>), time elapsed: 368.8314187526703 seconds\n",
      "Step 10400, loss: tensor(0.1507, grad_fn=<SubBackward0>), time elapsed: 372.6139051914215 seconds\n",
      "Step 10500, loss: tensor(0.1487, grad_fn=<SubBackward0>), time elapsed: 376.1228926181793 seconds\n",
      "Step 10600, loss: tensor(0.1443, grad_fn=<SubBackward0>), time elapsed: 379.62853145599365 seconds\n",
      "Step 10700, loss: tensor(0.1432, grad_fn=<SubBackward0>), time elapsed: 383.39378237724304 seconds\n",
      "Step 10800, loss: tensor(0.1469, grad_fn=<SubBackward0>), time elapsed: 386.95499563217163 seconds\n",
      "Step 10900, loss: tensor(0.1470, grad_fn=<SubBackward0>), time elapsed: 390.4906234741211 seconds\n",
      "Step 11000, loss: tensor(0.1418, grad_fn=<SubBackward0>), time elapsed: 394.211740732193 seconds\n",
      "Step 11100, loss: tensor(0.1449, grad_fn=<SubBackward0>), time elapsed: 397.77101969718933 seconds\n",
      "Step 11200, loss: tensor(0.1461, grad_fn=<SubBackward0>), time elapsed: 401.33335065841675 seconds\n",
      "Step 11300, loss: tensor(0.1473, grad_fn=<SubBackward0>), time elapsed: 405.1186685562134 seconds\n",
      "Step 11400, loss: tensor(0.1477, grad_fn=<SubBackward0>), time elapsed: 408.66120409965515 seconds\n",
      "Step 11500, loss: tensor(0.1422, grad_fn=<SubBackward0>), time elapsed: 412.2208466529846 seconds\n",
      "Step 11600, loss: tensor(0.1465, grad_fn=<SubBackward0>), time elapsed: 415.745450258255 seconds\n",
      "Step 11700, loss: tensor(0.1481, grad_fn=<SubBackward0>), time elapsed: 419.54737091064453 seconds\n",
      "Step 11800, loss: tensor(0.1432, grad_fn=<SubBackward0>), time elapsed: 423.15106201171875 seconds\n",
      "Step 11900, loss: tensor(0.1473, grad_fn=<SubBackward0>), time elapsed: 426.75894355773926 seconds\n",
      "Step 12000, loss: tensor(0.1421, grad_fn=<SubBackward0>), time elapsed: 430.6036286354065 seconds\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(1.0820, grad_fn=<SubBackward0>), time elapsed: 0.03587985038757324 seconds\n",
      "Step 100, loss: tensor(1.0281, grad_fn=<SubBackward0>), time elapsed: 3.607551336288452 seconds\n",
      "Step 200, loss: tensor(0.9198, grad_fn=<SubBackward0>), time elapsed: 7.114226341247559 seconds\n",
      "Step 300, loss: tensor(0.7993, grad_fn=<SubBackward0>), time elapsed: 10.823481798171997 seconds\n",
      "Step 400, loss: tensor(0.7043, grad_fn=<SubBackward0>), time elapsed: 14.321379661560059 seconds\n",
      "Step 500, loss: tensor(0.6420, grad_fn=<SubBackward0>), time elapsed: 17.821983814239502 seconds\n",
      "Step 600, loss: tensor(0.5815, grad_fn=<SubBackward0>), time elapsed: 21.5376980304718 seconds\n",
      "Step 700, loss: tensor(0.5366, grad_fn=<SubBackward0>), time elapsed: 25.02492594718933 seconds\n",
      "Step 800, loss: tensor(0.4979, grad_fn=<SubBackward0>), time elapsed: 28.555362939834595 seconds\n",
      "Step 900, loss: tensor(0.4603, grad_fn=<SubBackward0>), time elapsed: 32.307552337646484 seconds\n",
      "Step 1000, loss: tensor(0.4202, grad_fn=<SubBackward0>), time elapsed: 35.825867891311646 seconds\n",
      "Step 1100, loss: tensor(0.3914, grad_fn=<SubBackward0>), time elapsed: 39.30446243286133 seconds\n",
      "Step 1200, loss: tensor(0.3505, grad_fn=<SubBackward0>), time elapsed: 42.991875648498535 seconds\n",
      "Step 1300, loss: tensor(0.3184, grad_fn=<SubBackward0>), time elapsed: 46.47540616989136 seconds\n",
      "Step 1400, loss: tensor(0.2863, grad_fn=<SubBackward0>), time elapsed: 49.9685263633728 seconds\n",
      "Step 1500, loss: tensor(0.2663, grad_fn=<SubBackward0>), time elapsed: 53.69526672363281 seconds\n",
      "Step 1600, loss: tensor(0.2430, grad_fn=<SubBackward0>), time elapsed: 57.21800112724304 seconds\n",
      "Step 1700, loss: tensor(0.2274, grad_fn=<SubBackward0>), time elapsed: 60.759055614471436 seconds\n",
      "Step 1800, loss: tensor(0.2166, grad_fn=<SubBackward0>), time elapsed: 64.47933888435364 seconds\n",
      "Step 1900, loss: tensor(0.2109, grad_fn=<SubBackward0>), time elapsed: 67.98301649093628 seconds\n",
      "Step 2000, loss: tensor(0.2028, grad_fn=<SubBackward0>), time elapsed: 71.49242687225342 seconds\n",
      "Step 2100, loss: tensor(0.2001, grad_fn=<SubBackward0>), time elapsed: 75.22169828414917 seconds\n",
      "Step 2200, loss: tensor(0.1914, grad_fn=<SubBackward0>), time elapsed: 78.71827459335327 seconds\n",
      "Step 2300, loss: tensor(0.1909, grad_fn=<SubBackward0>), time elapsed: 82.21419858932495 seconds\n",
      "Step 2400, loss: tensor(0.1904, grad_fn=<SubBackward0>), time elapsed: 85.89784383773804 seconds\n",
      "Step 2500, loss: tensor(0.1880, grad_fn=<SubBackward0>), time elapsed: 89.4217631816864 seconds\n",
      "Step 2600, loss: tensor(0.1880, grad_fn=<SubBackward0>), time elapsed: 92.93715405464172 seconds\n",
      "Step 2700, loss: tensor(0.1807, grad_fn=<SubBackward0>), time elapsed: 96.61845874786377 seconds\n",
      "Step 2800, loss: tensor(0.1854, grad_fn=<SubBackward0>), time elapsed: 100.13314270973206 seconds\n",
      "Step 2900, loss: tensor(0.1775, grad_fn=<SubBackward0>), time elapsed: 103.64074444770813 seconds\n",
      "Step 3000, loss: tensor(0.1809, grad_fn=<SubBackward0>), time elapsed: 107.3470721244812 seconds\n",
      "Step 3100, loss: tensor(0.1784, grad_fn=<SubBackward0>), time elapsed: 110.81415796279907 seconds\n",
      "Step 3200, loss: tensor(0.1829, grad_fn=<SubBackward0>), time elapsed: 114.33875489234924 seconds\n",
      "Step 3300, loss: tensor(0.1784, grad_fn=<SubBackward0>), time elapsed: 117.83357572555542 seconds\n",
      "Step 3400, loss: tensor(0.1784, grad_fn=<SubBackward0>), time elapsed: 121.51887536048889 seconds\n",
      "Step 3500, loss: tensor(0.1791, grad_fn=<SubBackward0>), time elapsed: 125.04158592224121 seconds\n",
      "Step 3600, loss: tensor(0.1821, grad_fn=<SubBackward0>), time elapsed: 128.5440011024475 seconds\n",
      "Step 3700, loss: tensor(0.1815, grad_fn=<SubBackward0>), time elapsed: 132.25490808486938 seconds\n",
      "Step 3800, loss: tensor(0.1796, grad_fn=<SubBackward0>), time elapsed: 135.76129484176636 seconds\n",
      "Step 3900, loss: tensor(0.1792, grad_fn=<SubBackward0>), time elapsed: 139.24313354492188 seconds\n",
      "Step 4000, loss: tensor(0.1788, grad_fn=<SubBackward0>), time elapsed: 142.94516563415527 seconds\n",
      "Step 4100, loss: tensor(0.1798, grad_fn=<SubBackward0>), time elapsed: 146.42937707901 seconds\n",
      "Step 4200, loss: tensor(0.1815, grad_fn=<SubBackward0>), time elapsed: 149.94210600852966 seconds\n",
      "Step 4300, loss: tensor(0.1813, grad_fn=<SubBackward0>), time elapsed: 153.6495656967163 seconds\n",
      "Step 4400, loss: tensor(0.1814, grad_fn=<SubBackward0>), time elapsed: 157.13827896118164 seconds\n",
      "Step 4500, loss: tensor(0.1825, grad_fn=<SubBackward0>), time elapsed: 160.62580180168152 seconds\n",
      "Step 4600, loss: tensor(0.1832, grad_fn=<SubBackward0>), time elapsed: 164.3379590511322 seconds\n",
      "Step 4700, loss: tensor(0.1832, grad_fn=<SubBackward0>), time elapsed: 167.889888048172 seconds\n",
      "Step 4800, loss: tensor(0.1820, grad_fn=<SubBackward0>), time elapsed: 171.417906999588 seconds\n",
      "Step 4900, loss: tensor(0.1827, grad_fn=<SubBackward0>), time elapsed: 175.13647985458374 seconds\n",
      "Step 5000, loss: tensor(0.1810, grad_fn=<SubBackward0>), time elapsed: 178.63688802719116 seconds\n",
      "Step 5100, loss: tensor(0.1809, grad_fn=<SubBackward0>), time elapsed: 182.13329029083252 seconds\n",
      "Step 5200, loss: tensor(0.1828, grad_fn=<SubBackward0>), time elapsed: 185.86543107032776 seconds\n",
      "Step 5300, loss: tensor(0.1808, grad_fn=<SubBackward0>), time elapsed: 189.38060903549194 seconds\n",
      "Step 5400, loss: tensor(0.1812, grad_fn=<SubBackward0>), time elapsed: 192.91926670074463 seconds\n",
      "Step 5500, loss: tensor(0.1787, grad_fn=<SubBackward0>), time elapsed: 196.64219737052917 seconds\n",
      "Step 5600, loss: tensor(0.1782, grad_fn=<SubBackward0>), time elapsed: 200.15680623054504 seconds\n",
      "Step 5700, loss: tensor(0.1793, grad_fn=<SubBackward0>), time elapsed: 203.66282176971436 seconds\n",
      "Step 5800, loss: tensor(0.1769, grad_fn=<SubBackward0>), time elapsed: 207.41183853149414 seconds\n",
      "Step 5900, loss: tensor(0.1760, grad_fn=<SubBackward0>), time elapsed: 210.9510805606842 seconds\n",
      "Step 6000, loss: tensor(0.1765, grad_fn=<SubBackward0>), time elapsed: 214.49306273460388 seconds\n",
      "Step 6100, loss: tensor(0.1736, grad_fn=<SubBackward0>), time elapsed: 218.0268361568451 seconds\n",
      "Step 6200, loss: tensor(0.1777, grad_fn=<SubBackward0>), time elapsed: 221.73819065093994 seconds\n",
      "Step 6300, loss: tensor(0.1741, grad_fn=<SubBackward0>), time elapsed: 225.26626634597778 seconds\n",
      "Step 6400, loss: tensor(0.1722, grad_fn=<SubBackward0>), time elapsed: 228.80576515197754 seconds\n",
      "Step 6500, loss: tensor(0.1728, grad_fn=<SubBackward0>), time elapsed: 232.5100326538086 seconds\n",
      "Step 6600, loss: tensor(0.1736, grad_fn=<SubBackward0>), time elapsed: 236.01100039482117 seconds\n",
      "Step 6700, loss: tensor(0.1722, grad_fn=<SubBackward0>), time elapsed: 239.50560903549194 seconds\n",
      "Step 6800, loss: tensor(0.1727, grad_fn=<SubBackward0>), time elapsed: 243.2409644126892 seconds\n",
      "Step 6900, loss: tensor(0.1680, grad_fn=<SubBackward0>), time elapsed: 246.732097864151 seconds\n",
      "Step 7000, loss: tensor(0.1688, grad_fn=<SubBackward0>), time elapsed: 250.2585551738739 seconds\n",
      "Step 7100, loss: tensor(0.1705, grad_fn=<SubBackward0>), time elapsed: 253.99259209632874 seconds\n",
      "Step 7200, loss: tensor(0.1664, grad_fn=<SubBackward0>), time elapsed: 257.512943983078 seconds\n",
      "Step 7300, loss: tensor(0.1677, grad_fn=<SubBackward0>), time elapsed: 261.05347299575806 seconds\n",
      "Step 7400, loss: tensor(0.1651, grad_fn=<SubBackward0>), time elapsed: 264.77218651771545 seconds\n",
      "Step 7500, loss: tensor(0.1671, grad_fn=<SubBackward0>), time elapsed: 268.26732301712036 seconds\n",
      "Step 7600, loss: tensor(0.1617, grad_fn=<SubBackward0>), time elapsed: 271.7756578922272 seconds\n",
      "Step 7700, loss: tensor(0.1646, grad_fn=<SubBackward0>), time elapsed: 275.49390506744385 seconds\n",
      "Step 7800, loss: tensor(0.1631, grad_fn=<SubBackward0>), time elapsed: 278.9858841896057 seconds\n",
      "Step 7900, loss: tensor(0.1588, grad_fn=<SubBackward0>), time elapsed: 282.5009677410126 seconds\n",
      "Step 8000, loss: tensor(0.1618, grad_fn=<SubBackward0>), time elapsed: 286.2351839542389 seconds\n",
      "Step 8100, loss: tensor(0.1583, grad_fn=<SubBackward0>), time elapsed: 289.7688777446747 seconds\n",
      "Step 8200, loss: tensor(0.1566, grad_fn=<SubBackward0>), time elapsed: 293.30094146728516 seconds\n",
      "Step 8300, loss: tensor(0.1584, grad_fn=<SubBackward0>), time elapsed: 296.99652123451233 seconds\n",
      "Step 8400, loss: tensor(0.1532, grad_fn=<SubBackward0>), time elapsed: 300.5392861366272 seconds\n",
      "Step 8500, loss: tensor(0.1525, grad_fn=<SubBackward0>), time elapsed: 304.04668045043945 seconds\n",
      "Step 8600, loss: tensor(0.1515, grad_fn=<SubBackward0>), time elapsed: 307.5784742832184 seconds\n",
      "Step 8700, loss: tensor(0.1548, grad_fn=<SubBackward0>), time elapsed: 311.2934834957123 seconds\n",
      "Step 8800, loss: tensor(0.1555, grad_fn=<SubBackward0>), time elapsed: 314.8076345920563 seconds\n",
      "Step 8900, loss: tensor(0.1475, grad_fn=<SubBackward0>), time elapsed: 318.3135275840759 seconds\n",
      "Step 9000, loss: tensor(0.1539, grad_fn=<SubBackward0>), time elapsed: 322.03874945640564 seconds\n",
      "Step 9100, loss: tensor(0.1501, grad_fn=<SubBackward0>), time elapsed: 325.54772543907166 seconds\n",
      "Step 9200, loss: tensor(0.1548, grad_fn=<SubBackward0>), time elapsed: 329.10153222084045 seconds\n",
      "Step 9300, loss: tensor(0.1539, grad_fn=<SubBackward0>), time elapsed: 332.84367656707764 seconds\n",
      "Step 9400, loss: tensor(0.1522, grad_fn=<SubBackward0>), time elapsed: 336.39817810058594 seconds\n",
      "Step 9500, loss: tensor(0.1479, grad_fn=<SubBackward0>), time elapsed: 339.88970041275024 seconds\n",
      "Step 9600, loss: tensor(0.1495, grad_fn=<SubBackward0>), time elapsed: 343.6217269897461 seconds\n",
      "Step 9700, loss: tensor(0.1555, grad_fn=<SubBackward0>), time elapsed: 347.1602694988251 seconds\n",
      "Step 9800, loss: tensor(0.1504, grad_fn=<SubBackward0>), time elapsed: 350.6562306880951 seconds\n",
      "Step 9900, loss: tensor(0.1513, grad_fn=<SubBackward0>), time elapsed: 354.39972734451294 seconds\n",
      "Step 10000, loss: tensor(0.1508, grad_fn=<SubBackward0>), time elapsed: 357.9197664260864 seconds\n",
      "Step 10100, loss: tensor(0.1474, grad_fn=<SubBackward0>), time elapsed: 361.45532178878784 seconds\n",
      "Step 10200, loss: tensor(0.1464, grad_fn=<SubBackward0>), time elapsed: 365.2065734863281 seconds\n",
      "Step 10300, loss: tensor(0.1479, grad_fn=<SubBackward0>), time elapsed: 368.7325119972229 seconds\n",
      "Step 10400, loss: tensor(0.1484, grad_fn=<SubBackward0>), time elapsed: 372.2763521671295 seconds\n",
      "Step 10500, loss: tensor(0.1486, grad_fn=<SubBackward0>), time elapsed: 376.03006410598755 seconds\n",
      "Step 10600, loss: tensor(0.1397, grad_fn=<SubBackward0>), time elapsed: 379.5584759712219 seconds\n",
      "Step 10700, loss: tensor(0.1467, grad_fn=<SubBackward0>), time elapsed: 383.1346755027771 seconds\n",
      "Step 10800, loss: tensor(0.1405, grad_fn=<SubBackward0>), time elapsed: 386.8777129650116 seconds\n",
      "Step 10900, loss: tensor(0.1435, grad_fn=<SubBackward0>), time elapsed: 390.42251682281494 seconds\n",
      "Step 11000, loss: tensor(0.1371, grad_fn=<SubBackward0>), time elapsed: 394.0101969242096 seconds\n",
      "Step 11100, loss: tensor(0.1383, grad_fn=<SubBackward0>), time elapsed: 397.53039169311523 seconds\n",
      "Step 11200, loss: tensor(0.1392, grad_fn=<SubBackward0>), time elapsed: 401.2341375350952 seconds\n",
      "Step 11300, loss: tensor(0.1445, grad_fn=<SubBackward0>), time elapsed: 404.81556844711304 seconds\n",
      "Step 11400, loss: tensor(0.1342, grad_fn=<SubBackward0>), time elapsed: 408.3417365550995 seconds\n",
      "Step 11500, loss: tensor(0.1418, grad_fn=<SubBackward0>), time elapsed: 412.1179325580597 seconds\n",
      "Step 11600, loss: tensor(0.1360, grad_fn=<SubBackward0>), time elapsed: 415.66507482528687 seconds\n",
      "Step 11700, loss: tensor(0.1349, grad_fn=<SubBackward0>), time elapsed: 419.212984085083 seconds\n",
      "Step 11800, loss: tensor(0.1350, grad_fn=<SubBackward0>), time elapsed: 422.9509222507477 seconds\n",
      "Step 11900, loss: tensor(0.1342, grad_fn=<SubBackward0>), time elapsed: 426.54736518859863 seconds\n",
      "Step 12000, loss: tensor(0.1388, grad_fn=<SubBackward0>), time elapsed: 430.15990376472473 seconds\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(1.1435, grad_fn=<SubBackward0>), time elapsed: 0.03488326072692871 seconds\n",
      "Step 100, loss: tensor(1.0219, grad_fn=<SubBackward0>), time elapsed: 3.770111083984375 seconds\n",
      "Step 200, loss: tensor(0.8766, grad_fn=<SubBackward0>), time elapsed: 7.2673351764678955 seconds\n",
      "Step 300, loss: tensor(0.7414, grad_fn=<SubBackward0>), time elapsed: 10.796419858932495 seconds\n",
      "Step 400, loss: tensor(0.6166, grad_fn=<SubBackward0>), time elapsed: 14.47688627243042 seconds\n",
      "Step 500, loss: tensor(0.5142, grad_fn=<SubBackward0>), time elapsed: 17.974830389022827 seconds\n",
      "Step 600, loss: tensor(0.4667, grad_fn=<SubBackward0>), time elapsed: 21.4605815410614 seconds\n",
      "Step 700, loss: tensor(0.4004, grad_fn=<SubBackward0>), time elapsed: 25.186843156814575 seconds\n",
      "Step 800, loss: tensor(0.3382, grad_fn=<SubBackward0>), time elapsed: 28.70294165611267 seconds\n",
      "Step 900, loss: tensor(0.2955, grad_fn=<SubBackward0>), time elapsed: 32.19865679740906 seconds\n",
      "Step 1000, loss: tensor(0.2563, grad_fn=<SubBackward0>), time elapsed: 35.92510437965393 seconds\n",
      "Step 1100, loss: tensor(0.2370, grad_fn=<SubBackward0>), time elapsed: 39.44827127456665 seconds\n",
      "Step 1200, loss: tensor(0.2102, grad_fn=<SubBackward0>), time elapsed: 42.98093509674072 seconds\n",
      "Step 1300, loss: tensor(0.2052, grad_fn=<SubBackward0>), time elapsed: 46.66730761528015 seconds\n",
      "Step 1400, loss: tensor(0.2010, grad_fn=<SubBackward0>), time elapsed: 50.129913330078125 seconds\n",
      "Step 1500, loss: tensor(0.1946, grad_fn=<SubBackward0>), time elapsed: 53.629154920578 seconds\n",
      "Step 1600, loss: tensor(0.1885, grad_fn=<SubBackward0>), time elapsed: 57.32390117645264 seconds\n",
      "Step 1700, loss: tensor(0.1856, grad_fn=<SubBackward0>), time elapsed: 60.828683614730835 seconds\n",
      "Step 1800, loss: tensor(0.1858, grad_fn=<SubBackward0>), time elapsed: 64.31352591514587 seconds\n",
      "Step 1900, loss: tensor(0.1773, grad_fn=<SubBackward0>), time elapsed: 67.97081518173218 seconds\n",
      "Step 2000, loss: tensor(0.1729, grad_fn=<SubBackward0>), time elapsed: 71.44914531707764 seconds\n",
      "Step 2100, loss: tensor(0.1683, grad_fn=<SubBackward0>), time elapsed: 74.92171001434326 seconds\n",
      "Step 2200, loss: tensor(0.1661, grad_fn=<SubBackward0>), time elapsed: 78.59698700904846 seconds\n",
      "Step 2300, loss: tensor(0.1569, grad_fn=<SubBackward0>), time elapsed: 82.05218982696533 seconds\n",
      "Step 2400, loss: tensor(0.1588, grad_fn=<SubBackward0>), time elapsed: 85.55417704582214 seconds\n",
      "Step 2500, loss: tensor(0.1536, grad_fn=<SubBackward0>), time elapsed: 89.23261427879333 seconds\n",
      "Step 2600, loss: tensor(0.1508, grad_fn=<SubBackward0>), time elapsed: 92.69297623634338 seconds\n",
      "Step 2700, loss: tensor(0.1448, grad_fn=<SubBackward0>), time elapsed: 96.19687986373901 seconds\n",
      "Step 2800, loss: tensor(0.1413, grad_fn=<SubBackward0>), time elapsed: 99.6856472492218 seconds\n",
      "Step 2900, loss: tensor(0.1395, grad_fn=<SubBackward0>), time elapsed: 103.33450889587402 seconds\n",
      "Step 3000, loss: tensor(0.1413, grad_fn=<SubBackward0>), time elapsed: 106.8403639793396 seconds\n",
      "Step 3100, loss: tensor(0.1326, grad_fn=<SubBackward0>), time elapsed: 110.3014543056488 seconds\n",
      "Step 3200, loss: tensor(0.1354, grad_fn=<SubBackward0>), time elapsed: 113.98516249656677 seconds\n",
      "Step 3300, loss: tensor(0.1319, grad_fn=<SubBackward0>), time elapsed: 117.47165322303772 seconds\n",
      "Step 3400, loss: tensor(0.1290, grad_fn=<SubBackward0>), time elapsed: 120.97892260551453 seconds\n",
      "Step 3500, loss: tensor(0.1322, grad_fn=<SubBackward0>), time elapsed: 124.68332648277283 seconds\n",
      "Step 3600, loss: tensor(0.1351, grad_fn=<SubBackward0>), time elapsed: 128.1603343486786 seconds\n",
      "Step 3700, loss: tensor(0.1359, grad_fn=<SubBackward0>), time elapsed: 131.63498258590698 seconds\n",
      "Step 3800, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 135.30304598808289 seconds\n",
      "Step 3900, loss: tensor(0.1296, grad_fn=<SubBackward0>), time elapsed: 138.79400658607483 seconds\n",
      "Step 4000, loss: tensor(0.1288, grad_fn=<SubBackward0>), time elapsed: 142.2970678806305 seconds\n",
      "Step 4100, loss: tensor(0.1268, grad_fn=<SubBackward0>), time elapsed: 145.98999333381653 seconds\n",
      "Step 4200, loss: tensor(0.1257, grad_fn=<SubBackward0>), time elapsed: 149.4831883907318 seconds\n",
      "Step 4300, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 152.9892063140869 seconds\n",
      "Step 4400, loss: tensor(0.1267, grad_fn=<SubBackward0>), time elapsed: 156.6753077507019 seconds\n",
      "Step 4500, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 160.16789388656616 seconds\n",
      "Step 4600, loss: tensor(0.1255, grad_fn=<SubBackward0>), time elapsed: 163.63565826416016 seconds\n",
      "Step 4700, loss: tensor(0.1300, grad_fn=<SubBackward0>), time elapsed: 167.29802775382996 seconds\n",
      "Step 4800, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 170.77924060821533 seconds\n",
      "Step 4900, loss: tensor(0.1254, grad_fn=<SubBackward0>), time elapsed: 174.2670271396637 seconds\n",
      "Step 5000, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 177.93798208236694 seconds\n",
      "Step 5100, loss: tensor(0.1210, grad_fn=<SubBackward0>), time elapsed: 181.4082899093628 seconds\n",
      "Step 5200, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 184.88124871253967 seconds\n",
      "Step 5300, loss: tensor(0.1267, grad_fn=<SubBackward0>), time elapsed: 188.5760543346405 seconds\n",
      "Step 5400, loss: tensor(0.1246, grad_fn=<SubBackward0>), time elapsed: 192.0443296432495 seconds\n",
      "Step 5500, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 195.51018691062927 seconds\n",
      "Step 5600, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 198.96115517616272 seconds\n",
      "Step 5700, loss: tensor(0.1246, grad_fn=<SubBackward0>), time elapsed: 202.64636373519897 seconds\n",
      "Step 5800, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 206.16446924209595 seconds\n",
      "Step 5900, loss: tensor(0.1288, grad_fn=<SubBackward0>), time elapsed: 209.66699934005737 seconds\n",
      "Step 6000, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 213.35772943496704 seconds\n",
      "Step 6100, loss: tensor(0.1275, grad_fn=<SubBackward0>), time elapsed: 216.85531330108643 seconds\n",
      "Step 6200, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 220.36945104599 seconds\n",
      "Step 6300, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 224.06240797042847 seconds\n",
      "Step 6400, loss: tensor(0.1236, grad_fn=<SubBackward0>), time elapsed: 227.55681896209717 seconds\n",
      "Step 6500, loss: tensor(0.1269, grad_fn=<SubBackward0>), time elapsed: 231.01854729652405 seconds\n",
      "Step 6600, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 234.71656227111816 seconds\n",
      "Step 6700, loss: tensor(0.1263, grad_fn=<SubBackward0>), time elapsed: 238.19183325767517 seconds\n",
      "Step 6800, loss: tensor(0.1235, grad_fn=<SubBackward0>), time elapsed: 241.6573359966278 seconds\n",
      "Step 6900, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 245.33190941810608 seconds\n",
      "Step 7000, loss: tensor(0.1264, grad_fn=<SubBackward0>), time elapsed: 248.83342146873474 seconds\n",
      "Step 7100, loss: tensor(0.1212, grad_fn=<SubBackward0>), time elapsed: 252.33455801010132 seconds\n",
      "Step 7200, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 256.050683259964 seconds\n",
      "Step 7300, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 259.5264718532562 seconds\n",
      "Step 7400, loss: tensor(0.1265, grad_fn=<SubBackward0>), time elapsed: 263.0013334751129 seconds\n",
      "Step 7500, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 266.68131613731384 seconds\n",
      "Step 7600, loss: tensor(0.1215, grad_fn=<SubBackward0>), time elapsed: 270.1884262561798 seconds\n",
      "Step 7700, loss: tensor(0.1177, grad_fn=<SubBackward0>), time elapsed: 273.70292258262634 seconds\n",
      "Step 7800, loss: tensor(0.1212, grad_fn=<SubBackward0>), time elapsed: 277.37595438957214 seconds\n",
      "Step 7900, loss: tensor(0.1204, grad_fn=<SubBackward0>), time elapsed: 280.83979654312134 seconds\n",
      "Step 8000, loss: tensor(0.1228, grad_fn=<SubBackward0>), time elapsed: 284.3016550540924 seconds\n",
      "Step 8100, loss: tensor(0.1242, grad_fn=<SubBackward0>), time elapsed: 287.7875304222107 seconds\n",
      "Step 8200, loss: tensor(0.1218, grad_fn=<SubBackward0>), time elapsed: 291.48431420326233 seconds\n",
      "Step 8300, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 294.93886280059814 seconds\n",
      "Step 8400, loss: tensor(0.1181, grad_fn=<SubBackward0>), time elapsed: 298.42220425605774 seconds\n",
      "Step 8500, loss: tensor(0.1246, grad_fn=<SubBackward0>), time elapsed: 302.13606667518616 seconds\n",
      "Step 8600, loss: tensor(0.1236, grad_fn=<SubBackward0>), time elapsed: 305.6122934818268 seconds\n",
      "Step 8700, loss: tensor(0.1201, grad_fn=<SubBackward0>), time elapsed: 309.1112959384918 seconds\n",
      "Step 8800, loss: tensor(0.1215, grad_fn=<SubBackward0>), time elapsed: 312.8212661743164 seconds\n",
      "Step 8900, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 316.29765605926514 seconds\n",
      "Step 9000, loss: tensor(0.1213, grad_fn=<SubBackward0>), time elapsed: 319.81809067726135 seconds\n",
      "Step 9100, loss: tensor(0.1163, grad_fn=<SubBackward0>), time elapsed: 323.5240092277527 seconds\n",
      "Step 9200, loss: tensor(0.1219, grad_fn=<SubBackward0>), time elapsed: 327.04846024513245 seconds\n",
      "Step 9300, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 330.5727915763855 seconds\n",
      "Step 9400, loss: tensor(0.1272, grad_fn=<SubBackward0>), time elapsed: 334.2978081703186 seconds\n",
      "Step 9500, loss: tensor(0.1206, grad_fn=<SubBackward0>), time elapsed: 337.8120675086975 seconds\n",
      "Step 9600, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 341.3048930168152 seconds\n",
      "Step 9700, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 345.0132145881653 seconds\n",
      "Step 9800, loss: tensor(0.1218, grad_fn=<SubBackward0>), time elapsed: 348.5397262573242 seconds\n",
      "Step 9900, loss: tensor(0.1211, grad_fn=<SubBackward0>), time elapsed: 352.061155796051 seconds\n",
      "Step 10000, loss: tensor(0.1216, grad_fn=<SubBackward0>), time elapsed: 355.75561141967773 seconds\n",
      "Step 10100, loss: tensor(0.1229, grad_fn=<SubBackward0>), time elapsed: 359.2511451244354 seconds\n",
      "Step 10200, loss: tensor(0.1262, grad_fn=<SubBackward0>), time elapsed: 362.7614703178406 seconds\n",
      "Step 10300, loss: tensor(0.1209, grad_fn=<SubBackward0>), time elapsed: 366.4611189365387 seconds\n",
      "Step 10400, loss: tensor(0.1183, grad_fn=<SubBackward0>), time elapsed: 369.9743173122406 seconds\n",
      "Step 10500, loss: tensor(0.1186, grad_fn=<SubBackward0>), time elapsed: 373.4715096950531 seconds\n",
      "Step 10600, loss: tensor(0.1194, grad_fn=<SubBackward0>), time elapsed: 376.9833312034607 seconds\n",
      "Step 10700, loss: tensor(0.1221, grad_fn=<SubBackward0>), time elapsed: 380.70921778678894 seconds\n",
      "Step 10800, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 384.23450326919556 seconds\n",
      "Step 10900, loss: tensor(0.1214, grad_fn=<SubBackward0>), time elapsed: 387.7155375480652 seconds\n",
      "Step 11000, loss: tensor(0.1170, grad_fn=<SubBackward0>), time elapsed: 391.422504901886 seconds\n",
      "Step 11100, loss: tensor(0.1181, grad_fn=<SubBackward0>), time elapsed: 394.91770219802856 seconds\n",
      "Step 11200, loss: tensor(0.1233, grad_fn=<SubBackward0>), time elapsed: 398.4300289154053 seconds\n",
      "Step 11300, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 402.16988706588745 seconds\n",
      "Step 11400, loss: tensor(0.1196, grad_fn=<SubBackward0>), time elapsed: 405.709276676178 seconds\n",
      "Step 11500, loss: tensor(0.1211, grad_fn=<SubBackward0>), time elapsed: 409.24177527427673 seconds\n",
      "Step 11600, loss: tensor(0.1223, grad_fn=<SubBackward0>), time elapsed: 413.00892448425293 seconds\n",
      "Step 11700, loss: tensor(0.1199, grad_fn=<SubBackward0>), time elapsed: 416.5715181827545 seconds\n",
      "Step 11800, loss: tensor(0.1182, grad_fn=<SubBackward0>), time elapsed: 420.14592719078064 seconds\n",
      "Step 11900, loss: tensor(0.1197, grad_fn=<SubBackward0>), time elapsed: 423.93831157684326 seconds\n",
      "Step 12000, loss: tensor(0.1203, grad_fn=<SubBackward0>), time elapsed: 427.52855229377747 seconds\n"
     ]
    }
   ],
   "source": [
    "#train on diffusion data\n",
    "n, na = 4, 1 # number of data and ancilla qubits\n",
    "T = 20 # number of diffusion steps\n",
    "L = 6 # layer of backward PQC\n",
    "Ndata = 1000 # number of data in the training data set\n",
    "epochs = 12001 # number of training epochs\n",
    "\n",
    "#gen random samples at t = T\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_T = diffModel.HaarSampleGeneration(Ndata, seed = 22)\n",
    "\n",
    "#load diffusion process\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "model.set_diffusionSet(states_diff)\n",
    "\n",
    "params_total = []\n",
    "loss_hist_total = []\n",
    "\n",
    "for t in range(T - 1, -1, -1):\n",
    "    params_tot = np.load('params_total.npy')\n",
    "    for tt in range(t+1, T):\n",
    "        print(tt)\n",
    "        params_tot[tt] = np.load('paramsandloss_squaremodel/params_t%d.npy'%tt)\n",
    "    \n",
    "    params, loss_hist = Training_t(model, t, inputs_T, params_tot, Ndata, epochs)\n",
    "\n",
    "    np.save('paramsandloss_squaremodel/params_t%d'%t, params.detach().numpy())\n",
    "    np.save('paramsandloss_squaremodel/loss_t%d'%t, loss_hist.detach().numpy())\n",
    "    \n",
    "    #params_total = np.append(params_total, params, axis = 1)\n",
    "    #loss_hist_total = np.append(loss_hist_total, loss_hist, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, na = 4, 1\n",
    "T = 20\n",
    "L = 6\n",
    "Ndata = 2000\n",
    "epochs = 20000 + 1\n",
    "\n",
    "params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "loss_tot = np.zeros((T, epochs))\n",
    "f0_tot = np.zeros((T, epochs))\n",
    "\n",
    "for t in range(T):\n",
    "    params_tot[t] = np.load('paramsandloss_squaremodel/params_t%d.npy'%t)\n",
    "    loss_tot[t] = np.load('paramsandloss_squaremodel/loss_t%d.npy'%t)\n",
    "    \n",
    "\n",
    "np.save(\"params_total_2000Ndata_20kEpochs\", params_tot)\n",
    "np.save(\"loss_tot_2000Ndata_20kEpochs\", loss_tot)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fd8e672270>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create random 16 pixel data\n",
    "Ndata = 2000\n",
    "n = 4\n",
    "random_images = np.zeros((Ndata, 4, 4))\n",
    "amplitude_vals = np.zeros(Ndata)\n",
    "\n",
    "for i in range(0, Ndata):\n",
    "    random_images[i] = np.random.rand(4,4)\n",
    "    amplitude_vals[i] = np.sum(random_images[i] ** 2) ** 0.5\n",
    "\n",
    "plt.imshow(random_images[1], cmap = 'grey', interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amplitude encode image data into qubits\n",
    "random_images_qubits = np.zeros((Ndata, 2**n)) + 1j * np.zeros((Ndata, 2**n))\n",
    "for i in range(0, Ndata):\n",
    "    random_images_qubits[i] = np.ravel(random_images[i] / amplitude_vals[i] + 0j)\n",
    "\n",
    "#print(random_images_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run trained model on random image data\n",
    "diffused_images = np.load('Xout_testimage.npy')\n",
    "test_data_T20 = torch.tensor(diffused_images[20], dtype = torch.complex64)\n",
    "#print(Xout[0])\n",
    "n, na = 4, 1\n",
    "T = 20\n",
    "L = 6\n",
    "Ndata = 2000\n",
    "\n",
    "params_tot = np.load('params_total_2000Ndata_20kEpochs.npy')\n",
    "\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_te = diffModel.HaarSampleGeneration(Ndata, seed=22)\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "\n",
    "\n",
    "#data_te = model.backDataGeneration(test_data_T20, params_tot, Ndata)[:, :, :2**n].numpy()\n",
    "data_te = model.backDataGeneration(torch.from_numpy(random_images_qubits), params_tot, Ndata)[:, :, :2**n].numpy()\n",
    "\n",
    "np.save(\"test_backwardsgen\", data_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from backwards_gen decode the nxn array for the generated image from qubit states\n",
    "backwards_gen = np.load('test_backwardsgen.npy')\n",
    "dim = 4#int((final_output_flattened.size) ** 0.5)\n",
    "final_output_nxn = np.zeros((T + 1, Ndata, dim, dim))\n",
    "\n",
    "for z in range(0, 21):\n",
    "    final_output_flattened = backwards_gen[z]\n",
    "    final_output_flattened = np.abs(final_output_flattened)\n",
    "    multiplier = np.max(final_output_flattened)\n",
    "    for i in range(0, np.size(amplitude_vals)):\n",
    "        final_output_flattened[:][i] *= amplitude_vals[i]\n",
    "    #final_output_flattened = final_output_flattened.flatten()\n",
    "\n",
    "    for i in range(0, dim):\n",
    "        for j in range(0, dim):\n",
    "            for nth_data in range(0, Ndata):\n",
    "                final_output_nxn[z][nth_data][i][j] = final_output_flattened[nth_data][(i*dim) + j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.24780219e-02-5.93114994e-04j -1.56621207e-02-3.90787749e-03j\n",
      "  -1.06544131e-02+1.51402375e-03j -9.98933241e-03-2.14363588e-03j\n",
      "  -9.34913103e-03-5.34133660e-03j  7.96415610e-04-1.13453472e-03j\n",
      "  -3.73336533e-03-6.98780292e-04j -1.23603605e-02-1.58969907e-03j\n",
      "  -9.58481710e-03-3.65941273e-03j  7.95269851e-04-3.49259353e-04j\n",
      "  -1.61837623e-03+3.09350016e-03j -5.21578500e-03-1.00545306e-03j\n",
      "  -1.12097086e-02-1.18416594e-03j -9.32759047e-03-4.10621113e-04j\n",
      "  -1.16118966e-02+9.28505498e-04j -9.86696314e-03+6.98716438e-04j]\n",
      " [-8.44608876e-04-3.30406148e-03j -1.32088619e-03+6.58079283e-04j\n",
      "  -1.88586221e-03-1.06912012e-04j -2.61170161e-03+7.28171377e-04j\n",
      "  -4.04873258e-03+1.15694059e-03j -2.18625553e-03+4.84092208e-03j\n",
      "  -3.84756038e-03+3.25297797e-03j -1.94246415e-03+2.34429119e-03j\n",
      "  -3.89945391e-03+1.88835105e-03j -8.48368625e-04+5.44398185e-03j\n",
      "  -3.10110091e-03+4.44780011e-03j -1.08371978e-03+2.83264392e-03j\n",
      "  -1.19579432e-03-4.51016845e-03j -2.45223683e-03+9.52430419e-04j\n",
      "  -4.45459649e-04+1.14746625e-03j -2.30062869e-03+4.06538398e-04j]\n",
      " [ 3.10543436e-03-7.64208962e-04j -3.54366843e-03+1.13952835e-03j\n",
      "  -2.15010645e-04+4.77871450e-04j -9.49896290e-04+1.11274421e-03j\n",
      "   9.64582025e-04+2.83518038e-03j -1.49177169e-04+1.47154322e-03j\n",
      "   2.46550859e-04-4.40635689e-04j -1.87414559e-03-1.18299155e-04j\n",
      "   3.81448190e-03+4.02002857e-04j -3.13448487e-03+1.27423450e-03j\n",
      "  -1.15413556e-03+1.35804433e-03j -5.08204044e-04-5.02490322e-04j\n",
      "   3.36875394e-03+1.11910154e-03j -2.63977505e-04-3.76360229e-04j\n",
      "   4.18569613e-03+6.09857263e-04j  2.76714843e-03-1.59840147e-05j]\n",
      " [-2.22913641e-03-2.56605749e-03j -5.49505581e-04-1.39855267e-03j\n",
      "   3.46562266e-03+6.41426584e-03j  1.05884252e-03+2.79409275e-03j\n",
      "   2.96640932e-03+3.75736505e-04j  2.65045534e-03+3.36201396e-03j\n",
      "  -1.87646801e-04-3.67839122e-03j  2.43904022e-03+3.01977922e-03j\n",
      "  -1.08625740e-03+2.28242041e-03j -2.74364999e-03-2.53380649e-03j\n",
      "   3.07071535e-03+4.38581657e-04j  9.53826413e-04+1.90337643e-03j\n",
      "   3.50029930e-03+8.02543852e-03j  6.27415115e-03+5.57411788e-03j\n",
      "   3.80195561e-03-3.85973742e-03j  2.09671538e-03+1.30323868e-03j]\n",
      " [ 1.86993298e-03+8.78024753e-03j -4.91725886e-03+2.33528030e-04j\n",
      "  -5.90528781e-03+5.23001235e-03j -6.55351067e-03+4.38343175e-03j\n",
      "  -2.56731850e-03+1.43636868e-03j -1.19279174e-03+2.70959135e-05j\n",
      "   8.51051975e-03+2.46252341e-04j -1.46666188e-02-8.59234540e-04j\n",
      "  -6.96972385e-03+4.09851922e-03j  3.98581382e-03-4.23406716e-03j\n",
      "  -1.50740205e-03-4.46566846e-03j -6.14654971e-03+5.20443497e-03j\n",
      "  -1.00585828e-02+2.82418588e-03j -9.71665699e-03+5.07377880e-03j\n",
      "  -4.31702100e-03+2.52426602e-03j -2.41301488e-03+1.06774252e-02j]\n",
      " [-7.22406991e-03+1.75573607e-03j  1.04867425e-02-1.12866727e-03j\n",
      "   5.95335802e-03-8.77062883e-03j  1.26229378e-03-5.60209062e-03j\n",
      "   4.65014065e-03-5.16378507e-03j -5.30063058e-04+1.71316252e-03j\n",
      "   2.79434514e-03+5.36471722e-04j  8.50143656e-03-5.82674565e-03j\n",
      "   1.87407772e-03-3.45788780e-03j  8.93250282e-04+3.09487828e-03j\n",
      "  -1.57441315e-03-8.56498082e-04j -7.96492561e-04-5.58523927e-03j\n",
      "   9.73568019e-03-8.86984810e-04j  1.02564786e-02-5.73905371e-03j\n",
      "   3.58222285e-03+2.36866501e-04j -1.70464348e-03-8.61055963e-03j]\n",
      " [ 8.35510809e-03+9.65136569e-03j  5.15940087e-03-1.39033911e-03j\n",
      "  -5.92008280e-03-2.80804303e-03j  3.79926339e-03+3.12472042e-03j\n",
      "  -7.42464885e-03-4.55757632e-04j -1.75444165e-03-1.76784722e-03j\n",
      "  -4.54220036e-03-2.24483013e-03j -3.17651802e-03+1.19714532e-03j\n",
      "  -4.94658574e-03+3.05539044e-03j  3.61300260e-03+4.25968901e-04j\n",
      "  -1.28595554e-03+1.30660459e-03j  2.55804951e-03-4.75345738e-03j\n",
      "   1.24989275e-03-8.27717595e-03j  6.65536802e-03-2.24199891e-03j\n",
      "  -2.77284184e-03-4.42160806e-03j -4.24341299e-03+5.65376272e-03j]\n",
      " [-3.00574512e-03-8.25577322e-03j  6.59086322e-03-5.66657539e-03j\n",
      "  -2.92682601e-03+5.87909296e-03j -1.00637386e-02+1.84624875e-03j\n",
      "   4.97209746e-03-3.69969173e-03j  4.20466159e-03+1.49514980e-03j\n",
      "  -1.98380952e-03+9.47253313e-04j  3.87089502e-04+7.26272585e-03j\n",
      "   8.30180850e-03+1.50287943e-03j -3.03093763e-03+2.51618284e-03j\n",
      "  -2.07830756e-03-1.38290098e-03j -4.48543718e-03+3.38719552e-03j\n",
      "  -6.86393026e-03-7.13721674e-04j -6.03964226e-03+6.04208233e-03j\n",
      "  -3.47834593e-03-2.45943363e-03j  4.51223599e-03+3.80785554e-03j]\n",
      " [ 7.23603461e-03+1.81012880e-03j  2.84263305e-03-1.05149010e-02j\n",
      "  -2.75701215e-03+1.70716073e-03j  1.32273631e-02+1.23156477e-02j\n",
      "   2.38259626e-03-4.29070089e-03j  3.57657700e-04-3.78216966e-03j\n",
      "  -1.03559075e-02+9.34717059e-03j  1.76206306e-02-4.23783576e-03j\n",
      "   1.99440378e-03+7.08169164e-03j -6.08500151e-04+1.41397677e-03j\n",
      "   1.19677065e-02+2.02858821e-03j -6.81258025e-05-7.39656342e-03j\n",
      "  -6.68324530e-03-1.23276678e-03j -5.50081395e-03-6.31590001e-03j\n",
      "  -1.72260385e-02+7.78946048e-03j -6.18985668e-03+1.04131065e-02j]\n",
      " [ 2.17894930e-03+1.79670565e-03j  1.11185727e-05-1.15138059e-02j\n",
      "  -3.84032866e-03-9.25832987e-03j  1.72432866e-02-5.80166187e-03j\n",
      "   1.79593321e-02+1.26229972e-02j  2.63316487e-03-5.83542278e-04j\n",
      "   4.30334621e-04+8.26807134e-03j -6.65150676e-03-4.97362716e-03j\n",
      "   1.35080976e-04+6.75031124e-03j -2.77454662e-03+1.00556994e-02j\n",
      "  -1.23310480e-02+7.23170396e-03j  7.45536061e-03-9.88946296e-03j\n",
      "  -7.21068541e-03+8.71346053e-03j  4.38275718e-04+8.79015587e-03j\n",
      "   3.10522248e-03+1.39605915e-02j  1.15791978e-02+1.23602422e-02j]\n",
      " [-3.34482262e-04-8.58869404e-03j  1.44834735e-03+2.16820594e-02j\n",
      "  -1.56735424e-02-1.37999863e-03j  2.48994920e-02-3.97483865e-03j\n",
      "   2.42685433e-03+6.14038529e-03j -5.79530280e-03-1.14299608e-02j\n",
      "   2.79938267e-03+3.40493396e-03j  2.82704756e-02+2.82284594e-03j\n",
      "  -1.99159421e-02-1.30481040e-02j  7.20610609e-03-1.19196400e-02j\n",
      "   7.44269555e-03-7.32730981e-03j -7.17250444e-03+2.15821643e-03j\n",
      "   2.42580334e-03-2.60855281e-03j -7.45660486e-03-2.45068353e-02j\n",
      "  -4.98110242e-03+2.88941315e-04j  3.37721780e-02-3.62612749e-03j]\n",
      " [-1.50233144e-02-1.11536849e-02j  2.95407921e-02-1.88882630e-02j\n",
      "  -1.74933496e-06+1.85830996e-03j -2.15993449e-02+1.07688168e-02j\n",
      "   1.24350311e-02-2.51147114e-02j -6.80192700e-03-2.42566019e-02j\n",
      "   2.36854609e-02+1.34551302e-02j -7.08903559e-03+3.48003721e-03j\n",
      "   3.03148502e-03+2.83446093e-03j  2.53943820e-02-2.39294879e-02j\n",
      "   6.07578084e-03+1.02423262e-02j  1.68455634e-02-2.62710154e-02j\n",
      "  -2.23555341e-02+1.27634434e-02j  1.62128489e-02-1.33996299e-02j\n",
      "   3.50131094e-03-1.73078813e-02j  1.31878154e-02-1.39907822e-02j]\n",
      " [ 8.11636727e-03-2.95377653e-02j  2.53582597e-02+1.10957371e-02j\n",
      "   2.73494013e-02-4.17432971e-02j  8.29867553e-03+1.56000573e-02j\n",
      "  -1.44889178e-02-3.03025125e-03j -1.09956600e-02+3.14710699e-02j\n",
      "   1.59622952e-02+1.22774660e-03j -1.69264879e-02-1.42143331e-02j\n",
      "  -1.50465395e-03-8.84425547e-03j -3.02206203e-02-3.98407429e-02j\n",
      "   1.19995617e-03+2.57104840e-02j -1.54722610e-03-2.29298081e-02j\n",
      "   1.64152603e-04-3.91956940e-02j  2.68717273e-03+9.36300057e-05j\n",
      "   7.14433333e-03+1.88135281e-02j  1.89303327e-02+2.94551509e-03j]\n",
      " [-1.33162094e-02-2.95095537e-02j  3.35633475e-03+1.71887893e-02j\n",
      "   4.58459975e-03-3.36217257e-04j -3.57885510e-02-6.05074652e-02j\n",
      "   3.73661369e-02+5.59944436e-02j  2.65581366e-02+4.04892750e-02j\n",
      "   1.20891072e-02-1.98220983e-02j -6.05545044e-02+4.00248915e-02j\n",
      "   8.87067989e-03+2.78912834e-03j -4.84610759e-02-7.30184764e-02j\n",
      "  -4.64406013e-02+1.74983367e-02j  1.28834266e-02-1.74676869e-02j\n",
      "   2.68430449e-02-1.55765563e-02j -4.78370339e-02+1.90225802e-02j\n",
      "  -7.92166591e-02+4.79043648e-02j  1.25920260e-02-3.36326957e-02j]\n",
      " [-2.24800929e-02-1.33745866e-02j  1.09910099e-02-1.02003608e-02j\n",
      "  -1.19408723e-02-1.37352133e-02j  1.13725336e-02-9.52623412e-02j\n",
      "  -3.93770169e-03-1.19969044e-02j -2.41502505e-02-2.47600693e-02j\n",
      "   2.83343880e-03+3.30370776e-02j  3.07234637e-02-2.96149645e-02j\n",
      "   4.52344166e-03-2.73087192e-02j -2.67085079e-02-3.34701985e-02j\n",
      "  -9.00762826e-02+9.80375335e-02j  8.07428882e-02-2.59384532e-02j\n",
      "  -1.40658915e-02+1.65854804e-02j -5.39710447e-02+4.84146141e-02j\n",
      "  -5.85687421e-02-1.93630178e-02j  5.97641394e-02+3.56550477e-02j]\n",
      " [-4.73435409e-02+9.83261690e-03j  5.49229272e-02+6.75534131e-03j\n",
      "   4.84203994e-02+3.53253353e-03j -3.29932608e-02+4.05784696e-02j\n",
      "   6.29957393e-02+6.57271966e-03j -8.02381486e-02-9.09870788e-02j\n",
      "   4.48285602e-02-1.32524157e-02j -5.09202182e-02-5.32383844e-02j\n",
      "  -4.27248469e-03-1.06320120e-02j  2.06857678e-02-4.58433554e-02j\n",
      "   6.67235479e-02-5.73419258e-02j -6.44643530e-02-6.15224652e-02j\n",
      "   1.99632812e-02-2.30591316e-02j -3.72508653e-02-2.87189484e-02j\n",
      "   3.26159559e-02+3.06283403e-02j -1.35436459e-02-7.53808171e-02j]\n",
      " [-1.47612114e-02-3.74097973e-02j  2.79195141e-03+4.09918241e-02j\n",
      "  -1.60753783e-02-5.37985377e-02j  7.35195056e-02-8.03564489e-02j\n",
      "   5.46594290e-03+5.50547205e-02j -2.66708303e-02-6.37446791e-02j\n",
      "   9.40340757e-03-7.45665878e-02j  1.81236453e-02+2.46429052e-02j\n",
      "  -2.88028419e-02-4.35262807e-02j -6.00875821e-03+2.86239367e-02j\n",
      "  -1.73297636e-02-2.33577229e-02j  1.69364974e-01-5.88052832e-02j\n",
      "  -3.22114229e-02+4.23195660e-02j  8.05006772e-02-1.91090088e-02j\n",
      "   5.05773053e-02+1.33020030e-02j  9.85382721e-02-2.29237489e-02j]\n",
      " [-7.36911446e-02-5.10767847e-03j -3.31189372e-02-9.97224264e-03j\n",
      "  -1.55750230e-01+6.59286305e-02j -1.33403921e-02+3.57787423e-02j\n",
      "  -4.31446917e-02+4.17740084e-03j  3.20982304e-03+6.76948726e-02j\n",
      "  -2.13712394e-01+1.30273163e-01j  8.88145790e-02-3.79571058e-02j\n",
      "  -6.31075799e-02+4.29517403e-02j  7.06156790e-02-4.49932739e-02j\n",
      "  -1.91352218e-02+9.56643373e-02j -3.83939296e-02+1.28054097e-01j\n",
      "  -1.79728307e-02+3.61758232e-01j  1.51456827e-02+1.66521490e-01j\n",
      "   1.65323064e-01+1.74747959e-01j  7.87870660e-02+2.07193997e-02j]\n",
      " [ 8.34685266e-02-1.53497025e-01j  4.67734449e-02+1.38384506e-01j\n",
      "  -5.75507507e-02+2.56524142e-02j -5.98125570e-02-1.09141722e-01j\n",
      "   2.81565054e-03-1.67208552e-01j -7.90316332e-03-9.26040858e-02j\n",
      "  -9.86256599e-02-6.55010790e-02j  2.09248558e-01+1.48764387e-01j\n",
      "  -1.85929146e-02-1.90932944e-01j -7.72706373e-03+2.19489053e-01j\n",
      "  -3.08770575e-02+1.90833196e-01j -1.13808177e-01+8.91899019e-02j\n",
      "  -5.66035211e-02+1.48960024e-01j -1.12702765e-01-2.70556789e-02j\n",
      "   1.15565091e-01+2.69267932e-02j -4.32365648e-02+1.01731801e-02j]\n",
      " [ 2.28946686e-01-1.59583360e-01j  6.61842823e-02-8.17808360e-02j\n",
      "   1.12090982e-01-2.00352132e-01j -1.04896016e-01-4.76055332e-02j\n",
      "   3.82693037e-02-1.53040484e-01j -1.35608912e-01+5.85770048e-02j\n",
      "   1.47829223e-02-5.76299266e-04j -1.00221694e-01-7.20638037e-02j\n",
      "   6.60852641e-02-2.14565858e-01j  5.97001286e-03+1.15073502e-01j\n",
      "   9.15438905e-02-1.44551605e-01j  2.69394815e-02+5.63136674e-02j\n",
      "   1.81239218e-01+1.47838116e-01j -3.85905174e-03+7.80616933e-03j\n",
      "  -8.26215819e-02+5.21098338e-02j  8.98018032e-02-1.29101574e-01j]\n",
      " [ 2.18364418e-01+0.00000000e+00j  2.16589332e-01+0.00000000e+00j\n",
      "   2.11218879e-01+0.00000000e+00j  2.13595465e-01+0.00000000e+00j\n",
      "   2.16765583e-01+0.00000000e+00j  2.21146733e-01+0.00000000e+00j\n",
      "   2.16290012e-01+0.00000000e+00j  2.18250662e-01+0.00000000e+00j\n",
      "   2.18513578e-01+0.00000000e+00j  2.20058829e-01+0.00000000e+00j\n",
      "   2.16640264e-01+0.00000000e+00j  2.12011769e-01+0.00000000e+00j\n",
      "   2.18997434e-01+0.00000000e+00j  2.17403799e-01+0.00000000e+00j\n",
      "   2.17757359e-01+0.00000000e+00j  2.17499688e-01+0.00000000e+00j]]\n",
      "1.0740064960514244\n",
      "1.0132715265853545\n",
      "0.992898923759149\n",
      "0.9861545899392121\n",
      "1.041569117355383\n",
      "0.9729131513224244\n",
      "1.0000899982700464\n",
      "1.0067142147439234\n",
      "0.9969131628623646\n",
      "0.9771605333804334\n",
      "0.982181181124206\n",
      "0.9898631099334758\n",
      "0.9754872875782215\n",
      "1.1185361916060528\n",
      "1.039551921495852\n",
      "1.0823165323875357\n",
      "0.8570521346707378\n",
      "1.4336471165792177\n",
      "1.3907015005267311\n",
      "1.1037846973625307\n",
      "0.2344390115155412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'T')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACCdUlEQVR4nO3dd3wT9f8H8Ndd2nQP6GRv2cJPhFJkqBTKUCyiQFGBFgGRXQdDZQgKDvYUpYAMGYqIgCAgMqQIUlBBlsj4gnQwOuhMc5/fH2kjoWlpStpLm9fz8eij6eVzd+/LJbl3P/cZkhBCgIiIiMiOyWoHQERERKQ2JkRERERk95gQERERkd1jQkRERER2jwkRERER2T0mRERERGT3mBARERGR3WNCRERERHaPCRERERHZPSZERDZg4MCBcHd3VzsMKgN+/vlnSJKEn3/+We1QjGrWrImBAweqHUaBpkyZAkmScPPmTbVDscjKlSshSRIuX75s9W0PHDgQNWvWNFl29+5dvPrqqwgMDIQkSRgzZgwAID4+Hi+88AJ8fHwgSRLmzp1r9XgkScKUKVOsvl1LMCHKlffGkyQJhw4dyve8EALVqlWDJEl45plnVIiw6LKzszFv3jz83//9Hzw9PeHt7Y3GjRtjyJAhOHv2rNrhlai8i0VBP+vXr1c7xFKzePFirFy5Uu0wSsy9n9n7f8aPH692eGTG/efJzc0NjRo1wvTp05Genq52eGVWXsKX9+Pq6orq1avj2WefxYoVK5CVlVWk7Xz44YdYuXIlhg0bhtWrV+OVV14BAIwdOxa7du3ChAkTsHr1anTp0qUkD0c1DmoHYGucnZ2xbt06tG3b1mT5/v37ce3aNTg5OakUWdH16tULP/zwA8LDwzF48GDodDqcPXsW27ZtQ5s2bdCgQQO1Qyxxo0aNQsuWLfMtDw4OViEadSxevBi+vr42/Z+7Nbz//vuoVauWybImTZqoFA09SKdOndC/f38AhhqJgwcP4r333sPvv/+OTZs2qRxd2bZkyRK4u7sjKysL169fx65duxAZGYm5c+di27ZtqFatmrHs559/DkVRTNb/6aef0Lp1a0yePDnf8ueeew5vvvlmicWekZEBBwd1UxImRPfp1q0bNm3ahPnz55ucnHXr1qFFixY2X+V67NgxbNu2DR988AEmTpxo8tzChQuRlJSkTmBFkJaWBjc3N6tsq127dnjhhRcsWkdRFGRnZ8PZ2blEYktPT4erq+tDbYPy69q1Kx5//HGrb9ea70dLCCGQmZkJFxeXUt93aXjkkUfw8ssvG/9+7bXXkJ2djc2bNyMzM9Ps56+8Kuw7pzheeOEF+Pr6Gv+eNGkS1q5di/79++PFF1/EkSNHjM85OjrmWz8hIQGNGjUyu9zb29sqMRbEFs47b5ndJzw8HLdu3cLu3buNy7Kzs/H111+jX79+ZtdRFAVz585F48aN4ezsjICAAAwdOhR37twxKffdd9+he/fuqFy5MpycnFCnTh1MmzYNer3epNyTTz6JJk2a4K+//sJTTz0FV1dXVKlSBR9//PED47948SIA4Iknnsj3nEajgY+Pj8myQ4cOoWXLlnB2dkadOnXw2WefGatf81y+fBmSJJm9/XL/fd8rV67g9ddfR/369eHi4gIfHx+8+OKL+e6B593u2L9/P15//XX4+/ujatWqxud/+OEHtGvXDm5ubvDw8ED37t1x+vTpBx6/JSRJwogRI7B27Vo0btwYTk5O2Llz5wNjW7x4sbF85cqVMXz48HyJZt45PH78ONq3bw9XV9d8Cao5//zzD0JDQ+Hm5obKlSvj/fffhxDCpExR3m81a9bE6dOnsX//fmM1+pNPPomkpCRoNBrMnz/fWPbmzZuQZRk+Pj4m+xo2bBgCAwNN9v3rr7+iS5cu8PLygqurKzp06IBffvkl33Fcv34dkZGRCAgIgJOTExo3bozo6GiTMnm3Nzdu3IgPPvgAVatWhbOzMzp27Ii///77ga9VUf3000/G95K3tzeee+45nDlzxqRM3nv+r7/+Qr9+/VChQgW0bdsWW7duhSRJ+OOPP4xlv/nmG0iShOeff95kGw0bNkSfPn2Mf69YsQJPP/00/P394eTkhEaNGmHJkiX54qtZsyaeeeYZ7Nq1C48//jhcXFzw2WefAQCuXbuGsLAwuLm5wd/fH2PHji3y7Q9LP4u//PILoqKi4OfnBzc3N/Ts2ROJiYkmZYUQmD59OqpWrQpXV1c89dRTVvlc5rVZufef0IMHD+LFF19E9erV4eTkhGrVqmHs2LHIyMjIt/7Zs2fRu3dv+Pn5wcXFBfXr18c777xT6D6vXLmCunXrokmTJoiPj8f8+fOh0WhMPsuzZs2CJEmIiooyLtPr9fDw8MC4ceOMyz799FO0adMGPj4+cHFxQYsWLfD111/n22dB3zkAcPr0aTz99NNwcXFB1apVMX369Hw1OMXx0ksv4dVXX8Wvv/5qcl27tw1R3mfx0qVL2L59u/E7I++9IYTAokWLjMsB5LtO5DHX7um3335DaGgofH194eLiglq1aiEyMjLfa3N/G6ITJ06ga9eu8PT0hLu7Ozp27GiS1N27v6K8fx+ENUT3qVmzJoKDg/HVV1+ha9euAAwX5+TkZPTt29fkQpJn6NChWLlyJSIiIjBq1ChcunQJCxcuxIkTJ/DLL78YM/GVK1fC3d0dUVFRcHd3x08//YRJkyYhJSUFn3zyick279y5gy5duuD5559H79698fXXX2PcuHFo2rSpMS5zatSoAQBYu3YtnnjiiUKrIP/880907twZfn5+mDJlCnJycjB58mQEBARY/LrlOXbsGA4fPoy+ffuiatWquHz5MpYsWYInn3wSf/31V74aktdffx1+fn6YNGkS0tLSAACrV6/GgAEDEBoaio8++gjp6elYsmQJ2rZtixMnTuRrCGhOamqq2dq8vEaBeX766Sds3LgRI0aMgK+vL2rWrImTJ08WGNuUKVMwdepUhISEYNiwYTh37hyWLFmCY8eOmZxrALh16xa6du2Kvn374uWXX37g66rX69GlSxe0bt0aH3/8MXbu3InJkycjJycH77//vrFcUd5vc+fOxciRI+Hu7m68MAQEBMDb2xtNmjTBgQMHMGrUKACGpFiSJNy+fRt//fUXGjduDMBwQWrXrp3Ja9W1a1e0aNECkydPhizLxov+wYMH0apVKwCGBpitW7c2fvn7+fnhhx9+wKBBg5CSkmJsqJln5syZkGUZb775JpKTk/Hxxx/jpZdewq+//lro65UnOTk537nO+y95z5496Nq1K2rXro0pU6YgIyMDCxYswBNPPIHY2Nh876UXX3wR9erVw4cffgghBNq2bQtJknDgwAE8+uijxtdFlmWTtoaJiYk4e/YsRowYYVy2ZMkSNG7cGD169ICDgwO+//57vP7661AUBcOHDzfZ77lz5xAeHo6hQ4di8ODBqF+/PjIyMtCxY0dcvXoVo0aNQuXKlbF69Wr89NNPRXpdLP0sjhw5EhUqVMDkyZNx+fJlzJ07FyNGjMCGDRuMZSZNmoTp06ejW7du6NatG2JjY9G5c2dkZ2cXKSYAyMzMNJ6vtLQ0/PLLL1i1ahX69etn8n21adMmpKenY9iwYfDx8cHRo0exYMECXLt2zeTW2h9//IF27drB0dERQ4YMQc2aNXHx4kV8//33+OCDD8zGcPHiRTz99NOoWLEidu/eDV9fX7Rr1w6KouDQoUPGdqJ55/rgwYPGdU+cOIG7d++iffv2xmXz5s1Djx498NJLLyE7Oxvr16/Hiy++iG3btqF79+4m+zb3nRMXF4ennnoKOTk5GD9+PNzc3LBs2TKr1RK+8sorWLZsGX788Ud06tQp3/MNGzbE6tWrMXbsWFStWhVvvPEGAOD//u//jG2J7r3VaYmEhATjdWb8+PHw9vbG5cuXsXnz5kLXO336NNq1awdPT0+8/fbbcHR0xGeffYYnn3wS+/fvR1BQkEn5orx/H0iQEEKIFStWCADi2LFjYuHChcLDw0Okp6cLIYR48cUXxVNPPSWEEKJGjRqie/fuxvUOHjwoAIi1a9eabG/nzp35ludt715Dhw4Vrq6uIjMz07isQ4cOAoD48ssvjcuysrJEYGCg6NWrV6HHoSiKcf2AgAARHh4uFi1aJK5cuZKvbFhYmHB2djZ57q+//hIajUbc+9a4dOmSACBWrFiRbxsAxOTJkws9xpiYmHzHk/d6t23bVuTk5BiXp6amCm9vbzF48GCTbcTFxQkvL698y++3b98+AaDAnxs3bpjELsuyOH36tMk2CootISFBaLVa0blzZ6HX643LFy5cKACI6Oho47K8c7B06dJC480zYMAAAUCMHDnSuExRFNG9e3eh1WpFYmKiEMKy91vjxo1Fhw4d8u1r+PDhIiAgwPh3VFSUaN++vfD39xdLliwRQghx69YtIUmSmDdvnjGWevXqidDQUKEoinHd9PR0UatWLdGpUyfjskGDBolKlSqJmzdvmuy3b9++wsvLy/geyTtXDRs2FFlZWcZy8+bNEwDEn3/+WehrlneezP3kad68ufD39xe3bt0yLvv999+FLMuif//+xmWTJ08WAER4eHi+/TRu3Fj07t3b+Pdjjz0mXnzxRQFAnDlzRgghxObNmwUA8fvvv5u8NvcLDQ0VtWvXNllWo0YNAUDs3LnTZPncuXMFALFx40bjsrS0NFG3bl0BQOzbt6/Q18fSz2JISIjJuR07dqzQaDQiKSlJCPHf+7979+4m5SZOnCgAiAEDBhQajxCiwPMVFhZm8h1YUPwzZswQkiSZfGe1b99eeHh45PuOuzfGvPObmJgozpw5IypXrixatmwpbt++bSyj1+uFp6enePvtt43r+/j4iBdffFFoNBqRmpoqhBBi9uzZQpZlcefOnQJjzc7OFk2aNBFPP/10vuM3950zZswYAUD8+uuvxmUJCQnCy8tLABCXLl3K91rc697jM+fOnTsCgOjZs6dx2YABA0SNGjVMyt1/fbs37uHDh5vd5/3y3k95MX/77bfGa2th7r+WhIWFCa1WKy5evGhc9u+//woPDw/Rvn37fPt70Pu3KHjLzIzevXsjIyMD27ZtQ2pqKrZt21bg7bJNmzbBy8sLnTp1ws2bN40/LVq0gLu7O/bt22cse2+2n1eD0a5dO6Snp+fr/eXu7m5yn12r1aJVq1b4559/Co1dkiTs2rUL06dPR4UKFfDVV19h+PDhqFGjBvr06WOsDtbr9di1axfCwsJQvXp14/oNGzZEaGhokV+r+917jDqdDrdu3ULdunXh7e2N2NjYfOUHDx4MjUZj/Hv37t1ISkpCeHi4yeup0WgQFBRk8noWZtKkSdi9e3e+n4oVK5qU69Chg9l75uZi27NnD7KzszFmzBjIsmxSztPTE9u3bzdZ38nJCREREUWKN8+9NQx5NSzZ2dnYs2cPAMvebwVp164d4uPjce7cOQCG/4Lbt2+Pdu3aGf8TPnToEIQQxhqikydP4sKFC+jXrx9u3bpl3G9aWho6duyIAwcOQFEUCCHwzTff4Nlnn4UQwiTG0NBQJCcn53sfREREQKvVmsQH4IHv9TyLFi3Kd54B4MaNGzh58iQGDhxoct4fffRRdOrUCTt27Mi3rddee83s65X3uqSmpuL333/HkCFD4Ovra1x+8OBBY+1bnns/C3m1WB06dMA///yD5ORkk33UqlUr3+dux44dqFSpkklbOFdXVwwZMqRIr4uln8UhQ4aY1J62a9cOer0eV65cAfDf+3/kyJEm5e6v8XuQ5557znievvvuO0yYMAE7d+5Ev379TG7Z3ht/Wloabt68iTZt2kAIgRMnTgAw1MwdOHAAkZGRJt9jAMzezjl16hQ6dOiAmjVrYs+ePahQoYLxOVmW0aZNGxw4cAAAcObMGdy6dQvjx4+HEAIxMTEADOe6SZMmJm1q7o31zp07SE5ORrt27cy+zua+c3bs2IHWrVsba1kBwM/PDy+99FLBL6QF8ob0SE1Ntcr2LJH3Om3btg06na5I6+j1evz4448ICwtD7dq1jcsrVaqEfv364dChQ0hJSTFZ50Hv36LgLTMz/Pz8EBISgnXr1iE9PR16vb7ABroXLlxAcnIy/P39zT6fkJBgfHz69Gm8++67+Omnn/KdzPu/IKtWrZrvA12hQgWTtgwFcXJywjvvvIN33nkHN27cwP79+zFv3jxs3LgRjo6OWLNmDRITE5GRkYF69erlW79+/fpmLxZFkZGRgRkzZmDFihW4fv26yRfc/ccIIF/voAsXLgAAnn76abPb9/T0LFIcTZs2RUhIyAPL3b//wp7L+2DVr1/fZLlWq0Xt2rXzffCqVKlicqF/EFmWTT78gKEBKgDj/XhL3m8FyUs4Dh48iKpVq+LEiROYPn06/Pz88Omnnxqf8/T0RLNmzYz7BYABAwYUuN3k5GTodDokJSVh2bJlWLZsWZFivP9ClneRur8NXkFatWpltlF1QecLMCT+u3btytdw2tz7oV27dli6dCn+/vtvXLx4EZIkITg42JgoDR48GAcPHsQTTzxhkij/8ssvmDx5MmJiYvJ1KU9OToaXl1eh+81r43L/94C54zHH0s/ig85D3ut5/3eGn5+fSWLxIFWrVjX5bPbo0QM+Pj548803sW3bNjz77LMAgKtXr2LSpEnYunVrvvdCXvx5SXNRexU+++yzCAgIwK5du8yO+9WuXTvjrdWDBw+iUqVKeOyxx9CsWTMcPHgQnTp1wqFDh9C7d2+T9bZt24bp06fj5MmTJm28zCVlBZ3r+28BAUU/1w9y9+5dAICHh4dVtmeJDh06oFevXpg6dSrmzJmDJ598EmFhYejXr1+BvbYTExORnp5e4GdXURT873//M97eBx7+ewRgQlSgfv36YfDgwYiLi0PXrl0LbGGvKAr8/f2xdu1as8/7+fkBAJKSktChQwd4enri/fffR506deDs7IzY2FiMGzcuX+O5e2sm7nXvl1pRVKpUCX379kWvXr3QuHFjbNy40eKxacx9qAHkawwOGO7jrlixAmPGjEFwcDC8vLwgSRL69u1rtoHg/ffI88qsXr06X4NeAFbvllnYPfqHvX9fEr2Eivp+K0zlypVRq1YtHDhwADVr1oQQAsHBwfDz88Po0aNx5coVHDx4EG3atDFe4PPOyyeffILmzZub3a67uztu3boFAHj55ZcLTJ7y2uLksdZ73RrMnbO8ITgOHDiAf/75B4899hjc3NzQrl07zJ8/H3fv3sWJEydM2qtcvHgRHTt2RIMGDTB79mxUq1YNWq0WO3bswJw5c/J9FkrivWLpZ1HN89CxY0cAhtf42WefhV6vR6dOnXD79m2MGzcODRo0gJubG65fv46BAwcWu7Fxr169sGrVKqxduxZDhw7N93zbtm2h0+kQExNj0oYuL/k9e/YsEhMTTdrWHTx4ED169ED79u2xePFiVKpUCY6OjlixYgXWrVuXbx9q9B48deoUAKBu3bpW22ZRrwuSJOHrr7/GkSNH8P333xuHApg1axaOHDlitQFprfH+ZUJUgJ49e2Lo0KE4cuRIoY2y6tSpgz179uCJJ54o9I3+888/49atW9i8ebNJY7xLly5ZNe6CODo64tFHH8WFCxdw8+ZNY2+MvP/875V3KyVPXqZ9f08qc1WRX3/9NQYMGIBZs2YZl2VmZha5u3+dOnUAAP7+/kWq4SlNeQ3Wz507Z1KTk52djUuXLj10vIqi4J9//jHWCgHA+fPnAcDY+Leo7zeg4C8swPAFf+DAAdSqVQvNmzeHh4cHmjVrBi8vL+zcuROxsbGYOnWqsXzeefH09Cz0OP38/ODh4QG9Xq/6+bv3fN3v7Nmz8PX1LVK3+urVq6N69eo4ePAg/vnnH+PFsH379oiKisKmTZug1+tNPtfff/89srKysHXrVpP/XIt6yzcv/lOnTkEIYXIuzR2POQ/7WTQXD2CoLbz3/Z+YmGjRf+Hm5OTkAPivJuPPP//E+fPnsWrVKpOGvPf2kgJgjCPvgv8gn3zyCRwcHPD666/Dw8MjX1OIVq1aQavV4uDBgzh48CDeeustAIZz/fnnn2Pv3r3Gv/N88803cHZ2xq5du0xqPFasWFGkmADDa1uU7+LiWr16NQA8VHOI+917Xbi3wqCgW1StW7dG69at8cEHH2DdunV46aWXsH79erz66qv5yvr5+cHV1bXAz64syyZjKlkL2xAVwN3dHUuWLMGUKVOMVbjm9O7dG3q9HtOmTcv3XE5OjvHLJy97vTdbzc7OxuLFi60a94ULF3D16tV8y5OSkhATE4MKFSrAz88PGo0GoaGh2LJli0n5M2fOYNeuXSbrenp6wtfX13hvPY+52DUaTb6MfMGCBWZrk8wJDQ2Fp6cnPvzwQ7P3my3tRmlNISEh0Gq1mD9/vskxLl++HMnJyfl6kxTHwoULjY+FEFi4cCEcHR2N/0EX9f0GAG5ubgVe/Nq1a4fLly9jw4YNxgt8XhuK2bNnQ6fTmfwX3KJFC9SpUweffvqp8aJ1r7zzotFo0KtXL3zzzTdmL1Klef4qVaqE5s2bY9WqVSavw6lTp/Djjz+iW7duRd5Wu3bt8NNPP+Ho0aPG1yUvkZw5c6axq3Uec5/35ORkiy6S3bp1w7///mvSfTs9Pb3AW5H3e9jP4v1CQkLg6OiIBQsWmGzXGtM4fP/99wBgvEVr7vUTQmDevHkm6/n5+aF9+/aIjo7O971nrmZAkiQsW7YML7zwAgYMGICtW7eaPO/s7IyWLVviq6++wtWrV01qiDIyMjB//nzUqVMHlSpVMq6j0WggSZLJ63r58mVs2bKlyMffrVs3HDlyBEePHjUuS0xMLLAm2BLr1q3DF198geDgYOP3iDXk/ZN073UhLS0Nq1atMil3586dfOcir5a5oCEkNBoNOnfujO+++86k+358fLxx4OSiNp+wBGuIClFYe4k8HTp0wNChQzFjxgycPHkSnTt3hqOjIy5cuIBNmzZh3rx5eOGFF9CmTRtUqFABAwYMwKhRoyBJElavXm316ujff/8d/fr1Q9euXdGuXTtUrFgR169fx6pVq/Dvv/9i7ty5xi+bqVOnYufOnWjXrh1ef/115OTkYMGCBWjcuHG+tkqvvvoqZs6ciVdffRWPP/44Dhw4YKy9uNczzzyD1atXw8vLC40aNUJMTAz27NmTb/yjgnh6emLJkiV45ZVX8Nhjj6Fv377w8/PD1atXsX37djzxxBMmSUNBDh48iMzMzHzLH3300Xy3bIrKz88PEyZMwNSpU9GlSxf06NED586dw+LFi9GyZUuTRvDF4ezsjJ07d2LAgAEICgrCDz/8gO3bt2PixInGW2FFfb8BhiRmyZIlmD59OurWrQt/f39j26y8L/pz587hww8/NMbQvn17/PDDD3BycjIZ6VuWZXzxxRfo2rUrGjdujIiICFSpUgXXr1/Hvn374OnpabyozZw5E/v27UNQUBAGDx6MRo0a4fbt24iNjcWePXtw+/bth3qdLPHJJ5+ga9euCA4OxqBBg4zd7r28vCyaN6ldu3ZYu3YtJEky3kLTaDRo06YNdu3ahSeffNKkvVjnzp2h1Wrx7LPPYujQobh79y4+//xz+Pv748aNG0Xa5+DBg7Fw4UL0798fx48fR6VKlbB69eoiD+75sJ/F+/n5+eHNN9/EjBkz8Mwzz6Bbt244ceIEfvjhB5PBAB/k/PnzWLNmDQBDgnfkyBGsWrUKdevWNU4V0aBBA9SpUwdvvvkmrl+/Dk9PT3zzzTdma6Lmz5+Ptm3b4rHHHsOQIUNQq1YtXL58Gdu3bzcOoXEvWZaxZs0ahIWFoXfv3tixY4dJm8V27dph5syZ8PLyQtOmTQEYaqzr16+Pc+fO5Rv5vXv37pg9eza6dOmCfv36ISEhAYsWLULdunWL1OYTAN5++23jlBijR482druvUaNGkbcBGGoF3d3dkZ2dbRyp+pdffkGzZs2sPgp4586dUb16dQwaNAhvvfUWNBoNoqOjjd/XeVatWoXFixejZ8+eqFOnDlJTU/H555/D09Oz0H9Kpk+fjt27d6Nt27Z4/fXX4eDggM8++wxZWVlFGpOvWIrcH62cu7fbfWEK6pa4bNky0aJFC+Hi4iI8PDxE06ZNxdtvvy3+/fdfY5lffvlFtG7dWri4uIjKlSuLt99+W+zatStfF9oOHTqIxo0b59uHuW6S94uPjxczZ84UHTp0EJUqVRIODg6iQoUK4umnnxZff/11vvL79+8XLVq0EFqtVtSuXVssXbrUbHfK9PR0MWjQIOHl5SU8PDxE7969RUJCQr6uknfu3BERERHC19dXuLu7i9DQUHH27FlRo0YNk265D3q99+3bJ0JDQ4WXl5dwdnYWderUEQMHDhS//fZbocf/oG7398YKM11JixLbwoULRYMGDYSjo6MICAgQw4YNM+mCK0TB57AgAwYMEG5ubuLixYuic+fOwtXVVQQEBIjJkyebdPHPU5T3W1xcnOjevbvw8PAQAPJ1wff39xcARHx8vHHZoUOHBADRrl07s3GeOHFCPP/888LHx0c4OTmJGjVqiN69e4u9e/ealIuPjxfDhw8X1apVE46OjiIwMFB07NhRLFu2zFgm71xt2rTJZN3Chnm4V1E/s3v27BFPPPGEcHFxEZ6enuLZZ58Vf/31l0mZB3VbPn36tHGIgHtNnz5dABDvvfdevnW2bt0qHn30UeHs7Cxq1qwpPvroIxEdHZ2vG3VB3ylCCHHlyhXRo0cP4erqKnx9fcXo0aONQyw8qNv9w34W887PvfvR6/Vi6tSpolKlSsLFxUU8+eST4tSpU/m2WZD7P48ajUZUrVpVDBkyxOR9KIRhCJCQkBDh7u4ufH19xeDBg8Xvv/9u9r1x6tQp0bNnT+Ht7S2cnZ1F/fr1Tc6JufObnp4uOnToINzd3cWRI0eMy7dv3y4AiK5du5rs49VXXxUAxPLly/Md1/Lly0W9evWEk5OTaNCggVixYoXZ79GCvnOEEOKPP/4QHTp0EM7OzqJKlSpi2rRpYvny5RZ1u8/7cXZ2FlWrVhXPPPOMiI6OzjekgRAP3+1eCCGOHz8ugoKChFarFdWrVxezZ8/O1+0+NjZWhIeHi+rVqwsnJyfh7+8vnnnmmXzf5fd/P+etGxoaKtzd3YWrq6t46qmnxOHDh03KWPL+fRApNxAio7zBB/nWICIie8E2RERERGT3mBARERGR3WNCRERERHaPbYiIiIjI7rHb/T1ycnLyDY5VsWJFk+H4iYiIyLYoipJvSI969epZNLsBE6J7XLhwocCJPomIiKjs+Ouvv9CwYcMil2fVBxEREdk9JkRERERk95gQERERkd1jG6J7VKxYMd+yv/76y6J5eoiIiKh03bx5M18bYHPX9MIwIbqHud5kvr6+xok1iYiIqGywtIc4b5kRERGR3WNCRERERHaPCRERERHZPSZEREREZPeYEBEREZHdY0JEREREdo8JEREREdk91ROiRYsWoWbNmnB2dkZQUBCOHj1aaPlNmzahQYMGcHZ2RtOmTbFjxw6T5+Pj4zFw4EBUrlwZrq6u6NKlS74Z7ImIiIjupWpCtGHDBkRFRWHy5MmIjY1Fs2bNEBoaioSEBLPlDx8+jPDwcAwaNAgnTpxAWFgYwsLCcOrUKQCAEAJhYWH4559/8N133+HEiROoUaMGQkJCkJaWVpqHRkRERGWIJIQQau08KCgILVu2xMKFCwEAiqKgWrVqGDlyJMaPH5+vfJ8+fZCWloZt27YZl7Vu3RrNmzfH0qVLcf78edSvXx+nTp1C48aNjdsMDAzEhx9+iFdffbXQeBITE+Hv72+yLCEhgSNVExER2TBrXL9VqyHKzs7G8ePHERIS8l8wsoyQkBDExMSYXScmJsakPACEhoYay2dlZQEAnJ2dTbbp5OSEQ4cOFRhLVlYWUlJSkJqaWuzjISIiorJLtYTo5s2b0Ov1CAgIMFkeEBCAuLg4s+vExcUVWr5BgwaoXr06JkyYgDt37iA7OxsfffQRrl27hhs3bhQYy4wZM+Dl5YU6deo85FERERHld3DXNly7ckntMKgQqjeqtiZHR0ds3rwZ58+fR8WKFeHq6op9+/aha9euhU7yNmHCBCQnJ+PixYulGC0REdmDtQs/xtHoaGwcNx7zIiKYGNko1Wa79/X1hUajQXx8vMny+Ph4BAYGml0nMDDwgeVbtGiBkydPIjk5GdnZ2fDz80NQUBAef/zxAmNxcnKCk5OT8ZYbERGRtSScOgsgG0JkIyc9DZvGvQPJ0xmRH8+Gp7e32uFRLtVqiLRaLVq0aIG9e/calymKgr179yI4ONjsOsHBwSblAWD37t1my3t5ecHPzw8XLlzAb7/9hueee866B0BERFQEmnTDb0lyB+AERaRAn5yA6NdGYc6Iwjv7UOlRrYYIAKKiojBgwAA8/vjjaNWqFebOnYu0tDREREQAAPr3748qVapgxowZAIDRo0ejQ4cOmDVrFrp3747169fjt99+w7Jly4zb3LRpE/z8/FC9enX8+eefGD16NMLCwtC5c2dVjpGIiOyblGP47ahxRdXOrXBt91Fk625DL24DicC88IFQKjlj7Oyl6gZq51RNiPr06YPExERMmjQJcXFxaN68OXbu3GlsOH316lWTtj9t2rTBunXr8O6772LixImoV68etmzZgiZNmhjL3LhxA1FRUYiPj0elSpXQv39/vPfee6V+bERERACgCMXwWwv0HPAaMOA1rPhkCu6euIps/U3kKDeB68D88Eho6lXA8PdnqRyxfVJ1HCJbw3GIiIjI2ub0DYciUiH7BGDs4uUmzy18ZwyUf5KhUxJzl8hw1PjCu2Ud9B/7TukHW0aV6XGIiIiIyrtLF85AEYaZEtwq5784j/hgLkZ9tQJy5apwkHwBKNDpE5B45BjmvxyJ7etXlHLE9osJERERUQnZuWY5AAWAjG6vDC2w3Ng5SzF6/UrIPgHQSBUA5ECnS8DZb7dhfv9IHDu4t8B1yTqYEBEREZWQzIQ7AABZckfVGrUeWH7s4uWIXLoAGk9/yJIngCzoshJwcNEyzIvkGEYliQkRERFRCZEzDA2qZUlb5HU8vb0x5vNohE17Dw6ufpAkVwiRhpy0RGwa9y7mDIlESlJSCUVsv5gQERERlRBJJwEAZMnyy22teg0xesUKBA+NhKOTPwxjGCVDMY5hNMjK0do3JkREREQlRCiGjtzCsfgduoOf6oJRX0ajdrdO0Dr4A3CAXtyGkhiPeeEDMe+NYVaK1r4xISIiIiohisgGAOhdpIfeVs8Br2Hk2mhUaNEMjhp/ABJylJvIuXaNI15bARMiIiKiEnAr/oaxy72jj6fVthv59lSMWhcNbd3acJB8AAhobilW2769YkJERERUArasXAwgB4CEzuERVt/+yA/mQalgmHAiR6TiVvwNq+/DnjAhIiIiKgEp//sXACBJbnikSbMS2UfHgQMAOECIDKz+aGqJ7MNeMCEiIiIqAXKa4TaWBs4lto9Hg9rDQfY27C8xu8T2Yw+YEBEREZUAOdvQkFqSS/ZSK2kN2xc6fYnup7xjQkRERFQC8rrcw6Fk96Op7gUAyBF38OPXa0p2Z+UYEyIiIqISoBc6w++Su2MGABg+bQ4kyQOAgr9+/Klkd1aOMSEiIiKyspSkJCgiHQCg8XYp8f05yIZ9yHdLfFflFhMiIiIiK/t+zTIAhkbOwc8+X+L703sYfuco6ZznrJiYEBEREVlZwrkLAABJckXLdh1LfH9Nu4cCkCHEXaz6eFKJ7688YkJERERkZVJaDgBAg5K/XQYAIT36wEGqAAAQ13jfrDiYEBEREVnZf13uNaW2T0lr2JfQcRqP4mBCREREZG15QwKVXj4E4WfozpajJOHYwb2lt+NyggkRERGRlSm5Xe6VEu5yf6+I9z6EJLkCyMGhdetKb8flBBMiIiIiK9OLDMMDD22p7dPT2xsOsjsAwCFFlNp+ywsmRERERFb07aqlADIBAI3atC3VfStuht85+sxS3W95wISIiIjIii6fPAEAkCRndH7h5VLdd7XgxwBIUEQKlk0fX6r7LuuYEBEREVmRlGoYkFGGa6nvu1fkCGhyu99nXIwv9f2XZUyIiIiIrEjOMnS5l6USntW1ABoHw34l3jWzCBMiIiIia8rtcq9SPgS9j6Gvv05JxvlTv6sTRBnEhIiIiMiKFMUwSrW+9DqYmXghaiIAJwDZ2L50gTpBlEFMiIiIiKxIye1hBvdSHJXxHlVr1IKj7AkA0CRx1OqiYkJERERkJft3fAMh0gEAVR9tqlocIncKNSUnW7UYyhomRERERFZycm/elBlOeGHQKNXi8GxYAwCgF0lYPf9D1eIoS5gQERERWYmSahihWiOVfpf7e0W8NQWy5A0AuPX7BVVjKSuYEBEREVmJJlPdLvf3ctAYWnXL6ZLKkZQNTIiIiIisJSf3t0b9JCSngiGGHJGCW/E3VI7G9qmeEC1atAg1a9aEs7MzgoKCcPTo0ULLb9q0CQ0aNICzszOaNm2KHTt2mDx/9+5djBgxAlWrVoWLiwsaNWqEpUuXluQhEBERAQCEYhiESGjVn1y1U+RQAI4QIhNfzpyidjg2T9WEaMOGDYiKisLkyZMRGxuLZs2aITQ0FAkJCWbLHz58GOHh4Rg0aBBOnDiBsLAwhIWF4dSpU8YyUVFR2LlzJ9asWYMzZ85gzJgxGDFiBLZu3Vpah0VERHZKjywAgOKqTpf7ezV5rBUcZG8AgOamTt1gygBVE6LZs2dj8ODBiIiIMNbkuLq6Ijo62mz5efPmoUuXLnjrrbfQsGFDTJs2DY899hgWLlxoLHP48GEMGDAATz75JGrWrIkhQ4agWbNmD6x5IiIiehh//HoAQqQBACrUrq5yNLmcDL8UXU7h5Ui9hCg7OxvHjx9HSEjIf8HIMkJCQhATE2N2nZiYGJPyABAaGmpSvk2bNti6dSuuX78OIQT27duH8+fPo3PnzgXGkpWVhZSUFKSmpj7kURERkb3a/+3XAAQABzwfqV6X+3tpa/gAAPTiDr5f+4XK0dg21RKimzdvQq/XIyAgwGR5QEAA4uLizK4TFxf3wPILFixAo0aNULVqVWi1WnTp0gWLFi1C+/btC4xlxowZ8PLyQp06dR7iiIiIyJ7lJN0FAMiSGzy9vdUNJtewqZ9CljwBCFz8+Re1w7FpqjeqtrYFCxbgyJEj2Lp1K44fP45Zs2Zh+PDh2LNnT4HrTJgwAcnJybh48WIpRkpEROWJJj33t6TSJGYFcJCdAQBymvo932yZagMl+Pr6QqPRID4+3mR5fHw8AgMDza4TGBhYaPmMjAxMnDgR3377Lbp37w4AePTRR3Hy5El8+umn+W635XFycoKTkxOysrIe9rCIiMhOSbnNdCTZthKPHE8Ad4AcJQ0pSUk2U3tla1SrIdJqtWjRogX2Goc5BxRFwd69exEcHGx2neDgYJPyALB7925jeZ1OB51OB1k2PSyNRgNF4QR3RERUchRhuM4oNtDl/l6P93wegAZCpGHljHfVDsdmqTqUZlRUFAYMGIDHH38crVq1wty5c5GWloaIiAgAQP/+/VGlShXMmDEDADB69Gh06NABs2bNQvfu3bF+/Xr89ttvWLZsGQDA09MTHTp0wFtvvQUXFxfUqFED+/fvx5dffonZs2erdpxERFT+KSK3y72LbbVGaRf6DGJXfI0ccRO4ka52ODZL1YSoT58+SExMxKRJkxAXF4fmzZtj586dxobTV69eNantadOmDdatW4d3330XEydORL169bBlyxY0adLEWGb9+vWYMGECXnrpJdy+fRs1atTABx98gNdee63Uj4+IiOzDpQtnoOR2uXer7KdyNPlJWhnIAkQ275YURBJC2FbdnooSExPh7+9vsiwhIQF+frb35iYiItuxZPKbSD97FoAGA+cvhU9AJbVDMjH/7eHQXbkCQINWkYPRLvQZtUOyKmtcv22rXo+IiKgMyky4A8DQ5d7WkiEAGDjxA0iSGwA9fvt2s9rh2CQmRERERA9JzjDcipJtrMt9Hk9vbzjIbgAAhxSVg7FRTIiIiIgekqQzdLWXJdu9rCpuhhYyOUqmypHYJts9c0RERGWEUAzJhnBUOZBC1HnyCQASFJGCJZPfVDscm8OEiIiI6CEpIhsAoHdROZBCPPvSq9BIFQAA2VduqRyN7WFCRERE9BBuxd8wdrl39PFUOZrCyY6G0XYkTsyQDxMiIiKih7Bl5WIAOQAkdA6PUDucQul9Dff0dEoSTsUeVTka28KEiIiI6CGk/O9fAIAkueORJs1UjqZw/cdPgSQ5A9Bhd/RnaodjU5gQERERPQQ5zdDlXgMnlSN5MJ+ASnCQDLf1HO5wXOZ7MSEiIiJ6CHK2ocu9JJeNS6riktv9Xp+tciS2pWycPSIiIhuV1+Ve3dlBi86neT0AgCKSsHL2+ypHYzuYEBERET0EvdAZfjurHEgRvTJqIjSSNwAg+dRlVWOxJUyIiIiIiiklKQmKSAcAaLxteBCi+8gOhilGpAyVA7EhTIiIiIiK6fs1ywAY2uIEP/u8usFYQO9tuPzrlGRcu3JJ5WhsAxMiIiKiYko4dwEAIEluaNmuo8rRFF3310YC0ALIwtezP1Q7HJvAhIiIiKiYpLQcAIAGZaQBUa5HmjSDo+wFANDc1qscjW1gQkRERFRM/3W516gcieVEbg6n1+WoG4iNYEJERERUXHmVK2UvH4JLnQAAgF7cwTfRC1WORn1MiIiIiIpJye1yr5StO2YAgCHvzoQseQIQ+F9MrNrhqI4JERERUTHpRW6/dQ+tuoEUk4NsyOTkNJUDsQFMiIiIiIrh21VLAWQCABq1aatuMMWU42loA5Wj3EVKUpK6waiMCREREVExXD55AgAgSS7o/MLLKkdTPG1f6gfAAUKkY8X0iWqHoyomRERERMUgpRoGZJRRdkaovl/Ldh3hIHsDAKSETHWDURkTIiIiomKQswy3m2SpjMzqWgDJ0ZAKiGz7Ho+ICREREVFx5OYPZTwfglTZHQCQI+5g/45vVI5GPUyIiIiIikFRDAMa6p2EypE8nAHj34ckuQNQEPvddrXDUQ0TIiIiomJQcnuYwa1sVxF5envDQXYFAGhSVQ5GRUyIiIiILLR/xzcQIh0AUPXRpipH8/AUw10z6JUMdQNRERMiIiIiC53cuzf3kRNeGDRK1VisoVHnpwHIUEQqFr4zRu1wVMGEiIiIyEJKqqEmRSO5qhyJdXR+4WVopAoAAOVaisrRqIMJERERkYU0meWjy/29ZEfDDLUiW1E5EnUwISIiIrJUTu5vjaRqGNak+BnmY8tRkvDHrwdUjqb0MSEiIiKykFAMgxAJbdnucn+vyEkzIUkuAHKwd9UqtcMpdUyIiIiILKRHFgBAcdWoHIn1eHp7w0HyAABokstPoldUNpEQLVq0CDVr1oSzszOCgoJw9OjRQstv2rQJDRo0gLOzM5o2bYodO3aYPC9JktmfTz75pCQPg4iI7MAfvx6AEGkAgAq1q6scjXUpboZESK/PUjmS0qd6QrRhwwZERUVh8uTJiI2NRbNmzRAaGoqEhASz5Q8fPozw8HAMGjQIJ06cQFhYGMLCwnDq1CljmRs3bpj8REdHQ5Ik9OrVq7QOi4iIyqn9334NQABwwPORZb/L/b0CH2sCAFBEMqI/nqxyNKVLEkKoWi8WFBSEli1bYuHChQAARVFQrVo1jBw5EuPHj89Xvk+fPkhLS8O2bduMy1q3bo3mzZtj6dKlZvcRFhaG1NRU7DWOG2FeYmIi/P39TZYlJCTAz8/P0sMiIqJyas7QSChJCZAlb4xdv0btcKxubt9XoBd34ODqh9ErVqgdTpFY4/qtag1RdnY2jh8/jpCQEOMyWZYREhKCmJgYs+vExMSYlAeA0NDQAsvHx8dj+/btGDRoUIFxZGVlISUlBampdjxmORHZnaXvv4154QMxLyJC7VDKFE3uYM4ayVHdQEqI7GA4LilT5UBKmaoJ0c2bN6HX6xEQEGCyPCAgAHFxcWbXiYuLs6j8qlWr4OHhgeeff77AOGbMmAEvLy/UqVPHwiMgIiq7ss4kIEe5iZz0RMzvz6SoqKTcLveSXH663N9LX8HQUFynpODShTMqR1N6VG9DVNKio6Px0ksvwdnZucAyEyZMQHJyMi5evFiKkRERqWfOsEHIUW4a/9ZlJWL+K5EqRlR2KMIwcKFSjrrc3ytsVBQALYAsbJk/W+1wSo2qCZGvry80Gg3i4+NNlsfHxyMwMNDsOoGBgUUuf/DgQZw7dw6vvvpqoXE4OTnB09MTHh4eFh4BEVHZc+3KJeCOoReRVuMPraOh7YUuOwELmBQ9kCJyu9y7lM86hVr1GsJR9gIAaO7oVY6m9Kh6NrVaLVq0aGHS2FlRFOzduxfBwcFm1wkODs7XOHr37t1myy9fvhwtWrRAs2bNrBs4EVEZ9s2U96GIJABaBHRoipFrouGoNSRF2dkJmP9yJFKSktQM0WZdunAGSm6Xe7fK5bfDjci9qaLk6NQNpBSpnt5GRUXh888/x6pVq3DmzBkMGzYMaWlpiMht5Ne/f39MmDDBWH706NHYuXMnZs2ahbNnz2LKlCn47bffMGLECJPtpqSkYNOmTQ+sHSIisid7tm6APuMuAMDRyRu9h44FAIxaHQ1Hp9yaIl0CVo2IYlJkxs41ywEoADToNXi02uGUGKdahmRPL5IMNYp2QPWEqE+fPvj0008xadIkNG/eHCdPnsTOnTuNDaevXr2KGzduGMu3adMG69atw7Jly9CsWTN8/fXX2LJlC5o0aWKy3fXr10MIgfDw8FI9HiIiW/bXxl0QIgOS5IEnhw80eW7Ul/8lRdm6BKwczqTofpkJdwAAsuQGn4BKKkdTckLD8xrZC+z77itVYyktqo9DZEs4DhERlWfLPpiA1D/OAMiBxtMPYz43P8bM/AER0GUmAjC0MRqweDY8vb1LL1AbNm9gBHIyEuEg+2L0VyvVDqdEze77AoTIhEPVahg9a4na4RSqzI9DREREpSfzVByAHGikioj8ZE6B5UatWgEHl9yaIn0CVr7OmqI8ks7Q1V6Wyv/lU4ITAEBJy1Y5ktJR/s8oERFh7pih0CmGWh8Eujywxmf0ymg4uOa2KcpNim7F3yh0HXsgFMNNFVE+x2Q0IcMBACBl28eNJCZERETlXEpSEhBnGF7ZUfbDmLmfFWm90Sui4eBmuOWg0ydg7dh3VE+K5r35Ohb0i8ScYQXPPlCSFGGoLdG7lM9BGe8l5daCyTkqB1JKmBAREZVz0W+NhV7cBuAA5ybmx3gryOjoFdC4+wGQDEnRmHdU6XW09P23MT88Ejn/u4psfQLEnZRSj+NW/A1jl3tHHzsYty53JG7JToYiYkJUio4d3IsFL0Xi+7VfqB0KEdmJP349ACXVMCmVo2NFDHlnhsXbGLN8BTQevgAk6JQEbB4/rdSmdFi78GMs6BeJtNNnoVMScpfKECIDm2ZOL5UY8mxZuRhADgAJncPtYKoTTe7tQUXlOEoJE6JSFLNkLbJzEnDh+z2YM/Y1tcMhIjvw86KVECIVkuSCRr1Di72dMV+sgMbTD4AMnZKA7yd9jPOnfrdeoPf58es1WPByJOIOxiBbnwBAgYPkA7lSZThqfAEAmqTSbduS8r9/AQCS5I5HmpT/AX+V3HZSQthHFRETolKk+GkgSW4Q4i6Uf//FvIEcDZaISs76xbOgy0oCAGhc3BHSo89DbW/M59GQvXxgSIoSsfODuVZPiv749QDm94/An5u2IFuXAEAHjeQNuWIARq9fhbFzlyGnouHSpVPu4OCubVbdf2HkNENViSa391V5J7SG11mBfTQiYkJUisbM+xw1OreDg+wLQEFORgJWDhuLH79eo3ZoRFQOJR46DSAbsuSNXlMmWWWbY5etgOzti3uTolOxRx96u9euXMK8yAjsmbMIuqxEAJmQJQ9oPP0RuXQhxi5Zbiz76vRPIUnuAHQ4vumbh953UcnZuW1qZPu4dMpuud3uRabKkZQO+zirNqRX5AhELJkLR6e8qudEnP56O+a9MUzt0IioHJk/fgSy9bnd7Cs4oWqNWlbb9tjPoiFX8AWggU5JxN5PFuOPXw8Ua1spSUmYOyQSm8a9g5y0RAiRBklyhYOrH8KmTcKYz6PzDRHg6e0NBwdXAICUVnq3zfK63Of2Ri/3AmrXyX2UVaK3R22FRQmRTqdDZGQkLl2yj3lNSoqntzdGfbkCsn8AJMkVikhFzrV/MS8igrfQiMgqxJW7AAQcZF+T2hVrGbs0GnJFQ1KUo9zEvjnRiNm306JtzBkxCNGvjYQ+OQGKSAHgBEcnfwQPjcToFStQq17DgleuYkiIcpTbWL94VvEPxAJ6YZjoVO9cKrtTXade/ZCXJhzc/rW6wZQCixIiR0dHfPNN6VVPlndjF3yOqk8F595C0yMnPRErh0Vhz9YNaodGRGXYnGGDkKPcBCBDruFZYvsZu2Q5ZB9fAA7IETdx9LOvitSmZ+7YoZjXdyCUxHjoxR0ADtA6+qN2t04Y9WU0gp/q8sBtjPpoIWTJG4CC+KOnH/ZQHiglKQmKSAcAaLxdSnx/tsAnoBIkyZD9pcQlPKB02WfxLbOwsDBs2bKlBEKxT72HjsXLc2fAUeuPvC6tf677DvPefF3t0IioDLp25RJwJwsAoNX4YuTM+SW6v7GLl0P2y0uKbiF25Sbs32H+H+eF74zB/PAI6P+9jhxhSNgcNf6o+HhzjFwTjZ4DLOt9q9FqAQAis+Qb/X6/ZhkAw6CMwc8+X+L7sxUSDK8xMnTqBlIKLL4TWq9ePbz//vv45Zdf0KJFC7i5uZk8P2rUKKsFZy98Aiph1OpozBkxCOJmKhSRAuV/6ZgXGYHR0eYnXyQiMuebKe9DEUkAtAjo0LRU9jl24ReYPeJViMRbyFFu4cSX3yErPQOdX3gZABD98WTcPfk/6PQ3ARh6ajnKftDU9cbwaQXPqfYg7k2q4c7xBOjFbSyZ/CaGTf3UGodjVsK5CwAASXJDy3YdS2w/tkaWHKAIQM4q/9N3WJwQLV++HN7e3jh+/DiOHz9u8pwkSUyIHsLYhcuxev6HuH34PHLETeSkJWJ+v0j834Dn0S70GbXDK1Pmvz0cjdq2f+huxkRlyZ6tG6DPuAsAcHTyRu+hY0tt31ELv8CcUYOhxN+CXtzGX1//iMtnTyHrTAKyc24DuV23HWRfiErOGDV76UPvM/LtqZjXdwByxC3o/rn90NsrjJRmiF8j2UkDolx503dIOeV/qhKLEyI2qC5Zr4yaiFt9bmDdm+8gOzsROn0CflvxFU7s24lRMxeqHZ7N+37tF7i07TB0SgL+uPot/jp0AKM+XqR2WESl4q+NuyBEBiTJA08OH1jq+x87/3PMGTMEyo2b0IvbSP3zvyRFI1WE8NVi9EIrj9TvKgNpgF6XiZSkpAdOWltccrYEPQBJ0pTI9m2VlJsH2cP0HQ/V7V4IASHKfzVaafMJqISRq6Mh+fhDklygiGToLl3D3EGRaodm0+a8PggXvt9tHN5fiLvQXbmh2iSQRKVp2QcToNMZEhDZwxmPBrVXJY6xc5dBruwP5A5eKEtekL38Ebl0PsZaOxkCULdzewAaKCIFy6eOt/r2jfISAvvKhyByj9cervXFSoi+/PJLNG3aFC4uLnBxccGjjz6K1atXWzs2uxe1eDkqtmoKB9kHQA70dxOwoF+kxV1by7uYfTuxoF8klFvxuWOYuEP29odGqgggG8rtBMyLsIN5h8iuZZ6KA5ADjVQRkZ8Uv12ONYydsxQeTetBruCPFz+ajrHL8o8lZC3d+0bAUa4IAHCIzy6RfQCAktvlXrGvO2ZQtIZESLGD6TssTohmz56NYcOGoVu3bti4cSM2btyILl264LXXXsOcOep+CMujgVGT0GvmFDg6+gMAsvUJOPLZGix4Z7TKkdmGuWOG4shnX+bOdQQ4avzxaL+eGPtZNJ4e+yocZX8AAjnpiZj/UqQqs3QTlbS5Y4ZCp+QOwhjoUmLJhyWGvDsTY5dGW3VAyILoPQz3dXRKaol9xvUiw/DAQ1si27dVwtmQJgiU/15mFidECxYswJIlS/DRRx+hR48e6NGjBz7++GMsXrwY8+eXbPdOe1W1Ri2MWhMNuWIAJMkZikhC9t9XMfdV+631OH/qdyx4ORL6G/9CESmQJBfIFfwxal20sSH1o0HtMXDJbGi1hmRSl5OAzePfL7BLMFFZlJKUBMQZLtaOsh/GzP1M3YBU0Om11wA4QYgMbJo53erb/3bVUgCG6SsatWlr9e3bMgd3Q09yRWSpHEnJszghunHjBtq0aZNveZs2bXDjxg2rBEXmjV2yHN6PNc69FaSDPtXQC624Q+aXVfPHj8CO6Z/mTvwo4Cj7oepTbTB2aXS+sp7e3hi5Ohoaj/+mSjnx5beI/nhyqcdNVBKi3xoLvbgNwAHOTQLVDkcVTR5rBa3GCwCgSbJ+W5fLJ08AACTJxTiUgL2o2fTR3EfZ5b65hsUJUd26dbFx48Z8yzds2IB69epZJSgqWOTbU/HctAnQOuTWeugT8NOcZVg06Q2VIyt5t+JvYH7/SOgu/S93dFstNB5+GLhkzgO7F4/5YgUkP38AztCLJCTFnsGcKMsGgSOyNX/8egBKqqHmwtGxIoa8M0PliNSjq2i4nOmUO0UaLdsSUqqhbZIM+xih+l4dur+AvJbkJ/fvVTeYEmZxQjR16lRMmjQJXbp0wbRp0zBt2jR06dIFU6dOxfvvv18SMdJ9atVriJFroyFXMPTk0IskZJ67hLmREeX2dtCy6eOxZvR46LISAOjhIPnC+7HGGPPFiiK3l4ha+AWcH6kGWfKEEOlQrsdjzlD23KOy6+dFKyFEKiTJBY16h6odjqpenf4pJMkdgA7HN1n3e1DOMrRRkiU7mdX1Hp7e3sbpOzLvJKkbTAmzOCHq1asXjh49Cl9fX2zZsgVbtmyBr68vjh49ip49e5ZEjFSAsUuj4fHoI8beVPq0RPy2ahXm9u2P+S9HYs6wQVb/T6m0pSQlYW5kJFL/PI8ccQuAAxyd/RGxdC4GjZtm8faGT5uD6p3bwEHyAaCDkpSI+QPsty0WlV3rF8+CLisJAKBxcbf7QUg9vb3h4GCY8FVKs/Jts9wOVnaYDwEA5NzpO6SM8t3TzKLTq9PpMHToULz33ntYs2ZNScVEFhjyzgycP/U7ds6ch5ycNAiRBr24Db0OwG3gaPQyHF+xEbKDA/SeEtqG9yszw86vXfgxbv1yBvrc3jMaqQI0dSpi5AfzHmq7vSJH4Hyr37Hzw3nQ6ROgyzS0xXp26luFz65dBpyKPYpfd3+P1PhEiAwdpGwBWQdAAMJRQO8sw8nXG51698cjTZqpHS49hMRDpwFkQ5a80WvKJLXDsQ1VXIHLQI5yG+sXz0Lf163TlEBRDKNU653K/1g85kiSAyAAqeRGNbAJkrBwtCUvLy+cPHkStWqVfFfK0paYmAh/f3+TZQkJCfDz81MpIsstmfwmsq/egpQpIUfchcidnfk/GjhI3pAdNMjxltApciiaPNZKlVgLM2dIJERKKoTIACDDUeuLZydZN2FJSUrCypFR0GUbuuw7yL5o2CvEphpNxuzbiT9/+Rlpt+4AGTpI2YCcYxg1VghACAVCKFCggxBZyJt8sigkyRUynCFLGkiyBMXRMMaKxtsV//dUF04XY8Pmjx8B3aUrAATkigEYu2S52iHZjDl9X4YikuDg4o/RK/N3tCiO2X17Q4h0yJUqY+zcZVbZZlmyoF8ksvUJ0Dr6Y+Qa67ym1maN67fFFYB5s92PHVt6c+RQ0d07uWFKUhK+/GQK9P8m5yZIhgQjR9wCdAASgV0ffYi9khckRw30FWR0HzJS1ZqDH79eg7Obf4KSO66QLHlBquSOUXOs35XY09sbo1ZHY+7gSOhTbiFHuYnTX/+Ay+dOl2rj1GtXLmHTR9PhkCSgCAVC6HMTnEzkzf90rwdXWkuQJGdI0EKWHCBBBiQJQtFDQTYUkQ5ADyHSoUc69AKG+TZzAGQAyh3g6KWlOLZiJWS4GNpNyBLgIKB3kgB3LWo2a27xzORkPeLKXQACDrIvRjMZMqHRaqFkASIz/2enOPbv+Mb4j2W1pk2sss2yRsgwfPEoakdSsjjbfTnm6e2NER/MNf6dkpSEVTMnQYlLhZSF3AQp05AgZQOIB76fNhkOkjckRxlKRQ2eGxFVareR5ox4FeLmXQiROzmlgz+CX3upxG/xjfk8GnNHD4YSnwRFJOPun39j7pihJTqey634G/hy2rvQ3BbIUQyJasF1O/J/CQ40kCSNYX4hDaBoDLfChJMMjbsLAmvVQcewvvAJqFTovr9f+znuXPkfpLQcyNkSkGOobVKELjdhMiRkemSaJkyZAJKBf65fw6wdP0KWXKCRtBBOEqRAVwwcP90mBgUsz+YMGwRFuQlAhlzDU+1wbI57k2q4czwRenEbiya9geHvz3qo7Z3cm9ezygkvDLLP65twAKAr/9N3WHzLrLBbZZIk4Z9//nnooNRSHm6ZWSIlKQkrpk+ElJgBZAM6JQXA/YNvOcJB9oIsyRAawwdDcRQQWg0cvdxQ9ZGG6Phc34e6CB47uBcxS9dCl2OoFZIkd0i+7iUy71Fhlr7/NjL+ug5FJANwgOxVAWOXrbDa9m/F38CX0yfB4ZYeOuX+25lOcNR4QTgJCEcJwlmGo4c7ajRqgg7dXyjVJCMlKQm7N6/B//78EyItG5pMCVKO4ctQEbrcEXvND9ImSc7QSB6QtIDwdcHLb79XaHJGlrl25RI2jXsPikiCVuOPkets8/aF2ub1HYAccQtarT9Grn6412jOkAgoyYnQSBUwZr19TlE1d3AE9CmJ0EgVMWb9l2qHY1ap3zITQuDnn3+Gv78/XFzsbzyG8sbT2xujP11s/PtW/A2s+fh9SIlZgE4YE6Qc5aahgB4mTVSyE4F//r6If3bsMFuDYUigci/wTho4erqhVtNmaNelp/ECPzfqNYh/U3OTEMPUG3V6tEP3vqXf8+u1SR9j+/oV+HvLfuSIm1CSEzG/fyQGzp9d7IQkJSkJ0ZPfguamHjq9odH7fy+hFo6yN/ReQKchr9lMWy5Pb2/0ihxRaJk9Wzfgr0MHoE/JgOYuIHL0yBFJuTWOmYaapGvAylEj4Ch7Ao6A3scJL44ZVypTOZSU86d+x/6tG3E3IRFIz4Gsg6HRupKXMOohSxrAUYLeVUKFR2rg+chRVktov5nyPhSRBECLgA5NrbLNcslVBtKAHF0mUpKSHur112RKUGCfXe7zCBcNkAIoFrRRLIssqiFSFAXOzs44ffp0uRyE0d5qiB7k2pVL+HruTMi3syHlSPc04tXf04i3OPPb/HcLSBEpABTD356eGLtM/f94L104g+8nfwJdbjsmrcYfoRNHF7ltVUpSEpZPHQeHBB10+nTjLUADRzjKFaD3ADoOGqjajOQl4Y9fD2Dv6lVwSBZQdHrkiGTkbwOlhYPsabglW0GDboNHqN7bLSUpCfu3f41Lf56EPiUDUrYCOVsyNFxXRG6j9ZzcqQsyi7EHZzhIbpBlDRQnAeGlRdBzPRH8VBeLtrJn6wb8se5rCJEBRyd/jPpS/c+Krdq+fgXOfrsFgB5y5aoYO2dpsbc1v18kdPoEODr6Y5SNNiguaYsmvYHMc+cAOGDwZytt8ra4Na7fFt8ya9y4MZYvX47WrVtbslqZwITIcvf3gpKzASlHQNKbS6DMNxJ2lP3g80RDvDTi7dI/gAKkJCVh5aio3IEgAQfJB9W7BhfYkDglKQnL3x8Ph/hs6PQZECL1nmcdjElQ6/A+Fl8Iy6rzp37H9mULobmjh8hRkKMkI38C7WBss5bjJaNz5JBi15SlJCXh9IkjOPvbEaTevAl9ehag0xvejzmGBEdSDL3zkFuboyC7wPdlwQwJvQyn3NpQGZANt5OFg4AmS4LIEdCLLCgiFYC5r1gJsuQBjeQEyUGC3gVwqlQRvV9/o8BbjPNfjoROlwBJ8kDI2GHlKpkuCfPDI6BTEh/61mLe7TcHNz+MjrbeLfSyZP+Ob/DbKsOxN32xr031xM2jSkL0/fff4+OPP8aSJUvQpEn5anHPhKjkHdy1Dad/PYDM28kQmToIBxmvTv/UJv/jAAzd/5Xk2wByIEuecK5f2aQn37w3X4cUlwl9TkbuxS+PIQlS3CW0eOF5dmGHoeZty+K50NzSQegU5CipyN8WyQEOkpdhWAh3CYDIl9BAGG5PCSEgoIdADgR0ECIbD9cNRgtZcoYMR0iSDEmSIDSA4ggIJwmymxMCatdBp179itwuKmbfThzZ+i3kpGzIWRIURY8ckYaCa5ocoZE8IMsOhtuMHjLqBrfGjcsXkfrHGQA50Hj6Yczn9nlhtoThs5sASXJB748+Lvat2tl9+0KIu5D8AhFVyu0abcmsPj0B6OBUt65JZx1boUpCVKFCBaSnpyMnJwdarTZfW6Lbt29bsjmbwoSIzJk7diiUG3cgRLqhZsDDE5oMICcnM/eWXx4NHOWKUNyAJj262P3IwQ9yK/4GvvxoKjQ3s3PbrKWieLek7icB0EKSHCHBIbddm2HoAUmCoTZHBhStgHB2gHNFLzR/MqTUau5SkpKwOXo+7py/Ak26Augk6EW28faxeY4AdNBIFRG5dL7N/gNhS07FHsWujz4CkFXssZr++PUAds/+BICAT1ArDIyy3wEw5/QNhyJSIfsFlnqHl6JQZRyiuXPnWrpKoRYtWoRPPvkEcXFxaNasGRYsWIBWrQquMt+0aRPee+89XL58GfXq1cNHH32Ebt26mZQ5c+YMxo0bh/379yMnJweNGjXCN998g+rVq1s1drIPY+Z8huUfvYeUE5egiCToUzLvGQtIhoNcEcJVQqOuHW2yKtlW+QRUwtjZ/7XtSElKwsoP34FIyICUZbjlBMgFJDQCikYydAfWSpCdnOBS0Qs16zdG65BnbDph8PT2NnthPX/qd+z8chmUWxnQZBrabOpFBoRIg+FWowQEutj0sdmSJo+1wj6NF7L1CdAkFa+7+P5vv4bhlqcDno+0zy73eSQ4Gn5nld/BiCxOiAYMGGC1nW/YsAFRUVFYunQpgoKCMHfuXISGhuLcuXP5Mj0AOHz4MMLDwzFjxgw888wzWLduHcLCwhAbG2u8fXfx4kW0bdsWgwYNwtSpU+Hp6YnTp0/D2dnZanGT/Rk0bpqhZ9VXO6FTbsFBrgi4yKjeoRUHKLQST29vjPp4kdphqOaRJs3wiJnj/3bVUlw+dgzQOmLM7OI3DrZHuooykAjolDs4uGubxbeuc+4YOkTIkrvdJ6KypIFeAHI57mhW5FtmGzduRFhYGLRawyRv165dQ+XKlSHLhvlh09PTsXDhQrz9dtEbxgYFBaFly5ZYuHAhAMN/RNWqVcPIkSMxfvz4fOX79OmDtLQ0bNv234SlrVu3RvPmzbF0qeGLom/fvnB0dMTq1ZaPF8FbZlQU165cKtNdx4nsRUpSEr547TUIcRcaDz+M+cKytlfz+0dCl5UAR9kPo76y73Zbtt7bzhrX7yLPdh8eHo6kpCTj340aNcLly5eNf6empmLChAlF3nF2djaOHz+OkJCQ/4KRZYSEhCAmJsbsOjExMSblASA0NNRYXlEUbN++HY888ghCQ0Ph7++PoKAgbNmypchxET0IkyGissHT2xsODq4AACnN8ttmUm7nQ0mWrBlWmSRpch+U3ztmRU+I7q9IetghvG/evAm9Xo+AgACT5QEBAYiLizO7TlxcXKHlExIScPfuXcycORNdunTBjz/+iJ49e+L555/H/v37C4wlKysLKSkpSE1NLbAMERGVQVUMCVGOchvrF1s2jYciDFd/RVu+p6woCsXB8BoIUX4zoiInRGWBohhO1HPPPYexY8eiefPmGD9+PJ555hnjLTVzZsyYAS8vL9SpU6e0QiUiolIw6qOFkCVvAAoSfj1t0bqGwTgBvUu5ulQWi+JoqCUT4sHTS5dVqp1lX19faDQaxMfHmyyPj49HYGCg2XUCAwMLLe/r6wsHBwc0atTIpEzDhg1x9erVAmOZMGECkpOTcfHixeIcChER2TBNbttXJavoA3BeunAGikgDALhXZjtSuBr6YJXn6TssSoh27dqFrVu3YuvWrVAUBXv37jX+vWvXLot2rNVq0aJFC+w1ziQM4zaDg4PNrhMcHGxSHgB2795tLK/VatGyZUucO3fOpMz58+dRo0aNAmNxcnKCp6cnPDw8LDoGIiKyfe5NqgGQoBe3sWjSG0VaZ+ea5TA0mNGg1+DRJRlemeDmWxEAIEQGUu5pT1yeWNTt/v4u90OHDjX5W5Isa3gWFRWFAQMG4PHHH0erVq0wd+5cpKWlISLCMLFn//79UaVKFcyYMQMAMHr0aHTo0AGzZs1C9+7dsX79evz2229YtmyZcZtvvfUW+vTpg/bt2+Opp57Czp078f333+Pnn3+2KDYiIiofIt+eapyCQ7l0p0jrZCYYysmSW5FHJi/PWnfqht1/ngKgYPfmNQ+cALosKnJClNc+x5r69OmDxMRETJo0CXFxcWjevDl27txpbDh99epVY7d+AGjTpg3WrVuHd999FxMnTkS9evWwZcsWkylEevbsiaVLl2LGjBkYNWoU6tevj2+++QZt27a1evxERFRGuMpAGpCjy0RKUtIDxxWSM5TcWe61pRKerXs0qD12Yx6ALFw/d1btcEqExVN3lGcch4iIqHzavn4Fzn67BYAecuUqGDvns0LLz38pErqchIeeHLY8mdO3HxSRAsknAFGLLZ8KpSSV6jhEREREZVX3vhFwlA3tYBzidQ8sL5TcbuaOJRpWmSLnTt8hZ5XPehQmREREZBf0HoZ2rjolFdeuXCq0rCIMvan0LhyUMY8kGVIGWceEiIiIqMzq9NprAJwgRAY2zZxeYLlb8TeMXe61fp6lFJ3ty+s4JenLZ5LIhIiIiOxCk8daQavxAgBokgqu5diycjGAHAASOvUZWCqxlQUid/qO8jpYdbESoqSkJHzxxReYMGECbt++DQCIjY3F9evXrRocERGRNeX4Gi57OuUODu7aZrZMyv/+BQBIkjseadKs1GKzdcKxfE/fYXFC9Mcff+CRRx7BRx99hE8//dQ44evmzZstmtyViIiotA16/1NIkjsAHY5v3Gy2jJxmuOBr4FSKkdk+RWu4Vaag6CN+lyUWJ0RRUVEYOHAgLly4AGdnZ+Pybt264cCBA1YNjoiIyJo8vb3h4GCY8FVKN1/TIWfntpWR2arkXpKLoZeZQJbKkZQMi8/2sWPH8o1QDQBVqlQpcJZ6IiIim1HFkBDlKLexfvGsfE/ndbm3bC6H8s+rsmHEbiEyH9hLryyyOCFycnJCSkpKvuXnz5/nAIZERGTzRn20ELLkDUBBwq+n8z2vF4ZxivTO+Z6ya0+G9cl9JLDvu69UjaUkWJwQ9ejRA++//z50OsMbRpIkXL16FePGjUOvXr2sHiAREZG1abSGKTmULNP2MClJSVBEuqFMBddSj8uW1arXEJJkyBJvX/mfytFYn8UJ0axZs3D37l34+/sjIyMDHTp0QN26deHh4YEPPvigJGIkIiKyKo+m1QFI0IvbWDTpDePy79csA2AYlDH4mZ7qBGfDpNyG5kp6tsqRWJ/Fd0i9vLywe/du/PLLL/j9999x9+5dPPbYYwgJCSmJ+IiIiKwu4q0pmNd3AHLELSiX7hiXJ5y7AACQJDe0bNdRrfBslgwHKACkcjh9h0UJkU6ng4uLC06ePIknnngCTzzxREnFRUREVLJcZSANyNFlIiUpCZ7e3pDSDLfQNBIbEJkjSTIgALkc9ry36JaZo6MjqlevDr1eX1LxEBERlYq6ndsDcIAiUrB86jgA93S5lzQqRmbD5NxpO8phGmBxG6J33nkHEydONI5QTUREVBZ17xsBR7kCAMAh3tBRyHihZz5knib3Vlk5HKza4jZECxcuxN9//43KlSujRo0acHNzM3k+NjbWasERERGVJL2HBCQDOiUV165cgpLb5V7hHTOzFEcAWYAQ5a+KyOKEKCwsrATCICIiKn2dXnsNuz76CEJkYNPM6VBEBgBAeGhVjsw2Ca3hxlJ5nL7D4oRo8uTJJREHERFRqWvyWCvs03ghW58AOUmBgkwAQOM2bVWOzDbJbk5QbgOKKH/Td3CiFiIisms5voZLYY6SCACQJBd0fuFlNUOyWQG16+Q+ysT5U7+rGou1WZwQ6fV6fPrpp2jVqhUCAwNRsWJFkx8iIqKyZND7n0KS3I1/y3BRMRrb1qlXPwCGnmYHt3+tbjBWZnFCNHXqVMyePRt9+vRBcnIyoqKi8Pzzz0OWZUyZMqUEQiQiIio5nt7ecHD4b5oOWeKsrgXxCagESTIkjClxiSpHY10WJ0Rr167F559/jjfeeAMODg4IDw/HF198gUmTJuHIkSMlESMREVHJqvpfQsR8qHASchucZ5Sv6TssToji4uLQtGlTAIC7uzuSk5MBAM888wy2b99u3eiIiIhKwaiZC6GRDGMS5biUv2kprCmvBk0uZ9N3WJwQVa1aFTdu3AAA1KlTBz/++CMA4NixY3BycrJudERERKXEqX4AHFz90H/ah2qHYtMkyZA6SDmSypFYl8UVgz179sTevXsRFBSEkSNH4uWXX8by5ctx9epVjB07tiRiJCIiKnHDpn6qdghlgpSbB0nlbGxGixOimTNnGh/36dMH1atXR0xMDOrVq4dnn33WqsERERGRbREaADmAEOXrltlDNx0LDg5GcHCwNWIhIiIiG6doBZAFKOVs+g6LE6Ivv/yy0Of79+9f7GCIiIjItglnGUgFRDmbvsPihGj06NEmf+t0OqSnp0Or1cLV1ZUJERERUTnm4O6G7ERAEZlqh2JVFvcyu3PnjsnP3bt3ce7cObRt2xZfffVVScRIRERENqJqw0a5j7Jx7OBeVWOxJqvMZVavXj3MnDkzX+0RERERlS8dn+sLQAMAOL5vl7rBWJHVJnd1cHDAv//+a63NERERkQ3y9PaGJDkDADJu3VE5GuuxuA3R1q1bTf4WQuDGjRtYuHAhnnjiCasFRkRERLZJhhZ6pEHKKD89zSxOiMLCwkz+liQJfn5+ePrppzFr1ixrxUVEREQ2SpI0gAAkndqRWI/Ft8wURTH50ev1iIuLw7p161CpUqViBbFo0SLUrFkTzs7OCAoKwtGjRwstv2nTJjRo0ADOzs5o2rQpduzYYfL8wIEDIUmSyU+XLl2KFRsRERGZknOn75DtOSGytg0bNiAqKgqTJ09GbGwsmjVrhtDQUCQkJJgtf/jwYYSHh2PQoEE4ceIEwsLCEBYWhlOnTpmU69KlC27cuGH8YQ84IiIi6xB52YOiahhWJQkLx96OiooqctnZs2c/sExQUBBatmyJhQsXAjDUQFWrVg0jR47E+PHj85Xv06cP0tLSsG3bNuOy1q1bo3nz5li6dCkAQw1RUlIStmzZUuRYASAxMRH+/v4myxISEuDn52fRdoiIiMqz+QMioctMgKPsh1FfrVA7HKtcvy1uQ3TixAmcOHECOp0O9evXBwCcP38eGo0Gjz32mLGcJD14Ftzs7GwcP34cEyZMMC6TZRkhISGIiYkxu05MTEy+pCw0NDRf8vPzzz/D398fFSpUwNNPP43p06fDx8fH7DazsrKQlZWF1NTUB8ZMRERk7xStADLL1/QdFidEzz77LDw8PLBq1SpUqFABgGGwxoiICLRr1w5vvPFGkbd18+ZN6PV6BAQEmCwPCAjA2bNnza4TFxdntnxcXJzx7y5duuD5559HrVq1cPHiRUycOBFdu3ZFTEwMNBpNvm3OmDEDU6dOLXLcRERE9ky4aIAUQEG22qFYjcVtiGbNmoUZM2YYkyEAqFChAqZPn24zvcz69u2LHj16oGnTpggLC8O2bdtw7Ngx/Pzzz2bLT5gwAcnJybh48WLpBkpERFQGab08AACiHE3fYXFClJKSgsTExHzLExMTLb7l5OvrC41Gg/j4eJPl8fHxCAwMNLtOYGCgReUBoHbt2vD19cXff/9t9nknJyd4enrCw8PDoviJiIjsUf2WrXMf5WDP1g2qxmItFidEPXv2REREBDZv3oxr167h2rVr+OabbzBo0CA8//zzFm1Lq9WiRYsW2Lv3v7lQFEXB3r17ERwcbHad4OBgk/IAsHv37gLLA8C1a9dw69atYg8LQERERP8J6dEHgCMA4Oyv5tv8ljUWtyFaunQp3nzzTfTr1w86nWEAAgcHBwwaNAiffPKJxQFERUVhwIABePzxx9GqVSvMnTsXaWlpiIiIAAD0798fVapUwYwZMwAAo0ePRocOHTBr1ix0794d69evx2+//YZly5YBAO7evYupU6eiV69eCAwMxMWLF/H222+jbt26CA0NtTg+IiIiyk+SnCGEDrrku2qHYhUWJ0Surq5YvHgxPvnkE2Obmzp16sDNza1YAfTp0weJiYmYNGkS4uLi0Lx5c+zcudPYcPrq1auQ5f8qstq0aYN169bh3XffxcSJE1GvXj1s2bIFTZo0AQBoNBr88ccfWLVqFZKSklC5cmV07twZ06ZNg5OTU7FiJCIiIlMyHKEHIGWVj8GILB6H6H5XrlxBWloaGjRoYJK4lEUch4iIiKho5odHQKckwtHZH6NWRasaizWu30XOYKKjo/MNtDhkyBDUrl0bTZs2RZMmTfC///2vyDsmIiKiMixvvMFyMhRRkROiZcuWmXS137lzJ1asWIEvv/wSx44dg7e3N8fyISIishNS3rB+5eOOWdHbEF24cAGPP/648e/vvvsOzz33HF566SUAwIcffmhsCE1ERETlm+IggGxAiPKRERW5higjIwOenp7Gvw8fPoz27dsb/65du7bJaNFERERUfimOhltmopxM31HkhKhGjRo4fvw4AMOUG6dPn8YTTzxhfD4uLg5eXl7Wj5CIiIhsjuRiuGdWXqbvKPItswEDBmD48OE4ffo0fvrpJzRo0AAtWrQwPn/48GFj13ciIiIq31z9fJAadwNCZCAlKQme3t5qh/RQilxD9Pbbb2Pw4MHYvHkznJ2dsWnTJpPnf/nlF4SHh1s9QCIiIrI9rTt1y32kYPfmNarGYg0PPQ5RecJxiIiIiIpuVp/nAWTDsWZNjPpooWpxlOo4RERERET3kiVnAIA+NUPlSB4eEyIiIiIqFjl3glcpq+zfbGJCRERERMUiSYY0QtYxISIiIiI7JeVO3yHpJZUjeXhMiIiIiKhYRO70HeWhe1aRxyHKo9frsXLlSuzduxcJCQlQFNMhu3/66SerBUdERES2SzjmTt+hlP3pOyxOiEaPHo2VK1eie/fuaNKkibG6jIiIiOyLopWANEBBjtqhPDSLE6L169dj48aN6Nat24MLExERUbkluTgCdwCBLLVDeWgWtyHSarWoW7duScRCREREZYhX5UoAACEyce3KJZWjeTgWJ0RvvPEG5s2bBw5wTUREZN+eDOuT+0hg33dfqRrLw7L4ltmhQ4ewb98+/PDDD2jcuDEcHR1Nnt+8ebPVgiMiIiLbVateQ0iSM4TIxO0r19QO56FYnBB5e3ujZ8+eJRELERERlTESnCCQCSW9bLcjsjghWrFiRUnEQURERGWQDAcoKPvTd3BgRiIiIio2STKMziiX8Z73FtcQAcDXX3+NjRs34urVq8jOzjZ5LjY21iqBERERURkgA1AA6NUO5OFYXEM0f/58REREICAgACdOnECrVq3g4+ODf/75B127di2JGImIiMhW5U7fgTI+WLXFCdHixYuxbNkyLFiwAFqtFm+//TZ2796NUaNGITk5uSRiJCIiIhulOBjaDglRtquILE6Irl69ijZt2gAAXFxckJqaCgB45ZVX8NVXZXsMAiIiIrKMcDKkEmV9+g6LE6LAwEDcvn0bAFC9enUcOXIEAHDp0iUO1khERGRnZDcnAIAiyna3e4sToqeffhpbt24FAERERGDs2LHo1KkT+vTpw/GJiIiI7IxvzZq5jzJx/tTvaobyUCzuZbZs2TIoiqHl1PDhw+Hj44PDhw+jR48eGDp0qNUDJCIiItvV5cVXsPLgQQACh3ZsxiNNmqkdUrFYnBDJsgxZ/q9iqW/fvujbt69VgyIiIqKywSegEiTJBUKkI/lGvNrhFFuxBmY8ePAgXn75ZQQHB+P69esAgNWrV+PQoUNWDY6IiIhsnwSt4UFGduEFbZjFCdE333yD0NBQuLi44MSJE8jKMjSiSk5Oxocffmj1AImIiMi2yZLhhpNchqfvsDghmj59OpYuXYrPP//cZKb7J554gqNUExER2SFJMqQTUo6kciTFZ3FCdO7cObRv3z7fci8vLyQlJVkjJiIiIipDpNw8SCrDYzMWaxyiv//+O9/yQ4cOoXbt2sUKYtGiRahZsyacnZ0RFBSEo0ePFlp+06ZNaNCgAZydndG0aVPs2LGjwLKvvfYaJEnC3LlzixUbERERFU7kTt9RlscjtDghGjx4MEaPHo1ff/0VkiTh33//xdq1a/Hmm29i2LBhFgewYcMGREVFYfLkyYiNjUWzZs0QGhqKhIQEs+UPHz6M8PBwDBo0CCdOnEBYWBjCwsJw6tSpfGW//fZbHDlyBJUrV7Y4LiIiIioaRWtIhBRRdic0szghGj9+PPr164eOHTvi7t27aN++PV599VUMHToUI0eOtDiA2bNnY/DgwYiIiECjRo2wdOlSuLq6Ijo62mz5efPmoUuXLnjrrbfQsGFDTJs2DY899hgWLlxoUu769esYOXIk1q5da9LWiYiIiKxLOBvSCQGdypEUn8UJkSRJeOedd3D79m2cOnUKR44cQWJiIqZNm2bxzrOzs3H8+HGEhIT8F5AsIyQkBDExMWbXiYmJMSkPAKGhoSblFUXBK6+8grfeeguNGzd+YBxZWVlISUkxzstGRERERadxdwEAKCJT5UiKr1jjEAGAVqtFo0aN0KpVK7i7uxdrGzdv3oRer0dAQIDJ8oCAAMTFxZldJy4u7oHlP/roIzg4OGDUqFFFimPGjBnw8vJCnTp1LDwCIiIiqtawSe6jbBw7uFfVWIqryCNVR0ZGFqlcQbe6Ssvx48cxb948xMbGQpKK1v1vwoQJiIqKws2bN5kUERERWajjc33xz44fAOhxfN8utGzXUe2QLFbkhGjlypWoUaMG/u///s9qrch9fX2h0WgQH2861Hd8fDwCAwPNrhMYGFho+YMHDyIhIQHVq1c3Pq/X6/HGG29g7ty5uHz5cr5tOjk5wcnJyTjIJBERERWdp7c3JMkZQqQh49YdtcMpliInRMOGDcNXX32FS5cuISIiAi+//DIqVqz4UDvXarVo0aIF9u7di7CwMACG9j979+7FiBEjzK4THByMvXv3YsyYMcZlu3fvRnBwMADglVdeMdvG6JVXXkFERMRDxUtERETmydBCjzQgo2wORlTkNkSLFi3CjRs38Pbbb+P7779HtWrV0Lt3b+zateuhaoyioqLw+eefY9WqVThz5gyGDRuGtLQ0Y/LSv39/TJgwwVh+9OjR2LlzJ2bNmoWzZ89iypQp+O2334wJlI+PD5o0aWLy4+joiMDAQNSvX7/YcRIREVHBJMkwGJFcRjuaWTTbvZOTE8LDwxEeHo4rV65g5cqVeP3115GTk4PTp08Xq3F1nz59kJiYiEmTJiEuLg7NmzfHzp07jQ2nr169Cln+L29r06YN1q1bh3fffRcTJ05EvXr1sGXLFjRp0qSgXRAREVEJy5u+wy4SonvJsgxJkiCEgF7/cNVjI0aMKPAW2c8//5xv2YsvvogXX3yxyNs3126IiIiIrEgGoAdQRsdmtKjbfVZWFr766it06tQJjzzyCP78808sXLgQV69eLXbXeyIiIioHyvj0HUWuIXr99dexfv16VKtWDZGRkfjqq6/g6+tbkrERERFRGaFoAWQCiiibjaqLnBAtXboU1atXR+3atbF//37s37/fbLnNmzdbLTgiIiIqGxQnw00nBdkqR1I8RU6I+vfvX+SBDomIiMi+OFXwQGZiHEQZnb7DooEZiYiIiMyp37I1fj9/AUAO9mzdgJAefdQOySLFnsuMiIiIKI8hATLUs5z91fwE7baMCRERERFZhSQZZr3PTr6rciSWY0JEREREViHD0fA7q+wNRsSEiIiIiKxCzpu+owx2NGNCRERERNaR1xu9DA5FxISIiIiIrCK3gqhMTt/BhIiIiIisQskdzEeIspcRMSEiIiIiq1AMbaohyuD0HUyIiIiIyCokF8M9s7I4fQcTIiIiIrIKVz8fAIAQGUhJSlI3GAsxISIiIiKr+L8OnXIfKdi9eY2qsViKCRERERFZRct2HQFoAQDXz59VNxgLMSEiIiIiq5ElZwCAPiVD5Ugsw4SIiIiIrEbKnb5DyhIqR2IZJkRERERkNbJkSC1kHRMiIiIislNS7vQdkl5SORLLMCEiIiIiqxG503eIslVBxISIiIiIrEc4GDIhoZSt6TuYEBEREZHVKE6GW2UKclSOxDJMiIiIiMh6XAzjEAlkqRyIZZgQERERkdV4VQoAAAiRiVvxN1SOpuiYEBEREZHVtO32fO4jgZ2bVqsaiyWYEBEREZHVPNKkGaTc0apvXrqsbjAWYEJEREREViXBCQCgpJeddkRMiIiIiMiqZDgAKFvTdzAhIiIiIquSJMPojHIZ6nnPhIiIiIisKy+70KsahUWYEBEREZF15U7fgTI0WDUTIiIiIrIqJW/6DlF2qohsIiFatGgRatasCWdnZwQFBeHo0aOFlt+0aRMaNGgAZ2dnNG3aFDt27DB5fsqUKWjQoAHc3NxQoUIFhISE4Ndffy3JQyAiIqJcwsmQXpSl6TtUT4g2bNiAqKgoTJ48GbGxsWjWrBlCQ0ORkJBgtvzhw4cRHh6OQYMG4cSJEwgLC0NYWBhOnTplLPPII49g4cKF+PPPP3Ho0CHUrFkTnTt3RmJiYmkdFhERkd2SXXO73Yuy0+1eEkKo2icuKCgILVu2xMKFCwEAiqKgWrVqGDlyJMaPH5+vfJ8+fZCWloZt27YZl7Vu3RrNmzfH0qVLze4jJSUFXl5e2LNnDzp27FhgLImJifD39zdZlpCQAD8/v+IcGhERkV1au/BjxB08AAB49r0P8EiTZiW6P2tcv1WtIcrOzsbx48cREhJiXCbLMkJCQhATE2N2nZiYGJPyABAaGlpg+ezsbCxbtgxeXl5o1sz8CcnKykJKSgpSU1OLeSRERESUp8uLrwAwzHp/aMdmdYMpIlUTops3b0Kv1yMgIMBkeUBAAOLi4syuExcXV6Ty27Ztg7u7O5ydnTFnzhzs3r0bvr6+Zrc5Y8YMeHl5oU6dOg9xNERERAQAPgGVjNN3JN+IVzmaolG9DVFJeeqpp3Dy5EkcPnwYXbp0Qe/evQtslzRhwgQkJyfj4sWLpRwlERFR+ZQ3fQcystUNpIhUTYh8fX2h0WgQH2+aPcbHxyMwMNDsOoGBgUUq7+bmhrp166J169ZYvnw5HBwcsHz5crPbdHJygqenJzw8PB7iaIiIiCiPLBmm75DLyPQdqiZEWq0WLVq0wN69e43LFEXB3r17ERwcbHad4OBgk/IAsHv37gLL37vdrKyy09qdiIioLJMkQ4oh5UgqR1I0DmoHEBUVhQEDBuDxxx9Hq1atMHfuXKSlpSEiIgIA0L9/f1SpUgUzZswAAIwePRodOnTArFmz0L17d6xfvx6//fYbli1bBgBIS0vDBx98gB49eqBSpUq4efMmFi1ahOvXr+PFF19U7TiJiIjsiZSbB0llZGxG1ROiPn36IDExEZMmTUJcXByaN2+OnTt3GhtOX716FbL8X0VWmzZtsG7dOrz77ruYOHEi6tWrhy1btqBJkyYAAI1Gg7Nnz2LVqlW4efMmfHx80LJlSxw8eBCNGzdW5RiJiIjsjdAAyAFUHt2nyFQfh8iWcBwiIiIi65g7KBL6uwnQSD4Ys35Vie6rzI9DREREROWTcDLcMxPQqRxJ0TAhIiIiIqvTeLoAABSRqXIkRcOEiIiIiKyuyiMNch9l49jBvYWWtQVMiIiIiMjqOj3/MvLSjOP7dqkbTBEwISIiIiKr8/T2hiQZbptl3LqjcjQPxoSIiIiISoQMreFBhu0PRsSEiIiIiEqEJGkAAHIZ6GjGhIiIiIhKRN70HUyIiIiIyH7lZhlCUTeMomBCRERERCVDk/u7DEyKwYSIiIiISoSS26ZaEWxUTURERHZKcTKkGUoZmL6DCRERERGVCK2XOwBAiAyVI3kwJkRERERUIhoEBec+ysGerRtUjeVBmBARERFRiQjp0QeAAwDg7K8x6gbzAEyIiIiIqMTkTd+RnXxX5UgKx4SIiIiISowMR8PvLNsejIgJEREREZUYOW/6jmyVA3kAJkRERERUciTJ8NvGhyJiQkREREQlRsrLNGz7jhkTIiIiIio5iqEJEYSNT2jGhIiIiIhKzH8JkW3fM2NCRERERCXHxdCoWoFtt6pmQkREREQlxsWnAgDD9B0pSUnqBlMIJkRERERUYlo8FZr7SMHe79arGkthmBARERFRiWnZriMALQDgf2dOqRtMIZgQERERUYmSJWcAgD7Fdme9Z0JEREREJUrKnb5DyhIqR1IwJkRERERUouTc0RllncqBFIIJEREREZUoKXf6DsmGhyJiQkREREQlSmhyf9vuHTMmRERERFSyhIMhE7Ll6TuYEBEREVGJUpwMt8wUkaNyJAVjQkREREQly8UwDpFAlsqBFMwmEqJFixahZs2acHZ2RlBQEI4ePVpo+U2bNqFBgwZwdnZG06ZNsWPHDuNzOp0O48aNQ9OmTeHm5obKlSujf//++Pfff0v6MIiIiMgMr0oBAAAhMnEr/obK0ZinekK0YcMGREVFYfLkyYiNjUWzZs0QGhqKhIQEs+UPHz6M8PBwDBo0CCdOnEBYWBjCwsJw6pRh9Mv09HTExsbivffeQ2xsLDZv3oxz586hR48epXlYRERElKttt+dzHwns3LRa1VgKIgmhbpvvoKAgtGzZEgsXLgQAKIqCatWqYeTIkRg/fny+8n369EFaWhq2bdtmXNa6dWs0b94cS5cuNbuPY8eOoVWrVrhy5QqqV69eYCyJiYnw9/c3WZaQkAA/P7/iHBoRERHlmtXnBQCZcKhaHaNnLbbqtq1x/Va1hig7OxvHjx9HSEiIcZksywgJCUFMTIzZdWJiYkzKA0BoaGiB5QEgOTkZkiTB29vb7PNZWVlISUlBamqq5QdBREREDyRLTgAAJd022xGpmhDdvHkTer0eAQEBJssDAgIQFxdndp24uDiLymdmZmLcuHEIDw+Hp6en2TIzZsyAl5cX6tSpU4yjICIiogeR4QAAkLJss+u96m2ISpJOp0Pv3r0hhMCSJUsKLDdhwgQkJyfj4sWLpRgdERGR/ZAkw+iMco6kciTmOai5c19fX2g0GsTHx5ssj4+PR2BgoNl1AgMDi1Q+Lxm6cuUKfvrppwJrhwDAyckJTk5OyMqyzWo8IiKiMk8GoACw0ek7VK0h0mq1aNGiBfbu3WtcpigK9u7di+DgYLPrBAcHm5QHgN27d5uUz0uGLly4gD179sDHx6dkDoCIiIiKJnf6DtjmHTN1a4gAICoqCgMGDMDjjz+OVq1aYe7cuUhLS0NERAQAoH///qhSpQpmzJgBABg9ejQ6dOiAWbNmoXv37li/fj1+++03LFu2DIAhGXrhhRcQGxuLbdu2Qa/XG9sXVaxYEVqtVp0DJSIismOKA4AsQAjbrCJSPSHq06cPEhMTMWnSJMTFxaF58+bYuXOnseH01atXIcv/VWS1adMG69atw7vvvouJEyeiXr162LJlC5o0aQIAuH79OrZu3QoAaN68ucm+9u3bhyeffLJUjouIiIj+I5wkIA1QYJvTd6g+DpEt4ThEREREJWPeG68j59pVSJIzotZ/bdVtl/lxiIiIiMg+VKxRFYBh+o7zp35XOZr8mBARERFRiXvquXAAhi73h3ZsVjcYM5gQERERUYmrWqMWJMkZAJB8I/4BpUsfEyIiIiIqFRIM03cgI1vdQMxgQkRERESlIm/6DjnL9vpzMSEiIiKiUiHlDqMj2eD0HUyIiIiIqFRIuXmQZINjMzIhIiIiolIhNIZbZbY4BCITIiIiIioViqOhikgRtjehGRMiIiIiKhXCyZAQCehUjiQ/JkRERERUKjQeLgAARWSqHEl+TIiIiIioVFSp3yD3UTaOHdyraiz3Y0JEREREpaLT8y8jL/U4vm+XusHchwkRERERlQpPb29IkuG2WeatOypHY4oJEREREZUaGVoAgMiwrcGImBARERFRqZEkDQBAtrGOZkyIiIiIqNRIkiH1kHNUDuQ+TIiIiIio9MgAoIGtDVbtoHYAREREZD+enfwWfPwqwdPbW+1QTDAhIiIiolJTq15DtUMwi7fMiIiIyO4xISIiIiK7x4SIiIiI7B4TIiIiIrJ7TIiIiIjI7jEhIiIiIrvHhIiIiIjsHhMiIiIisntMiIiIiMjuMSEiIiIiu8eEiIiIiOwe5zK7h6Io+ZbdvHlThUiIiIioqMxdq81d0wvDhOget2/fzresUaNGKkRCRERED+P27dsICAgocnneMiMiIiK7x4SIiIiI7B4TIiIiIrJ7khBCqB2ErcjJycGFCxdMllWsWBGybL28MTU1FXXq1MHFixfh4eFhte3aCh5f2Vfej5HHV/aV92Pk8VlOUZR87YDr1asHB4eiN5Vmo+p7ODg4oGHDhiW6DycnJwCAr68vPD09S3RfauDxlX3l/Rh5fGVfeT9GHl/xWNKA2hzeMiMiIiK7x4SolDk5OWHy5MnGDLm84fGVfeX9GHl8ZV95P0YenzrYhoiIiIjsHmuIiIiIyO4xISIiIiK7x4SIiIiI7B4TIiIiIrJ7TIhKwKJFi1CzZk04OzsjKCgIR48eLbT8pk2b0KBBAzg7O6Np06bYsWNHKUVqmRkzZqBly5bw8PCAv78/wsLCcO7cuULXWblyJSRJMvlxdnYupYgtM2XKlHyxNmjQoNB1ysq5y1OzZs18xyhJEoYPH262vK2fvwMHDuDZZ59F5cqVIUkStmzZYvK8EAKTJk1CpUqV4OLigpCQkHyDr5pj6We4JBV2jDqdDuPGjUPTpk3h5uaGypUro3///vj3338L3WZx3usl5UHncODAgfli7dKlywO3ayvn8EHHZ+7zKEkSPvnkkwK3aUvnryjXhczMTAwfPhw+Pj5wd3dHr169EB8fX+h2i/vZfRhMiKxsw4YNiIqKwuTJkxEbG4tmzZohNDQUCQkJZssfPnwY4eHhGDRoEE6cOIGwsDCEhYXh1KlTpRz5g+3fvx/Dhw/HkSNHsHv3buh0OnTu3BlpaWmFrufp6YkbN24Yf65cuVJKEVuucePGJrEeOnSowLJl6dzlOXbsmMnx7d69GwDw4osvFriOLZ+/tLQ0NGvWDIsWLTL7/Mcff4z58+dj6dKl+PXXX+Hm5obQ0FBkZmYWuE1LP8MlrbBjTE9PR2xsLN577z3ExsZi8+bNOHfuHHr06PHA7VryXi9JDzqHANClSxeTWL/66qtCt2lL5/BBx3fvcd24cQPR0dGQJAm9evUqdLu2cv6Kcl0YO3Ysvv/+e2zatAn79+/Hv//+i+eff77Q7Rbns/vQBFlVq1atxPDhw41/6/V6UblyZTFjxgyz5Xv37i26d+9usiwoKEgMHTq0ROO0hoSEBAFA7N+/v8AyK1asEF5eXqUX1EOYPHmyaNasWZHLl+Vzl2f06NGiTp06QlEUs8+XpfMHQHz77bfGvxVFEYGBgeKTTz4xLktKShJOTk7iq6++KnA7ln6GS9P9x2jO0aNHBQBx5cqVAstY+l4vLeaOb8CAAeK5556zaDu2eg6Lcv6ee+458fTTTxdaxlbPnxD5rwtJSUnC0dFRbNq0yVjmzJkzAoCIiYkxu43ifnYfFmuIrCg7OxvHjx9HSEiIcZksywgJCUFMTIzZdWJiYkzKA0BoaGiB5W1JcnIyAMN8b4W5e/cuatSogWrVquG5557D6dOnSyO8Yrlw4QIqV66M2rVr46WXXsLVq1cLLFuWzx1geL+uWbMGkZGRkCSpwHJl6fzd69KlS4iLizM5R15eXggKCirwHBXnM2xrkpOTIUkSvL29Cy1nyXtdbT///DP8/f1Rv359DBs2DLdu3SqwbFk+h/Hx8di+fTsGDRr0wLK2ev7uvy4cP34cOp3O5Hw0aNAA1atXL/B8FOezaw1MiKzo5s2b0Ov1+eZTCQgIQFxcnNl14uLiLCpvKxRFwZgxY/DEE0+gSZMmBZarX78+oqOj8d1332HNmjVQFAVt2rTBtWvXSjHaogkKCsLKlSuxc+dOLFmyBJcuXUK7du2QmppqtnxZPXd5tmzZgqSkJAwcOLDAMmXp/N0v7zxYco6K8xm2JZmZmRg3bhzCw8MLnSPK0ve6mrp06YIvv/wSe/fuxUcffYT9+/eja9eu0Ov1ZsuX5XO4atUqeHh4PPB2kq2eP3PXhbi4OGi12nwJ+oOui3llirqONXByVyqW4cOH49SpUw+8bx0cHIzg4GDj323atEHDhg3x2WefYdq0aSUdpkW6du1qfPzoo48iKCgINWrUwMaNG4v0H1tZs3z5cnTt2hWVK1cusExZOn/2TqfToXfv3hBCYMmSJYWWLUvv9b59+xofN23aFI8++ijq1KmDn3/+GR07dlQxMuuLjo7GSy+99MCOC7Z6/op6XbBVrCGyIl9fX2g0mnyt5+Pj4xEYGGh2ncDAQIvK24IRI0Zg27Zt2LdvH6pWrWrRuo6Ojvi///s//P333yUUnfV4e3vjkUceKTDWsnju8ly5cgV79uzBq6++atF6Zen85Z0HS85RcT7DtiAvGbpy5Qp2795t8QziD3qv25LatWvD19e3wFjL6jk8ePAgzp07Z/FnErCN81fQdSEwMBDZ2dlISkoyKf+g62JemaKuYw1MiKxIq9WiRYsW2Lt3r3GZoijYu3evyX/Z9woODjYpDwC7d+8usLyahBAYMWIEvv32W/z000+oVauWxdvQ6/X4888/UalSpRKI0Lru3r2LixcvFhhrWTp391uxYgX8/f3RvXt3i9YrS+evVq1aCAwMNDlHKSkp+PXXXws8R8X5DKstLxm6cOEC9uzZAx8fH4u38aD3ui25du0abt26VWCsZfEcAoYa2xYtWqBZs2YWr6vm+XvQdaFFixZwdHQ0OR/nzp3D1atXCzwfxfnsWkWJNde2U+vXrxdOTk5i5cqV4q+//hJDhgwR3t7eIi4uTgghxCuvvCLGjx9vLP/LL78IBwcH8emnn4ozZ86IyZMnC0dHR/Hnn3+qdQgFGjZsmPDy8hI///yzuHHjhvEnPT3dWOb+45s6darYtWuXuHjxojh+/Ljo27evcHZ2FqdPn1bjEAr1xhtviJ9//llcunRJ/PLLLyIkJET4+vqKhIQEIUTZPnf30uv1onr16mLcuHH5nitr5y81NVWcOHFCnDhxQgAQs2fPFidOnDD2sJo5c6bw9vYW3333nfjjjz/Ec889J2rVqiUyMjKM23j66afFggULjH8/6DNc2go7xuzsbNGjRw9RtWpVcfLkSZPPZVZWlnEb9x/jg97rtnJ8qamp4s033xQxMTHi0qVLYs+ePeKxxx4T9erVE5mZmQUeny2dwwe9R4UQIjk5Wbi6uoolS5aY3YYtn7+iXBdee+01Ub16dfHTTz+J3377TQQHB4vg4GCT7dSvX19s3rzZ+HdRPrvWxoSoBCxYsEBUr15daLVa0apVK3HkyBHjcx06dBADBgwwKb9x40bxyCOPCK1WKxo3biy2b99eyhEXDQCzPytWrDCWuf/4xowZY3wtAgICRLdu3URsbGzpB18Effr0EZUqVRJarVZUqVJF9OnTR/z999/G58vyubvXrl27BABx7ty5fM+VtfO3b98+s+/JvGNQFEW89957IiAgQDg5OYmOHTvmO+4aNWqIyZMnmywr7DNc2go7xkuXLhX4udy3b59xG/cf44Pe66WpsONLT08XnTt3Fn5+fsLR0VHUqFFDDB48OF9iY8vn8EHvUSGE+Oyzz4SLi4tISkoyuw1bPn9FuS5kZGSI119/XVSoUEG4urqKnj17ihs3buTbzr3rFOWza21SbiBEREREdottiIiIiMjuMSEiIiIiu8eEiIiIiOweEyIiIiKye0yIiIiIyO4xISIiIiK7x4SIiIiI7B4TIiIiIrJ7TIiIqNyTJKnQnylTpqgdIhGpzEHtAIiIStqNGzeMjzds2IBJkybh3LlzxmXu7u5qhEVENoQJERGVe4GBgcbHXl5ekCTJZBkREW+ZERERkd1jQkRERER2jwkRERER2T0mRERERGT3mBARERGR3WNCRERERHaPCRERERHZPUkIIdQOgoiIiEhNrCEiIiIiu8eEiIiIiOweEyIiIiKye0yIiIiIyO4xISIiIiK7x4SIiIiI7B4TIiIiIrJ7TIiIiIjI7jEhIiIiIrvHhIiIiIjsHhMiIiIisntMiIiIiMju/T8JkxxE1edueAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Will change to showing loss instead of mean square error\n",
    "#Xout[T][Ndata][2**n]\n",
    "#backwards_gen[T][Ndata][2**n]\n",
    "mse_calc = np.zeros((21, 16))\n",
    "avg_backwards_vector = np.sum(backwards_gen, axis = 1)/Ndata\n",
    "print(avg_backwards_vector)\n",
    "\n",
    "orig_vals = np.sum((np.load('training_data.npy')), axis = 0) / 2000\n",
    "#print(orig_vals)\n",
    "for i in range(0, 21):\n",
    "    temp1 = 0\n",
    "    for j in range(0, 2**n):\n",
    "        temp1 += (np.abs(orig_vals[j] - avg_backwards_vector[i][j]))**2\n",
    "    #print(temp1)\n",
    "    mse_calc[i] = temp1/ 16\n",
    "\n",
    "#mse_calc = np.abs(np.sum(mse_calc, axis = 1)) / 16\n",
    "\n",
    "plt.plot(range(0, 21), mse_calc)\n",
    "plt.title(\"Mean Square Error between Forward and Backward Diffusion\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.xlabel(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize average output of the model\n",
    "#final_output_nxn[T][Ndata][4][4]\n",
    "#print_this_1 is average output of model\n",
    "#fig, axs = plt.subplots(1, 1, figsize = (10, 20))\n",
    "num1 = np.max(np.abs(avg_backwards_vector))\n",
    "print_this = np.abs(avg_backwards_vector) /num1\n",
    "print_this_1 = np.zeros((4,4))\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 4):\n",
    "        print_this_1[i][j] = print_this[0][4*i + j]\n",
    "\n",
    "\n",
    "for z in range(0, 1):\n",
    "    #print(final_output_nxn[z])\n",
    "    plt.imshow(\n",
    "        #final_output_nxn[0][z]\n",
    "        print_this_1, cmap = 'grey', interpolation = 'nearest')\n",
    "    plt.title('Average Output at T = %d'%0)\n",
    "\n",
    "#plt.suptitle('Backwards Diffusion At T = 0')\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
