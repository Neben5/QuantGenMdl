{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import unitary_group\n",
    "from skimage.measure import block_reduce\n",
    "from opt_einsum import contract\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from src.QDDPM_torch import DiffusionModel, QDDPM, naturalDistance\n",
    "#import src.ImageEncode as ie\n",
    "rc('text', usetex=False)\n",
    "rc('axes', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Torch will use CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Torch will use CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training(values: np.array, n_train: int, scale: float, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    n = values.size\n",
    "    noise = abs(np.random.randn(n_train,n))+0j*np.random.randn(n_train,n) \n",
    "    print(noise.shape)\n",
    "    print(values.shape)\n",
    "    states = (noise*scale) + values\n",
    "    states/=np.tile(np.linalg.norm(states, axis=1).reshape(1,n_train), (n,1)).T\n",
    "    return states\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#will not work with mixed state probabilities\n",
    "#   ***This cell is not being used right now***\n",
    "from PIL import Image\n",
    "scale = 40\n",
    "img = Image.open('images.png')\n",
    "img_greyscale = img.convert(\"L\")\n",
    "img_resized = img_greyscale.resize((scale, scale))\n",
    "\n",
    "img_array = np.array(img_resized, ndmin = 2)\n",
    "img_resized.save('dog_resized_greyscale.jpg')\n",
    "\n",
    "collapsed_array = np.zeros(img_array.size)\n",
    "\n",
    "for i in range(0, img_array.size):\n",
    "    collapsed_array[i] = img_array[int(i/scale)][i%scale]\n",
    "\n",
    "test_data = np.zeros((int(img_array.size / 4), 4)) + 1j * np.zeros((int(img_array.size / 4), 4))\n",
    "temp_normalize = max(collapsed_array) * 4\n",
    "for i in range(0, int(img_array.size / 4)):\n",
    "    test_data[i] = collapsed_array[4*i : 4*i+4] / temp_normalize + 1j * np.zeros(4)\n",
    "\n",
    "plt.imshow(img_array, cmap='grey',interpolation = 'nearest')\n",
    "\n",
    "# Find where NaN values are\n",
    "print(np.isnan(test_data).any())\n",
    "\n",
    "# Get the indices of the NaN values\n",
    "#nan_indices = np.where(img_array)\n",
    "\n",
    "#print(nan_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Square 4 by 4 to add noise and turn into training data\n",
    "temp_test = np.array([[1, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1], [1, 1, 1, 1]])\n",
    "plt.imshow(temp_test, cmap='grey',interpolation = 'nearest')\n",
    "temp_test = temp_test.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "#generate training data for diffusion\n",
    "n = 4\n",
    "T = 20\n",
    "Ndata = 2000\n",
    "\n",
    "diff_hs = torch.from_numpy(np.linspace(0.5, 4., T))\n",
    "\n",
    "model_diff = DiffusionModel(n, T, Ndata)\n",
    "\n",
    "X = torch.from_numpy(generate_training(temp_test, Ndata, 0.05))\n",
    "\n",
    "np.save('training_data', np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2800x1400 with 50 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize training data before they are forward diffused\n",
    "training_temp = np.abs(np.array(X))\n",
    "multiplier = np.max(training_temp)\n",
    "training_temp /= multiplier\n",
    "fig, axs = plt.subplots(5, 10, figsize = (28, 14))\n",
    "for index_1 in range(0, 50):\n",
    "    training_images = training_temp[index_1]\n",
    "\n",
    "    picture_training  = np.zeros((4, 4))\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            picture_training[i][j] = training_images[i*4 + j]\n",
    "\n",
    "    axs[int(index_1 / 10)][index_1 % 10].imshow(picture_training, cmap='grey',interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#diffuse training data\n",
    "Xout = np.zeros((T+1, Ndata, 2**n), dtype = np.complex64)\n",
    "Xout[0] = X.numpy()\n",
    "\n",
    "for t in range(1, T+1):\n",
    "    Xout[t] = model_diff.set_diffusionData_t(t, X, diff_hs[:t], seed = t).numpy()\n",
    "    print(t)\n",
    "\n",
    "np.save(\"states_diff\", Xout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         1.         ... 1.         0.99999988 1.        ]\n",
      " [0.95379411 0.94283223 0.95989051 ... 0.96230815 0.93162313 0.94562588]\n",
      " [0.93342679 0.94968429 0.80878709 ... 0.84761103 0.76824997 0.73139394]\n",
      " ...\n",
      " [0.03011609 0.10262213 0.04946043 ... 0.09197472 0.0871414  0.150233  ]\n",
      " [0.05082321 0.06157269 0.08411071 ... 0.16418842 0.0980667  0.00735945]\n",
      " [0.08787546 0.0059782  0.01721599 ... 0.04829054 0.00670319 0.0607175 ]]\n"
     ]
    }
   ],
   "source": [
    "#calculate fidelity of forward diffusion\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "fidelity = np.zeros((T + 1, Ndata))\n",
    "for i in range(0, T + 1):\n",
    "    for j in range(0, Ndata):\n",
    "        #different calculations for mixed state fidelity\n",
    "        fidelity[i][j] = np.abs(np.vdot(states_diff[i][j], states_diff[0][j])) ** 2\n",
    "\n",
    "\n",
    "fidelity_mean = np.mean(fidelity, axis = 1)\n",
    "print(fidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the fidelity decay in the diffusion process\n",
    "n = 2\n",
    "T = 20\n",
    "Ndata = 600\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "indices = np.random.permutation(Ndata)\n",
    "\n",
    "ax.plot(range(T+1), fidelity_mean, 'o--', markersize=8, lw=2, c='r')\n",
    "ax.plot(range(T+1), 0.25*np.ones(T+1), '--', lw=2, c='gold')\n",
    "ax.set_ylabel(r'$F_0$', fontsize=30)\n",
    "ax.set_xlabel(r'$t$', fontsize=30)\n",
    "ax.set_ylim(0,1)\n",
    "ax.tick_params(direction='in', length=10, width=3, top='on', right='on', labelsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_t(model, t, inputs_T, params_tot, Ndata, epochs):\n",
    "    '''\n",
    "    the trianing for the backward PQC at step t\n",
    "    input_tplus1: the output from step t+1, as the role of input at step t\n",
    "    Args:\n",
    "    model: the QDDPM model\n",
    "    t: the diffusion step\n",
    "    inputs_T: the input data at step t=T\n",
    "    params_tot: collection of PQC parameters before step t\n",
    "    Ndata: number of samples in dataset\n",
    "    epochs: the number of iterations\n",
    "    '''\n",
    "    input_tplus1 = model.prepareInput_t(inputs_T, params_tot, t, Ndata) # prepare input\n",
    "    states_diff = model.states_diff\n",
    "    loss_hist = [] # record of training history\n",
    "\n",
    "    # initialize parameters\n",
    "    np.random.seed()\n",
    "    params_t = torch.tensor(np.random.normal(size=2 * model.n_tot * model.L), requires_grad=True)\n",
    "    # set optimizer and learning rate decay\n",
    "    optimizer = torch.optim.Adam([params_t], lr=0.0005)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for step in range(epochs):\n",
    "        indices = np.random.choice(states_diff.shape[1], size=Ndata, replace=False)\n",
    "        true_data = states_diff[t, indices]\n",
    "\n",
    "        output_t = model.backwardOutput_t(input_tplus1, params_t)\n",
    "        loss = naturalDistance(output_t, true_data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_hist.append(loss) # record the current loss\n",
    "        \n",
    "        if step%100 == 0:\n",
    "            loss_value = loss_hist[-1]\n",
    "            print(\"Step %s, loss: %s, time elapsed: %s seconds\"%(step, loss_value, time.time() - t0))\n",
    "\n",
    "    return params_t, torch.stack(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: tensor(0.0207, grad_fn=<SubBackward0>), time elapsed: 0.08776545524597168 seconds\n",
      "Step 100, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 5.974656581878662 seconds\n",
      "Step 200, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 11.656195878982544 seconds\n",
      "Step 300, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 17.42595076560974 seconds\n",
      "Step 400, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 23.094358682632446 seconds\n",
      "Step 500, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 28.875133991241455 seconds\n",
      "Step 600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 34.505762815475464 seconds\n",
      "Step 700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 40.16259956359863 seconds\n",
      "Step 800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 45.904818058013916 seconds\n",
      "Step 900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 51.573132038116455 seconds\n",
      "Step 1000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 57.461182832717896 seconds\n",
      "Step 1100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 63.14864706993103 seconds\n",
      "Step 1200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 73.8249843120575 seconds\n",
      "Step 1300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 84.26455616950989 seconds\n",
      "Step 1400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 90.52780675888062 seconds\n",
      "Step 1500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 96.37097215652466 seconds\n",
      "Step 1600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 102.04809379577637 seconds\n",
      "Step 1700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 107.82140445709229 seconds\n",
      "Step 1800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 113.50526857376099 seconds\n",
      "Step 1900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 119.28499579429626 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 124.91267013549805 seconds\n",
      "Step 2100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 130.5266933441162 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 136.22830533981323 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 141.84679102897644 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 147.64419674873352 seconds\n",
      "Step 2500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 153.37086033821106 seconds\n",
      "Step 2600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 159.22691679000854 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 164.91325569152832 seconds\n",
      "Step 2800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 170.6092870235443 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 176.402024269104 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 182.05828619003296 seconds\n",
      "Step 3100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 187.77012276649475 seconds\n",
      "Step 3200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 193.3716254234314 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 198.9502112865448 seconds\n",
      "Step 3400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 204.69045686721802 seconds\n",
      "Step 3500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 210.27309131622314 seconds\n",
      "Step 3600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 216.0008246898651 seconds\n",
      "Step 3700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 221.58672618865967 seconds\n",
      "Step 3800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 227.2997806072235 seconds\n",
      "Step 3900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 232.8690001964569 seconds\n",
      "Step 4000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 238.43914341926575 seconds\n",
      "Step 4100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 244.17954397201538 seconds\n",
      "Step 4200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 249.7433729171753 seconds\n",
      "Step 4300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 255.45767426490784 seconds\n",
      "Step 4400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 261.04651141166687 seconds\n",
      "Step 4500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 266.7733254432678 seconds\n",
      "Step 4600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 272.38298058509827 seconds\n",
      "Step 4700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 277.95547699928284 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 283.71546053886414 seconds\n",
      "Step 4900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 289.3171441555023 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 295.043607711792 seconds\n",
      "Step 5100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 300.6451301574707 seconds\n",
      "Step 5200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 306.2850778102875 seconds\n",
      "Step 5300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 312.0064699649811 seconds\n",
      "Step 5400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 317.58821272850037 seconds\n",
      "Step 5500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 323.2793040275574 seconds\n",
      "Step 5600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 328.86541628837585 seconds\n",
      "Step 5700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 334.60022234916687 seconds\n",
      "Step 5800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 340.198618888855 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 345.77422547340393 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 351.5049104690552 seconds\n",
      "Step 6100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 357.09384059906006 seconds\n",
      "Step 6200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 362.8831944465637 seconds\n",
      "Step 6300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 368.5614335536957 seconds\n",
      "Step 6400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 374.37548446655273 seconds\n",
      "Step 6500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 380.0462725162506 seconds\n",
      "Step 6600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 385.62692856788635 seconds\n",
      "Step 6700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 391.36196851730347 seconds\n",
      "Step 6800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 396.97758412361145 seconds\n",
      "Step 6900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 402.6984622478485 seconds\n",
      "Step 7000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 408.2749478816986 seconds\n",
      "Step 7100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 413.84303545951843 seconds\n",
      "Step 7200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 419.5717387199402 seconds\n",
      "Step 7300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 425.16720056533813 seconds\n",
      "Step 7400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 430.87031865119934 seconds\n",
      "Step 7500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 436.4744963645935 seconds\n",
      "Step 7600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 442.2051031589508 seconds\n",
      "Step 7700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 447.7859218120575 seconds\n",
      "Step 7800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 453.3707320690155 seconds\n",
      "Step 7900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 459.1055762767792 seconds\n",
      "Step 8000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 464.6851496696472 seconds\n",
      "Step 8100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 470.4130356311798 seconds\n",
      "Step 8200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 475.98518466949463 seconds\n",
      "Step 8300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 481.7326707839966 seconds\n",
      "Step 8400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 487.3258566856384 seconds\n",
      "Step 8500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 492.90542697906494 seconds\n",
      "Step 8600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 498.6289699077606 seconds\n",
      "Step 8700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 504.21269726753235 seconds\n",
      "Step 8800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 509.90289330482483 seconds\n",
      "Step 8900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 515.4835481643677 seconds\n",
      "Step 9000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 521.0700507164001 seconds\n",
      "Step 9100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 526.80335688591 seconds\n",
      "Step 9200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 532.3881828784943 seconds\n",
      "Step 9300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 538.1210653781891 seconds\n",
      "Step 9400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 543.716493844986 seconds\n",
      "Step 9500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 549.4710667133331 seconds\n",
      "Step 9600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 555.0423882007599 seconds\n",
      "Step 9700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 560.6240239143372 seconds\n",
      "Step 9800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 566.3434481620789 seconds\n",
      "Step 9900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 571.939670085907 seconds\n",
      "Step 10000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 577.7028732299805 seconds\n",
      "Step 10100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 583.2835233211517 seconds\n",
      "Step 10200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 588.8662810325623 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 594.5845608711243 seconds\n",
      "Step 10400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 600.1444575786591 seconds\n",
      "Step 10500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 605.8670387268066 seconds\n",
      "Step 10600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 611.4726901054382 seconds\n",
      "Step 10700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 617.2210702896118 seconds\n",
      "Step 10800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 622.8272202014923 seconds\n",
      "Step 10900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 628.4271876811981 seconds\n",
      "Step 11000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 634.1830296516418 seconds\n",
      "Step 11100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 639.8001091480255 seconds\n",
      "Step 11200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 645.5154976844788 seconds\n",
      "Step 11300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 651.0804150104523 seconds\n",
      "Step 11400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 656.6432869434357 seconds\n",
      "Step 11500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 662.3532481193542 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 667.9591240882874 seconds\n",
      "Step 11700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 673.7064137458801 seconds\n",
      "Step 11800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 679.3329601287842 seconds\n",
      "Step 11900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 685.0793371200562 seconds\n",
      "Step 12000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 690.6391694545746 seconds\n",
      "Step 12100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 696.1969494819641 seconds\n",
      "Step 12200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 701.9360990524292 seconds\n",
      "Step 12300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 707.4968898296356 seconds\n",
      "Step 12400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 713.204799413681 seconds\n",
      "Step 12500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 718.7854459285736 seconds\n",
      "Step 12600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 724.3671233654022 seconds\n",
      "Step 12700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 730.1178212165833 seconds\n",
      "Step 12800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 735.6859042644501 seconds\n",
      "Step 12900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 741.4041991233826 seconds\n",
      "Step 13000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 746.9848494529724 seconds\n",
      "Step 13100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 752.7046675682068 seconds\n",
      "Step 13200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 758.3015847206116 seconds\n",
      "Step 13300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 763.9196977615356 seconds\n",
      "Step 13400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 769.6747081279755 seconds\n",
      "Step 13500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 775.276083946228 seconds\n",
      "Step 13600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 781.0153167247772 seconds\n",
      "Step 13700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 786.6000807285309 seconds\n",
      "Step 13800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 792.1765446662903 seconds\n",
      "Step 13900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 797.9010469913483 seconds\n",
      "Step 14000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 803.4837744235992 seconds\n",
      "Step 14100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 809.2062485218048 seconds\n",
      "Step 14200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 814.8213429450989 seconds\n",
      "Step 14300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 820.5678265094757 seconds\n",
      "Step 14400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 826.1275656223297 seconds\n",
      "Step 14500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 831.7125403881073 seconds\n",
      "Step 14600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 837.4349126815796 seconds\n",
      "Step 14700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 843.0207612514496 seconds\n",
      "Step 14800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 848.7557277679443 seconds\n",
      "Step 14900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 854.3478269577026 seconds\n",
      "Step 15000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 859.9119589328766 seconds\n",
      "Step 15100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 865.6436989307404 seconds\n",
      "Step 15200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 871.2274353504181 seconds\n",
      "Step 15300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 876.9709184169769 seconds\n",
      "Step 15400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 882.5847780704498 seconds\n",
      "Step 15500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 888.3250193595886 seconds\n",
      "Step 15600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 893.9045903682709 seconds\n",
      "Step 15700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 899.4852366447449 seconds\n",
      "Step 15800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 905.1910164356232 seconds\n",
      "Step 15900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 910.7841885089874 seconds\n",
      "Step 16000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 916.5568814277649 seconds\n",
      "Step 16100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 922.1634564399719 seconds\n",
      "Step 16200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 927.7377715110779 seconds\n",
      "Step 16300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 933.4446671009064 seconds\n",
      "Step 16400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 939.0129773616791 seconds\n",
      "Step 16500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 944.7353339195251 seconds\n",
      "Step 16600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 950.3034207820892 seconds\n",
      "Step 16700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 956.0426437854767 seconds\n",
      "Step 16800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 961.6284630298615 seconds\n",
      "Step 16900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 967.2288956642151 seconds\n",
      "Step 17000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 972.9847331047058 seconds\n",
      "Step 17100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 978.5642747879028 seconds\n",
      "Step 17200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 984.3002684116364 seconds\n",
      "Step 17300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 989.8821139335632 seconds\n",
      "Step 17400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 995.4491639137268 seconds\n",
      "Step 17500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1001.2258477210999 seconds\n",
      "Step 17600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1006.8232746124268 seconds\n",
      "Step 17700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1012.545661687851 seconds\n",
      "Step 17800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1018.1429851055145 seconds\n",
      "Step 17900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1023.7028107643127 seconds\n",
      "Step 18000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1029.4398577213287 seconds\n",
      "Step 18100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1035.0393619537354 seconds\n",
      "Step 18200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1040.7409782409668 seconds\n",
      "Step 18300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1046.339313030243 seconds\n",
      "Step 18400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1052.083645105362 seconds\n",
      "Step 18500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1057.6707282066345 seconds\n",
      "Step 18600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1063.2555565834045 seconds\n",
      "Step 18700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1069.0028676986694 seconds\n",
      "Step 18800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1074.5782589912415 seconds\n",
      "Step 18900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1080.2851340770721 seconds\n",
      "Step 19000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1085.8449602127075 seconds\n",
      "Step 19100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1091.4256162643433 seconds\n",
      "Step 19200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1097.1845240592957 seconds\n",
      "Step 19300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1102.7705788612366 seconds\n",
      "Step 19400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1108.5231728553772 seconds\n",
      "Step 19500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1114.0860579013824 seconds\n",
      "Step 19600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1119.645866394043 seconds\n",
      "Step 19700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1125.3850240707397 seconds\n",
      "Step 19800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1130.9645643234253 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1136.6808953285217 seconds\n",
      "Step 20000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1142.2438867092133 seconds\n",
      "19\n",
      "Step 0, loss: tensor(0.0104, grad_fn=<SubBackward0>), time elapsed: 0.058374881744384766 seconds\n",
      "Step 100, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 5.801647424697876 seconds\n",
      "Step 200, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 11.42495846748352 seconds\n",
      "Step 300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 17.071259021759033 seconds\n",
      "Step 400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 22.814573764801025 seconds\n",
      "Step 500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 28.391058444976807 seconds\n",
      "Step 600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 34.088496685028076 seconds\n",
      "Step 700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 39.69000840187073 seconds\n",
      "Step 800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 45.45019221305847 seconds\n",
      "Step 900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 51.03490948677063 seconds\n",
      "Step 1000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 56.62801694869995 seconds\n",
      "Step 1100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 62.36923670768738 seconds\n",
      "Step 1200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 67.95630884170532 seconds\n",
      "Step 1300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 73.66719365119934 seconds\n",
      "Step 1400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 79.24682450294495 seconds\n",
      "Step 1500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 84.78997111320496 seconds\n",
      "Step 1600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 90.51026940345764 seconds\n",
      "Step 1700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 96.08256483078003 seconds\n",
      "Step 1800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 101.79669046401978 seconds\n",
      "Step 1900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 107.39629673957825 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 113.10710000991821 seconds\n",
      "Step 2100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 118.68465971946716 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 124.2715630531311 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 129.9815800189972 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 135.55382990837097 seconds\n",
      "Step 2500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 141.2119631767273 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 146.7509047985077 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 152.41690349578857 seconds\n",
      "Step 2800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 157.93621492385864 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 163.46743059158325 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 169.148024559021 seconds\n",
      "Step 3100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 174.65367937088013 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 180.31016373634338 seconds\n",
      "Step 3300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 185.80308055877686 seconds\n",
      "Step 3400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 191.32202553749084 seconds\n",
      "Step 3500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 196.9888575077057 seconds\n",
      "Step 3600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 202.51524376869202 seconds\n",
      "Step 3700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 208.2020902633667 seconds\n",
      "Step 3800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 213.71729397773743 seconds\n",
      "Step 3900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 219.3866970539093 seconds\n",
      "Step 4000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 224.8956959247589 seconds\n",
      "Step 4100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 230.4185950756073 seconds\n",
      "Step 4200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 236.07876300811768 seconds\n",
      "Step 4300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 241.58919095993042 seconds\n",
      "Step 4400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 247.2524573802948 seconds\n",
      "Step 4500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 252.7995743751526 seconds\n",
      "Step 4600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 258.49875354766846 seconds\n",
      "Step 4700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 264.02956795692444 seconds\n",
      "Step 4800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 269.55335426330566 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 275.2741873264313 seconds\n",
      "Step 5000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 280.8207392692566 seconds\n",
      "Step 5100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 286.50381875038147 seconds\n",
      "Step 5200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 292.0773093700409 seconds\n",
      "Step 5300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 298.2805278301239 seconds\n",
      "Step 5400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 303.80914282798767 seconds\n",
      "Step 5500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 309.316472530365 seconds\n",
      "Step 5600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 314.9686539173126 seconds\n",
      "Step 5700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 320.5187096595764 seconds\n",
      "Step 5800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 326.18671011924744 seconds\n",
      "Step 5900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 331.72266006469727 seconds\n",
      "Step 6000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 337.23882031440735 seconds\n",
      "Step 6100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 342.9203288555145 seconds\n",
      "Step 6200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 348.4531433582306 seconds\n",
      "Step 6300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 354.10708355903625 seconds\n",
      "Step 6400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 359.6233592033386 seconds\n",
      "Step 6500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 365.3012685775757 seconds\n",
      "Step 6600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 370.8075361251831 seconds\n",
      "Step 6700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 376.3182158470154 seconds\n",
      "Step 6800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 382.0273394584656 seconds\n",
      "Step 6900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 387.5771391391754 seconds\n",
      "Step 7000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 393.21324944496155 seconds\n",
      "Step 7100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 398.75151658058167 seconds\n",
      "Step 7200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 404.4300117492676 seconds\n",
      "Step 7300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 409.9746880531311 seconds\n",
      "Step 7400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 415.51358819007874 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 421.6877932548523 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 427.23864555358887 seconds\n",
      "Step 7700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 432.9344983100891 seconds\n",
      "Step 7800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 438.4451639652252 seconds\n",
      "Step 7900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 443.9789729118347 seconds\n",
      "Step 8000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 449.64538526535034 seconds\n",
      "Step 8100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 455.1568660736084 seconds\n",
      "Step 8200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 460.8026900291443 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 466.3437066078186 seconds\n",
      "Step 8400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 472.04855465888977 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 477.5796058177948 seconds\n",
      "Step 8600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 483.1040585041046 seconds\n",
      "Step 8700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 488.7916717529297 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 494.3114278316498 seconds\n",
      "Step 8900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 499.97879338264465 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 505.52699065208435 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 511.0474889278412 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 517.0570693016052 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 522.5619921684265 seconds\n",
      "Step 9400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 528.223531961441 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 533.7782607078552 seconds\n",
      "Step 9600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 539.4412994384766 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 544.9314336776733 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 550.4685337543488 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 556.1628398895264 seconds\n",
      "Step 10000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 561.7453455924988 seconds\n",
      "Step 10100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 567.5853319168091 seconds\n",
      "Step 10200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 573.1457998752594 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 578.6866915225983 seconds\n",
      "Step 10400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 584.3572628498077 seconds\n",
      "Step 10500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 589.8765025138855 seconds\n",
      "Step 10600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 595.5953402519226 seconds\n",
      "Step 10700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 601.794201374054 seconds\n",
      "Step 10800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 607.7107000350952 seconds\n",
      "Step 10900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 613.3706028461456 seconds\n",
      "Step 11000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 619.0325446128845 seconds\n",
      "Step 11100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 624.8671395778656 seconds\n",
      "Step 11200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 630.4649994373322 seconds\n",
      "Step 11300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 636.1563482284546 seconds\n",
      "Step 11400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 641.7118434906006 seconds\n",
      "Step 11500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 647.2883071899414 seconds\n",
      "Step 11600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 653.010769367218 seconds\n",
      "Step 11700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 658.6101982593536 seconds\n",
      "Step 11800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 664.3516283035278 seconds\n",
      "Step 11900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 669.9404547214508 seconds\n",
      "Step 12000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 675.6589121818542 seconds\n",
      "Step 12100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 681.2310450077057 seconds\n",
      "Step 12200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 686.8409245014191 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 692.5696141719818 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 698.160760641098 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 703.9040834903717 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 709.5180451869965 seconds\n",
      "Step 12700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 715.1278913021088 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 720.8964087963104 seconds\n",
      "Step 12900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 726.4895005226135 seconds\n",
      "Step 13000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 732.2285516262054 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 737.8300592899323 seconds\n",
      "Step 13200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 743.6026475429535 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 749.2417891025543 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 754.851667881012 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 760.5833697319031 seconds\n",
      "Step 13600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 766.1713087558746 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 771.933349609375 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 777.5286271572113 seconds\n",
      "Step 13900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 783.1134610176086 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 788.8672134876251 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 794.4501094818115 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 800.189094543457 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 805.9508016109467 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 811.7497093677521 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 817.3398704528809 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 822.8912694454193 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 828.6137325763702 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 834.2079782485962 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 839.9962565898895 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 845.9523403644562 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 851.511935710907 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 857.2512607574463 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 862.8485758304596 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 868.6002421379089 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 874.185010433197 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 880.5038776397705 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 886.088648557663 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 891.6694524288177 seconds\n",
      "Step 15900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 897.4574053287506 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 903.0611670017242 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 908.9273829460144 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 914.5716750621796 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 920.1690411567688 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 925.9290316104889 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 931.5294466018677 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 937.2696888446808 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 942.8670382499695 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 948.6144218444824 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 954.1837546825409 seconds\n",
      "Step 17000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 959.7643678188324 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 965.4836752414703 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 971.0499148368835 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 976.7836513519287 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 982.3572070598602 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 987.9544520378113 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 993.6977257728577 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 999.290864944458 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1005.0133445262909 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1010.6106743812561 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1016.1998291015625 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1021.9513595104218 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1027.5320234298706 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1033.2857480049133 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1038.8893601894379 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1044.6462273597717 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1050.2570781707764 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1055.8419208526611 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1061.563381433487 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1067.130455493927 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1072.8986976146698 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1078.5075600147247 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1084.1487710475922 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1089.9087979793549 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1095.4883630275726 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1101.2108438014984 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1106.7914848327637 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1112.380485534668 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1118.1072130203247 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1123.6962678432465 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1129.4437868595123 seconds\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0089, grad_fn=<SubBackward0>), time elapsed: 0.07213568687438965 seconds\n",
      "Step 100, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 5.702676773071289 seconds\n",
      "Step 200, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 11.432360410690308 seconds\n",
      "Step 300, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 17.02261996269226 seconds\n",
      "Step 400, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 22.58644461631775 seconds\n",
      "Step 500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 28.31742525100708 seconds\n",
      "Step 600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 33.90096640586853 seconds\n",
      "Step 700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 39.603694915771484 seconds\n",
      "Step 800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 45.1989209651947 seconds\n",
      "Step 900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 50.94024586677551 seconds\n",
      "Step 1000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 56.504154205322266 seconds\n",
      "Step 1100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 62.07868552207947 seconds\n",
      "Step 1200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 67.742835521698 seconds\n",
      "Step 1300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 73.31718039512634 seconds\n",
      "Step 1400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 79.0425820350647 seconds\n",
      "Step 1500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 84.62322425842285 seconds\n",
      "Step 1600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 90.20921802520752 seconds\n",
      "Step 1700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 95.89614868164062 seconds\n",
      "Step 1800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 101.46222853660583 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 107.18639063835144 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 112.76535034179688 seconds\n",
      "Step 2100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 118.4712917804718 seconds\n",
      "Step 2200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 124.03617334365845 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 129.6032645702362 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 135.36324501037598 seconds\n",
      "Step 2500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 140.9647672176361 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 146.6705687046051 seconds\n",
      "Step 2700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 152.23658204078674 seconds\n",
      "Step 2800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 157.93215131759644 seconds\n",
      "Step 2900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 163.50429129600525 seconds\n",
      "Step 3000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 169.11848998069763 seconds\n",
      "Step 3100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 174.85750198364258 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 180.43701457977295 seconds\n",
      "Step 3300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 186.14391112327576 seconds\n",
      "Step 3400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 191.72472882270813 seconds\n",
      "Step 3500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 197.42504858970642 seconds\n",
      "Step 3600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 203.0027937889099 seconds\n",
      "Step 3700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 208.6000781059265 seconds\n",
      "Step 3800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 214.30727100372314 seconds\n",
      "Step 3900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 219.87411737442017 seconds\n",
      "Step 4000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 225.60625433921814 seconds\n",
      "Step 4100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 231.202054977417 seconds\n",
      "Step 4200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 236.78688645362854 seconds\n",
      "Step 4300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 242.50936102867126 seconds\n",
      "Step 4400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 248.09725069999695 seconds\n",
      "Step 4500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 253.78753018379211 seconds\n",
      "Step 4600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 259.3639278411865 seconds\n",
      "Step 4700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 265.0781292915344 seconds\n",
      "Step 4800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 270.6754114627838 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 276.2435607910156 seconds\n",
      "Step 5000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 281.9745125770569 seconds\n",
      "Step 5100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 287.5633451938629 seconds\n",
      "Step 5200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 293.29723501205444 seconds\n",
      "Step 5300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 298.87903237342834 seconds\n",
      "Step 5400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 304.6056230068207 seconds\n",
      "Step 5500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 310.17392015457153 seconds\n",
      "Step 5600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 315.7972083091736 seconds\n",
      "Step 5700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 321.9440426826477 seconds\n",
      "Step 5800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 327.62495732307434 seconds\n",
      "Step 5900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 333.83731031417847 seconds\n",
      "Step 6000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 339.48271799087524 seconds\n",
      "Step 6100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 345.09772968292236 seconds\n",
      "Step 6200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 350.8420581817627 seconds\n",
      "Step 6300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 356.4874620437622 seconds\n",
      "Step 6400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 362.2355601787567 seconds\n",
      "Step 6500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 368.91233801841736 seconds\n",
      "Step 6600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 374.61380410194397 seconds\n",
      "Step 6700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 380.13073110580444 seconds\n",
      "Step 6800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 385.6799032688141 seconds\n",
      "Step 6900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 391.30949878692627 seconds\n",
      "Step 7000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 396.8428235054016 seconds\n",
      "Step 7100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 402.50303506851196 seconds\n",
      "Step 7200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 408.03455114364624 seconds\n",
      "Step 7300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 413.7323684692383 seconds\n",
      "Step 7400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 419.2495596408844 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 424.77498483657837 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 430.412823677063 seconds\n",
      "Step 7700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 435.95179176330566 seconds\n",
      "Step 7800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 441.6513900756836 seconds\n",
      "Step 7900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 447.199355840683 seconds\n",
      "Step 8000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 452.73592877388 seconds\n",
      "Step 8100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 460.0047376155853 seconds\n",
      "Step 8200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 465.51171708106995 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 471.1926112174988 seconds\n",
      "Step 8400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 476.7417731285095 seconds\n",
      "Step 8500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 482.42599272727966 seconds\n",
      "Step 8600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 488.7171206474304 seconds\n",
      "Step 8700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 494.2690713405609 seconds\n",
      "Step 8800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 499.9383523464203 seconds\n",
      "Step 8900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 505.50703024864197 seconds\n",
      "Step 9000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 511.1655933856964 seconds\n",
      "Step 9100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 516.6693913936615 seconds\n",
      "Step 9200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 522.1936485767365 seconds\n",
      "Step 9300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 527.8661193847656 seconds\n",
      "Step 9400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 533.4165332317352 seconds\n",
      "Step 9500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 539.1052796840668 seconds\n",
      "Step 9600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 544.6478817462921 seconds\n",
      "Step 9700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 550.3583629131317 seconds\n",
      "Step 9800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 555.8918981552124 seconds\n",
      "Step 9900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 561.410961151123 seconds\n",
      "Step 10000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 567.1268889904022 seconds\n",
      "Step 10100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 572.6700189113617 seconds\n",
      "Step 10200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 578.3548314571381 seconds\n",
      "Step 10300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 584.6038529872894 seconds\n",
      "Step 10400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 590.1354458332062 seconds\n",
      "Step 10500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 595.8037419319153 seconds\n",
      "Step 10600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 601.3213684558868 seconds\n",
      "Step 10700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 606.9813573360443 seconds\n",
      "Step 10800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 612.5085990428925 seconds\n",
      "Step 10900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 618.170820236206 seconds\n",
      "Step 11000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 623.6957125663757 seconds\n",
      "Step 11100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 629.2658047676086 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 634.9730806350708 seconds\n",
      "Step 11300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 640.5438039302826 seconds\n",
      "Step 11400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 646.2682814598083 seconds\n",
      "Step 11500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 652.1674718856812 seconds\n",
      "Step 11600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 657.7212634086609 seconds\n",
      "Step 11700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 663.5137190818787 seconds\n",
      "Step 11800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 669.0915653705597 seconds\n",
      "Step 11900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 674.8000164031982 seconds\n",
      "Step 12000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 680.7899897098541 seconds\n",
      "Step 12100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 686.4711961746216 seconds\n",
      "Step 12200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 692.024816274643 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 697.5640442371368 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 703.277664899826 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 708.8495781421661 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 714.9754374027252 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 720.5669205188751 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 726.1101379394531 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 731.7787818908691 seconds\n",
      "Step 13000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 737.3340086936951 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 743.0262529850006 seconds\n",
      "Step 13200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 748.5793573856354 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 754.3052518367767 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 759.8499412536621 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 765.4170308113098 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 771.1133246421814 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 776.6520760059357 seconds\n",
      "Step 13800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 782.3717305660248 seconds\n",
      "Step 13900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 788.3748669624329 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 793.8731474876404 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 799.5488178730011 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 805.0909724235535 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 810.8368880748749 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 816.3797569274902 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 822.0870585441589 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 827.6078948974609 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 833.1421041488647 seconds\n",
      "Step 14800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 838.9936938285828 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 845.0170059204102 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 850.7043809890747 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 856.3578882217407 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 861.8811047077179 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 867.5721724033356 seconds\n",
      "Step 15400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 873.2960715293884 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 879.3264861106873 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 884.835547208786 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 890.447329044342 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 896.1392221450806 seconds\n",
      "Step 15900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 901.9465727806091 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 908.0802664756775 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 913.9507586956024 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 919.6411929130554 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 925.239275932312 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 930.7642331123352 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 936.8066973686218 seconds\n",
      "Step 16600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 942.3281955718994 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 948.0843377113342 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 953.6345121860504 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 959.1827626228333 seconds\n",
      "Step 17000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 965.2668035030365 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 970.8107781410217 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 977.1673505306244 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 982.7004380226135 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 988.3624258041382 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 993.8855166435242 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 999.4374523162842 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1005.2706332206726 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1010.7855477333069 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1017.4338436126709 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1022.9700982570648 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1028.497498512268 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1034.1771285533905 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1039.7039353847504 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1045.3899302482605 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1050.9129424095154 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1056.4646155834198 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1062.1542375087738 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1067.6971607208252 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1073.3832304477692 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1078.915206670761 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1084.6644222736359 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1090.225491285324 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1095.7653496265411 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1101.4763867855072 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1107.0297546386719 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1112.7675850391388 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1118.7667953968048 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1124.3330490589142 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1130.0458707809448 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1135.6064257621765 seconds\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0252, grad_fn=<SubBackward0>), time elapsed: 0.06104612350463867 seconds\n",
      "Step 100, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 5.734208106994629 seconds\n",
      "Step 200, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 11.243655681610107 seconds\n",
      "Step 300, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 16.90635848045349 seconds\n",
      "Step 400, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 22.405108213424683 seconds\n",
      "Step 500, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 27.919087886810303 seconds\n",
      "Step 600, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 33.56527519226074 seconds\n",
      "Step 700, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 39.0777690410614 seconds\n",
      "Step 800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 44.70958733558655 seconds\n",
      "Step 900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 50.22515296936035 seconds\n",
      "Step 1000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 55.87300491333008 seconds\n",
      "Step 1100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 61.37632870674133 seconds\n",
      "Step 1200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 66.89590001106262 seconds\n",
      "Step 1300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 72.55580139160156 seconds\n",
      "Step 1400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 78.10654592514038 seconds\n",
      "Step 1500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 83.76004600524902 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 89.26222348213196 seconds\n",
      "Step 1700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 94.7647716999054 seconds\n",
      "Step 1800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 100.39011716842651 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 105.9054548740387 seconds\n",
      "Step 2000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 111.5659122467041 seconds\n",
      "Step 2100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 117.08203721046448 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 122.74281072616577 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 128.22500729560852 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 133.71634936332703 seconds\n",
      "Step 2500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 139.36959624290466 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 144.87011861801147 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 150.5031237602234 seconds\n",
      "Step 2800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 156.02144527435303 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 161.67922067642212 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 167.1741099357605 seconds\n",
      "Step 3100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 172.69362664222717 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 178.32041931152344 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 183.8279025554657 seconds\n",
      "Step 3400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 189.45561265945435 seconds\n",
      "Step 3500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 194.95166087150574 seconds\n",
      "Step 3600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 200.4891641139984 seconds\n",
      "Step 3700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 206.13868761062622 seconds\n",
      "Step 3800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 211.67858266830444 seconds\n",
      "Step 3900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 217.33155989646912 seconds\n",
      "Step 4000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 222.84700298309326 seconds\n",
      "Step 4100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 228.48756742477417 seconds\n",
      "Step 4200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 234.02415776252747 seconds\n",
      "Step 4300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 239.53168082237244 seconds\n",
      "Step 4400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 245.175635099411 seconds\n",
      "Step 4500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 250.69162678718567 seconds\n",
      "Step 4600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 256.3186237812042 seconds\n",
      "Step 4700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 261.85623121261597 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 267.52527141571045 seconds\n",
      "Step 4900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 273.0266168117523 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 278.5254530906677 seconds\n",
      "Step 5100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 284.14933824539185 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.69140362739563 seconds\n",
      "Step 5300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 295.3877422809601 seconds\n",
      "Step 5400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 300.91175532341003 seconds\n",
      "Step 5500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 306.4403042793274 seconds\n",
      "Step 5600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 312.1159977912903 seconds\n",
      "Step 5700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 317.66580390930176 seconds\n",
      "Step 5800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 323.3741171360016 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 328.9233949184418 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 334.5922694206238 seconds\n",
      "Step 6100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 340.1170918941498 seconds\n",
      "Step 6200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 345.66538524627686 seconds\n",
      "Step 6300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 351.3634717464447 seconds\n",
      "Step 6400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 356.8904733657837 seconds\n",
      "Step 6500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 362.5532171726227 seconds\n",
      "Step 6600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 368.07183027267456 seconds\n",
      "Step 6700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 373.7512695789337 seconds\n",
      "Step 6800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 379.2815718650818 seconds\n",
      "Step 6900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 384.8277907371521 seconds\n",
      "Step 7000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 390.498074054718 seconds\n",
      "Step 7100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 396.02922320365906 seconds\n",
      "Step 7200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 401.69940423965454 seconds\n",
      "Step 7300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 407.2379856109619 seconds\n",
      "Step 7400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 412.76017713546753 seconds\n",
      "Step 7500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 418.4042887687683 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 423.9156403541565 seconds\n",
      "Step 7700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 429.56330704689026 seconds\n",
      "Step 7800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 435.06485056877136 seconds\n",
      "Step 7900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 440.760849237442 seconds\n",
      "Step 8000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 446.3157515525818 seconds\n",
      "Step 8100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 451.87577724456787 seconds\n",
      "Step 8200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 457.56143736839294 seconds\n",
      "Step 8300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 463.17198944091797 seconds\n",
      "Step 8400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 468.9332127571106 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 474.55150151252747 seconds\n",
      "Step 8600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 480.25520038604736 seconds\n",
      "Step 8700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 486.53687143325806 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 492.05949783325195 seconds\n",
      "Step 8900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 497.74524569511414 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 503.27477741241455 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 508.9784564971924 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 514.4897887706757 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 519.9938724040985 seconds\n",
      "Step 9400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 525.666699886322 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 531.2187542915344 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 536.911702632904 seconds\n",
      "Step 9700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 542.457923412323 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 550.9693074226379 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 556.53067445755 seconds\n",
      "Step 10000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 562.0457453727722 seconds\n",
      "Step 10100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 567.7209243774414 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 573.2366614341736 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 578.8986282348633 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 584.4017679691315 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 589.9175248146057 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 595.5809977054596 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 601.0804109573364 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 606.7408878803253 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 612.2701210975647 seconds\n",
      "Step 11000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 617.9665439128876 seconds\n",
      "Step 11100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 623.4912898540497 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 629.0605225563049 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 634.7732992172241 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 640.3244531154633 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 646.0181910991669 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 651.5625641345978 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 657.6509099006653 seconds\n",
      "Step 11800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 663.4806363582611 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 669.013991355896 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 674.6753304004669 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 680.1907546520233 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 685.8702132701874 seconds\n",
      "Step 12300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 691.4003713130951 seconds\n",
      "Step 12400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 697.212644815445 seconds\n",
      "Step 12500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 702.8921728134155 seconds\n",
      "Step 12600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 708.4204947948456 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 714.1057593822479 seconds\n",
      "Step 12800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 719.6401913166046 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 725.1491310596466 seconds\n",
      "Step 13000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 730.8135621547699 seconds\n",
      "Step 13100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 736.3296029567719 seconds\n",
      "Step 13200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 742.0219097137451 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 747.5938534736633 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 753.3035175800323 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 758.8491270542145 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 764.3891711235046 seconds\n",
      "Step 13700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 770.0951337814331 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 775.6590394973755 seconds\n",
      "Step 13900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 781.3513419628143 seconds\n",
      "Step 14000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 786.9010636806488 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 792.6151530742645 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 798.330073595047 seconds\n",
      "Step 14300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 803.9007883071899 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 809.6133894920349 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 815.7921347618103 seconds\n",
      "Step 14600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 821.302880525589 seconds\n",
      "Step 14700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 826.962860584259 seconds\n",
      "Step 14800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 832.4860274791718 seconds\n",
      "Step 14900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 838.1615381240845 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 843.7139041423798 seconds\n",
      "Step 15100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 849.4184215068817 seconds\n",
      "Step 15200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 854.9286379814148 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 860.4413890838623 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 866.0908088684082 seconds\n",
      "Step 15500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 871.6336581707001 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 877.2795188426971 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 882.786422252655 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 888.3053781986237 seconds\n",
      "Step 15900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 894.018976688385 seconds\n",
      "Step 16000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 899.546947479248 seconds\n",
      "Step 16100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 905.2550935745239 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 910.7914497852325 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 916.48561835289 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 921.9959180355072 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 927.5466601848602 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 933.2862815856934 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 938.833646774292 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 944.5360510349274 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 950.0856931209564 seconds\n",
      "Step 17000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 955.6079568862915 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 961.3405108451843 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 966.8856563568115 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 972.5947830677032 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 978.1390635967255 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 983.8699791431427 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 989.4102246761322 seconds\n",
      "Step 17700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 994.9983336925507 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1000.8181262016296 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1006.3431444168091 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1012.0386373996735 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1017.5774040222168 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1023.7795171737671 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1029.4762136936188 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1034.9983923435211 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1040.67418050766 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1046.195878982544 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1051.7172181606293 seconds\n",
      "Step 18800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1057.3927767276764 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1062.9259150028229 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1068.598335981369 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1074.1397347450256 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1079.8325889110565 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1085.3824725151062 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1090.9379818439484 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1096.6433193683624 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1102.1663262844086 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1108.1260755062103 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1113.6901490688324 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1119.2373585700989 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1124.994999885559 seconds\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 0.062408447265625 seconds\n",
      "Step 100, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 5.696375370025635 seconds\n",
      "Step 200, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 11.356139898300171 seconds\n",
      "Step 300, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 16.874448776245117 seconds\n",
      "Step 400, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 22.549752712249756 seconds\n",
      "Step 500, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 28.05535912513733 seconds\n",
      "Step 600, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 33.564220666885376 seconds\n",
      "Step 700, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 39.20470595359802 seconds\n",
      "Step 800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 44.69365906715393 seconds\n",
      "Step 900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 50.32076048851013 seconds\n",
      "Step 1000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 55.82115364074707 seconds\n",
      "Step 1100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 61.33740735054016 seconds\n",
      "Step 1200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 66.94639563560486 seconds\n",
      "Step 1300, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 72.43548440933228 seconds\n",
      "Step 1400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 78.05605030059814 seconds\n",
      "Step 1500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 83.5745849609375 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 89.18767786026001 seconds\n",
      "Step 1700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 94.65889430046082 seconds\n",
      "Step 1800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 100.16338777542114 seconds\n",
      "Step 1900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 105.81823682785034 seconds\n",
      "Step 2000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 111.33467555046082 seconds\n",
      "Step 2100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 117.00308394432068 seconds\n",
      "Step 2200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 122.5227313041687 seconds\n",
      "Step 2300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 128.17369866371155 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 133.6724145412445 seconds\n",
      "Step 2500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 139.1772608757019 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 144.83270025253296 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 150.35105776786804 seconds\n",
      "Step 2800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 155.9951992034912 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 161.5031032562256 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 167.16948318481445 seconds\n",
      "Step 3100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 172.6698796749115 seconds\n",
      "Step 3200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 178.2107813358307 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 183.8490424156189 seconds\n",
      "Step 3400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 189.34838724136353 seconds\n",
      "Step 3500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 194.99189949035645 seconds\n",
      "Step 3600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 200.49588656425476 seconds\n",
      "Step 3700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 206.0148115158081 seconds\n",
      "Step 3800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 211.67961025238037 seconds\n",
      "Step 3900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 217.21962904930115 seconds\n",
      "Step 4000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 222.84956073760986 seconds\n",
      "Step 4100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 228.3407280445099 seconds\n",
      "Step 4200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 233.98933792114258 seconds\n",
      "Step 4300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 239.49882793426514 seconds\n",
      "Step 4400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 244.9912793636322 seconds\n",
      "Step 4500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 250.60625171661377 seconds\n",
      "Step 4600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 256.1503973007202 seconds\n",
      "Step 4700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 261.8335111141205 seconds\n",
      "Step 4800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 267.3791344165802 seconds\n",
      "Step 4900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 273.0315291881561 seconds\n",
      "Step 5000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 278.54478788375854 seconds\n",
      "Step 5100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 284.051869392395 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.6912434101105 seconds\n",
      "Step 5300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 295.2274441719055 seconds\n",
      "Step 5400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 301.0412838459015 seconds\n",
      "Step 5500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 306.63375067710876 seconds\n",
      "Step 5600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 312.36538100242615 seconds\n",
      "Step 5700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 318.01928544044495 seconds\n",
      "Step 5800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 323.6002857685089 seconds\n",
      "Step 5900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 329.2669520378113 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 335.10569739341736 seconds\n",
      "Step 6100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 340.7737216949463 seconds\n",
      "Step 6200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 346.3189344406128 seconds\n",
      "Step 6300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 351.8970708847046 seconds\n",
      "Step 6400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 357.5411503314972 seconds\n",
      "Step 6500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 363.60115242004395 seconds\n",
      "Step 6600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 369.2760157585144 seconds\n",
      "Step 6700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 374.77955651283264 seconds\n",
      "Step 6800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 380.4307096004486 seconds\n",
      "Step 6900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 385.94662284851074 seconds\n",
      "Step 7000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 391.5105664730072 seconds\n",
      "Step 7100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 397.2053031921387 seconds\n",
      "Step 7200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 402.73500084877014 seconds\n",
      "Step 7300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 409.0199065208435 seconds\n",
      "Step 7400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 414.5445201396942 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 420.08415818214417 seconds\n",
      "Step 7600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 425.7440912723541 seconds\n",
      "Step 7700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 431.2938895225525 seconds\n",
      "Step 7800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 437.077285528183 seconds\n",
      "Step 7900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 442.6989121437073 seconds\n",
      "Step 8000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 448.4052336215973 seconds\n",
      "Step 8100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 454.1779203414917 seconds\n",
      "Step 8200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 459.6638250350952 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 465.3725280761719 seconds\n",
      "Step 8400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 470.89377331733704 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 476.59131383895874 seconds\n",
      "Step 8600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 482.1314516067505 seconds\n",
      "Step 8700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 488.12305974960327 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 493.63715386390686 seconds\n",
      "Step 8900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 499.23192381858826 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 504.88069701194763 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 510.6939241886139 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 516.3896825313568 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 522.0544168949127 seconds\n",
      "Step 9400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 527.6032629013062 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 533.5042760372162 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 539.0410859584808 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 544.8525738716125 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 550.3919503688812 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 556.363445520401 seconds\n",
      "Step 10000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 561.968624830246 seconds\n",
      "Step 10100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 567.5850212574005 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 573.4762668609619 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 579.0196993350983 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 585.4795236587524 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 591.0104584693909 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 596.5257363319397 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 602.2011423110962 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 607.7054314613342 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 613.398533821106 seconds\n",
      "Step 11000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 618.9451117515564 seconds\n",
      "Step 11100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 624.6660213470459 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 630.2028577327728 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 635.7669126987457 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 642.2804756164551 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 647.8170714378357 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 653.476095199585 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 659.0052664279938 seconds\n",
      "Step 11800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 664.7066004276276 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 670.512247800827 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 676.0521655082703 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 682.4996745586395 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 688.0339295864105 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 693.757485628128 seconds\n",
      "Step 12400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 699.279896736145 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 704.8117897510529 seconds\n",
      "Step 12600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 710.4937098026276 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 716.0311918258667 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 721.7476274967194 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 727.3132901191711 seconds\n",
      "Step 13000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 732.8407299518585 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 738.5507116317749 seconds\n",
      "Step 13200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 744.0761578083038 seconds\n",
      "Step 13300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 749.7764909267426 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 755.3499977588654 seconds\n",
      "Step 13500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 761.059056520462 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 766.5749368667603 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 772.1401298046112 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 777.8281714916229 seconds\n",
      "Step 13900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 783.3666870594025 seconds\n",
      "Step 14000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 789.0803029537201 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 794.6418609619141 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 800.1977670192719 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 805.9134199619293 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 812.5471160411835 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 818.2593183517456 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 823.7959759235382 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 829.5334892272949 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 835.0652041435242 seconds\n",
      "Step 14900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 840.6293125152588 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 846.3370172977448 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 852.6689028739929 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 858.3804221153259 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 863.8879086971283 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 869.3940896987915 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 875.0835852622986 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 880.6328010559082 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 886.3191637992859 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 891.8448326587677 seconds\n",
      "Step 15900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 897.5148179531097 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 903.0317509174347 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 908.5612268447876 seconds\n",
      "Step 16200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 914.202761888504 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 919.7247595787048 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 925.4048388004303 seconds\n",
      "Step 16500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 930.9378428459167 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 936.4750442504883 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 942.184654712677 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 947.7223062515259 seconds\n",
      "Step 16900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 953.4235413074493 seconds\n",
      "Step 17000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 958.9469783306122 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 964.495992898941 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 970.2228078842163 seconds\n",
      "Step 17300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 975.9410753250122 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 981.6579887866974 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 987.3075630664825 seconds\n",
      "Step 17600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 993.0174686908722 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 998.5818662643433 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1004.1122608184814 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1009.9207649230957 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1015.4350085258484 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1021.2610034942627 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1026.776380777359 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1032.660828113556 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1038.3512954711914 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1043.8595705032349 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1049.548374414444 seconds\n",
      "Step 18700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1055.0907137393951 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1060.6728491783142 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1066.396125793457 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1071.949861049652 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1077.701786994934 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1083.9018244743347 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1089.6335468292236 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1095.1713914871216 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1100.731683731079 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1106.5035297870636 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1112.1160607337952 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1117.8304777145386 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1123.6561300754547 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1129.2146844863892 seconds\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 0.05976462364196777 seconds\n",
      "Step 100, loss: tensor(0.0083, grad_fn=<SubBackward0>), time elapsed: 5.809586524963379 seconds\n",
      "Step 200, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 11.32264757156372 seconds\n",
      "Step 300, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 16.962754487991333 seconds\n",
      "Step 400, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 22.47853684425354 seconds\n",
      "Step 500, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 28.122157096862793 seconds\n",
      "Step 600, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 33.64934706687927 seconds\n",
      "Step 700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 39.155890226364136 seconds\n",
      "Step 800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 44.795947551727295 seconds\n",
      "Step 900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 50.298255443573 seconds\n",
      "Step 1000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 55.91584014892578 seconds\n",
      "Step 1100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 61.42700242996216 seconds\n",
      "Step 1200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 67.08045244216919 seconds\n",
      "Step 1300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 72.58123207092285 seconds\n",
      "Step 1400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 78.10072040557861 seconds\n",
      "Step 1500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 83.7224509716034 seconds\n",
      "Step 1600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 89.22169899940491 seconds\n",
      "Step 1700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 94.8899896144867 seconds\n",
      "Step 1800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 100.3969612121582 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 105.90123867988586 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 111.55184674263 seconds\n",
      "Step 2100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 117.06472325325012 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 122.74810338020325 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 128.26601195335388 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 133.9359667301178 seconds\n",
      "Step 2500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 139.42031359672546 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 144.92477250099182 seconds\n",
      "Step 2700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 150.55643343925476 seconds\n",
      "Step 2800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 156.08165979385376 seconds\n",
      "Step 2900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 161.72348046302795 seconds\n",
      "Step 3000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 167.2283239364624 seconds\n",
      "Step 3100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 172.8768548965454 seconds\n",
      "Step 3200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 178.37858819961548 seconds\n",
      "Step 3300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 183.90256023406982 seconds\n",
      "Step 3400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 189.52807712554932 seconds\n",
      "Step 3500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 195.03188014030457 seconds\n",
      "Step 3600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 200.6740539073944 seconds\n",
      "Step 3700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 206.1911153793335 seconds\n",
      "Step 3800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 211.84776282310486 seconds\n",
      "Step 3900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 217.3700668811798 seconds\n",
      "Step 4000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 222.87488913536072 seconds\n",
      "Step 4100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 228.51852774620056 seconds\n",
      "Step 4200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 234.01812863349915 seconds\n",
      "Step 4300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 239.66209411621094 seconds\n",
      "Step 4400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 245.19171023368835 seconds\n",
      "Step 4500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 250.71590948104858 seconds\n",
      "Step 4600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 256.37010407447815 seconds\n",
      "Step 4700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 261.9004592895508 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 267.5456736087799 seconds\n",
      "Step 4900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 273.0667414665222 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 278.7213418483734 seconds\n",
      "Step 5100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 284.2301585674286 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.74536323547363 seconds\n",
      "Step 5300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 295.3888351917267 seconds\n",
      "Step 5400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 300.8954334259033 seconds\n",
      "Step 5500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 306.5755937099457 seconds\n",
      "Step 5600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 312.1050035953522 seconds\n",
      "Step 5700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 317.7699990272522 seconds\n",
      "Step 5800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 323.25768280029297 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 328.78535199165344 seconds\n",
      "Step 6000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 334.45333766937256 seconds\n",
      "Step 6100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 339.950156211853 seconds\n",
      "Step 6200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 345.5843434333801 seconds\n",
      "Step 6300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 351.0767834186554 seconds\n",
      "Step 6400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 356.57354402542114 seconds\n",
      "Step 6500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 362.22276043891907 seconds\n",
      "Step 6600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 367.73783230781555 seconds\n",
      "Step 6700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 373.3661983013153 seconds\n",
      "Step 6800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 378.874351978302 seconds\n",
      "Step 6900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 384.508811712265 seconds\n",
      "Step 7000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 390.0481927394867 seconds\n",
      "Step 7100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 395.5883390903473 seconds\n",
      "Step 7200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 401.2607045173645 seconds\n",
      "Step 7300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 406.7698905467987 seconds\n",
      "Step 7400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 412.4196426868439 seconds\n",
      "Step 7500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 417.949515581131 seconds\n",
      "Step 7600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 423.6239261627197 seconds\n",
      "Step 7700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 429.1613030433655 seconds\n",
      "Step 7800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 434.6909170150757 seconds\n",
      "Step 7900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 440.34901785850525 seconds\n",
      "Step 8000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 445.8953266143799 seconds\n",
      "Step 8100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 451.5611057281494 seconds\n",
      "Step 8200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 457.10882568359375 seconds\n",
      "Step 8300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 462.6309506893158 seconds\n",
      "Step 8400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 468.28206610679626 seconds\n",
      "Step 8500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 473.7941725254059 seconds\n",
      "Step 8600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 479.46640038490295 seconds\n",
      "Step 8700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 485.00726199150085 seconds\n",
      "Step 8800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 490.70447301864624 seconds\n",
      "Step 8900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 496.2120637893677 seconds\n",
      "Step 9000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 501.74134635925293 seconds\n",
      "Step 9100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 507.4003572463989 seconds\n",
      "Step 9200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 512.91335105896 seconds\n",
      "Step 9300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 518.5798783302307 seconds\n",
      "Step 9400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 524.1065108776093 seconds\n",
      "Step 9500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 529.6429951190948 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 535.3118045330048 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 540.8547384738922 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 546.5418450832367 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 552.1296229362488 seconds\n",
      "Step 10000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 557.814975976944 seconds\n",
      "Step 10100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 563.3574223518372 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 568.9001142978668 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 574.6053383350372 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 580.1724507808685 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 585.8739233016968 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 591.6204581260681 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 597.1847190856934 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 603.0862953662872 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 608.6527094841003 seconds\n",
      "Step 11000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 614.3305974006653 seconds\n",
      "Step 11100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 619.8812961578369 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 625.5981063842773 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 631.4706566333771 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 636.9842014312744 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 642.6877584457397 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 648.1973164081573 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 653.8606839179993 seconds\n",
      "Step 11800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 659.3813669681549 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 664.9133498668671 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 670.6357507705688 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 676.1817166805267 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 681.8598594665527 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 687.753500699997 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 693.42937541008 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 698.944155216217 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 704.4872894287109 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 710.147319316864 seconds\n",
      "Step 12800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 715.6894958019257 seconds\n",
      "Step 12900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 721.37824177742 seconds\n",
      "Step 13000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 726.9223260879517 seconds\n",
      "Step 13100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 732.5756614208221 seconds\n",
      "Step 13200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 738.3206334114075 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 743.8849663734436 seconds\n",
      "Step 13400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 749.5849361419678 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 755.2100439071655 seconds\n",
      "Step 13600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 761.2333796024323 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 766.8138699531555 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 772.3450062274933 seconds\n",
      "Step 13900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 778.0236189365387 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 783.5857994556427 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 789.3931787014008 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 794.9230906963348 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 800.4816346168518 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 806.4294850826263 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 811.9610040187836 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 817.6270923614502 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 823.1551222801208 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 828.8125762939453 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 834.3238263130188 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 839.8569347858429 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 845.7151257991791 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 851.2468118667603 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 856.9983921051025 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 862.9393923282623 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 868.4626414775848 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 874.1329071521759 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 879.639327287674 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 885.3315443992615 seconds\n",
      "Step 15900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 890.8633298873901 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 896.5782256126404 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 902.1314287185669 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 907.681314945221 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 913.6124680042267 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 919.1493232250214 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 924.8423783779144 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 930.37637591362 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 935.9592838287354 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 942.3118922710419 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 947.9203164577484 seconds\n",
      "Step 17000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 953.6250727176666 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 959.1420073509216 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 964.6573147773743 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 970.3173906803131 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 975.8465120792389 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 981.5108513832092 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 987.0350692272186 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 992.7310254573822 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 998.3001310825348 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1003.8164455890656 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1009.5417423248291 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1015.0845959186554 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1020.7926044464111 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1026.3616268634796 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1031.934430360794 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1037.676941871643 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1043.2564463615417 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1048.97469496727 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1054.5201840400696 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1060.1435840129852 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1065.8962247371674 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1071.4344387054443 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1077.132113456726 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1083.4435346126556 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1089.176728963852 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1094.6822063922882 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1100.236656665802 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1105.9095611572266 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1111.442922115326 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1117.1323268413544 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1122.678377866745 seconds\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0250, grad_fn=<SubBackward0>), time elapsed: 0.061799049377441406 seconds\n",
      "Step 100, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 5.660330295562744 seconds\n",
      "Step 200, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 11.341418266296387 seconds\n",
      "Step 300, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 16.84400200843811 seconds\n",
      "Step 400, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 22.479936599731445 seconds\n",
      "Step 500, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 27.972734928131104 seconds\n",
      "Step 600, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 33.606866121292114 seconds\n",
      "Step 700, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 39.133134841918945 seconds\n",
      "Step 800, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 44.65788245201111 seconds\n",
      "Step 900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 50.2734580039978 seconds\n",
      "Step 1000, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 55.77788162231445 seconds\n",
      "Step 1100, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 61.40591788291931 seconds\n",
      "Step 1200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 66.91237020492554 seconds\n",
      "Step 1300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 72.57054924964905 seconds\n",
      "Step 1400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 78.0735821723938 seconds\n",
      "Step 1500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 83.5859203338623 seconds\n",
      "Step 1600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 89.21130442619324 seconds\n",
      "Step 1700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 94.71198868751526 seconds\n",
      "Step 1800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 100.3637261390686 seconds\n",
      "Step 1900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 105.8610270023346 seconds\n",
      "Step 2000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 111.35819053649902 seconds\n",
      "Step 2100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 116.98390054702759 seconds\n",
      "Step 2200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 122.49946522712708 seconds\n",
      "Step 2300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 128.14981269836426 seconds\n",
      "Step 2400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 133.6891098022461 seconds\n",
      "Step 2500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 139.33144164085388 seconds\n",
      "Step 2600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 144.8365375995636 seconds\n",
      "Step 2700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 150.34799909591675 seconds\n",
      "Step 2800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 155.9775846004486 seconds\n",
      "Step 2900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 161.5023069381714 seconds\n",
      "Step 3000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 167.14320492744446 seconds\n",
      "Step 3100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 172.64663672447205 seconds\n",
      "Step 3200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 178.31255960464478 seconds\n",
      "Step 3300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 183.83059668540955 seconds\n",
      "Step 3400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 189.34862685203552 seconds\n",
      "Step 3500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 195.01672267913818 seconds\n",
      "Step 3600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 200.52543878555298 seconds\n",
      "Step 3700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 206.1606080532074 seconds\n",
      "Step 3800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 211.6711084842682 seconds\n",
      "Step 3900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 217.31331181526184 seconds\n",
      "Step 4000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 222.8430380821228 seconds\n",
      "Step 4100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 228.35319757461548 seconds\n",
      "Step 4200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 234.0042462348938 seconds\n",
      "Step 4300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 239.52733278274536 seconds\n",
      "Step 4400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 245.1570703983307 seconds\n",
      "Step 4500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 250.675213098526 seconds\n",
      "Step 4600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 256.18557834625244 seconds\n",
      "Step 4700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 261.8235104084015 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 267.3484137058258 seconds\n",
      "Step 4900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 272.9973909854889 seconds\n",
      "Step 5000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 278.51500701904297 seconds\n",
      "Step 5100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 284.19960737228394 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 289.7401623725891 seconds\n",
      "Step 5300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 295.460077047348 seconds\n",
      "Step 5400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 301.21001982688904 seconds\n",
      "Step 5500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 307.832879781723 seconds\n",
      "Step 5600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 313.52512669563293 seconds\n",
      "Step 5700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 319.0414996147156 seconds\n",
      "Step 5800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 324.6967785358429 seconds\n",
      "Step 5900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 330.19734382629395 seconds\n",
      "Step 6000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 335.7126712799072 seconds\n",
      "Step 6100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 341.3656198978424 seconds\n",
      "Step 6200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 346.90833568573 seconds\n",
      "Step 6300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 352.56276631355286 seconds\n",
      "Step 6400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 358.0993494987488 seconds\n",
      "Step 6500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 363.63742232322693 seconds\n",
      "Step 6600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 370.0353240966797 seconds\n",
      "Step 6700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 375.5560111999512 seconds\n",
      "Step 6800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 381.19084191322327 seconds\n",
      "Step 6900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 386.7077338695526 seconds\n",
      "Step 7000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 392.3817820549011 seconds\n",
      "Step 7100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 397.89482402801514 seconds\n",
      "Step 7200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 403.4397830963135 seconds\n",
      "Step 7300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 409.11099553108215 seconds\n",
      "Step 7400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 414.64569306373596 seconds\n",
      "Step 7500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 420.32745337486267 seconds\n",
      "Step 7600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 425.8335521221161 seconds\n",
      "Step 7700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 431.5099992752075 seconds\n",
      "Step 7800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 437.07602071762085 seconds\n",
      "Step 7900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 442.6216678619385 seconds\n",
      "Step 8000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 448.31846475601196 seconds\n",
      "Step 8100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 453.88017892837524 seconds\n",
      "Step 8200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 459.55634784698486 seconds\n",
      "Step 8300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 465.12262654304504 seconds\n",
      "Step 8400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 470.7962911128998 seconds\n",
      "Step 8500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 477.00541591644287 seconds\n",
      "Step 8600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 482.57770109176636 seconds\n",
      "Step 8700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 488.2362823486328 seconds\n",
      "Step 8800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 493.74257946014404 seconds\n",
      "Step 8900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 499.4379527568817 seconds\n",
      "Step 9000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 504.9460859298706 seconds\n",
      "Step 9100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 510.4827947616577 seconds\n",
      "Step 9200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 516.1157612800598 seconds\n",
      "Step 9300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 521.6283257007599 seconds\n",
      "Step 9400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 528.0898678302765 seconds\n",
      "Step 9500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 533.608996629715 seconds\n",
      "Step 9600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 539.1386592388153 seconds\n",
      "Step 9700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 544.810733795166 seconds\n",
      "Step 9800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 550.3305387496948 seconds\n",
      "Step 9900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 556.0183525085449 seconds\n",
      "Step 10000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 561.539644241333 seconds\n",
      "Step 10100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 567.1914527416229 seconds\n",
      "Step 10200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 572.7143774032593 seconds\n",
      "Step 10300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 578.275105714798 seconds\n",
      "Step 10400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 583.989669084549 seconds\n",
      "Step 10500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 589.5851624011993 seconds\n",
      "Step 10600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 595.2860250473022 seconds\n",
      "Step 10700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 600.824627161026 seconds\n",
      "Step 10800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 606.3589744567871 seconds\n",
      "Step 10900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 612.0505800247192 seconds\n",
      "Step 11000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 617.6011657714844 seconds\n",
      "Step 11100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 623.7899308204651 seconds\n",
      "Step 11200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 629.3196806907654 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 634.9811079502106 seconds\n",
      "Step 11400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 640.5906221866608 seconds\n",
      "Step 11500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 646.1348209381104 seconds\n",
      "Step 11600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 651.8354620933533 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 657.3812236785889 seconds\n",
      "Step 11800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 663.0984754562378 seconds\n",
      "Step 11900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 668.714750289917 seconds\n",
      "Step 12000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 674.2591607570648 seconds\n",
      "Step 12100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 680.37104845047 seconds\n",
      "Step 12200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 685.8985962867737 seconds\n",
      "Step 12300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 691.5675346851349 seconds\n",
      "Step 12400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 697.1311497688293 seconds\n",
      "Step 12500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 702.8532121181488 seconds\n",
      "Step 12600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 708.4333341121674 seconds\n",
      "Step 12700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 713.9993612766266 seconds\n",
      "Step 12800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 719.7555305957794 seconds\n",
      "Step 12900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 725.34046626091 seconds\n",
      "Step 13000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 731.0693123340607 seconds\n",
      "Step 13100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 737.2306094169617 seconds\n",
      "Step 13200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 742.7955956459045 seconds\n",
      "Step 13300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 748.4694221019745 seconds\n",
      "Step 13400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 754.0130026340485 seconds\n",
      "Step 13500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 759.6879122257233 seconds\n",
      "Step 13600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 765.2382304668427 seconds\n",
      "Step 13700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 770.9484333992004 seconds\n",
      "Step 13800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 776.5252594947815 seconds\n",
      "Step 13900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 782.0712270736694 seconds\n",
      "Step 14000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 787.7569055557251 seconds\n",
      "Step 14100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 793.280636548996 seconds\n",
      "Step 14200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 798.9899642467499 seconds\n",
      "Step 14300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 804.5455746650696 seconds\n",
      "Step 14400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 810.1033136844635 seconds\n",
      "Step 14500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 815.7945032119751 seconds\n",
      "Step 14600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 821.3442947864532 seconds\n",
      "Step 14700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 827.0399839878082 seconds\n",
      "Step 14800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 832.6054513454437 seconds\n",
      "Step 14900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 838.3030550479889 seconds\n",
      "Step 15000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 844.1448571681976 seconds\n",
      "Step 15100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 849.7440888881683 seconds\n",
      "Step 15200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 855.4814622402191 seconds\n",
      "Step 15300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 861.0702221393585 seconds\n",
      "Step 15400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 867.111118555069 seconds\n",
      "Step 15500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 872.6824307441711 seconds\n",
      "Step 15600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 878.2149593830109 seconds\n",
      "Step 15700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 883.9048316478729 seconds\n",
      "Step 15800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 890.0393249988556 seconds\n",
      "Step 15900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 895.7453844547272 seconds\n",
      "Step 16000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 901.2815432548523 seconds\n",
      "Step 16100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 906.959956407547 seconds\n",
      "Step 16200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 912.6565179824829 seconds\n",
      "Step 16300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 918.59921002388 seconds\n",
      "Step 16400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 924.3131613731384 seconds\n",
      "Step 16500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 929.8558692932129 seconds\n",
      "Step 16600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 935.5197038650513 seconds\n",
      "Step 16700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 941.0682702064514 seconds\n",
      "Step 16800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 946.6302216053009 seconds\n",
      "Step 16900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 952.3437156677246 seconds\n",
      "Step 17000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 958.3360795974731 seconds\n",
      "Step 17100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 964.0122828483582 seconds\n",
      "Step 17200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 969.6032409667969 seconds\n",
      "Step 17300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 975.2918267250061 seconds\n",
      "Step 17400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 980.8757512569427 seconds\n",
      "Step 17500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 986.6980590820312 seconds\n",
      "Step 17600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 992.4060673713684 seconds\n",
      "Step 17700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 998.0267913341522 seconds\n",
      "Step 17800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1003.7246916294098 seconds\n",
      "Step 17900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1009.3897104263306 seconds\n",
      "Step 18000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1014.928740978241 seconds\n",
      "Step 18100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1021.4289302825928 seconds\n",
      "Step 18200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1026.961228132248 seconds\n",
      "Step 18300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1032.6658635139465 seconds\n",
      "Step 18400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1038.1978125572205 seconds\n",
      "Step 18500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1043.7438399791718 seconds\n",
      "Step 18600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1049.419662952423 seconds\n",
      "Step 18700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1054.962968826294 seconds\n",
      "Step 18800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1060.6817548274994 seconds\n",
      "Step 18900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1066.2147316932678 seconds\n",
      "Step 19000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1071.9323184490204 seconds\n",
      "Step 19100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1077.5120632648468 seconds\n",
      "Step 19200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1083.0840439796448 seconds\n",
      "Step 19300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1088.809496164322 seconds\n",
      "Step 19400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1095.9188432693481 seconds\n",
      "Step 19500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1101.6735444068909 seconds\n",
      "Step 19600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1107.2107396125793 seconds\n",
      "Step 19700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1112.755295753479 seconds\n",
      "Step 19800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1118.449541568756 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1124.0046863555908 seconds\n",
      "Step 20000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1129.7078301906586 seconds\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 0.06112265586853027 seconds\n",
      "Step 100, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 5.6908814907073975 seconds\n",
      "Step 200, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 11.253300905227661 seconds\n",
      "Step 300, loss: tensor(0.0078, grad_fn=<SubBackward0>), time elapsed: 16.877676963806152 seconds\n",
      "Step 400, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 22.41338610649109 seconds\n",
      "Step 500, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 28.03304672241211 seconds\n",
      "Step 600, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 33.51479983329773 seconds\n",
      "Step 700, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 39.17566895484924 seconds\n",
      "Step 800, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 44.6731059551239 seconds\n",
      "Step 900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 50.18402409553528 seconds\n",
      "Step 1000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 55.817137241363525 seconds\n",
      "Step 1100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 61.31541442871094 seconds\n",
      "Step 1200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 66.97697710990906 seconds\n",
      "Step 1300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 72.50171113014221 seconds\n",
      "Step 1400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 78.15946841239929 seconds\n",
      "Step 1500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 83.6525981426239 seconds\n",
      "Step 1600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 89.15808701515198 seconds\n",
      "Step 1700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 94.79078030586243 seconds\n",
      "Step 1800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 100.31637978553772 seconds\n",
      "Step 1900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 105.96645379066467 seconds\n",
      "Step 2000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 111.49436521530151 seconds\n",
      "Step 2100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 117.1558837890625 seconds\n",
      "Step 2200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 122.63043403625488 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 128.1326949596405 seconds\n",
      "Step 2400, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 133.76214003562927 seconds\n",
      "Step 2500, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 139.2493588924408 seconds\n",
      "Step 2600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 144.87441444396973 seconds\n",
      "Step 2700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 150.392094373703 seconds\n",
      "Step 2800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 155.90868210792542 seconds\n",
      "Step 2900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 161.58039355278015 seconds\n",
      "Step 3000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 167.081556558609 seconds\n",
      "Step 3100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 172.73565459251404 seconds\n",
      "Step 3200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 178.22690439224243 seconds\n",
      "Step 3300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 183.86473536491394 seconds\n",
      "Step 3400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 189.35118961334229 seconds\n",
      "Step 3500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 194.90471696853638 seconds\n",
      "Step 3600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 200.542720079422 seconds\n",
      "Step 3700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 206.06767988204956 seconds\n",
      "Step 3800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 211.70816946029663 seconds\n",
      "Step 3900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 217.23409056663513 seconds\n",
      "Step 4000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 222.9183418750763 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 228.41643166542053 seconds\n",
      "Step 4200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 233.9362223148346 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 239.57948565483093 seconds\n",
      "Step 4400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 245.09160661697388 seconds\n",
      "Step 4500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 250.73205494880676 seconds\n",
      "Step 4600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 256.28328251838684 seconds\n",
      "Step 4700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 261.94808173179626 seconds\n",
      "Step 4800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 267.4451491832733 seconds\n",
      "Step 4900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 272.942346572876 seconds\n",
      "Step 5000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 278.7833602428436 seconds\n",
      "Step 5100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 284.31241631507874 seconds\n",
      "Step 5200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 289.96419072151184 seconds\n",
      "Step 5300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 295.4799373149872 seconds\n",
      "Step 5400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 301.03256821632385 seconds\n",
      "Step 5500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 306.71249198913574 seconds\n",
      "Step 5600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 313.09924149513245 seconds\n",
      "Step 5700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 318.74930572509766 seconds\n",
      "Step 5800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 324.25660014152527 seconds\n",
      "Step 5900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 329.9054524898529 seconds\n",
      "Step 6000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 335.40904927253723 seconds\n",
      "Step 6100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 340.9369385242462 seconds\n",
      "Step 6200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 346.62188267707825 seconds\n",
      "Step 6300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 352.1423535346985 seconds\n",
      "Step 6400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 357.7920687198639 seconds\n",
      "Step 6500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 363.3174641132355 seconds\n",
      "Step 6600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 368.9721426963806 seconds\n",
      "Step 6700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 374.4760067462921 seconds\n",
      "Step 6800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 379.99012088775635 seconds\n",
      "Step 6900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 385.643741607666 seconds\n",
      "Step 7000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 391.1918406486511 seconds\n",
      "Step 7100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 396.8559534549713 seconds\n",
      "Step 7200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 402.3805627822876 seconds\n",
      "Step 7300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 407.9272060394287 seconds\n",
      "Step 7400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 413.5831370353699 seconds\n",
      "Step 7500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 419.0840301513672 seconds\n",
      "Step 7600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 424.73356533050537 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 430.26845383644104 seconds\n",
      "Step 7800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 435.96921730041504 seconds\n",
      "Step 7900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 442.13990020751953 seconds\n",
      "Step 8000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 447.80983304977417 seconds\n",
      "Step 8100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 453.5088768005371 seconds\n",
      "Step 8200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 459.06612610816956 seconds\n",
      "Step 8300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 464.7229058742523 seconds\n",
      "Step 8400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 470.28172612190247 seconds\n",
      "Step 8500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 475.9722673892975 seconds\n",
      "Step 8600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 481.5122084617615 seconds\n",
      "Step 8700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 487.07589197158813 seconds\n",
      "Step 8800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 492.75380539894104 seconds\n",
      "Step 8900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 498.312997341156 seconds\n",
      "Step 9000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 504.0104477405548 seconds\n",
      "Step 9100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 509.52489590644836 seconds\n",
      "Step 9200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 515.1092052459717 seconds\n",
      "Step 9300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 520.8030064105988 seconds\n",
      "Step 9400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 526.3554470539093 seconds\n",
      "Step 9500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 532.043062210083 seconds\n",
      "Step 9600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 537.5941445827484 seconds\n",
      "Step 9700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 543.3008749485016 seconds\n",
      "Step 9800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 548.8534915447235 seconds\n",
      "Step 9900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 554.3870849609375 seconds\n",
      "Step 10000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 560.0944769382477 seconds\n",
      "Step 10100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 565.6457297801971 seconds\n",
      "Step 10200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 571.3359162807465 seconds\n",
      "Step 10300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 576.8693356513977 seconds\n",
      "Step 10400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 582.4192280769348 seconds\n",
      "Step 10500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 588.1041910648346 seconds\n",
      "Step 10600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 593.6185824871063 seconds\n",
      "Step 10700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 599.3015546798706 seconds\n",
      "Step 10800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 604.8150551319122 seconds\n",
      "Step 10900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 610.4908089637756 seconds\n",
      "Step 11000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 616.0177667140961 seconds\n",
      "Step 11100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 621.5889620780945 seconds\n",
      "Step 11200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 627.2767026424408 seconds\n",
      "Step 11300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 632.8309011459351 seconds\n",
      "Step 11400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 638.529034614563 seconds\n",
      "Step 11500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 644.5270462036133 seconds\n",
      "Step 11600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 650.2873976230621 seconds\n",
      "Step 11700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 655.9683110713959 seconds\n",
      "Step 11800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 661.5073118209839 seconds\n",
      "Step 11900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 667.2088575363159 seconds\n",
      "Step 12000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 672.777704000473 seconds\n",
      "Step 12100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 678.4796140193939 seconds\n",
      "Step 12200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 684.0217189788818 seconds\n",
      "Step 12300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 689.5660378932953 seconds\n",
      "Step 12400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 695.2492597103119 seconds\n",
      "Step 12500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 700.7891080379486 seconds\n",
      "Step 12600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 706.4721493721008 seconds\n",
      "Step 12700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 712.0518372058868 seconds\n",
      "Step 12800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 717.6230647563934 seconds\n",
      "Step 12900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 724.5221168994904 seconds\n",
      "Step 13000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 730.2406938076019 seconds\n",
      "Step 13100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 735.9319496154785 seconds\n",
      "Step 13200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 741.4710423946381 seconds\n",
      "Step 13300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 747.1395437717438 seconds\n",
      "Step 13400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 752.6574084758759 seconds\n",
      "Step 13500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 758.1944925785065 seconds\n",
      "Step 13600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 763.8698241710663 seconds\n",
      "Step 13700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 769.3798594474792 seconds\n",
      "Step 13800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 775.0618126392365 seconds\n",
      "Step 13900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 780.5935757160187 seconds\n",
      "Step 14000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 786.1173238754272 seconds\n",
      "Step 14100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 791.8119449615479 seconds\n",
      "Step 14200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 797.345109462738 seconds\n",
      "Step 14300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 803.06614112854 seconds\n",
      "Step 14400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 808.6055293083191 seconds\n",
      "Step 14500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 814.2987298965454 seconds\n",
      "Step 14600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 819.8265981674194 seconds\n",
      "Step 14700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 825.3613367080688 seconds\n",
      "Step 14800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 831.0511152744293 seconds\n",
      "Step 14900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 836.5973334312439 seconds\n",
      "Step 15000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 842.3046855926514 seconds\n",
      "Step 15100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 847.8324384689331 seconds\n",
      "Step 15200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 853.3770852088928 seconds\n",
      "Step 15300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 859.0734372138977 seconds\n",
      "Step 15400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 864.630752325058 seconds\n",
      "Step 15500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 870.3100626468658 seconds\n",
      "Step 15600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 875.8544249534607 seconds\n",
      "Step 15700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 881.5466902256012 seconds\n",
      "Step 15800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 887.1100120544434 seconds\n",
      "Step 15900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 892.7027177810669 seconds\n",
      "Step 16000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 898.4006206989288 seconds\n",
      "Step 16100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 903.9397990703583 seconds\n",
      "Step 16200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 909.6600046157837 seconds\n",
      "Step 16300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 915.2060735225677 seconds\n",
      "Step 16400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 920.7318696975708 seconds\n",
      "Step 16500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 926.4629645347595 seconds\n",
      "Step 16600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 932.350263595581 seconds\n",
      "Step 16700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 938.0861051082611 seconds\n",
      "Step 16800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 943.6199135780334 seconds\n",
      "Step 16900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 949.1571185588837 seconds\n",
      "Step 17000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 954.8675410747528 seconds\n",
      "Step 17100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 960.387957572937 seconds\n",
      "Step 17200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 966.0365252494812 seconds\n",
      "Step 17300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 971.5534858703613 seconds\n",
      "Step 17400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 977.6738111972809 seconds\n",
      "Step 17500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 983.2184689044952 seconds\n",
      "Step 17600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 988.7656257152557 seconds\n",
      "Step 17700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 994.4567379951477 seconds\n",
      "Step 17800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 999.9815113544464 seconds\n",
      "Step 17900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1005.6468670368195 seconds\n",
      "Step 18000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1011.1681079864502 seconds\n",
      "Step 18100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1016.7290353775024 seconds\n",
      "Step 18200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1022.4385027885437 seconds\n",
      "Step 18300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1027.9890275001526 seconds\n",
      "Step 18400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1033.7388770580292 seconds\n",
      "Step 18500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1039.3075449466705 seconds\n",
      "Step 18600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1045.0683472156525 seconds\n",
      "Step 18700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1051.0265078544617 seconds\n",
      "Step 18800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1056.5481066703796 seconds\n",
      "Step 18900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1062.230807542801 seconds\n",
      "Step 19000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1067.7657644748688 seconds\n",
      "Step 19100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1073.4520344734192 seconds\n",
      "Step 19200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1079.0016479492188 seconds\n",
      "Step 19300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1084.5529115200043 seconds\n",
      "Step 19400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1090.2863986492157 seconds\n",
      "Step 19500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1095.8261258602142 seconds\n",
      "Step 19600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1101.5595638751984 seconds\n",
      "Step 19700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1107.5469899177551 seconds\n",
      "Step 19800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 1113.3721668720245 seconds\n",
      "Step 19900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1119.0896763801575 seconds\n",
      "Step 20000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1124.6610758304596 seconds\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0268, grad_fn=<SubBackward0>), time elapsed: 0.06131720542907715 seconds\n",
      "Step 100, loss: tensor(0.0200, grad_fn=<SubBackward0>), time elapsed: 5.839563608169556 seconds\n",
      "Step 200, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 11.38443398475647 seconds\n",
      "Step 300, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 17.046024322509766 seconds\n",
      "Step 400, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 22.55169177055359 seconds\n",
      "Step 500, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 28.0757577419281 seconds\n",
      "Step 600, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 33.7706573009491 seconds\n",
      "Step 700, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 39.29278326034546 seconds\n",
      "Step 800, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 44.92242741584778 seconds\n",
      "Step 900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 50.447142124176025 seconds\n",
      "Step 1000, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 56.091723680496216 seconds\n",
      "Step 1100, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 61.616432905197144 seconds\n",
      "Step 1200, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 67.13268160820007 seconds\n",
      "Step 1300, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 72.78461956977844 seconds\n",
      "Step 1400, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 78.31994485855103 seconds\n",
      "Step 1500, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 83.9662516117096 seconds\n",
      "Step 1600, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 89.45564222335815 seconds\n",
      "Step 1700, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 94.95372128486633 seconds\n",
      "Step 1800, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 100.57231950759888 seconds\n",
      "Step 1900, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 106.08648037910461 seconds\n",
      "Step 2000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 111.7385573387146 seconds\n",
      "Step 2100, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 117.25725102424622 seconds\n",
      "Step 2200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 122.90696167945862 seconds\n",
      "Step 2300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 128.42523550987244 seconds\n",
      "Step 2400, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 133.92213582992554 seconds\n",
      "Step 2500, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 139.5522484779358 seconds\n",
      "Step 2600, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 145.06816172599792 seconds\n",
      "Step 2700, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 150.7189700603485 seconds\n",
      "Step 2800, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 156.23301005363464 seconds\n",
      "Step 2900, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 161.88010048866272 seconds\n",
      "Step 3000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 167.40494179725647 seconds\n",
      "Step 3100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 172.93349647521973 seconds\n",
      "Step 3200, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 178.55584120750427 seconds\n",
      "Step 3300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 184.07965397834778 seconds\n",
      "Step 3400, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 189.7369782924652 seconds\n",
      "Step 3500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 195.2349066734314 seconds\n",
      "Step 3600, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 200.91847276687622 seconds\n",
      "Step 3700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 206.42460346221924 seconds\n",
      "Step 3800, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 211.95678758621216 seconds\n",
      "Step 3900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 217.5994291305542 seconds\n",
      "Step 4000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 223.1082468032837 seconds\n",
      "Step 4100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 228.78735089302063 seconds\n",
      "Step 4200, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 234.29518055915833 seconds\n",
      "Step 4300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 239.77857637405396 seconds\n",
      "Step 4400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 245.40501761436462 seconds\n",
      "Step 4500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 250.90009760856628 seconds\n",
      "Step 4600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 256.5462644100189 seconds\n",
      "Step 4700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 262.0764195919037 seconds\n",
      "Step 4800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 267.86612939834595 seconds\n",
      "Step 4900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 273.42171907424927 seconds\n",
      "Step 5000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 279.6571328639984 seconds\n",
      "Step 5100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 285.3035960197449 seconds\n",
      "Step 5200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 290.8480770587921 seconds\n",
      "Step 5300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 296.5155580043793 seconds\n",
      "Step 5400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 302.0284686088562 seconds\n",
      "Step 5500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 307.6857075691223 seconds\n",
      "Step 5600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 313.2034375667572 seconds\n",
      "Step 5700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 318.7359399795532 seconds\n",
      "Step 5800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 325.1374452114105 seconds\n",
      "Step 5900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 330.65584897994995 seconds\n",
      "Step 6000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 336.310396194458 seconds\n",
      "Step 6100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 341.8141996860504 seconds\n",
      "Step 6200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 347.33398246765137 seconds\n",
      "Step 6300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 353.04497957229614 seconds\n",
      "Step 6400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 358.5775668621063 seconds\n",
      "Step 6500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 364.2717969417572 seconds\n",
      "Step 6600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 369.8028004169464 seconds\n",
      "Step 6700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 375.46446776390076 seconds\n",
      "Step 6800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 381.0394403934479 seconds\n",
      "Step 6900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 386.57348370552063 seconds\n",
      "Step 7000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 392.2561309337616 seconds\n",
      "Step 7100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 397.8068974018097 seconds\n",
      "Step 7200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 403.48610067367554 seconds\n",
      "Step 7300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 409.18378925323486 seconds\n",
      "Step 7400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 414.88511538505554 seconds\n",
      "Step 7500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 421.13614797592163 seconds\n",
      "Step 7600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 426.6577410697937 seconds\n",
      "Step 7700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 432.33440923690796 seconds\n",
      "Step 7800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 437.86584639549255 seconds\n",
      "Step 7900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 443.57696747779846 seconds\n",
      "Step 8000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 449.1400074958801 seconds\n",
      "Step 8100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 454.8627619743347 seconds\n",
      "Step 8200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 460.3916103839874 seconds\n",
      "Step 8300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 465.9793174266815 seconds\n",
      "Step 8400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 471.6895558834076 seconds\n",
      "Step 8500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 477.9176287651062 seconds\n",
      "Step 8600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 483.5991325378418 seconds\n",
      "Step 8700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 489.1567442417145 seconds\n",
      "Step 8800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 494.6742241382599 seconds\n",
      "Step 8900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 500.3187026977539 seconds\n",
      "Step 9000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 505.9400351047516 seconds\n",
      "Step 9100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 511.67027020454407 seconds\n",
      "Step 9200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 517.1941056251526 seconds\n",
      "Step 9300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 522.9372363090515 seconds\n",
      "Step 9400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 528.4955642223358 seconds\n",
      "Step 9500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 535.0454368591309 seconds\n",
      "Step 9600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 540.7260000705719 seconds\n",
      "Step 9700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 546.2485568523407 seconds\n",
      "Step 9800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 551.8997542858124 seconds\n",
      "Step 9900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 557.413946390152 seconds\n",
      "Step 10000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 562.9285840988159 seconds\n",
      "Step 10100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 568.6069912910461 seconds\n",
      "Step 10200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 574.1355328559875 seconds\n",
      "Step 10300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 579.8190762996674 seconds\n",
      "Step 10400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 585.3363392353058 seconds\n",
      "Step 10500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 591.0576856136322 seconds\n",
      "Step 10600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 596.5686287879944 seconds\n",
      "Step 10700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 602.0881972312927 seconds\n",
      "Step 10800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 608.197968006134 seconds\n",
      "Step 10900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 613.7399406433105 seconds\n",
      "Step 11000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 619.4419963359833 seconds\n",
      "Step 11100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 624.9993886947632 seconds\n",
      "Step 11200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 630.5330069065094 seconds\n",
      "Step 11300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 636.1982622146606 seconds\n",
      "Step 11400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 641.7926287651062 seconds\n",
      "Step 11500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 647.4810819625854 seconds\n",
      "Step 11600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 653.00790143013 seconds\n",
      "Step 11700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 658.753500699997 seconds\n",
      "Step 11800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 664.3609337806702 seconds\n",
      "Step 11900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 670.6820402145386 seconds\n",
      "Step 12000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 676.4153277873993 seconds\n",
      "Step 12100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 681.9458644390106 seconds\n",
      "Step 12200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 687.6461405754089 seconds\n",
      "Step 12300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 693.1773157119751 seconds\n",
      "Step 12400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 698.7187836170197 seconds\n",
      "Step 12500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 704.4116010665894 seconds\n",
      "Step 12600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 709.9348213672638 seconds\n",
      "Step 12700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 715.6187319755554 seconds\n",
      "Step 12800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 721.2064707279205 seconds\n",
      "Step 12900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 726.8809566497803 seconds\n",
      "Step 13000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 732.410964012146 seconds\n",
      "Step 13100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 737.9129366874695 seconds\n",
      "Step 13200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 743.6152212619781 seconds\n",
      "Step 13300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 749.1899085044861 seconds\n",
      "Step 13400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 754.9095878601074 seconds\n",
      "Step 13500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 760.4685769081116 seconds\n",
      "Step 13600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 766.0448834896088 seconds\n",
      "Step 13700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 771.7226023674011 seconds\n",
      "Step 13800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 777.2999401092529 seconds\n",
      "Step 13900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 783.0130889415741 seconds\n",
      "Step 14000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 788.5712673664093 seconds\n",
      "Step 14100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 794.7644157409668 seconds\n",
      "Step 14200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 800.462112903595 seconds\n",
      "Step 14300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 806.0109021663666 seconds\n",
      "Step 14400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 811.7483701705933 seconds\n",
      "Step 14500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 817.27734541893 seconds\n",
      "Step 14600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 822.9725635051727 seconds\n",
      "Step 14700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 828.5277671813965 seconds\n",
      "Step 14800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 834.1033406257629 seconds\n",
      "Step 14900, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 839.8630497455597 seconds\n",
      "Step 15000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 845.4546074867249 seconds\n",
      "Step 15100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 851.2190372943878 seconds\n",
      "Step 15200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 856.861421585083 seconds\n",
      "Step 15300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 862.5458006858826 seconds\n",
      "Step 15400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 868.2785699367523 seconds\n",
      "Step 15500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 873.9349203109741 seconds\n",
      "Step 15600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 880.2749540805817 seconds\n",
      "Step 15700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 885.7829241752625 seconds\n",
      "Step 15800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 891.4853868484497 seconds\n",
      "Step 15900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 896.9966342449188 seconds\n",
      "Step 16000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 902.5558412075043 seconds\n",
      "Step 16100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 908.2156422138214 seconds\n",
      "Step 16200, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 913.7549374103546 seconds\n",
      "Step 16300, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 919.4506907463074 seconds\n",
      "Step 16400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 924.9835822582245 seconds\n",
      "Step 16500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 931.0790424346924 seconds\n",
      "Step 16600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 936.8122601509094 seconds\n",
      "Step 16700, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 942.3285694122314 seconds\n",
      "Step 16800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 948.0137917995453 seconds\n",
      "Step 16900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 953.6754026412964 seconds\n",
      "Step 17000, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 959.4469728469849 seconds\n",
      "Step 17100, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 965.0073657035828 seconds\n",
      "Step 17200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 971.2529299259186 seconds\n",
      "Step 17300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 976.9622611999512 seconds\n",
      "Step 17400, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 982.5097014904022 seconds\n",
      "Step 17500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 988.1981782913208 seconds\n",
      "Step 17600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 993.75603556633 seconds\n",
      "Step 17700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 999.3110389709473 seconds\n",
      "Step 17800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1005.0701427459717 seconds\n",
      "Step 17900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1010.6450095176697 seconds\n",
      "Step 18000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1016.3832488059998 seconds\n",
      "Step 18100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1021.9711904525757 seconds\n",
      "Step 18200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1028.5229308605194 seconds\n",
      "Step 18300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1034.0670578479767 seconds\n",
      "Step 18400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1039.5846590995789 seconds\n",
      "Step 18500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1045.258157491684 seconds\n",
      "Step 18600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1050.7683365345001 seconds\n",
      "Step 18700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1056.4794862270355 seconds\n",
      "Step 18800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1062.010468006134 seconds\n",
      "Step 18900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1067.5431339740753 seconds\n",
      "Step 19000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1073.2394423484802 seconds\n",
      "Step 19100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1078.7638280391693 seconds\n",
      "Step 19200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1084.469209909439 seconds\n",
      "Step 19300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 1089.9995205402374 seconds\n",
      "Step 19400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1095.524052619934 seconds\n",
      "Step 19500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1101.2212352752686 seconds\n",
      "Step 19600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1106.7711369991302 seconds\n",
      "Step 19700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1112.5334780216217 seconds\n",
      "Step 19800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1118.0958774089813 seconds\n",
      "Step 19900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1123.8363943099976 seconds\n",
      "Step 20000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1129.4083988666534 seconds\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0203, grad_fn=<SubBackward0>), time elapsed: 0.06705641746520996 seconds\n",
      "Step 100, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 5.6780173778533936 seconds\n",
      "Step 200, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 11.353258848190308 seconds\n",
      "Step 300, loss: tensor(0.0090, grad_fn=<SubBackward0>), time elapsed: 16.883031845092773 seconds\n",
      "Step 400, loss: tensor(0.0077, grad_fn=<SubBackward0>), time elapsed: 22.55505633354187 seconds\n",
      "Step 500, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 28.061743021011353 seconds\n",
      "Step 600, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 33.571738481521606 seconds\n",
      "Step 700, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 39.20498561859131 seconds\n",
      "Step 800, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 44.7062246799469 seconds\n",
      "Step 900, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 50.34315514564514 seconds\n",
      "Step 1000, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 55.83336138725281 seconds\n",
      "Step 1100, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 61.48166298866272 seconds\n",
      "Step 1200, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 66.98461055755615 seconds\n",
      "Step 1300, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 72.49153590202332 seconds\n",
      "Step 1400, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 78.11459541320801 seconds\n",
      "Step 1500, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 83.6406421661377 seconds\n",
      "Step 1600, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 89.26758170127869 seconds\n",
      "Step 1700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 94.77395558357239 seconds\n",
      "Step 1800, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 100.4152364730835 seconds\n",
      "Step 1900, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 105.91846442222595 seconds\n",
      "Step 2000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 111.46080112457275 seconds\n",
      "Step 2100, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 117.10937023162842 seconds\n",
      "Step 2200, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 122.6124975681305 seconds\n",
      "Step 2300, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 128.24838399887085 seconds\n",
      "Step 2400, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 133.73201513290405 seconds\n",
      "Step 2500, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 139.3998851776123 seconds\n",
      "Step 2600, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 144.90452218055725 seconds\n",
      "Step 2700, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 150.4198956489563 seconds\n",
      "Step 2800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 156.06393885612488 seconds\n",
      "Step 2900, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 161.58394026756287 seconds\n",
      "Step 3000, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 167.22536993026733 seconds\n",
      "Step 3100, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 172.76625752449036 seconds\n",
      "Step 3200, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 178.25373315811157 seconds\n",
      "Step 3300, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 183.88597893714905 seconds\n",
      "Step 3400, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 189.40049266815186 seconds\n",
      "Step 3500, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 195.047607421875 seconds\n",
      "Step 3600, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 200.57535767555237 seconds\n",
      "Step 3700, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 206.21688771247864 seconds\n",
      "Step 3800, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 211.70524215698242 seconds\n",
      "Step 3900, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 217.22563338279724 seconds\n",
      "Step 4000, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 222.84887552261353 seconds\n",
      "Step 4100, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 228.35923981666565 seconds\n",
      "Step 4200, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 234.01105642318726 seconds\n",
      "Step 4300, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 239.51981902122498 seconds\n",
      "Step 4400, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 245.1728298664093 seconds\n",
      "Step 4500, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 250.69545769691467 seconds\n",
      "Step 4600, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 256.22567868232727 seconds\n",
      "Step 4700, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 261.890841960907 seconds\n",
      "Step 4800, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 267.3979504108429 seconds\n",
      "Step 4900, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 273.0474591255188 seconds\n",
      "Step 5000, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 278.55936193466187 seconds\n",
      "Step 5100, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 284.07015323638916 seconds\n",
      "Step 5200, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 289.743070602417 seconds\n",
      "Step 5300, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 295.28937005996704 seconds\n",
      "Step 5400, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 300.950875043869 seconds\n",
      "Step 5500, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 306.463796377182 seconds\n",
      "Step 5600, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 312.0977261066437 seconds\n",
      "Step 5700, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 317.6065058708191 seconds\n",
      "Step 5800, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 323.13708114624023 seconds\n",
      "Step 5900, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 328.7541253566742 seconds\n",
      "Step 6000, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 334.26903009414673 seconds\n",
      "Step 6100, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 339.903391122818 seconds\n",
      "Step 6200, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 345.4150776863098 seconds\n",
      "Step 6300, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 351.07388186454773 seconds\n",
      "Step 6400, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 356.5990719795227 seconds\n",
      "Step 6500, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 362.09383034706116 seconds\n",
      "Step 6600, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 367.71875190734863 seconds\n",
      "Step 6700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 373.25284218788147 seconds\n",
      "Step 6800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 379.30522775650024 seconds\n",
      "Step 6900, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 384.84716725349426 seconds\n",
      "Step 7000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 390.35397505760193 seconds\n",
      "Step 7100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 396.04254055023193 seconds\n",
      "Step 7200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 401.5557379722595 seconds\n",
      "Step 7300, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 407.2283375263214 seconds\n",
      "Step 7400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 412.7572479248047 seconds\n",
      "Step 7500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 418.4595868587494 seconds\n",
      "Step 7600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 424.3928482532501 seconds\n",
      "Step 7700, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 429.92211079597473 seconds\n",
      "Step 7800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 435.61729621887207 seconds\n",
      "Step 7900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 441.161062002182 seconds\n",
      "Step 8000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 446.83539724349976 seconds\n",
      "Step 8100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 452.39935779571533 seconds\n",
      "Step 8200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 458.0862741470337 seconds\n",
      "Step 8300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 463.56697130203247 seconds\n",
      "Step 8400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 469.1097493171692 seconds\n",
      "Step 8500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 474.7859275341034 seconds\n",
      "Step 8600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 480.47344160079956 seconds\n",
      "Step 8700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 486.1473078727722 seconds\n",
      "Step 8800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 491.66640400886536 seconds\n",
      "Step 8900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 497.2025189399719 seconds\n",
      "Step 9000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 502.8779878616333 seconds\n",
      "Step 9100, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 508.42483139038086 seconds\n",
      "Step 9200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 514.3481080532074 seconds\n",
      "Step 9300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 519.8548171520233 seconds\n",
      "Step 9400, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 525.5374224185944 seconds\n",
      "Step 9500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 531.0579614639282 seconds\n",
      "Step 9600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 536.6481621265411 seconds\n",
      "Step 9700, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 542.3035864830017 seconds\n",
      "Step 9800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 548.3883180618286 seconds\n",
      "Step 9900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 554.1057894229889 seconds\n",
      "Step 10000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 559.7074346542358 seconds\n",
      "Step 10100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 565.3239862918854 seconds\n",
      "Step 10200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 571.1528384685516 seconds\n",
      "Step 10300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 576.7831814289093 seconds\n",
      "Step 10400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 582.6955370903015 seconds\n",
      "Step 10500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 588.3415598869324 seconds\n",
      "Step 10600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 594.0365252494812 seconds\n",
      "Step 10700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 599.6039750576019 seconds\n",
      "Step 10800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 605.2446012496948 seconds\n",
      "Step 10900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 611.0337474346161 seconds\n",
      "Step 11000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 617.4653191566467 seconds\n",
      "Step 11100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 623.1607673168182 seconds\n",
      "Step 11200, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 628.7004923820496 seconds\n",
      "Step 11300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 634.2120769023895 seconds\n",
      "Step 11400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 639.8757696151733 seconds\n",
      "Step 11500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 645.423020362854 seconds\n",
      "Step 11600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 651.0747210979462 seconds\n",
      "Step 11700, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 656.6125495433807 seconds\n",
      "Step 11800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 662.3138673305511 seconds\n",
      "Step 11900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 667.8371443748474 seconds\n",
      "Step 12000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 673.3654294013977 seconds\n",
      "Step 12100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 679.0387227535248 seconds\n",
      "Step 12200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 684.5854635238647 seconds\n",
      "Step 12300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 690.3098349571228 seconds\n",
      "Step 12400, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 695.8421823978424 seconds\n",
      "Step 12500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 701.3863387107849 seconds\n",
      "Step 12600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 707.0748591423035 seconds\n",
      "Step 12700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 712.6162066459656 seconds\n",
      "Step 12800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 719.0308451652527 seconds\n",
      "Step 12900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 724.555389881134 seconds\n",
      "Step 13000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 730.2425138950348 seconds\n",
      "Step 13100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 735.7752661705017 seconds\n",
      "Step 13200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 741.3007352352142 seconds\n",
      "Step 13300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 746.9991593360901 seconds\n",
      "Step 13400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 752.5670766830444 seconds\n",
      "Step 13500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 758.2496519088745 seconds\n",
      "Step 13600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 763.7871608734131 seconds\n",
      "Step 13700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 769.3311684131622 seconds\n",
      "Step 13800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 775.030237197876 seconds\n",
      "Step 13900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 780.6046378612518 seconds\n",
      "Step 14000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 786.2844824790955 seconds\n",
      "Step 14100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 791.813827753067 seconds\n",
      "Step 14200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 797.5106618404388 seconds\n",
      "Step 14300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 803.0260837078094 seconds\n",
      "Step 14400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 808.5793871879578 seconds\n",
      "Step 14500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 814.3032820224762 seconds\n",
      "Step 14600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 819.8707013130188 seconds\n",
      "Step 14700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 825.5951533317566 seconds\n",
      "Step 14800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 831.191930770874 seconds\n",
      "Step 14900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 836.7441592216492 seconds\n",
      "Step 15000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 842.506498336792 seconds\n",
      "Step 15100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 848.0676157474518 seconds\n",
      "Step 15200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 853.8258175849915 seconds\n",
      "Step 15300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 859.3778991699219 seconds\n",
      "Step 15400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 865.0970797538757 seconds\n",
      "Step 15500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 870.6274604797363 seconds\n",
      "Step 15600, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 876.2205767631531 seconds\n",
      "Step 15700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 881.9020705223083 seconds\n",
      "Step 15800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 887.4580714702606 seconds\n",
      "Step 15900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 893.1509921550751 seconds\n",
      "Step 16000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 898.6840958595276 seconds\n",
      "Step 16100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 904.2485272884369 seconds\n",
      "Step 16200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 909.9509944915771 seconds\n",
      "Step 16300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 916.0656545162201 seconds\n",
      "Step 16400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 921.7693846225739 seconds\n",
      "Step 16500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 927.2957646846771 seconds\n",
      "Step 16600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 932.8441982269287 seconds\n",
      "Step 16700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 938.5368502140045 seconds\n",
      "Step 16800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 944.0903992652893 seconds\n",
      "Step 16900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 949.7737166881561 seconds\n",
      "Step 17000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 955.3089420795441 seconds\n",
      "Step 17100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 961.0048727989197 seconds\n",
      "Step 17200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 966.5636699199677 seconds\n",
      "Step 17300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 972.1028883457184 seconds\n",
      "Step 17400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 977.860426902771 seconds\n",
      "Step 17500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 983.4076285362244 seconds\n",
      "Step 17600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 989.098210811615 seconds\n",
      "Step 17700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 994.7981579303741 seconds\n",
      "Step 17800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1000.3149869441986 seconds\n",
      "Step 17900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1006.0373125076294 seconds\n",
      "Step 18000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1011.5985879898071 seconds\n",
      "Step 18100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1017.3086831569672 seconds\n",
      "Step 18200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1022.8541932106018 seconds\n",
      "Step 18300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1028.6345341205597 seconds\n",
      "Step 18400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1034.1845893859863 seconds\n",
      "Step 18500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1040.3074204921722 seconds\n",
      "Step 18600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1045.995600938797 seconds\n",
      "Step 18700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1051.5416555404663 seconds\n",
      "Step 18800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1057.2655091285706 seconds\n",
      "Step 18900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1062.766709804535 seconds\n",
      "Step 19000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1068.2993099689484 seconds\n",
      "Step 19100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1073.9825901985168 seconds\n",
      "Step 19200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1079.527071237564 seconds\n",
      "Step 19300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1085.2407500743866 seconds\n",
      "Step 19400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1090.7718136310577 seconds\n",
      "Step 19500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1096.7864243984222 seconds\n",
      "Step 19600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1102.5511274337769 seconds\n",
      "Step 19700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1108.0683076381683 seconds\n",
      "Step 19800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1113.7643067836761 seconds\n",
      "Step 19900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1119.3246443271637 seconds\n",
      "Step 20000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1125.023999929428 seconds\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 0.06233978271484375 seconds\n",
      "Step 100, loss: tensor(0.0232, grad_fn=<SubBackward0>), time elapsed: 5.723978042602539 seconds\n",
      "Step 200, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 11.291936874389648 seconds\n",
      "Step 300, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 16.9344699382782 seconds\n",
      "Step 400, loss: tensor(0.0145, grad_fn=<SubBackward0>), time elapsed: 22.43358039855957 seconds\n",
      "Step 500, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 28.103668689727783 seconds\n",
      "Step 600, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 33.59884023666382 seconds\n",
      "Step 700, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 39.10199451446533 seconds\n",
      "Step 800, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 44.75360727310181 seconds\n",
      "Step 900, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 50.25390625 seconds\n",
      "Step 1000, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 55.90637683868408 seconds\n",
      "Step 1100, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 61.40460538864136 seconds\n",
      "Step 1200, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 67.05175185203552 seconds\n",
      "Step 1300, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 72.55066657066345 seconds\n",
      "Step 1400, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 78.06082701683044 seconds\n",
      "Step 1500, loss: tensor(0.0110, grad_fn=<SubBackward0>), time elapsed: 83.68422245979309 seconds\n",
      "Step 1600, loss: tensor(0.0105, grad_fn=<SubBackward0>), time elapsed: 89.22890996932983 seconds\n",
      "Step 1700, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 94.86425876617432 seconds\n",
      "Step 1800, loss: tensor(0.0108, grad_fn=<SubBackward0>), time elapsed: 100.3726851940155 seconds\n",
      "Step 1900, loss: tensor(0.0104, grad_fn=<SubBackward0>), time elapsed: 106.01401472091675 seconds\n",
      "Step 2000, loss: tensor(0.0101, grad_fn=<SubBackward0>), time elapsed: 111.52348184585571 seconds\n",
      "Step 2100, loss: tensor(0.0101, grad_fn=<SubBackward0>), time elapsed: 117.05768299102783 seconds\n",
      "Step 2200, loss: tensor(0.0102, grad_fn=<SubBackward0>), time elapsed: 122.70073056221008 seconds\n",
      "Step 2300, loss: tensor(0.0095, grad_fn=<SubBackward0>), time elapsed: 128.21632599830627 seconds\n",
      "Step 2400, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 133.8643536567688 seconds\n",
      "Step 2500, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 139.36913681030273 seconds\n",
      "Step 2600, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 145.02396202087402 seconds\n",
      "Step 2700, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 150.5463466644287 seconds\n",
      "Step 2800, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 156.06081438064575 seconds\n",
      "Step 2900, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 161.70347929000854 seconds\n",
      "Step 3000, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 167.19700574874878 seconds\n",
      "Step 3100, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 172.84527134895325 seconds\n",
      "Step 3200, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 178.37531423568726 seconds\n",
      "Step 3300, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 183.89143061637878 seconds\n",
      "Step 3400, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 189.52969527244568 seconds\n",
      "Step 3500, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 195.0445737838745 seconds\n",
      "Step 3600, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 200.69220209121704 seconds\n",
      "Step 3700, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 206.20357704162598 seconds\n",
      "Step 3800, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 211.86895108222961 seconds\n",
      "Step 3900, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 217.37724208831787 seconds\n",
      "Step 4000, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 222.8924605846405 seconds\n",
      "Step 4100, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 228.54268336296082 seconds\n",
      "Step 4200, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 234.05487871170044 seconds\n",
      "Step 4300, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 239.73361682891846 seconds\n",
      "Step 4400, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 245.25606298446655 seconds\n",
      "Step 4500, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 250.9119234085083 seconds\n",
      "Step 4600, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 256.4081394672394 seconds\n",
      "Step 4700, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 262.0629394054413 seconds\n",
      "Step 4800, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 267.72090196609497 seconds\n",
      "Step 4900, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 273.7256109714508 seconds\n",
      "Step 5000, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 279.3996465206146 seconds\n",
      "Step 5100, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 285.54602694511414 seconds\n",
      "Step 5200, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 291.06154465675354 seconds\n",
      "Step 5300, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 296.70413970947266 seconds\n",
      "Step 5400, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 302.2535455226898 seconds\n",
      "Step 5500, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 307.92690348625183 seconds\n",
      "Step 5600, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 313.4642143249512 seconds\n",
      "Step 5700, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 319.197212934494 seconds\n",
      "Step 5800, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 324.8245303630829 seconds\n",
      "Step 5900, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 330.38206243515015 seconds\n",
      "Step 6000, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 336.04999828338623 seconds\n",
      "Step 6100, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 341.58787631988525 seconds\n",
      "Step 6200, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 347.25838255882263 seconds\n",
      "Step 6300, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 352.7960422039032 seconds\n",
      "Step 6400, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 358.6399314403534 seconds\n",
      "Step 6500, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 364.25202083587646 seconds\n",
      "Step 6600, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 369.7840898036957 seconds\n",
      "Step 6700, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 375.89074420928955 seconds\n",
      "Step 6800, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 381.51100182533264 seconds\n",
      "Step 6900, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 387.27424573898315 seconds\n",
      "Step 7000, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 392.81840777397156 seconds\n",
      "Step 7100, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 398.325875043869 seconds\n",
      "Step 7200, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 403.99767661094666 seconds\n",
      "Step 7300, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 409.5253632068634 seconds\n",
      "Step 7400, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 415.4555072784424 seconds\n",
      "Step 7500, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 421.66302371025085 seconds\n",
      "Step 7600, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 427.3311712741852 seconds\n",
      "Step 7700, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 432.8495693206787 seconds\n",
      "Step 7800, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 438.3645496368408 seconds\n",
      "Step 7900, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 444.00831604003906 seconds\n",
      "Step 8000, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 449.578262090683 seconds\n",
      "Step 8100, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 455.27104330062866 seconds\n",
      "Step 8200, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 460.7971031665802 seconds\n",
      "Step 8300, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 466.7464988231659 seconds\n",
      "Step 8400, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 472.25097918510437 seconds\n",
      "Step 8500, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 477.88806891441345 seconds\n",
      "Step 8600, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 483.57876920700073 seconds\n",
      "Step 8700, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 489.1113452911377 seconds\n",
      "Step 8800, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 494.8112382888794 seconds\n",
      "Step 8900, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 500.3266201019287 seconds\n",
      "Step 9000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 506.597225189209 seconds\n",
      "Step 9100, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 512.2588629722595 seconds\n",
      "Step 9200, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 517.7998020648956 seconds\n",
      "Step 9300, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 523.4406147003174 seconds\n",
      "Step 9400, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 528.9471147060394 seconds\n",
      "Step 9500, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 534.7023177146912 seconds\n",
      "Step 9600, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 540.3270432949066 seconds\n",
      "Step 9700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 545.8698856830597 seconds\n",
      "Step 9800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 551.81081199646 seconds\n",
      "Step 9900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 557.3365721702576 seconds\n",
      "Step 10000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 563.3430712223053 seconds\n",
      "Step 10100, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 569.7208981513977 seconds\n",
      "Step 10200, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 575.2739131450653 seconds\n",
      "Step 10300, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 580.9762489795685 seconds\n",
      "Step 10400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 586.5085785388947 seconds\n",
      "Step 10500, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 592.2192385196686 seconds\n",
      "Step 10600, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 597.767112493515 seconds\n",
      "Step 10700, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 604.2446157932281 seconds\n",
      "Step 10800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 609.7682318687439 seconds\n",
      "Step 10900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 615.2788240909576 seconds\n",
      "Step 11000, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 620.9516227245331 seconds\n",
      "Step 11100, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 626.4922964572906 seconds\n",
      "Step 11200, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 632.1776237487793 seconds\n",
      "Step 11300, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 637.742990732193 seconds\n",
      "Step 11400, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 643.2694666385651 seconds\n",
      "Step 11500, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 649.0515460968018 seconds\n",
      "Step 11600, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 654.5677449703217 seconds\n",
      "Step 11700, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 660.2502286434174 seconds\n",
      "Step 11800, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 666.0377202033997 seconds\n",
      "Step 11900, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 671.7269186973572 seconds\n",
      "Step 12000, loss: tensor(0.0040, grad_fn=<SubBackward0>), time elapsed: 677.2341940402985 seconds\n",
      "Step 12100, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 682.7891128063202 seconds\n",
      "Step 12200, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 688.6123144626617 seconds\n",
      "Step 12300, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 694.1348204612732 seconds\n",
      "Step 12400, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 699.8256435394287 seconds\n",
      "Step 12500, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 705.3614857196808 seconds\n",
      "Step 12600, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 711.7527163028717 seconds\n",
      "Step 12700, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 717.4527184963226 seconds\n",
      "Step 12800, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 722.9872674942017 seconds\n",
      "Step 12900, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 728.6871538162231 seconds\n",
      "Step 13000, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 734.2187469005585 seconds\n",
      "Step 13100, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 739.9043757915497 seconds\n",
      "Step 13200, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 745.4230892658234 seconds\n",
      "Step 13300, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 750.9631769657135 seconds\n",
      "Step 13400, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 756.6762316226959 seconds\n",
      "Step 13500, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 762.2563395500183 seconds\n",
      "Step 13600, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 767.9647417068481 seconds\n",
      "Step 13700, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 773.5234129428864 seconds\n",
      "Step 13800, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 779.0766017436981 seconds\n",
      "Step 13900, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 784.7675385475159 seconds\n",
      "Step 14000, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 790.3265745639801 seconds\n",
      "Step 14100, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 796.0145297050476 seconds\n",
      "Step 14200, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 801.5558896064758 seconds\n",
      "Step 14300, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 807.2471420764923 seconds\n",
      "Step 14400, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 813.3567261695862 seconds\n",
      "Step 14500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 818.9959998130798 seconds\n",
      "Step 14600, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 824.6800301074982 seconds\n",
      "Step 14700, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 830.2172901630402 seconds\n",
      "Step 14800, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 835.9200096130371 seconds\n",
      "Step 14900, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 841.5322279930115 seconds\n",
      "Step 15000, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 847.1124696731567 seconds\n",
      "Step 15100, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 852.8409266471863 seconds\n",
      "Step 15200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 858.4093344211578 seconds\n",
      "Step 15300, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 864.1285221576691 seconds\n",
      "Step 15400, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 869.869048833847 seconds\n",
      "Step 15500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 875.5591149330139 seconds\n",
      "Step 15600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 881.1263227462769 seconds\n",
      "Step 15700, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 886.6857192516327 seconds\n",
      "Step 15800, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 893.1289749145508 seconds\n",
      "Step 15900, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 898.6922428607941 seconds\n",
      "Step 16000, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 904.4506340026855 seconds\n",
      "Step 16100, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 910.1264073848724 seconds\n",
      "Step 16200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 915.7011206150055 seconds\n",
      "Step 16300, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 921.388402223587 seconds\n",
      "Step 16400, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 927.007621049881 seconds\n",
      "Step 16500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 932.8700768947601 seconds\n",
      "Step 16600, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 938.4455947875977 seconds\n",
      "Step 16700, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 944.2621092796326 seconds\n",
      "Step 16800, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 949.8038580417633 seconds\n",
      "Step 16900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 955.5364000797272 seconds\n",
      "Step 17000, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 961.2221078872681 seconds\n",
      "Step 17100, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 966.7851836681366 seconds\n",
      "Step 17200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 972.5156166553497 seconds\n",
      "Step 17300, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 978.05206823349 seconds\n",
      "Step 17400, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 984.0827808380127 seconds\n",
      "Step 17500, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 989.7854316234589 seconds\n",
      "Step 17600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 995.3320541381836 seconds\n",
      "Step 17700, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1001.035924911499 seconds\n",
      "Step 17800, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1006.5751619338989 seconds\n",
      "Step 17900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1012.3049705028534 seconds\n",
      "Step 18000, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1018.004804611206 seconds\n",
      "Step 18100, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1023.5471153259277 seconds\n",
      "Step 18200, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1029.2805697917938 seconds\n",
      "Step 18300, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1034.8929278850555 seconds\n",
      "Step 18400, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1041.231393814087 seconds\n",
      "Step 18500, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1046.7825932502747 seconds\n",
      "Step 18600, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1052.319095134735 seconds\n",
      "Step 18700, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1058.0139565467834 seconds\n",
      "Step 18800, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1063.5568821430206 seconds\n",
      "Step 18900, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1069.2605321407318 seconds\n",
      "Step 19000, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1074.7930104732513 seconds\n",
      "Step 19100, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 1080.3536577224731 seconds\n",
      "Step 19200, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1086.085501909256 seconds\n",
      "Step 19300, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1091.6239614486694 seconds\n",
      "Step 19400, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1098.9929072856903 seconds\n",
      "Step 19500, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1104.5197582244873 seconds\n",
      "Step 19600, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 1110.0379328727722 seconds\n",
      "Step 19700, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1115.7503135204315 seconds\n",
      "Step 19800, loss: tensor(0.0026, grad_fn=<SubBackward0>), time elapsed: 1121.318619966507 seconds\n",
      "Step 19900, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 1127.0641932487488 seconds\n",
      "Step 20000, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 1132.618617773056 seconds\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 0.06322193145751953 seconds\n",
      "Step 100, loss: tensor(0.0689, grad_fn=<SubBackward0>), time elapsed: 5.83461856842041 seconds\n",
      "Step 200, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 11.412747859954834 seconds\n",
      "Step 300, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 16.91568922996521 seconds\n",
      "Step 400, loss: tensor(0.0408, grad_fn=<SubBackward0>), time elapsed: 22.545907735824585 seconds\n",
      "Step 500, loss: tensor(0.0369, grad_fn=<SubBackward0>), time elapsed: 28.069189310073853 seconds\n",
      "Step 600, loss: tensor(0.0338, grad_fn=<SubBackward0>), time elapsed: 33.705358028411865 seconds\n",
      "Step 700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 39.2135329246521 seconds\n",
      "Step 800, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 44.86344289779663 seconds\n",
      "Step 900, loss: tensor(0.0285, grad_fn=<SubBackward0>), time elapsed: 50.377275466918945 seconds\n",
      "Step 1000, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 55.92368268966675 seconds\n",
      "Step 1100, loss: tensor(0.0253, grad_fn=<SubBackward0>), time elapsed: 61.576396226882935 seconds\n",
      "Step 1200, loss: tensor(0.0252, grad_fn=<SubBackward0>), time elapsed: 67.10011005401611 seconds\n",
      "Step 1300, loss: tensor(0.0247, grad_fn=<SubBackward0>), time elapsed: 72.74263453483582 seconds\n",
      "Step 1400, loss: tensor(0.0240, grad_fn=<SubBackward0>), time elapsed: 78.24697232246399 seconds\n",
      "Step 1500, loss: tensor(0.0236, grad_fn=<SubBackward0>), time elapsed: 83.75945353507996 seconds\n",
      "Step 1600, loss: tensor(0.0237, grad_fn=<SubBackward0>), time elapsed: 89.40973711013794 seconds\n",
      "Step 1700, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 94.92094326019287 seconds\n",
      "Step 1800, loss: tensor(0.0225, grad_fn=<SubBackward0>), time elapsed: 100.57371044158936 seconds\n",
      "Step 1900, loss: tensor(0.0227, grad_fn=<SubBackward0>), time elapsed: 106.09973549842834 seconds\n",
      "Step 2000, loss: tensor(0.0228, grad_fn=<SubBackward0>), time elapsed: 111.7511990070343 seconds\n",
      "Step 2100, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 117.27934122085571 seconds\n",
      "Step 2200, loss: tensor(0.0217, grad_fn=<SubBackward0>), time elapsed: 122.79649782180786 seconds\n",
      "Step 2300, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 128.4202916622162 seconds\n",
      "Step 2400, loss: tensor(0.0217, grad_fn=<SubBackward0>), time elapsed: 133.92150163650513 seconds\n",
      "Step 2500, loss: tensor(0.0210, grad_fn=<SubBackward0>), time elapsed: 139.56496572494507 seconds\n",
      "Step 2600, loss: tensor(0.0207, grad_fn=<SubBackward0>), time elapsed: 145.07069373130798 seconds\n",
      "Step 2700, loss: tensor(0.0209, grad_fn=<SubBackward0>), time elapsed: 150.7434606552124 seconds\n",
      "Step 2800, loss: tensor(0.0206, grad_fn=<SubBackward0>), time elapsed: 156.2567753791809 seconds\n",
      "Step 2900, loss: tensor(0.0199, grad_fn=<SubBackward0>), time elapsed: 161.75443148612976 seconds\n",
      "Step 3000, loss: tensor(0.0200, grad_fn=<SubBackward0>), time elapsed: 167.39836502075195 seconds\n",
      "Step 3100, loss: tensor(0.0199, grad_fn=<SubBackward0>), time elapsed: 172.89619493484497 seconds\n",
      "Step 3200, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 178.5434534549713 seconds\n",
      "Step 3300, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 184.04618620872498 seconds\n",
      "Step 3400, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 189.705482006073 seconds\n",
      "Step 3500, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 195.21847891807556 seconds\n",
      "Step 3600, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 200.75083231925964 seconds\n",
      "Step 3700, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 206.37914180755615 seconds\n",
      "Step 3800, loss: tensor(0.0178, grad_fn=<SubBackward0>), time elapsed: 211.9063422679901 seconds\n",
      "Step 3900, loss: tensor(0.0176, grad_fn=<SubBackward0>), time elapsed: 217.54955053329468 seconds\n",
      "Step 4000, loss: tensor(0.0172, grad_fn=<SubBackward0>), time elapsed: 223.05497694015503 seconds\n",
      "Step 4100, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 228.54956603050232 seconds\n",
      "Step 4200, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 234.18584823608398 seconds\n",
      "Step 4300, loss: tensor(0.0174, grad_fn=<SubBackward0>), time elapsed: 239.73816084861755 seconds\n",
      "Step 4400, loss: tensor(0.0167, grad_fn=<SubBackward0>), time elapsed: 245.392560005188 seconds\n",
      "Step 4500, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 250.9028398990631 seconds\n",
      "Step 4600, loss: tensor(0.0169, grad_fn=<SubBackward0>), time elapsed: 256.55813479423523 seconds\n",
      "Step 4700, loss: tensor(0.0170, grad_fn=<SubBackward0>), time elapsed: 262.06742453575134 seconds\n",
      "Step 4800, loss: tensor(0.0170, grad_fn=<SubBackward0>), time elapsed: 267.5618414878845 seconds\n",
      "Step 4900, loss: tensor(0.0166, grad_fn=<SubBackward0>), time elapsed: 273.2138249874115 seconds\n",
      "Step 5000, loss: tensor(0.0173, grad_fn=<SubBackward0>), time elapsed: 278.72084856033325 seconds\n",
      "Step 5100, loss: tensor(0.0170, grad_fn=<SubBackward0>), time elapsed: 284.38018321990967 seconds\n",
      "Step 5200, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 289.9037330150604 seconds\n",
      "Step 5300, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 295.56076669692993 seconds\n",
      "Step 5400, loss: tensor(0.0164, grad_fn=<SubBackward0>), time elapsed: 301.0850987434387 seconds\n",
      "Step 5500, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 306.60190510749817 seconds\n",
      "Step 5600, loss: tensor(0.0158, grad_fn=<SubBackward0>), time elapsed: 312.2172601222992 seconds\n",
      "Step 5700, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 317.72573947906494 seconds\n",
      "Step 5800, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 323.3908944129944 seconds\n",
      "Step 5900, loss: tensor(0.0160, grad_fn=<SubBackward0>), time elapsed: 328.91293597221375 seconds\n",
      "Step 6000, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 334.4490351676941 seconds\n",
      "Step 6100, loss: tensor(0.0159, grad_fn=<SubBackward0>), time elapsed: 340.11591267585754 seconds\n",
      "Step 6200, loss: tensor(0.0159, grad_fn=<SubBackward0>), time elapsed: 345.6530306339264 seconds\n",
      "Step 6300, loss: tensor(0.0161, grad_fn=<SubBackward0>), time elapsed: 351.3206310272217 seconds\n",
      "Step 6400, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 356.8043203353882 seconds\n",
      "Step 6500, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 362.48076820373535 seconds\n",
      "Step 6600, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 367.9849989414215 seconds\n",
      "Step 6700, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 373.5193500518799 seconds\n",
      "Step 6800, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 379.1690740585327 seconds\n",
      "Step 6900, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 384.6955668926239 seconds\n",
      "Step 7000, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 390.39300060272217 seconds\n",
      "Step 7100, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 395.91093015670776 seconds\n",
      "Step 7200, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 401.56850957870483 seconds\n",
      "Step 7300, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 407.06886315345764 seconds\n",
      "Step 7400, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 412.5798943042755 seconds\n",
      "Step 7500, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 418.2148959636688 seconds\n",
      "Step 7600, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 423.76298546791077 seconds\n",
      "Step 7700, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 429.4297094345093 seconds\n",
      "Step 7800, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 434.955441236496 seconds\n",
      "Step 7900, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 440.4715487957001 seconds\n",
      "Step 8000, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 446.14420795440674 seconds\n",
      "Step 8100, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 451.6760342121124 seconds\n",
      "Step 8200, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 457.3647093772888 seconds\n",
      "Step 8300, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 462.85499572753906 seconds\n",
      "Step 8400, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 468.5393283367157 seconds\n",
      "Step 8500, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 474.06827878952026 seconds\n",
      "Step 8600, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 479.65496802330017 seconds\n",
      "Step 8700, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 485.52892804145813 seconds\n",
      "Step 8800, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 491.0392019748688 seconds\n",
      "Step 8900, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 496.78982186317444 seconds\n",
      "Step 9000, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 502.3131630420685 seconds\n",
      "Step 9100, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 507.8075785636902 seconds\n",
      "Step 9200, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 514.1696376800537 seconds\n",
      "Step 9300, loss: tensor(0.0153, grad_fn=<SubBackward0>), time elapsed: 519.6949594020844 seconds\n",
      "Step 9400, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 525.3654274940491 seconds\n",
      "Step 9500, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 530.8940539360046 seconds\n",
      "Step 9600, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 536.5531287193298 seconds\n",
      "Step 9700, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 542.0600581169128 seconds\n",
      "Step 9800, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 547.5935516357422 seconds\n",
      "Step 9900, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 553.2464897632599 seconds\n",
      "Step 10000, loss: tensor(0.0156, grad_fn=<SubBackward0>), time elapsed: 558.7731363773346 seconds\n",
      "Step 10100, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 564.4267761707306 seconds\n",
      "Step 10200, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 569.9404149055481 seconds\n",
      "Step 10300, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 575.4875888824463 seconds\n",
      "Step 10400, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 581.1637029647827 seconds\n",
      "Step 10500, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 586.7012557983398 seconds\n",
      "Step 10600, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 592.3531670570374 seconds\n",
      "Step 10700, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 597.8717601299286 seconds\n",
      "Step 10800, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 603.5743863582611 seconds\n",
      "Step 10900, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 609.6531407833099 seconds\n",
      "Step 11000, loss: tensor(0.0152, grad_fn=<SubBackward0>), time elapsed: 615.1632578372955 seconds\n",
      "Step 11100, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 620.819319486618 seconds\n",
      "Step 11200, loss: tensor(0.0157, grad_fn=<SubBackward0>), time elapsed: 626.3386220932007 seconds\n",
      "Step 11300, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 631.9864356517792 seconds\n",
      "Step 11400, loss: tensor(0.0146, grad_fn=<SubBackward0>), time elapsed: 637.55304646492 seconds\n",
      "Step 11500, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 643.0971143245697 seconds\n",
      "Step 11600, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 648.7832217216492 seconds\n",
      "Step 11700, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 654.366052865982 seconds\n",
      "Step 11800, loss: tensor(0.0150, grad_fn=<SubBackward0>), time elapsed: 660.1416389942169 seconds\n",
      "Step 11900, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 665.7342166900635 seconds\n",
      "Step 12000, loss: tensor(0.0148, grad_fn=<SubBackward0>), time elapsed: 671.5316574573517 seconds\n",
      "Step 12100, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 677.1423602104187 seconds\n",
      "Step 12200, loss: tensor(0.0145, grad_fn=<SubBackward0>), time elapsed: 682.6961607933044 seconds\n",
      "Step 12300, loss: tensor(0.0146, grad_fn=<SubBackward0>), time elapsed: 688.6000134944916 seconds\n",
      "Step 12400, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 694.1332025527954 seconds\n",
      "Step 12500, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 699.8304252624512 seconds\n",
      "Step 12600, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 705.3661167621613 seconds\n",
      "Step 12700, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 712.2589218616486 seconds\n",
      "Step 12800, loss: tensor(0.0145, grad_fn=<SubBackward0>), time elapsed: 717.9504818916321 seconds\n",
      "Step 12900, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 723.4808104038239 seconds\n",
      "Step 13000, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 729.1604189872742 seconds\n",
      "Step 13100, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 734.681806564331 seconds\n",
      "Step 13200, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 740.3595407009125 seconds\n",
      "Step 13300, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 745.8751895427704 seconds\n",
      "Step 13400, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 751.4045221805573 seconds\n",
      "Step 13500, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 757.1050667762756 seconds\n",
      "Step 13600, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 762.6351418495178 seconds\n",
      "Step 13700, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 768.2979371547699 seconds\n",
      "Step 13800, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 773.8227667808533 seconds\n",
      "Step 13900, loss: tensor(0.0142, grad_fn=<SubBackward0>), time elapsed: 779.3503980636597 seconds\n",
      "Step 14000, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 785.0508601665497 seconds\n",
      "Step 14100, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 790.609979391098 seconds\n",
      "Step 14200, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 796.3252012729645 seconds\n",
      "Step 14300, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 801.8861904144287 seconds\n",
      "Step 14400, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 807.6142647266388 seconds\n",
      "Step 14500, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 813.1602501869202 seconds\n",
      "Step 14600, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 818.734500169754 seconds\n",
      "Step 14700, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 824.4499049186707 seconds\n",
      "Step 14800, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 830.0058379173279 seconds\n",
      "Step 14900, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 835.7266659736633 seconds\n",
      "Step 15000, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 841.2882122993469 seconds\n",
      "Step 15100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 846.8453500270844 seconds\n",
      "Step 15200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 852.5889163017273 seconds\n",
      "Step 15300, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 858.1241493225098 seconds\n",
      "Step 15400, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 863.8294382095337 seconds\n",
      "Step 15500, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 869.3628761768341 seconds\n",
      "Step 15600, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 875.0767500400543 seconds\n",
      "Step 15700, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 880.6597876548767 seconds\n",
      "Step 15800, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 886.266224861145 seconds\n",
      "Step 15900, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 892.097069978714 seconds\n",
      "Step 16000, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 897.6483228206635 seconds\n",
      "Step 16100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 903.9805343151093 seconds\n",
      "Step 16200, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 909.5568645000458 seconds\n",
      "Step 16300, loss: tensor(0.0133, grad_fn=<SubBackward0>), time elapsed: 915.1116745471954 seconds\n",
      "Step 16400, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 920.8176999092102 seconds\n",
      "Step 16500, loss: tensor(0.0138, grad_fn=<SubBackward0>), time elapsed: 926.4150812625885 seconds\n",
      "Step 16600, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 932.183146238327 seconds\n",
      "Step 16700, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 937.7109007835388 seconds\n",
      "Step 16800, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 943.2594132423401 seconds\n",
      "Step 16900, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 948.9404628276825 seconds\n",
      "Step 17000, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 954.4753234386444 seconds\n",
      "Step 17100, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 960.1712663173676 seconds\n",
      "Step 17200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 965.7073264122009 seconds\n",
      "Step 17300, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 971.4736466407776 seconds\n",
      "Step 17400, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 977.0224945545197 seconds\n",
      "Step 17500, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 982.5719709396362 seconds\n",
      "Step 17600, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 988.3179149627686 seconds\n",
      "Step 17700, loss: tensor(0.0139, grad_fn=<SubBackward0>), time elapsed: 996.4139888286591 seconds\n",
      "Step 17800, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1002.131195306778 seconds\n",
      "Step 17900, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1007.690274477005 seconds\n",
      "Step 18000, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 1013.2291300296783 seconds\n",
      "Step 18100, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1018.9218950271606 seconds\n",
      "Step 18200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1024.470463514328 seconds\n",
      "Step 18300, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1030.168726682663 seconds\n",
      "Step 18400, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1035.703349351883 seconds\n",
      "Step 18500, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 1041.3901181221008 seconds\n",
      "Step 18600, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 1046.8937797546387 seconds\n",
      "Step 18700, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 1052.4473781585693 seconds\n",
      "Step 18800, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1058.1331470012665 seconds\n",
      "Step 18900, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1063.6907050609589 seconds\n",
      "Step 19000, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 1069.4047546386719 seconds\n",
      "Step 19100, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 1074.9407842159271 seconds\n",
      "Step 19200, loss: tensor(0.0135, grad_fn=<SubBackward0>), time elapsed: 1080.771758556366 seconds\n",
      "Step 19300, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 1086.502452135086 seconds\n",
      "Step 19400, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1092.1167259216309 seconds\n",
      "Step 19500, loss: tensor(0.0128, grad_fn=<SubBackward0>), time elapsed: 1097.8725972175598 seconds\n",
      "Step 19600, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 1103.4740679264069 seconds\n",
      "Step 19700, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 1109.0879645347595 seconds\n",
      "Step 19800, loss: tensor(0.0133, grad_fn=<SubBackward0>), time elapsed: 1114.8645322322845 seconds\n",
      "Step 19900, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1120.5005927085876 seconds\n",
      "Step 20000, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 1126.3305914402008 seconds\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.1059, grad_fn=<SubBackward0>), time elapsed: 0.06616783142089844 seconds\n",
      "Step 100, loss: tensor(0.0958, grad_fn=<SubBackward0>), time elapsed: 5.824069261550903 seconds\n",
      "Step 200, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 11.636232852935791 seconds\n",
      "Step 300, loss: tensor(0.0730, grad_fn=<SubBackward0>), time elapsed: 17.208541870117188 seconds\n",
      "Step 400, loss: tensor(0.0632, grad_fn=<SubBackward0>), time elapsed: 22.782327890396118 seconds\n",
      "Step 500, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 28.49082112312317 seconds\n",
      "Step 600, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 34.10900259017944 seconds\n",
      "Step 700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 39.83146905899048 seconds\n",
      "Step 800, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 45.426713943481445 seconds\n",
      "Step 900, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 51.175193786621094 seconds\n",
      "Step 1000, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 56.74862003326416 seconds\n",
      "Step 1100, loss: tensor(0.0456, grad_fn=<SubBackward0>), time elapsed: 62.387656688690186 seconds\n",
      "Step 1200, loss: tensor(0.0440, grad_fn=<SubBackward0>), time elapsed: 68.11637139320374 seconds\n",
      "Step 1300, loss: tensor(0.0458, grad_fn=<SubBackward0>), time elapsed: 73.72607755661011 seconds\n",
      "Step 1400, loss: tensor(0.0420, grad_fn=<SubBackward0>), time elapsed: 79.46331071853638 seconds\n",
      "Step 1500, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 85.06915163993835 seconds\n",
      "Step 1600, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 90.72067260742188 seconds\n",
      "Step 1700, loss: tensor(0.0432, grad_fn=<SubBackward0>), time elapsed: 96.44718384742737 seconds\n",
      "Step 1800, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 102.06744599342346 seconds\n",
      "Step 1900, loss: tensor(0.0425, grad_fn=<SubBackward0>), time elapsed: 107.82430076599121 seconds\n",
      "Step 2000, loss: tensor(0.0415, grad_fn=<SubBackward0>), time elapsed: 113.40604209899902 seconds\n",
      "Step 2100, loss: tensor(0.0415, grad_fn=<SubBackward0>), time elapsed: 119.14943146705627 seconds\n",
      "Step 2200, loss: tensor(0.0419, grad_fn=<SubBackward0>), time elapsed: 124.76355218887329 seconds\n",
      "Step 2300, loss: tensor(0.0416, grad_fn=<SubBackward0>), time elapsed: 130.34821605682373 seconds\n",
      "Step 2400, loss: tensor(0.0397, grad_fn=<SubBackward0>), time elapsed: 136.0873568058014 seconds\n",
      "Step 2500, loss: tensor(0.0410, grad_fn=<SubBackward0>), time elapsed: 141.6681718826294 seconds\n",
      "Step 2600, loss: tensor(0.0401, grad_fn=<SubBackward0>), time elapsed: 147.3904881477356 seconds\n",
      "Step 2700, loss: tensor(0.0395, grad_fn=<SubBackward0>), time elapsed: 153.02842211723328 seconds\n",
      "Step 2800, loss: tensor(0.0413, grad_fn=<SubBackward0>), time elapsed: 158.764568567276 seconds\n",
      "Step 2900, loss: tensor(0.0398, grad_fn=<SubBackward0>), time elapsed: 164.35350489616394 seconds\n",
      "Step 3000, loss: tensor(0.0398, grad_fn=<SubBackward0>), time elapsed: 169.94674110412598 seconds\n",
      "Step 3100, loss: tensor(0.0388, grad_fn=<SubBackward0>), time elapsed: 175.68269157409668 seconds\n",
      "Step 3200, loss: tensor(0.0408, grad_fn=<SubBackward0>), time elapsed: 181.29150938987732 seconds\n",
      "Step 3300, loss: tensor(0.0396, grad_fn=<SubBackward0>), time elapsed: 187.0442521572113 seconds\n",
      "Step 3400, loss: tensor(0.0389, grad_fn=<SubBackward0>), time elapsed: 192.66985297203064 seconds\n",
      "Step 3500, loss: tensor(0.0388, grad_fn=<SubBackward0>), time elapsed: 198.42760467529297 seconds\n",
      "Step 3600, loss: tensor(0.0385, grad_fn=<SubBackward0>), time elapsed: 204.025945186615 seconds\n",
      "Step 3700, loss: tensor(0.0385, grad_fn=<SubBackward0>), time elapsed: 209.6463177204132 seconds\n",
      "Step 3800, loss: tensor(0.0383, grad_fn=<SubBackward0>), time elapsed: 215.38855743408203 seconds\n",
      "Step 3900, loss: tensor(0.0387, grad_fn=<SubBackward0>), time elapsed: 220.9858911037445 seconds\n",
      "Step 4000, loss: tensor(0.0380, grad_fn=<SubBackward0>), time elapsed: 226.7168788909912 seconds\n",
      "Step 4100, loss: tensor(0.0378, grad_fn=<SubBackward0>), time elapsed: 232.41851997375488 seconds\n",
      "Step 4200, loss: tensor(0.0389, grad_fn=<SubBackward0>), time elapsed: 238.44736790657043 seconds\n",
      "Step 4300, loss: tensor(0.0379, grad_fn=<SubBackward0>), time elapsed: 244.4294035434723 seconds\n",
      "Step 4400, loss: tensor(0.0375, grad_fn=<SubBackward0>), time elapsed: 250.17819380760193 seconds\n",
      "Step 4500, loss: tensor(0.0378, grad_fn=<SubBackward0>), time elapsed: 255.92971086502075 seconds\n",
      "Step 4600, loss: tensor(0.0367, grad_fn=<SubBackward0>), time elapsed: 261.51554703712463 seconds\n",
      "Step 4700, loss: tensor(0.0381, grad_fn=<SubBackward0>), time elapsed: 267.24836468696594 seconds\n",
      "Step 4800, loss: tensor(0.0366, grad_fn=<SubBackward0>), time elapsed: 272.86780047416687 seconds\n",
      "Step 4900, loss: tensor(0.0365, grad_fn=<SubBackward0>), time elapsed: 278.46096420288086 seconds\n",
      "Step 5000, loss: tensor(0.0367, grad_fn=<SubBackward0>), time elapsed: 284.1875593662262 seconds\n",
      "Step 5100, loss: tensor(0.0359, grad_fn=<SubBackward0>), time elapsed: 289.78953981399536 seconds\n",
      "Step 5200, loss: tensor(0.0365, grad_fn=<SubBackward0>), time elapsed: 295.50725746154785 seconds\n",
      "Step 5300, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 301.0879113674164 seconds\n",
      "Step 5400, loss: tensor(0.0366, grad_fn=<SubBackward0>), time elapsed: 306.86059641838074 seconds\n",
      "Step 5500, loss: tensor(0.0359, grad_fn=<SubBackward0>), time elapsed: 312.4495904445648 seconds\n",
      "Step 5600, loss: tensor(0.0376, grad_fn=<SubBackward0>), time elapsed: 318.0509395599365 seconds\n",
      "Step 5700, loss: tensor(0.0357, grad_fn=<SubBackward0>), time elapsed: 323.7900860309601 seconds\n",
      "Step 5800, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 329.4051899909973 seconds\n",
      "Step 5900, loss: tensor(0.0356, grad_fn=<SubBackward0>), time elapsed: 335.1671905517578 seconds\n",
      "Step 6000, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 340.77831864356995 seconds\n",
      "Step 6100, loss: tensor(0.0372, grad_fn=<SubBackward0>), time elapsed: 346.3879904747009 seconds\n",
      "Step 6200, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 352.1354808807373 seconds\n",
      "Step 6300, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 357.7510552406311 seconds\n",
      "Step 6400, loss: tensor(0.0361, grad_fn=<SubBackward0>), time elapsed: 363.4886496067047 seconds\n",
      "Step 6500, loss: tensor(0.0347, grad_fn=<SubBackward0>), time elapsed: 369.13188123703003 seconds\n",
      "Step 6600, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 374.8877184391022 seconds\n",
      "Step 6700, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 380.48923349380493 seconds\n",
      "Step 6800, loss: tensor(0.0346, grad_fn=<SubBackward0>), time elapsed: 386.10632848739624 seconds\n",
      "Step 6900, loss: tensor(0.0360, grad_fn=<SubBackward0>), time elapsed: 391.87999534606934 seconds\n",
      "Step 7000, loss: tensor(0.0342, grad_fn=<SubBackward0>), time elapsed: 397.5149748325348 seconds\n",
      "Step 7100, loss: tensor(0.0360, grad_fn=<SubBackward0>), time elapsed: 403.2708339691162 seconds\n",
      "Step 7200, loss: tensor(0.0346, grad_fn=<SubBackward0>), time elapsed: 408.8512876033783 seconds\n",
      "Step 7300, loss: tensor(0.0347, grad_fn=<SubBackward0>), time elapsed: 414.5831563472748 seconds\n",
      "Step 7400, loss: tensor(0.0346, grad_fn=<SubBackward0>), time elapsed: 420.16295337677 seconds\n",
      "Step 7500, loss: tensor(0.0344, grad_fn=<SubBackward0>), time elapsed: 425.7872052192688 seconds\n",
      "Step 7600, loss: tensor(0.0352, grad_fn=<SubBackward0>), time elapsed: 431.49931502342224 seconds\n",
      "Step 7700, loss: tensor(0.0341, grad_fn=<SubBackward0>), time elapsed: 437.08839750289917 seconds\n",
      "Step 7800, loss: tensor(0.0349, grad_fn=<SubBackward0>), time elapsed: 442.83047103881836 seconds\n",
      "Step 7900, loss: tensor(0.0342, grad_fn=<SubBackward0>), time elapsed: 448.43738055229187 seconds\n",
      "Step 8000, loss: tensor(0.0354, grad_fn=<SubBackward0>), time elapsed: 454.030410528183 seconds\n",
      "Step 8100, loss: tensor(0.0345, grad_fn=<SubBackward0>), time elapsed: 459.80301094055176 seconds\n",
      "Step 8200, loss: tensor(0.0338, grad_fn=<SubBackward0>), time elapsed: 465.40860772132874 seconds\n",
      "Step 8300, loss: tensor(0.0336, grad_fn=<SubBackward0>), time elapsed: 471.1604859828949 seconds\n",
      "Step 8400, loss: tensor(0.0343, grad_fn=<SubBackward0>), time elapsed: 476.7702889442444 seconds\n",
      "Step 8500, loss: tensor(0.0341, grad_fn=<SubBackward0>), time elapsed: 482.5177619457245 seconds\n",
      "Step 8600, loss: tensor(0.0333, grad_fn=<SubBackward0>), time elapsed: 488.125364780426 seconds\n",
      "Step 8700, loss: tensor(0.0337, grad_fn=<SubBackward0>), time elapsed: 493.7207808494568 seconds\n",
      "Step 8800, loss: tensor(0.0335, grad_fn=<SubBackward0>), time elapsed: 499.44727063179016 seconds\n",
      "Step 8900, loss: tensor(0.0334, grad_fn=<SubBackward0>), time elapsed: 505.0487976074219 seconds\n",
      "Step 9000, loss: tensor(0.0326, grad_fn=<SubBackward0>), time elapsed: 510.8087956905365 seconds\n",
      "Step 9100, loss: tensor(0.0325, grad_fn=<SubBackward0>), time elapsed: 516.4437403678894 seconds\n",
      "Step 9200, loss: tensor(0.0326, grad_fn=<SubBackward0>), time elapsed: 522.0744252204895 seconds\n",
      "Step 9300, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 527.8428001403809 seconds\n",
      "Step 9400, loss: tensor(0.0323, grad_fn=<SubBackward0>), time elapsed: 533.448403596878 seconds\n",
      "Step 9500, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 539.2084066867828 seconds\n",
      "Step 9600, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 544.8234820365906 seconds\n",
      "Step 9700, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 550.6012051105499 seconds\n",
      "Step 9800, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 556.2175161838531 seconds\n",
      "Step 9900, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 561.8105053901672 seconds\n",
      "Step 10000, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 567.5789973735809 seconds\n",
      "Step 10100, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 573.1820876598358 seconds\n",
      "Step 10200, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 578.9475665092468 seconds\n",
      "Step 10300, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 584.5335021018982 seconds\n",
      "Step 10400, loss: tensor(0.0285, grad_fn=<SubBackward0>), time elapsed: 590.1267306804657 seconds\n",
      "Step 10500, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 595.8492512702942 seconds\n",
      "Step 10600, loss: tensor(0.0288, grad_fn=<SubBackward0>), time elapsed: 601.4506447315216 seconds\n",
      "Step 10700, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 607.1751706600189 seconds\n",
      "Step 10800, loss: tensor(0.0275, grad_fn=<SubBackward0>), time elapsed: 612.8165068626404 seconds\n",
      "Step 10900, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 618.5638966560364 seconds\n",
      "Step 11000, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 624.148665189743 seconds\n",
      "Step 11100, loss: tensor(0.0287, grad_fn=<SubBackward0>), time elapsed: 629.7835993766785 seconds\n",
      "Step 11200, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 635.5604162216187 seconds\n",
      "Step 11300, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 641.1909282207489 seconds\n",
      "Step 11400, loss: tensor(0.0288, grad_fn=<SubBackward0>), time elapsed: 646.965115070343 seconds\n",
      "Step 11500, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 652.5692844390869 seconds\n",
      "Step 11600, loss: tensor(0.0266, grad_fn=<SubBackward0>), time elapsed: 658.2081608772278 seconds\n",
      "Step 11700, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 663.9683468341827 seconds\n",
      "Step 11800, loss: tensor(0.0292, grad_fn=<SubBackward0>), time elapsed: 669.6072957515717 seconds\n",
      "Step 11900, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 675.3505568504333 seconds\n",
      "Step 12000, loss: tensor(0.0270, grad_fn=<SubBackward0>), time elapsed: 680.9812934398651 seconds\n",
      "Step 12100, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 686.7163825035095 seconds\n",
      "Step 12200, loss: tensor(0.0279, grad_fn=<SubBackward0>), time elapsed: 692.3448166847229 seconds\n",
      "Step 12300, loss: tensor(0.0287, grad_fn=<SubBackward0>), time elapsed: 697.9359226226807 seconds\n",
      "Step 12400, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 703.7169232368469 seconds\n",
      "Step 12500, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 709.3245310783386 seconds\n",
      "Step 12600, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 715.1262345314026 seconds\n",
      "Step 12700, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 720.7340157032013 seconds\n",
      "Step 12800, loss: tensor(0.0284, grad_fn=<SubBackward0>), time elapsed: 726.3482277393341 seconds\n",
      "Step 12900, loss: tensor(0.0269, grad_fn=<SubBackward0>), time elapsed: 732.1165990829468 seconds\n",
      "Step 13000, loss: tensor(0.0279, grad_fn=<SubBackward0>), time elapsed: 737.7251334190369 seconds\n",
      "Step 13100, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 743.4768006801605 seconds\n",
      "Step 13200, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 749.10036444664 seconds\n",
      "Step 13300, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 754.8687863349915 seconds\n",
      "Step 13400, loss: tensor(0.0278, grad_fn=<SubBackward0>), time elapsed: 760.4909715652466 seconds\n",
      "Step 13500, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 766.0884618759155 seconds\n",
      "Step 13600, loss: tensor(0.0278, grad_fn=<SubBackward0>), time elapsed: 771.865076303482 seconds\n",
      "Step 13700, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 777.4498534202576 seconds\n",
      "Step 13800, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 783.2098331451416 seconds\n",
      "Step 13900, loss: tensor(0.0273, grad_fn=<SubBackward0>), time elapsed: 788.8198375701904 seconds\n",
      "Step 14000, loss: tensor(0.0267, grad_fn=<SubBackward0>), time elapsed: 794.4630641937256 seconds\n",
      "Step 14100, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 800.2312560081482 seconds\n",
      "Step 14200, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 805.8621368408203 seconds\n",
      "Step 14300, loss: tensor(0.0280, grad_fn=<SubBackward0>), time elapsed: 811.6220273971558 seconds\n",
      "Step 14400, loss: tensor(0.0278, grad_fn=<SubBackward0>), time elapsed: 817.2432281970978 seconds\n",
      "Step 14500, loss: tensor(0.0272, grad_fn=<SubBackward0>), time elapsed: 822.9845175743103 seconds\n",
      "Step 14600, loss: tensor(0.0276, grad_fn=<SubBackward0>), time elapsed: 828.5151083469391 seconds\n",
      "Step 14700, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 834.0600528717041 seconds\n",
      "Step 14800, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 839.7112228870392 seconds\n",
      "Step 14900, loss: tensor(0.0259, grad_fn=<SubBackward0>), time elapsed: 845.247392654419 seconds\n",
      "Step 15000, loss: tensor(0.0259, grad_fn=<SubBackward0>), time elapsed: 850.9296207427979 seconds\n",
      "Step 15100, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 856.4918527603149 seconds\n",
      "Step 15200, loss: tensor(0.0260, grad_fn=<SubBackward0>), time elapsed: 862.0103023052216 seconds\n",
      "Step 15300, loss: tensor(0.0268, grad_fn=<SubBackward0>), time elapsed: 867.6603178977966 seconds\n",
      "Step 15400, loss: tensor(0.0265, grad_fn=<SubBackward0>), time elapsed: 873.1879260540009 seconds\n",
      "Step 15500, loss: tensor(0.0249, grad_fn=<SubBackward0>), time elapsed: 878.8714692592621 seconds\n",
      "Step 15600, loss: tensor(0.0247, grad_fn=<SubBackward0>), time elapsed: 884.4213447570801 seconds\n",
      "Step 15700, loss: tensor(0.0243, grad_fn=<SubBackward0>), time elapsed: 890.0947844982147 seconds\n",
      "Step 15800, loss: tensor(0.0234, grad_fn=<SubBackward0>), time elapsed: 895.6176810264587 seconds\n",
      "Step 15900, loss: tensor(0.0241, grad_fn=<SubBackward0>), time elapsed: 901.1578793525696 seconds\n",
      "Step 16000, loss: tensor(0.0236, grad_fn=<SubBackward0>), time elapsed: 906.8347179889679 seconds\n",
      "Step 16100, loss: tensor(0.0238, grad_fn=<SubBackward0>), time elapsed: 912.3777613639832 seconds\n",
      "Step 16200, loss: tensor(0.0245, grad_fn=<SubBackward0>), time elapsed: 918.0590560436249 seconds\n",
      "Step 16300, loss: tensor(0.0233, grad_fn=<SubBackward0>), time elapsed: 923.5902149677277 seconds\n",
      "Step 16400, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 929.1205244064331 seconds\n",
      "Step 16500, loss: tensor(0.0231, grad_fn=<SubBackward0>), time elapsed: 934.8303906917572 seconds\n",
      "Step 16600, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 940.3862192630768 seconds\n",
      "Step 16700, loss: tensor(0.0224, grad_fn=<SubBackward0>), time elapsed: 946.1100995540619 seconds\n",
      "Step 16800, loss: tensor(0.0234, grad_fn=<SubBackward0>), time elapsed: 951.6924631595612 seconds\n",
      "Step 16900, loss: tensor(0.0233, grad_fn=<SubBackward0>), time elapsed: 957.2688591480255 seconds\n",
      "Step 17000, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 962.9609189033508 seconds\n",
      "Step 17100, loss: tensor(0.0215, grad_fn=<SubBackward0>), time elapsed: 968.5173273086548 seconds\n",
      "Step 17200, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 974.2397768497467 seconds\n",
      "Step 17300, loss: tensor(0.0225, grad_fn=<SubBackward0>), time elapsed: 979.8187167644501 seconds\n",
      "Step 17400, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 985.5411322116852 seconds\n",
      "Step 17500, loss: tensor(0.0234, grad_fn=<SubBackward0>), time elapsed: 991.1137363910675 seconds\n",
      "Step 17600, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 996.6851723194122 seconds\n",
      "Step 17700, loss: tensor(0.0224, grad_fn=<SubBackward0>), time elapsed: 1002.4325168132782 seconds\n",
      "Step 17800, loss: tensor(0.0222, grad_fn=<SubBackward0>), time elapsed: 1007.994250535965 seconds\n",
      "Step 17900, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 1013.676043510437 seconds\n",
      "Step 18000, loss: tensor(0.0222, grad_fn=<SubBackward0>), time elapsed: 1019.1957190036774 seconds\n",
      "Step 18100, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 1024.7476246356964 seconds\n",
      "Step 18200, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 1030.4712381362915 seconds\n",
      "Step 18300, loss: tensor(0.0224, grad_fn=<SubBackward0>), time elapsed: 1036.0457317829132 seconds\n",
      "Step 18400, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 1041.7859308719635 seconds\n",
      "Step 18500, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 1047.3135526180267 seconds\n",
      "Step 18600, loss: tensor(0.0208, grad_fn=<SubBackward0>), time elapsed: 1053.0455577373505 seconds\n",
      "Step 18700, loss: tensor(0.0213, grad_fn=<SubBackward0>), time elapsed: 1058.5802955627441 seconds\n",
      "Step 18800, loss: tensor(0.0221, grad_fn=<SubBackward0>), time elapsed: 1064.1203138828278 seconds\n",
      "Step 18900, loss: tensor(0.0225, grad_fn=<SubBackward0>), time elapsed: 1069.8612401485443 seconds\n",
      "Step 19000, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 1075.4463152885437 seconds\n",
      "Step 19100, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 1081.168471813202 seconds\n",
      "Step 19200, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 1086.7281725406647 seconds\n",
      "Step 19300, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 1092.2716686725616 seconds\n",
      "Step 19400, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 1098.0013637542725 seconds\n",
      "Step 19500, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 1103.5487480163574 seconds\n",
      "Step 19600, loss: tensor(0.0229, grad_fn=<SubBackward0>), time elapsed: 1109.264904975891 seconds\n",
      "Step 19700, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 1114.833734512329 seconds\n",
      "Step 19800, loss: tensor(0.0207, grad_fn=<SubBackward0>), time elapsed: 1120.4274525642395 seconds\n",
      "Step 19900, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 1126.199602842331 seconds\n",
      "Step 20000, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 1131.801593542099 seconds\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.2090, grad_fn=<SubBackward0>), time elapsed: 0.062062740325927734 seconds\n",
      "Step 100, loss: tensor(0.1864, grad_fn=<SubBackward0>), time elapsed: 5.914261102676392 seconds\n",
      "Step 200, loss: tensor(0.1532, grad_fn=<SubBackward0>), time elapsed: 11.539004802703857 seconds\n",
      "Step 300, loss: tensor(0.1291, grad_fn=<SubBackward0>), time elapsed: 17.18545126914978 seconds\n",
      "Step 400, loss: tensor(0.1158, grad_fn=<SubBackward0>), time elapsed: 22.699230670928955 seconds\n",
      "Step 500, loss: tensor(0.1083, grad_fn=<SubBackward0>), time elapsed: 28.213378429412842 seconds\n",
      "Step 600, loss: tensor(0.1061, grad_fn=<SubBackward0>), time elapsed: 33.86864686012268 seconds\n",
      "Step 700, loss: tensor(0.0982, grad_fn=<SubBackward0>), time elapsed: 39.36850070953369 seconds\n",
      "Step 800, loss: tensor(0.0967, grad_fn=<SubBackward0>), time elapsed: 45.010740995407104 seconds\n",
      "Step 900, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 50.51896405220032 seconds\n",
      "Step 1000, loss: tensor(0.0895, grad_fn=<SubBackward0>), time elapsed: 56.18190121650696 seconds\n",
      "Step 1100, loss: tensor(0.0857, grad_fn=<SubBackward0>), time elapsed: 61.68891644477844 seconds\n",
      "Step 1200, loss: tensor(0.0828, grad_fn=<SubBackward0>), time elapsed: 67.23157668113708 seconds\n",
      "Step 1300, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 72.8789496421814 seconds\n",
      "Step 1400, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 78.40281128883362 seconds\n",
      "Step 1500, loss: tensor(0.0802, grad_fn=<SubBackward0>), time elapsed: 84.04602575302124 seconds\n",
      "Step 1600, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 89.56595087051392 seconds\n",
      "Step 1700, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 95.25049614906311 seconds\n",
      "Step 1800, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 100.75752830505371 seconds\n",
      "Step 1900, loss: tensor(0.0696, grad_fn=<SubBackward0>), time elapsed: 106.28764605522156 seconds\n",
      "Step 2000, loss: tensor(0.0709, grad_fn=<SubBackward0>), time elapsed: 111.93590235710144 seconds\n",
      "Step 2100, loss: tensor(0.0679, grad_fn=<SubBackward0>), time elapsed: 117.4632499217987 seconds\n",
      "Step 2200, loss: tensor(0.0669, grad_fn=<SubBackward0>), time elapsed: 123.10934591293335 seconds\n",
      "Step 2300, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 128.63653898239136 seconds\n",
      "Step 2400, loss: tensor(0.0664, grad_fn=<SubBackward0>), time elapsed: 134.15347528457642 seconds\n",
      "Step 2500, loss: tensor(0.0659, grad_fn=<SubBackward0>), time elapsed: 139.7987334728241 seconds\n",
      "Step 2600, loss: tensor(0.0657, grad_fn=<SubBackward0>), time elapsed: 145.32171392440796 seconds\n",
      "Step 2700, loss: tensor(0.0654, grad_fn=<SubBackward0>), time elapsed: 150.96953892707825 seconds\n",
      "Step 2800, loss: tensor(0.0665, grad_fn=<SubBackward0>), time elapsed: 156.52113819122314 seconds\n",
      "Step 2900, loss: tensor(0.0646, grad_fn=<SubBackward0>), time elapsed: 162.16967916488647 seconds\n",
      "Step 3000, loss: tensor(0.0647, grad_fn=<SubBackward0>), time elapsed: 167.6913652420044 seconds\n",
      "Step 3100, loss: tensor(0.0633, grad_fn=<SubBackward0>), time elapsed: 173.22821307182312 seconds\n",
      "Step 3200, loss: tensor(0.0637, grad_fn=<SubBackward0>), time elapsed: 178.8937427997589 seconds\n",
      "Step 3300, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 184.40462470054626 seconds\n",
      "Step 3400, loss: tensor(0.0609, grad_fn=<SubBackward0>), time elapsed: 190.06577563285828 seconds\n",
      "Step 3500, loss: tensor(0.0629, grad_fn=<SubBackward0>), time elapsed: 195.57733416557312 seconds\n",
      "Step 3600, loss: tensor(0.0620, grad_fn=<SubBackward0>), time elapsed: 201.23616933822632 seconds\n",
      "Step 3700, loss: tensor(0.0621, grad_fn=<SubBackward0>), time elapsed: 206.74548983573914 seconds\n",
      "Step 3800, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 212.27950763702393 seconds\n",
      "Step 3900, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 217.9434633255005 seconds\n",
      "Step 4000, loss: tensor(0.0576, grad_fn=<SubBackward0>), time elapsed: 223.47261452674866 seconds\n",
      "Step 4100, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 229.13537549972534 seconds\n",
      "Step 4200, loss: tensor(0.0557, grad_fn=<SubBackward0>), time elapsed: 234.64012360572815 seconds\n",
      "Step 4300, loss: tensor(0.0549, grad_fn=<SubBackward0>), time elapsed: 240.31853723526 seconds\n",
      "Step 4400, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 245.85525012016296 seconds\n",
      "Step 4500, loss: tensor(0.0535, grad_fn=<SubBackward0>), time elapsed: 251.4261782169342 seconds\n",
      "Step 4600, loss: tensor(0.0552, grad_fn=<SubBackward0>), time elapsed: 257.1024417877197 seconds\n",
      "Step 4700, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 262.64177656173706 seconds\n",
      "Step 4800, loss: tensor(0.0547, grad_fn=<SubBackward0>), time elapsed: 268.3113284111023 seconds\n",
      "Step 4900, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 273.82122588157654 seconds\n",
      "Step 5000, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 279.385142326355 seconds\n",
      "Step 5100, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 285.16333174705505 seconds\n",
      "Step 5200, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 290.68963384628296 seconds\n",
      "Step 5300, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 296.35582208633423 seconds\n",
      "Step 5400, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 301.911159992218 seconds\n",
      "Step 5500, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 307.6025302410126 seconds\n",
      "Step 5600, loss: tensor(0.0465, grad_fn=<SubBackward0>), time elapsed: 313.1151611804962 seconds\n",
      "Step 5700, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 319.11694526672363 seconds\n",
      "Step 5800, loss: tensor(0.0465, grad_fn=<SubBackward0>), time elapsed: 324.76825070381165 seconds\n",
      "Step 5900, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 330.26574420928955 seconds\n",
      "Step 6000, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 335.9507210254669 seconds\n",
      "Step 6100, loss: tensor(0.0449, grad_fn=<SubBackward0>), time elapsed: 341.48739290237427 seconds\n",
      "Step 6200, loss: tensor(0.0450, grad_fn=<SubBackward0>), time elapsed: 347.16762590408325 seconds\n",
      "Step 6300, loss: tensor(0.0439, grad_fn=<SubBackward0>), time elapsed: 352.67083287239075 seconds\n",
      "Step 6400, loss: tensor(0.0462, grad_fn=<SubBackward0>), time elapsed: 358.1870036125183 seconds\n",
      "Step 6500, loss: tensor(0.0450, grad_fn=<SubBackward0>), time elapsed: 363.8343918323517 seconds\n",
      "Step 6600, loss: tensor(0.0446, grad_fn=<SubBackward0>), time elapsed: 369.3621950149536 seconds\n",
      "Step 6700, loss: tensor(0.0448, grad_fn=<SubBackward0>), time elapsed: 374.99418210983276 seconds\n",
      "Step 6800, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 380.5062017440796 seconds\n",
      "Step 6900, loss: tensor(0.0439, grad_fn=<SubBackward0>), time elapsed: 386.0210180282593 seconds\n",
      "Step 7000, loss: tensor(0.0433, grad_fn=<SubBackward0>), time elapsed: 391.66776943206787 seconds\n",
      "Step 7100, loss: tensor(0.0434, grad_fn=<SubBackward0>), time elapsed: 397.18723583221436 seconds\n",
      "Step 7200, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 402.85593938827515 seconds\n",
      "Step 7300, loss: tensor(0.0409, grad_fn=<SubBackward0>), time elapsed: 408.3912341594696 seconds\n",
      "Step 7400, loss: tensor(0.0421, grad_fn=<SubBackward0>), time elapsed: 414.07877564430237 seconds\n",
      "Step 7500, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 419.5971019268036 seconds\n",
      "Step 7600, loss: tensor(0.0420, grad_fn=<SubBackward0>), time elapsed: 425.14655685424805 seconds\n",
      "Step 7700, loss: tensor(0.0417, grad_fn=<SubBackward0>), time elapsed: 430.87453508377075 seconds\n",
      "Step 7800, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 436.41236639022827 seconds\n",
      "Step 7900, loss: tensor(0.0422, grad_fn=<SubBackward0>), time elapsed: 442.12187337875366 seconds\n",
      "Step 8000, loss: tensor(0.0410, grad_fn=<SubBackward0>), time elapsed: 448.12255334854126 seconds\n",
      "Step 8100, loss: tensor(0.0414, grad_fn=<SubBackward0>), time elapsed: 453.64374113082886 seconds\n",
      "Step 8200, loss: tensor(0.0421, grad_fn=<SubBackward0>), time elapsed: 459.2941517829895 seconds\n",
      "Step 8300, loss: tensor(0.0417, grad_fn=<SubBackward0>), time elapsed: 464.8198552131653 seconds\n",
      "Step 8400, loss: tensor(0.0408, grad_fn=<SubBackward0>), time elapsed: 470.47235012054443 seconds\n",
      "Step 8500, loss: tensor(0.0397, grad_fn=<SubBackward0>), time elapsed: 475.9995937347412 seconds\n",
      "Step 8600, loss: tensor(0.0399, grad_fn=<SubBackward0>), time elapsed: 481.67139315605164 seconds\n",
      "Step 8700, loss: tensor(0.0377, grad_fn=<SubBackward0>), time elapsed: 487.20130038261414 seconds\n",
      "Step 8800, loss: tensor(0.0386, grad_fn=<SubBackward0>), time elapsed: 492.74923872947693 seconds\n",
      "Step 8900, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 498.4276397228241 seconds\n",
      "Step 9000, loss: tensor(0.0373, grad_fn=<SubBackward0>), time elapsed: 503.94417238235474 seconds\n",
      "Step 9100, loss: tensor(0.0372, grad_fn=<SubBackward0>), time elapsed: 509.6053774356842 seconds\n",
      "Step 9200, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 515.119019985199 seconds\n",
      "Step 9300, loss: tensor(0.0358, grad_fn=<SubBackward0>), time elapsed: 520.6624953746796 seconds\n",
      "Step 9400, loss: tensor(0.0354, grad_fn=<SubBackward0>), time elapsed: 526.8654954433441 seconds\n",
      "Step 9500, loss: tensor(0.0348, grad_fn=<SubBackward0>), time elapsed: 532.3984415531158 seconds\n",
      "Step 9600, loss: tensor(0.0350, grad_fn=<SubBackward0>), time elapsed: 538.051675081253 seconds\n",
      "Step 9700, loss: tensor(0.0340, grad_fn=<SubBackward0>), time elapsed: 543.5662083625793 seconds\n",
      "Step 9800, loss: tensor(0.0341, grad_fn=<SubBackward0>), time elapsed: 549.2285931110382 seconds\n",
      "Step 9900, loss: tensor(0.0335, grad_fn=<SubBackward0>), time elapsed: 554.750789642334 seconds\n",
      "Step 10000, loss: tensor(0.0321, grad_fn=<SubBackward0>), time elapsed: 560.2829451560974 seconds\n",
      "Step 10100, loss: tensor(0.0333, grad_fn=<SubBackward0>), time elapsed: 565.9349687099457 seconds\n",
      "Step 10200, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 571.4602138996124 seconds\n",
      "Step 10300, loss: tensor(0.0322, grad_fn=<SubBackward0>), time elapsed: 577.1338860988617 seconds\n",
      "Step 10400, loss: tensor(0.0323, grad_fn=<SubBackward0>), time elapsed: 582.669810295105 seconds\n",
      "Step 10500, loss: tensor(0.0327, grad_fn=<SubBackward0>), time elapsed: 588.1829915046692 seconds\n",
      "Step 10600, loss: tensor(0.0320, grad_fn=<SubBackward0>), time elapsed: 593.828771352768 seconds\n",
      "Step 10700, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 599.3431322574615 seconds\n",
      "Step 10800, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 605.0036957263947 seconds\n",
      "Step 10900, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 610.5558376312256 seconds\n",
      "Step 11000, loss: tensor(0.0327, grad_fn=<SubBackward0>), time elapsed: 616.2699506282806 seconds\n",
      "Step 11100, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 621.8092494010925 seconds\n",
      "Step 11200, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 627.3503448963165 seconds\n",
      "Step 11300, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 633.0298895835876 seconds\n",
      "Step 11400, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 638.5622799396515 seconds\n",
      "Step 11500, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 644.2561225891113 seconds\n",
      "Step 11600, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 649.7775931358337 seconds\n",
      "Step 11700, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 655.2888798713684 seconds\n",
      "Step 11800, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 660.997768163681 seconds\n",
      "Step 11900, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 666.5424988269806 seconds\n",
      "Step 12000, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 672.2548711299896 seconds\n",
      "Step 12100, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 677.842808008194 seconds\n",
      "Step 12200, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 683.568124294281 seconds\n",
      "Step 12300, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 689.0793750286102 seconds\n",
      "Step 12400, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 694.6087803840637 seconds\n",
      "Step 12500, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 700.3285026550293 seconds\n",
      "Step 12600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 705.9144427776337 seconds\n",
      "Step 12700, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 711.7207527160645 seconds\n",
      "Step 12800, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 717.495046377182 seconds\n",
      "Step 12900, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 723.0211520195007 seconds\n",
      "Step 13000, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 728.715469121933 seconds\n",
      "Step 13100, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 734.2805716991425 seconds\n",
      "Step 13200, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 740.1306819915771 seconds\n",
      "Step 13300, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 745.6607446670532 seconds\n",
      "Step 13400, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 751.3369817733765 seconds\n",
      "Step 13500, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 756.8519911766052 seconds\n",
      "Step 13600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 762.3826441764832 seconds\n",
      "Step 13700, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 768.0949206352234 seconds\n",
      "Step 13800, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 773.8802154064178 seconds\n",
      "Step 13900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 779.546767950058 seconds\n",
      "Step 14000, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 785.1143264770508 seconds\n",
      "Step 14100, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 790.6495192050934 seconds\n",
      "Step 14200, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 796.3689484596252 seconds\n",
      "Step 14300, loss: tensor(0.0322, grad_fn=<SubBackward0>), time elapsed: 801.9386873245239 seconds\n",
      "Step 14400, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 807.628674030304 seconds\n",
      "Step 14500, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 813.1860072612762 seconds\n",
      "Step 14600, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 818.7615053653717 seconds\n",
      "Step 14700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 824.6137399673462 seconds\n",
      "Step 14800, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 830.1577208042145 seconds\n",
      "Step 14900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 835.9692018032074 seconds\n",
      "Step 15000, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 841.5116305351257 seconds\n",
      "Step 15100, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 847.2027840614319 seconds\n",
      "Step 15200, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 852.8893115520477 seconds\n",
      "Step 15300, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 858.4654037952423 seconds\n",
      "Step 15400, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 864.1783652305603 seconds\n",
      "Step 15500, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 869.7130138874054 seconds\n",
      "Step 15600, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 875.4324028491974 seconds\n",
      "Step 15700, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 881.1670429706573 seconds\n",
      "Step 15800, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 886.7130236625671 seconds\n",
      "Step 15900, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 892.4312806129456 seconds\n",
      "Step 16000, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 897.9793190956116 seconds\n",
      "Step 16100, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 903.6672358512878 seconds\n",
      "Step 16200, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 909.2066395282745 seconds\n",
      "Step 16300, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 915.2378239631653 seconds\n",
      "Step 16400, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 920.7973339557648 seconds\n",
      "Step 16500, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 926.3321685791016 seconds\n",
      "Step 16600, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 932.0225365161896 seconds\n",
      "Step 16700, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 937.5666637420654 seconds\n",
      "Step 16800, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 943.2707257270813 seconds\n",
      "Step 16900, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 948.8427860736847 seconds\n",
      "Step 17000, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 954.3976554870605 seconds\n",
      "Step 17100, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 960.1059110164642 seconds\n",
      "Step 17200, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 965.7735812664032 seconds\n",
      "Step 17300, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 971.4908924102783 seconds\n",
      "Step 17400, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 977.1084272861481 seconds\n",
      "Step 17500, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 982.9147565364838 seconds\n",
      "Step 17600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 988.5093867778778 seconds\n",
      "Step 17700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 994.6740581989288 seconds\n",
      "Step 17800, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1000.3606269359589 seconds\n",
      "Step 17900, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 1005.8871023654938 seconds\n",
      "Step 18000, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 1011.6653780937195 seconds\n",
      "Step 18100, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 1017.2448973655701 seconds\n",
      "Step 18200, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 1022.8391525745392 seconds\n",
      "Step 18300, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 1028.5856583118439 seconds\n",
      "Step 18400, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 1034.2089929580688 seconds\n",
      "Step 18500, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 1039.9846034049988 seconds\n",
      "Step 18600, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 1045.57359457016 seconds\n",
      "Step 18700, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1051.1667740345001 seconds\n",
      "Step 18800, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 1056.9257068634033 seconds\n",
      "Step 18900, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 1062.5178349018097 seconds\n",
      "Step 19000, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 1068.3019132614136 seconds\n",
      "Step 19100, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1073.9440214633942 seconds\n",
      "Step 19200, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1079.7394556999207 seconds\n",
      "Step 19300, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 1085.360764503479 seconds\n",
      "Step 19400, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 1090.9862899780273 seconds\n",
      "Step 19500, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 1096.7807505130768 seconds\n",
      "Step 19600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1102.4147124290466 seconds\n",
      "Step 19700, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 1108.2215247154236 seconds\n",
      "Step 19800, loss: tensor(0.0321, grad_fn=<SubBackward0>), time elapsed: 1113.8512780666351 seconds\n",
      "Step 19900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 1119.5235431194305 seconds\n",
      "Step 20000, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 1125.352337360382 seconds\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.3176, grad_fn=<SubBackward0>), time elapsed: 0.05955243110656738 seconds\n",
      "Step 100, loss: tensor(0.2851, grad_fn=<SubBackward0>), time elapsed: 5.748473882675171 seconds\n",
      "Step 200, loss: tensor(0.2483, grad_fn=<SubBackward0>), time elapsed: 11.529330015182495 seconds\n",
      "Step 300, loss: tensor(0.2187, grad_fn=<SubBackward0>), time elapsed: 17.10581946372986 seconds\n",
      "Step 400, loss: tensor(0.2048, grad_fn=<SubBackward0>), time elapsed: 22.82937526702881 seconds\n",
      "Step 500, loss: tensor(0.1950, grad_fn=<SubBackward0>), time elapsed: 28.3880832195282 seconds\n",
      "Step 600, loss: tensor(0.1810, grad_fn=<SubBackward0>), time elapsed: 33.97398138046265 seconds\n",
      "Step 700, loss: tensor(0.1685, grad_fn=<SubBackward0>), time elapsed: 39.658910512924194 seconds\n",
      "Step 800, loss: tensor(0.1570, grad_fn=<SubBackward0>), time elapsed: 45.217639684677124 seconds\n",
      "Step 900, loss: tensor(0.1476, grad_fn=<SubBackward0>), time elapsed: 50.869389057159424 seconds\n",
      "Step 1000, loss: tensor(0.1388, grad_fn=<SubBackward0>), time elapsed: 56.43752861022949 seconds\n",
      "Step 1100, loss: tensor(0.1299, grad_fn=<SubBackward0>), time elapsed: 62.13061165809631 seconds\n",
      "Step 1200, loss: tensor(0.1263, grad_fn=<SubBackward0>), time elapsed: 67.71975088119507 seconds\n",
      "Step 1300, loss: tensor(0.1211, grad_fn=<SubBackward0>), time elapsed: 73.27947115898132 seconds\n",
      "Step 1400, loss: tensor(0.1207, grad_fn=<SubBackward0>), time elapsed: 78.97702932357788 seconds\n",
      "Step 1500, loss: tensor(0.1123, grad_fn=<SubBackward0>), time elapsed: 84.54190921783447 seconds\n",
      "Step 1600, loss: tensor(0.1115, grad_fn=<SubBackward0>), time elapsed: 90.22992014884949 seconds\n",
      "Step 1700, loss: tensor(0.1096, grad_fn=<SubBackward0>), time elapsed: 95.80231833457947 seconds\n",
      "Step 1800, loss: tensor(0.1050, grad_fn=<SubBackward0>), time elapsed: 101.53613567352295 seconds\n",
      "Step 1900, loss: tensor(0.1055, grad_fn=<SubBackward0>), time elapsed: 107.10846018791199 seconds\n",
      "Step 2000, loss: tensor(0.0995, grad_fn=<SubBackward0>), time elapsed: 112.7068784236908 seconds\n",
      "Step 2100, loss: tensor(0.0972, grad_fn=<SubBackward0>), time elapsed: 118.42626333236694 seconds\n",
      "Step 2200, loss: tensor(0.0975, grad_fn=<SubBackward0>), time elapsed: 124.01848006248474 seconds\n",
      "Step 2300, loss: tensor(0.0962, grad_fn=<SubBackward0>), time elapsed: 129.7324583530426 seconds\n",
      "Step 2400, loss: tensor(0.0922, grad_fn=<SubBackward0>), time elapsed: 135.30587649345398 seconds\n",
      "Step 2500, loss: tensor(0.0938, grad_fn=<SubBackward0>), time elapsed: 140.88127517700195 seconds\n",
      "Step 2600, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 146.58695912361145 seconds\n",
      "Step 2700, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 152.18027138710022 seconds\n",
      "Step 2800, loss: tensor(0.0886, grad_fn=<SubBackward0>), time elapsed: 157.92360711097717 seconds\n",
      "Step 2900, loss: tensor(0.0888, grad_fn=<SubBackward0>), time elapsed: 163.5072774887085 seconds\n",
      "Step 3000, loss: tensor(0.0882, grad_fn=<SubBackward0>), time elapsed: 169.2235562801361 seconds\n",
      "Step 3100, loss: tensor(0.0860, grad_fn=<SubBackward0>), time elapsed: 174.78645300865173 seconds\n",
      "Step 3200, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 180.34625053405762 seconds\n",
      "Step 3300, loss: tensor(0.0847, grad_fn=<SubBackward0>), time elapsed: 186.04778909683228 seconds\n",
      "Step 3400, loss: tensor(0.0844, grad_fn=<SubBackward0>), time elapsed: 191.60766863822937 seconds\n",
      "Step 3500, loss: tensor(0.0844, grad_fn=<SubBackward0>), time elapsed: 197.3248791694641 seconds\n",
      "Step 3600, loss: tensor(0.0846, grad_fn=<SubBackward0>), time elapsed: 202.87760734558105 seconds\n",
      "Step 3700, loss: tensor(0.0835, grad_fn=<SubBackward0>), time elapsed: 208.5832815170288 seconds\n",
      "Step 3800, loss: tensor(0.0837, grad_fn=<SubBackward0>), time elapsed: 214.13459300994873 seconds\n",
      "Step 3900, loss: tensor(0.0824, grad_fn=<SubBackward0>), time elapsed: 219.7100899219513 seconds\n",
      "Step 4000, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 225.413831949234 seconds\n",
      "Step 4100, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 230.96858596801758 seconds\n",
      "Step 4200, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 236.6866934299469 seconds\n",
      "Step 4300, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 242.24649930000305 seconds\n",
      "Step 4400, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 247.84800148010254 seconds\n",
      "Step 4500, loss: tensor(0.0784, grad_fn=<SubBackward0>), time elapsed: 253.56836581230164 seconds\n",
      "Step 4600, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 259.1647198200226 seconds\n",
      "Step 4700, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 264.88627195358276 seconds\n",
      "Step 4800, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 270.5042061805725 seconds\n",
      "Step 4900, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 277.21961522102356 seconds\n",
      "Step 5000, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 282.85013365745544 seconds\n",
      "Step 5100, loss: tensor(0.0695, grad_fn=<SubBackward0>), time elapsed: 288.43079805374146 seconds\n",
      "Step 5200, loss: tensor(0.0663, grad_fn=<SubBackward0>), time elapsed: 294.14910769462585 seconds\n",
      "Step 5300, loss: tensor(0.0638, grad_fn=<SubBackward0>), time elapsed: 299.7464406490326 seconds\n",
      "Step 5400, loss: tensor(0.0623, grad_fn=<SubBackward0>), time elapsed: 305.47313833236694 seconds\n",
      "Step 5500, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 311.0808160305023 seconds\n",
      "Step 5600, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 316.9085958003998 seconds\n",
      "Step 5700, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 322.5841727256775 seconds\n",
      "Step 5800, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 328.24615025520325 seconds\n",
      "Step 5900, loss: tensor(0.0583, grad_fn=<SubBackward0>), time elapsed: 333.9604423046112 seconds\n",
      "Step 6000, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 339.5507962703705 seconds\n",
      "Step 6100, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 345.2863390445709 seconds\n",
      "Step 6200, loss: tensor(0.0553, grad_fn=<SubBackward0>), time elapsed: 350.86925745010376 seconds\n",
      "Step 6300, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 356.44347739219666 seconds\n",
      "Step 6400, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 362.14718532562256 seconds\n",
      "Step 6500, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 367.7257409095764 seconds\n",
      "Step 6600, loss: tensor(0.0549, grad_fn=<SubBackward0>), time elapsed: 373.42944526672363 seconds\n",
      "Step 6700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 378.99349999427795 seconds\n",
      "Step 6800, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 384.7160475254059 seconds\n",
      "Step 6900, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 390.32161808013916 seconds\n",
      "Step 7000, loss: tensor(0.0554, grad_fn=<SubBackward0>), time elapsed: 395.931414604187 seconds\n",
      "Step 7100, loss: tensor(0.0536, grad_fn=<SubBackward0>), time elapsed: 401.68725204467773 seconds\n",
      "Step 7200, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 407.3096160888672 seconds\n",
      "Step 7300, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 413.08639192581177 seconds\n",
      "Step 7400, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 418.98705101013184 seconds\n",
      "Step 7500, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 424.7481164932251 seconds\n",
      "Step 7600, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 430.3663799762726 seconds\n",
      "Step 7700, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 436.347375869751 seconds\n",
      "Step 7800, loss: tensor(0.0547, grad_fn=<SubBackward0>), time elapsed: 442.07501316070557 seconds\n",
      "Step 7900, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 447.64105772972107 seconds\n",
      "Step 8000, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 453.37301683425903 seconds\n",
      "Step 8100, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 459.05995512008667 seconds\n",
      "Step 8200, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 464.7887306213379 seconds\n",
      "Step 8300, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 470.3860397338867 seconds\n",
      "Step 8400, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 477.26818799972534 seconds\n",
      "Step 8500, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 483.0259165763855 seconds\n",
      "Step 8600, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 488.60759115219116 seconds\n",
      "Step 8700, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 494.3688757419586 seconds\n",
      "Step 8800, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 499.93288135528564 seconds\n",
      "Step 8900, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 505.5008466243744 seconds\n",
      "Step 9000, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 511.20658707618713 seconds\n",
      "Step 9100, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 516.7726154327393 seconds\n",
      "Step 9200, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 522.5096955299377 seconds\n",
      "Step 9300, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 528.1070251464844 seconds\n",
      "Step 9400, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 533.8837847709656 seconds\n",
      "Step 9500, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 539.4603731632233 seconds\n",
      "Step 9600, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 545.0252559185028 seconds\n",
      "Step 9700, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 550.7676687240601 seconds\n",
      "Step 9800, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 556.3483068943024 seconds\n",
      "Step 9900, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 562.0873031616211 seconds\n",
      "Step 10000, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 567.6681394577026 seconds\n",
      "Step 10100, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 573.248613357544 seconds\n",
      "Step 10200, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 578.9878339767456 seconds\n",
      "Step 10300, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 584.6423873901367 seconds\n",
      "Step 10400, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 590.3868091106415 seconds\n",
      "Step 10500, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 596.0164141654968 seconds\n",
      "Step 10600, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 601.7693402767181 seconds\n",
      "Step 10700, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 607.3832886219025 seconds\n",
      "Step 10800, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 613.0182299613953 seconds\n",
      "Step 10900, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 618.8042025566101 seconds\n",
      "Step 11000, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 624.4296867847443 seconds\n",
      "Step 11100, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 630.2479376792908 seconds\n",
      "Step 11200, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 635.8705625534058 seconds\n",
      "Step 11300, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 641.5083866119385 seconds\n",
      "Step 11400, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 647.3099083900452 seconds\n",
      "Step 11500, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 652.9460439682007 seconds\n",
      "Step 11600, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 658.7352364063263 seconds\n",
      "Step 11700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 664.3619492053986 seconds\n",
      "Step 11800, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 670.7601130008698 seconds\n",
      "Step 11900, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 676.3865756988525 seconds\n",
      "Step 12000, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 682.0075442790985 seconds\n",
      "Step 12100, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 687.7606558799744 seconds\n",
      "Step 12200, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 693.3817830085754 seconds\n",
      "Step 12300, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 699.1449429988861 seconds\n",
      "Step 12400, loss: tensor(0.0533, grad_fn=<SubBackward0>), time elapsed: 704.8046991825104 seconds\n",
      "Step 12500, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 710.4418804645538 seconds\n",
      "Step 12600, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 716.2603561878204 seconds\n",
      "Step 12700, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 721.9607198238373 seconds\n",
      "Step 12800, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 727.7968726158142 seconds\n",
      "Step 12900, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 733.7320804595947 seconds\n",
      "Step 13000, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 739.630802154541 seconds\n",
      "Step 13100, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 745.3313539028168 seconds\n",
      "Step 13200, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 751.6402590274811 seconds\n",
      "Step 13300, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 757.4940614700317 seconds\n",
      "Step 13400, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 763.1889896392822 seconds\n",
      "Step 13500, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 769.0601093769073 seconds\n",
      "Step 13600, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 774.7170853614807 seconds\n",
      "Step 13700, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 780.3799502849579 seconds\n",
      "Step 13800, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 786.2182383537292 seconds\n",
      "Step 13900, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 791.9325187206268 seconds\n",
      "Step 14000, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 798.0648708343506 seconds\n",
      "Step 14100, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 803.8085134029388 seconds\n",
      "Step 14200, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 809.5701491832733 seconds\n",
      "Step 14300, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 815.2806940078735 seconds\n",
      "Step 14400, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 820.8605124950409 seconds\n",
      "Step 14500, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 826.596892118454 seconds\n",
      "Step 14600, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 832.5196936130524 seconds\n",
      "Step 14700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 838.2242531776428 seconds\n",
      "Step 14800, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 843.88321352005 seconds\n",
      "Step 14900, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 849.4241764545441 seconds\n",
      "Step 15000, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 855.1978225708008 seconds\n",
      "Step 15100, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 860.8924238681793 seconds\n",
      "Step 15200, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 866.610265493393 seconds\n",
      "Step 15300, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 872.3215978145599 seconds\n",
      "Step 15400, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 877.9235453605652 seconds\n",
      "Step 15500, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 883.6518213748932 seconds\n",
      "Step 15600, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 889.365291595459 seconds\n",
      "Step 15700, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 895.1727449893951 seconds\n",
      "Step 15800, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 900.726633310318 seconds\n",
      "Step 15900, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 906.5364263057709 seconds\n",
      "Step 16000, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 912.113046169281 seconds\n",
      "Step 16100, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 918.3234853744507 seconds\n",
      "Step 16200, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 924.0252883434296 seconds\n",
      "Step 16300, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 929.5860455036163 seconds\n",
      "Step 16400, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 935.2930870056152 seconds\n",
      "Step 16500, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 940.861466884613 seconds\n",
      "Step 16600, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 946.5521433353424 seconds\n",
      "Step 16700, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 952.3761126995087 seconds\n",
      "Step 16800, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 957.9234426021576 seconds\n",
      "Step 16900, loss: tensor(0.0538, grad_fn=<SubBackward0>), time elapsed: 963.9816210269928 seconds\n",
      "Step 17000, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 969.5239350795746 seconds\n",
      "Step 17100, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 975.2326884269714 seconds\n",
      "Step 17200, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 980.8031227588654 seconds\n",
      "Step 17300, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 986.3434529304504 seconds\n",
      "Step 17400, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 992.0465984344482 seconds\n",
      "Step 17500, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 997.6012053489685 seconds\n",
      "Step 17600, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 1003.3417098522186 seconds\n",
      "Step 17700, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 1008.9661452770233 seconds\n",
      "Step 17800, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 1015.2822494506836 seconds\n",
      "Step 17900, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 1021.0028312206268 seconds\n",
      "Step 18000, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1026.5667371749878 seconds\n",
      "Step 18100, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 1032.2854351997375 seconds\n",
      "Step 18200, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 1037.8567569255829 seconds\n",
      "Step 18300, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 1043.421864748001 seconds\n",
      "Step 18400, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 1049.1612088680267 seconds\n",
      "Step 18500, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 1054.7826571464539 seconds\n",
      "Step 18600, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 1060.4891011714935 seconds\n",
      "Step 18700, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 1066.0472767353058 seconds\n",
      "Step 18800, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 1071.7778539657593 seconds\n",
      "Step 18900, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 1077.310611963272 seconds\n",
      "Step 19000, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 1082.8800299167633 seconds\n",
      "Step 19100, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 1088.5931570529938 seconds\n",
      "Step 19200, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 1094.1426129341125 seconds\n",
      "Step 19300, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1099.8860943317413 seconds\n",
      "Step 19400, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 1105.4398539066315 seconds\n",
      "Step 19500, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 1111.002469778061 seconds\n",
      "Step 19600, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1116.7968497276306 seconds\n",
      "Step 19700, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 1123.875884771347 seconds\n",
      "Step 19800, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 1129.6128253936768 seconds\n",
      "Step 19900, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 1135.1553122997284 seconds\n",
      "Step 20000, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 1140.7229056358337 seconds\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.4988, grad_fn=<SubBackward0>), time elapsed: 0.0599675178527832 seconds\n",
      "Step 100, loss: tensor(0.4451, grad_fn=<SubBackward0>), time elapsed: 5.673086404800415 seconds\n",
      "Step 200, loss: tensor(0.3738, grad_fn=<SubBackward0>), time elapsed: 11.24161696434021 seconds\n",
      "Step 300, loss: tensor(0.2956, grad_fn=<SubBackward0>), time elapsed: 16.864001750946045 seconds\n",
      "Step 400, loss: tensor(0.2498, grad_fn=<SubBackward0>), time elapsed: 22.365018844604492 seconds\n",
      "Step 500, loss: tensor(0.2220, grad_fn=<SubBackward0>), time elapsed: 28.001725912094116 seconds\n",
      "Step 600, loss: tensor(0.2042, grad_fn=<SubBackward0>), time elapsed: 33.48745512962341 seconds\n",
      "Step 700, loss: tensor(0.1913, grad_fn=<SubBackward0>), time elapsed: 39.01809763908386 seconds\n",
      "Step 800, loss: tensor(0.1802, grad_fn=<SubBackward0>), time elapsed: 44.65255284309387 seconds\n",
      "Step 900, loss: tensor(0.1721, grad_fn=<SubBackward0>), time elapsed: 50.17551612854004 seconds\n",
      "Step 1000, loss: tensor(0.1664, grad_fn=<SubBackward0>), time elapsed: 55.83338451385498 seconds\n",
      "Step 1100, loss: tensor(0.1608, grad_fn=<SubBackward0>), time elapsed: 61.375001192092896 seconds\n",
      "Step 1200, loss: tensor(0.1562, grad_fn=<SubBackward0>), time elapsed: 67.01808547973633 seconds\n",
      "Step 1300, loss: tensor(0.1519, grad_fn=<SubBackward0>), time elapsed: 72.506676197052 seconds\n",
      "Step 1400, loss: tensor(0.1496, grad_fn=<SubBackward0>), time elapsed: 78.00006413459778 seconds\n",
      "Step 1500, loss: tensor(0.1481, grad_fn=<SubBackward0>), time elapsed: 83.63940644264221 seconds\n",
      "Step 1600, loss: tensor(0.1474, grad_fn=<SubBackward0>), time elapsed: 89.16807675361633 seconds\n",
      "Step 1700, loss: tensor(0.1444, grad_fn=<SubBackward0>), time elapsed: 94.81337904930115 seconds\n",
      "Step 1800, loss: tensor(0.1396, grad_fn=<SubBackward0>), time elapsed: 100.33804321289062 seconds\n",
      "Step 1900, loss: tensor(0.1367, grad_fn=<SubBackward0>), time elapsed: 105.97847986221313 seconds\n",
      "Step 2000, loss: tensor(0.1340, grad_fn=<SubBackward0>), time elapsed: 111.47847127914429 seconds\n",
      "Step 2100, loss: tensor(0.1355, grad_fn=<SubBackward0>), time elapsed: 116.9933431148529 seconds\n",
      "Step 2200, loss: tensor(0.1292, grad_fn=<SubBackward0>), time elapsed: 122.63233184814453 seconds\n",
      "Step 2300, loss: tensor(0.1278, grad_fn=<SubBackward0>), time elapsed: 128.14141654968262 seconds\n",
      "Step 2400, loss: tensor(0.1271, grad_fn=<SubBackward0>), time elapsed: 133.7721812725067 seconds\n",
      "Step 2500, loss: tensor(0.1267, grad_fn=<SubBackward0>), time elapsed: 139.28489136695862 seconds\n",
      "Step 2600, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 144.9409384727478 seconds\n",
      "Step 2700, loss: tensor(0.1192, grad_fn=<SubBackward0>), time elapsed: 150.46457171440125 seconds\n",
      "Step 2800, loss: tensor(0.1165, grad_fn=<SubBackward0>), time elapsed: 155.99446320533752 seconds\n",
      "Step 2900, loss: tensor(0.1190, grad_fn=<SubBackward0>), time elapsed: 161.61800456047058 seconds\n",
      "Step 3000, loss: tensor(0.1171, grad_fn=<SubBackward0>), time elapsed: 167.12133979797363 seconds\n",
      "Step 3100, loss: tensor(0.1149, grad_fn=<SubBackward0>), time elapsed: 172.74762296676636 seconds\n",
      "Step 3200, loss: tensor(0.1155, grad_fn=<SubBackward0>), time elapsed: 178.2496473789215 seconds\n",
      "Step 3300, loss: tensor(0.1132, grad_fn=<SubBackward0>), time elapsed: 183.78439331054688 seconds\n",
      "Step 3400, loss: tensor(0.1144, grad_fn=<SubBackward0>), time elapsed: 189.4445445537567 seconds\n",
      "Step 3500, loss: tensor(0.1141, grad_fn=<SubBackward0>), time elapsed: 194.9724669456482 seconds\n",
      "Step 3600, loss: tensor(0.1120, grad_fn=<SubBackward0>), time elapsed: 200.60442185401917 seconds\n",
      "Step 3700, loss: tensor(0.1114, grad_fn=<SubBackward0>), time elapsed: 206.11015701293945 seconds\n",
      "Step 3800, loss: tensor(0.1094, grad_fn=<SubBackward0>), time elapsed: 211.7900938987732 seconds\n",
      "Step 3900, loss: tensor(0.1118, grad_fn=<SubBackward0>), time elapsed: 217.28892970085144 seconds\n",
      "Step 4000, loss: tensor(0.1089, grad_fn=<SubBackward0>), time elapsed: 222.7831666469574 seconds\n",
      "Step 4100, loss: tensor(0.1087, grad_fn=<SubBackward0>), time elapsed: 228.40738797187805 seconds\n",
      "Step 4200, loss: tensor(0.1052, grad_fn=<SubBackward0>), time elapsed: 233.92565393447876 seconds\n",
      "Step 4300, loss: tensor(0.1025, grad_fn=<SubBackward0>), time elapsed: 239.6027045249939 seconds\n",
      "Step 4400, loss: tensor(0.1042, grad_fn=<SubBackward0>), time elapsed: 245.10628294944763 seconds\n",
      "Step 4500, loss: tensor(0.1023, grad_fn=<SubBackward0>), time elapsed: 250.7570915222168 seconds\n",
      "Step 4600, loss: tensor(0.1019, grad_fn=<SubBackward0>), time elapsed: 256.25228929519653 seconds\n",
      "Step 4700, loss: tensor(0.0986, grad_fn=<SubBackward0>), time elapsed: 261.7512278556824 seconds\n",
      "Step 4800, loss: tensor(0.0974, grad_fn=<SubBackward0>), time elapsed: 267.3765971660614 seconds\n",
      "Step 4900, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 272.9295725822449 seconds\n",
      "Step 5000, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 278.5631742477417 seconds\n",
      "Step 5100, loss: tensor(0.0956, grad_fn=<SubBackward0>), time elapsed: 284.0665936470032 seconds\n",
      "Step 5200, loss: tensor(0.0911, grad_fn=<SubBackward0>), time elapsed: 289.5810661315918 seconds\n",
      "Step 5300, loss: tensor(0.0850, grad_fn=<SubBackward0>), time elapsed: 295.242205619812 seconds\n",
      "Step 5400, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 300.7599060535431 seconds\n",
      "Step 5500, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 306.3815643787384 seconds\n",
      "Step 5600, loss: tensor(0.0856, grad_fn=<SubBackward0>), time elapsed: 311.87810349464417 seconds\n",
      "Step 5700, loss: tensor(0.0859, grad_fn=<SubBackward0>), time elapsed: 317.54471015930176 seconds\n",
      "Step 5800, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 323.0485098361969 seconds\n",
      "Step 5900, loss: tensor(0.0834, grad_fn=<SubBackward0>), time elapsed: 328.5654036998749 seconds\n",
      "Step 6000, loss: tensor(0.0827, grad_fn=<SubBackward0>), time elapsed: 334.244802236557 seconds\n",
      "Step 6100, loss: tensor(0.0815, grad_fn=<SubBackward0>), time elapsed: 339.77567434310913 seconds\n",
      "Step 6200, loss: tensor(0.0796, grad_fn=<SubBackward0>), time elapsed: 345.4236104488373 seconds\n",
      "Step 6300, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 350.9368839263916 seconds\n",
      "Step 6400, loss: tensor(0.0786, grad_fn=<SubBackward0>), time elapsed: 356.57995772361755 seconds\n",
      "Step 6500, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 362.0937993526459 seconds\n",
      "Step 6600, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 367.5864565372467 seconds\n",
      "Step 6700, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 373.2098114490509 seconds\n",
      "Step 6800, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 378.7232904434204 seconds\n",
      "Step 6900, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 384.3595414161682 seconds\n",
      "Step 7000, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 389.8657877445221 seconds\n",
      "Step 7100, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 395.40313720703125 seconds\n",
      "Step 7200, loss: tensor(0.0692, grad_fn=<SubBackward0>), time elapsed: 401.03588032722473 seconds\n",
      "Step 7300, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 406.54494428634644 seconds\n",
      "Step 7400, loss: tensor(0.0683, grad_fn=<SubBackward0>), time elapsed: 412.19260907173157 seconds\n",
      "Step 7500, loss: tensor(0.0698, grad_fn=<SubBackward0>), time elapsed: 417.67816519737244 seconds\n",
      "Step 7600, loss: tensor(0.0708, grad_fn=<SubBackward0>), time elapsed: 423.3643014431 seconds\n",
      "Step 7700, loss: tensor(0.0667, grad_fn=<SubBackward0>), time elapsed: 428.8981611728668 seconds\n",
      "Step 7800, loss: tensor(0.0668, grad_fn=<SubBackward0>), time elapsed: 434.4219686985016 seconds\n",
      "Step 7900, loss: tensor(0.0664, grad_fn=<SubBackward0>), time elapsed: 440.07550382614136 seconds\n",
      "Step 8000, loss: tensor(0.0687, grad_fn=<SubBackward0>), time elapsed: 445.5874009132385 seconds\n",
      "Step 8100, loss: tensor(0.0641, grad_fn=<SubBackward0>), time elapsed: 451.2412705421448 seconds\n",
      "Step 8200, loss: tensor(0.0663, grad_fn=<SubBackward0>), time elapsed: 456.7759494781494 seconds\n",
      "Step 8300, loss: tensor(0.0611, grad_fn=<SubBackward0>), time elapsed: 462.4183371067047 seconds\n",
      "Step 8400, loss: tensor(0.0625, grad_fn=<SubBackward0>), time elapsed: 467.9539477825165 seconds\n",
      "Step 8500, loss: tensor(0.0630, grad_fn=<SubBackward0>), time elapsed: 473.5014202594757 seconds\n",
      "Step 8600, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 479.1730053424835 seconds\n",
      "Step 8700, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 484.7386963367462 seconds\n",
      "Step 8800, loss: tensor(0.0602, grad_fn=<SubBackward0>), time elapsed: 490.38742303848267 seconds\n",
      "Step 8900, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 495.890825510025 seconds\n",
      "Step 9000, loss: tensor(0.0603, grad_fn=<SubBackward0>), time elapsed: 501.3968241214752 seconds\n",
      "Step 9100, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 507.034375667572 seconds\n",
      "Step 9200, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 512.5718734264374 seconds\n",
      "Step 9300, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 518.2810888290405 seconds\n",
      "Step 9400, loss: tensor(0.0550, grad_fn=<SubBackward0>), time elapsed: 523.8460538387299 seconds\n",
      "Step 9500, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 529.6781101226807 seconds\n",
      "Step 9600, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 535.1822938919067 seconds\n",
      "Step 9700, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 540.8322486877441 seconds\n",
      "Step 9800, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 546.6477773189545 seconds\n",
      "Step 9900, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 552.2231426239014 seconds\n",
      "Step 10000, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 557.9065403938293 seconds\n",
      "Step 10100, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 563.439530134201 seconds\n",
      "Step 10200, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 568.9841668605804 seconds\n",
      "Step 10300, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 574.7065677642822 seconds\n",
      "Step 10400, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 580.243301153183 seconds\n",
      "Step 10500, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 586.1811199188232 seconds\n",
      "Step 10600, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 591.7654511928558 seconds\n",
      "Step 10700, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 597.4948587417603 seconds\n",
      "Step 10800, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 603.0189030170441 seconds\n",
      "Step 10900, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 608.5819706916809 seconds\n",
      "Step 11000, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 614.2862317562103 seconds\n",
      "Step 11100, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 619.8410928249359 seconds\n",
      "Step 11200, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 625.5774357318878 seconds\n",
      "Step 11300, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 631.1902642250061 seconds\n",
      "Step 11400, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 637.24422955513 seconds\n",
      "Step 11500, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 642.9186019897461 seconds\n",
      "Step 11600, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 648.453046798706 seconds\n",
      "Step 11700, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 654.1252748966217 seconds\n",
      "Step 11800, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 659.661972284317 seconds\n",
      "Step 11900, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 665.3366413116455 seconds\n",
      "Step 12000, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 670.8865487575531 seconds\n",
      "Step 12100, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 676.3966162204742 seconds\n",
      "Step 12200, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 683.5611908435822 seconds\n",
      "Step 12300, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 689.0823228359222 seconds\n",
      "Step 12400, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 694.7885458469391 seconds\n",
      "Step 12500, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 700.3312077522278 seconds\n",
      "Step 12600, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 705.8883099555969 seconds\n",
      "Step 12700, loss: tensor(0.0490, grad_fn=<SubBackward0>), time elapsed: 711.5646901130676 seconds\n",
      "Step 12800, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 717.0920596122742 seconds\n",
      "Step 12900, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 723.0514805316925 seconds\n",
      "Step 13000, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 728.6466352939606 seconds\n",
      "Step 13100, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 734.3439521789551 seconds\n",
      "Step 13200, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 739.8784718513489 seconds\n",
      "Step 13300, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 745.4277634620667 seconds\n",
      "Step 13400, loss: tensor(0.0490, grad_fn=<SubBackward0>), time elapsed: 751.129613161087 seconds\n",
      "Step 13500, loss: tensor(0.0470, grad_fn=<SubBackward0>), time elapsed: 756.6651866436005 seconds\n",
      "Step 13600, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 762.350359916687 seconds\n",
      "Step 13700, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 768.2872657775879 seconds\n",
      "Step 13800, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 773.8382461071014 seconds\n",
      "Step 13900, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 779.5263504981995 seconds\n",
      "Step 14000, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 785.0602910518646 seconds\n",
      "Step 14100, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 790.748804807663 seconds\n",
      "Step 14200, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 796.2898495197296 seconds\n",
      "Step 14300, loss: tensor(0.0465, grad_fn=<SubBackward0>), time elapsed: 801.9644291400909 seconds\n",
      "Step 14400, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 807.4875628948212 seconds\n",
      "Step 14500, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 813.0062470436096 seconds\n",
      "Step 14600, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 818.7102348804474 seconds\n",
      "Step 14700, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 824.2460877895355 seconds\n",
      "Step 14800, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 829.9279897212982 seconds\n",
      "Step 14900, loss: tensor(0.0476, grad_fn=<SubBackward0>), time elapsed: 835.482593536377 seconds\n",
      "Step 15000, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 841.028082370758 seconds\n",
      "Step 15100, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 846.7138230800629 seconds\n",
      "Step 15200, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 852.2825059890747 seconds\n",
      "Step 15300, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 857.9814918041229 seconds\n",
      "Step 15400, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 863.5412068367004 seconds\n",
      "Step 15500, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 869.2450213432312 seconds\n",
      "Step 15600, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 874.8102378845215 seconds\n",
      "Step 15700, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 880.3904340267181 seconds\n",
      "Step 15800, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 886.1103668212891 seconds\n",
      "Step 15900, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 891.640810251236 seconds\n",
      "Step 16000, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 897.3554458618164 seconds\n",
      "Step 16100, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 902.890917301178 seconds\n",
      "Step 16200, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 908.4556124210358 seconds\n",
      "Step 16300, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 914.1698575019836 seconds\n",
      "Step 16400, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 919.7221763134003 seconds\n",
      "Step 16500, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 925.4267356395721 seconds\n",
      "Step 16600, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 930.9922246932983 seconds\n",
      "Step 16700, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 936.6995394229889 seconds\n",
      "Step 16800, loss: tensor(0.0474, grad_fn=<SubBackward0>), time elapsed: 942.26478099823 seconds\n",
      "Step 16900, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 948.070100069046 seconds\n",
      "Step 17000, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 953.7785315513611 seconds\n",
      "Step 17100, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 959.3301241397858 seconds\n",
      "Step 17200, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 965.0182387828827 seconds\n",
      "Step 17300, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 970.585462808609 seconds\n",
      "Step 17400, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 976.1398568153381 seconds\n",
      "Step 17500, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 981.8573267459869 seconds\n",
      "Step 17600, loss: tensor(0.0463, grad_fn=<SubBackward0>), time elapsed: 987.4308831691742 seconds\n",
      "Step 17700, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 993.1442363262177 seconds\n",
      "Step 17800, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 998.7782990932465 seconds\n",
      "Step 17900, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 1004.3922004699707 seconds\n",
      "Step 18000, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 1010.1226665973663 seconds\n",
      "Step 18100, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1015.7214493751526 seconds\n",
      "Step 18200, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1021.5352745056152 seconds\n",
      "Step 18300, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 1027.6863079071045 seconds\n",
      "Step 18400, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 1033.4458253383636 seconds\n",
      "Step 18500, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 1038.9813511371613 seconds\n",
      "Step 18600, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 1044.5038740634918 seconds\n",
      "Step 18700, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 1050.1986043453217 seconds\n",
      "Step 18800, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 1055.7481174468994 seconds\n",
      "Step 18900, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 1061.4542796611786 seconds\n",
      "Step 19000, loss: tensor(0.0494, grad_fn=<SubBackward0>), time elapsed: 1067.0076925754547 seconds\n",
      "Step 19100, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 1072.5519406795502 seconds\n",
      "Step 19200, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1078.256949186325 seconds\n",
      "Step 19300, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 1083.808467388153 seconds\n",
      "Step 19400, loss: tensor(0.0468, grad_fn=<SubBackward0>), time elapsed: 1089.5544850826263 seconds\n",
      "Step 19500, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 1095.1537899971008 seconds\n",
      "Step 19600, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 1101.854486465454 seconds\n",
      "Step 19700, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1107.5655677318573 seconds\n",
      "Step 19800, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1113.1126036643982 seconds\n",
      "Step 19900, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 1118.8268251419067 seconds\n",
      "Step 20000, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 1124.401051044464 seconds\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.7608, grad_fn=<SubBackward0>), time elapsed: 0.0603337287902832 seconds\n",
      "Step 100, loss: tensor(0.7097, grad_fn=<SubBackward0>), time elapsed: 5.879121780395508 seconds\n",
      "Step 200, loss: tensor(0.6380, grad_fn=<SubBackward0>), time elapsed: 11.447932243347168 seconds\n",
      "Step 300, loss: tensor(0.5466, grad_fn=<SubBackward0>), time elapsed: 16.93709635734558 seconds\n",
      "Step 400, loss: tensor(0.4489, grad_fn=<SubBackward0>), time elapsed: 22.574985027313232 seconds\n",
      "Step 500, loss: tensor(0.3625, grad_fn=<SubBackward0>), time elapsed: 28.082396745681763 seconds\n",
      "Step 600, loss: tensor(0.3036, grad_fn=<SubBackward0>), time elapsed: 33.736506938934326 seconds\n",
      "Step 700, loss: tensor(0.2621, grad_fn=<SubBackward0>), time elapsed: 39.241549491882324 seconds\n",
      "Step 800, loss: tensor(0.2312, grad_fn=<SubBackward0>), time elapsed: 44.88120126724243 seconds\n",
      "Step 900, loss: tensor(0.2048, grad_fn=<SubBackward0>), time elapsed: 50.38529968261719 seconds\n",
      "Step 1000, loss: tensor(0.1855, grad_fn=<SubBackward0>), time elapsed: 55.91467046737671 seconds\n",
      "Step 1100, loss: tensor(0.1687, grad_fn=<SubBackward0>), time elapsed: 61.550291776657104 seconds\n",
      "Step 1200, loss: tensor(0.1561, grad_fn=<SubBackward0>), time elapsed: 67.0875494480133 seconds\n",
      "Step 1300, loss: tensor(0.1446, grad_fn=<SubBackward0>), time elapsed: 72.74131965637207 seconds\n",
      "Step 1400, loss: tensor(0.1363, grad_fn=<SubBackward0>), time elapsed: 78.23391461372375 seconds\n",
      "Step 1500, loss: tensor(0.1338, grad_fn=<SubBackward0>), time elapsed: 83.74945855140686 seconds\n",
      "Step 1600, loss: tensor(0.1265, grad_fn=<SubBackward0>), time elapsed: 89.38594770431519 seconds\n",
      "Step 1700, loss: tensor(0.1239, grad_fn=<SubBackward0>), time elapsed: 94.94654273986816 seconds\n",
      "Step 1800, loss: tensor(0.1237, grad_fn=<SubBackward0>), time elapsed: 100.59437608718872 seconds\n",
      "Step 1900, loss: tensor(0.1207, grad_fn=<SubBackward0>), time elapsed: 106.10434412956238 seconds\n",
      "Step 2000, loss: tensor(0.1196, grad_fn=<SubBackward0>), time elapsed: 111.72513341903687 seconds\n",
      "Step 2100, loss: tensor(0.1191, grad_fn=<SubBackward0>), time elapsed: 117.22833752632141 seconds\n",
      "Step 2200, loss: tensor(0.1160, grad_fn=<SubBackward0>), time elapsed: 122.70956897735596 seconds\n",
      "Step 2300, loss: tensor(0.1175, grad_fn=<SubBackward0>), time elapsed: 128.3465437889099 seconds\n",
      "Step 2400, loss: tensor(0.1154, grad_fn=<SubBackward0>), time elapsed: 133.8225281238556 seconds\n",
      "Step 2500, loss: tensor(0.1156, grad_fn=<SubBackward0>), time elapsed: 139.45855569839478 seconds\n",
      "Step 2600, loss: tensor(0.1144, grad_fn=<SubBackward0>), time elapsed: 144.97835874557495 seconds\n",
      "Step 2700, loss: tensor(0.1159, grad_fn=<SubBackward0>), time elapsed: 150.63480591773987 seconds\n",
      "Step 2800, loss: tensor(0.1162, grad_fn=<SubBackward0>), time elapsed: 156.15611219406128 seconds\n",
      "Step 2900, loss: tensor(0.1149, grad_fn=<SubBackward0>), time elapsed: 161.65440964698792 seconds\n",
      "Step 3000, loss: tensor(0.1150, grad_fn=<SubBackward0>), time elapsed: 167.26637768745422 seconds\n",
      "Step 3100, loss: tensor(0.1139, grad_fn=<SubBackward0>), time elapsed: 172.77274107933044 seconds\n",
      "Step 3200, loss: tensor(0.1120, grad_fn=<SubBackward0>), time elapsed: 178.39787793159485 seconds\n",
      "Step 3300, loss: tensor(0.1135, grad_fn=<SubBackward0>), time elapsed: 183.91302037239075 seconds\n",
      "Step 3400, loss: tensor(0.1121, grad_fn=<SubBackward0>), time elapsed: 189.58109402656555 seconds\n",
      "Step 3500, loss: tensor(0.1138, grad_fn=<SubBackward0>), time elapsed: 195.10535264015198 seconds\n",
      "Step 3600, loss: tensor(0.1119, grad_fn=<SubBackward0>), time elapsed: 200.63884592056274 seconds\n",
      "Step 3700, loss: tensor(0.1123, grad_fn=<SubBackward0>), time elapsed: 206.3046178817749 seconds\n",
      "Step 3800, loss: tensor(0.1122, grad_fn=<SubBackward0>), time elapsed: 211.81570529937744 seconds\n",
      "Step 3900, loss: tensor(0.1120, grad_fn=<SubBackward0>), time elapsed: 217.4867389202118 seconds\n",
      "Step 4000, loss: tensor(0.1082, grad_fn=<SubBackward0>), time elapsed: 222.99141097068787 seconds\n",
      "Step 4100, loss: tensor(0.1100, grad_fn=<SubBackward0>), time elapsed: 228.51919984817505 seconds\n",
      "Step 4200, loss: tensor(0.1081, grad_fn=<SubBackward0>), time elapsed: 234.17058038711548 seconds\n",
      "Step 4300, loss: tensor(0.1074, grad_fn=<SubBackward0>), time elapsed: 239.69841647148132 seconds\n",
      "Step 4400, loss: tensor(0.1075, grad_fn=<SubBackward0>), time elapsed: 245.3645932674408 seconds\n",
      "Step 4500, loss: tensor(0.1080, grad_fn=<SubBackward0>), time elapsed: 250.9122474193573 seconds\n",
      "Step 4600, loss: tensor(0.1052, grad_fn=<SubBackward0>), time elapsed: 256.6622009277344 seconds\n",
      "Step 4700, loss: tensor(0.1060, grad_fn=<SubBackward0>), time elapsed: 262.16317343711853 seconds\n",
      "Step 4800, loss: tensor(0.1060, grad_fn=<SubBackward0>), time elapsed: 267.67206740379333 seconds\n",
      "Step 4900, loss: tensor(0.1040, grad_fn=<SubBackward0>), time elapsed: 273.32578134536743 seconds\n",
      "Step 5000, loss: tensor(0.1050, grad_fn=<SubBackward0>), time elapsed: 278.98210096359253 seconds\n",
      "Step 5100, loss: tensor(0.1009, grad_fn=<SubBackward0>), time elapsed: 284.689058303833 seconds\n",
      "Step 5200, loss: tensor(0.1028, grad_fn=<SubBackward0>), time elapsed: 290.24828815460205 seconds\n",
      "Step 5300, loss: tensor(0.1020, grad_fn=<SubBackward0>), time elapsed: 296.00524830818176 seconds\n",
      "Step 5400, loss: tensor(0.1004, grad_fn=<SubBackward0>), time elapsed: 301.8977732658386 seconds\n",
      "Step 5500, loss: tensor(0.1010, grad_fn=<SubBackward0>), time elapsed: 307.4168395996094 seconds\n",
      "Step 5600, loss: tensor(0.0986, grad_fn=<SubBackward0>), time elapsed: 313.0764813423157 seconds\n",
      "Step 5700, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 318.5865454673767 seconds\n",
      "Step 5800, loss: tensor(0.0965, grad_fn=<SubBackward0>), time elapsed: 324.3381977081299 seconds\n",
      "Step 5900, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 329.92324805259705 seconds\n",
      "Step 6000, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 335.46077966690063 seconds\n",
      "Step 6100, loss: tensor(0.0953, grad_fn=<SubBackward0>), time elapsed: 341.1923863887787 seconds\n",
      "Step 6200, loss: tensor(0.0956, grad_fn=<SubBackward0>), time elapsed: 346.74488592147827 seconds\n",
      "Step 6300, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 352.4346971511841 seconds\n",
      "Step 6400, loss: tensor(0.0947, grad_fn=<SubBackward0>), time elapsed: 358.008757352829 seconds\n",
      "Step 6500, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 363.67988109588623 seconds\n",
      "Step 6600, loss: tensor(0.0926, grad_fn=<SubBackward0>), time elapsed: 369.6839122772217 seconds\n",
      "Step 6700, loss: tensor(0.0881, grad_fn=<SubBackward0>), time elapsed: 375.2170674800873 seconds\n",
      "Step 6800, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 380.8889539241791 seconds\n",
      "Step 6900, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 386.43874168395996 seconds\n",
      "Step 7000, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 392.1860282421112 seconds\n",
      "Step 7100, loss: tensor(0.0903, grad_fn=<SubBackward0>), time elapsed: 397.7644066810608 seconds\n",
      "Step 7200, loss: tensor(0.0897, grad_fn=<SubBackward0>), time elapsed: 403.7826421260834 seconds\n",
      "Step 7300, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 409.29382252693176 seconds\n",
      "Step 7400, loss: tensor(0.0887, grad_fn=<SubBackward0>), time elapsed: 414.8438663482666 seconds\n",
      "Step 7500, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 420.53237438201904 seconds\n",
      "Step 7600, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 426.04705142974854 seconds\n",
      "Step 7700, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 432.03768968582153 seconds\n",
      "Step 7800, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 437.54259490966797 seconds\n",
      "Step 7900, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 443.1467490196228 seconds\n",
      "Step 8000, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 448.84343671798706 seconds\n",
      "Step 8100, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 454.3517143726349 seconds\n",
      "Step 8200, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 461.65694856643677 seconds\n",
      "Step 8300, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 467.16131830215454 seconds\n",
      "Step 8400, loss: tensor(0.0858, grad_fn=<SubBackward0>), time elapsed: 472.804869890213 seconds\n",
      "Step 8500, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 478.3243441581726 seconds\n",
      "Step 8600, loss: tensor(0.0856, grad_fn=<SubBackward0>), time elapsed: 483.83592915534973 seconds\n",
      "Step 8700, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 489.47475481033325 seconds\n",
      "Step 8800, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 495.0138421058655 seconds\n",
      "Step 8900, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 500.67922163009644 seconds\n",
      "Step 9000, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 506.224782705307 seconds\n",
      "Step 9100, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 511.7584102153778 seconds\n",
      "Step 9200, loss: tensor(0.0858, grad_fn=<SubBackward0>), time elapsed: 517.4158658981323 seconds\n",
      "Step 9300, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 522.9623482227325 seconds\n",
      "Step 9400, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 528.6061420440674 seconds\n",
      "Step 9500, loss: tensor(0.0856, grad_fn=<SubBackward0>), time elapsed: 534.1157503128052 seconds\n",
      "Step 9600, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 539.7808523178101 seconds\n",
      "Step 9700, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 545.2888009548187 seconds\n",
      "Step 9800, loss: tensor(0.0834, grad_fn=<SubBackward0>), time elapsed: 550.8068788051605 seconds\n",
      "Step 9900, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 556.490473985672 seconds\n",
      "Step 10000, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 562.0432133674622 seconds\n",
      "Step 10100, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 567.7386200428009 seconds\n",
      "Step 10200, loss: tensor(0.0829, grad_fn=<SubBackward0>), time elapsed: 573.2413337230682 seconds\n",
      "Step 10300, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 578.735963344574 seconds\n",
      "Step 10400, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 584.4220261573792 seconds\n",
      "Step 10500, loss: tensor(0.0838, grad_fn=<SubBackward0>), time elapsed: 589.9419734477997 seconds\n",
      "Step 10600, loss: tensor(0.0840, grad_fn=<SubBackward0>), time elapsed: 595.6065537929535 seconds\n",
      "Step 10700, loss: tensor(0.0854, grad_fn=<SubBackward0>), time elapsed: 601.1274037361145 seconds\n",
      "Step 10800, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 606.8149135112762 seconds\n",
      "Step 10900, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 612.3506281375885 seconds\n",
      "Step 11000, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 617.8863000869751 seconds\n",
      "Step 11100, loss: tensor(0.0852, grad_fn=<SubBackward0>), time elapsed: 623.5642166137695 seconds\n",
      "Step 11200, loss: tensor(0.0833, grad_fn=<SubBackward0>), time elapsed: 629.0907504558563 seconds\n",
      "Step 11300, loss: tensor(0.0839, grad_fn=<SubBackward0>), time elapsed: 634.7890930175781 seconds\n",
      "Step 11400, loss: tensor(0.0809, grad_fn=<SubBackward0>), time elapsed: 640.3459007740021 seconds\n",
      "Step 11500, loss: tensor(0.0845, grad_fn=<SubBackward0>), time elapsed: 646.2340710163116 seconds\n",
      "Step 11600, loss: tensor(0.0848, grad_fn=<SubBackward0>), time elapsed: 651.9247403144836 seconds\n",
      "Step 11700, loss: tensor(0.0837, grad_fn=<SubBackward0>), time elapsed: 657.4716284275055 seconds\n",
      "Step 11800, loss: tensor(0.0846, grad_fn=<SubBackward0>), time elapsed: 663.1652472019196 seconds\n",
      "Step 11900, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 668.6839876174927 seconds\n",
      "Step 12000, loss: tensor(0.0813, grad_fn=<SubBackward0>), time elapsed: 674.4261243343353 seconds\n",
      "Step 12100, loss: tensor(0.0826, grad_fn=<SubBackward0>), time elapsed: 679.942088842392 seconds\n",
      "Step 12200, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 686.2031238079071 seconds\n",
      "Step 12300, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 691.892386674881 seconds\n",
      "Step 12400, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 697.4202644824982 seconds\n",
      "Step 12500, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 703.091105222702 seconds\n",
      "Step 12600, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 708.6271376609802 seconds\n",
      "Step 12700, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 714.2588460445404 seconds\n",
      "Step 12800, loss: tensor(0.0819, grad_fn=<SubBackward0>), time elapsed: 720.7527046203613 seconds\n",
      "Step 12900, loss: tensor(0.0802, grad_fn=<SubBackward0>), time elapsed: 726.264258146286 seconds\n",
      "Step 13000, loss: tensor(0.0805, grad_fn=<SubBackward0>), time elapsed: 731.9313812255859 seconds\n",
      "Step 13100, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 737.490659236908 seconds\n",
      "Step 13200, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 743.1541695594788 seconds\n",
      "Step 13300, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 748.6570460796356 seconds\n",
      "Step 13400, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 754.1950271129608 seconds\n",
      "Step 13500, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 759.9047477245331 seconds\n",
      "Step 13600, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 765.4645259380341 seconds\n",
      "Step 13700, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 771.1920506954193 seconds\n",
      "Step 13800, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 776.7399156093597 seconds\n",
      "Step 13900, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 782.2929570674896 seconds\n",
      "Step 14000, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 788.0089118480682 seconds\n",
      "Step 14100, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 793.5523526668549 seconds\n",
      "Step 14200, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 799.4713568687439 seconds\n",
      "Step 14300, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 805.0211291313171 seconds\n",
      "Step 14400, loss: tensor(0.0720, grad_fn=<SubBackward0>), time elapsed: 810.8129241466522 seconds\n",
      "Step 14500, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 816.4064371585846 seconds\n",
      "Step 14600, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 822.0110766887665 seconds\n",
      "Step 14700, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 829.4356262683868 seconds\n",
      "Step 14800, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 834.9770126342773 seconds\n",
      "Step 14900, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 840.6798319816589 seconds\n",
      "Step 15000, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 846.2099134922028 seconds\n",
      "Step 15100, loss: tensor(0.0728, grad_fn=<SubBackward0>), time elapsed: 851.7436838150024 seconds\n",
      "Step 15200, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 857.438560962677 seconds\n",
      "Step 15300, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 862.9740941524506 seconds\n",
      "Step 15400, loss: tensor(0.0701, grad_fn=<SubBackward0>), time elapsed: 868.6587691307068 seconds\n",
      "Step 15500, loss: tensor(0.0721, grad_fn=<SubBackward0>), time elapsed: 874.1878924369812 seconds\n",
      "Step 15600, loss: tensor(0.0728, grad_fn=<SubBackward0>), time elapsed: 879.8690364360809 seconds\n",
      "Step 15700, loss: tensor(0.0700, grad_fn=<SubBackward0>), time elapsed: 885.392781496048 seconds\n",
      "Step 15800, loss: tensor(0.0719, grad_fn=<SubBackward0>), time elapsed: 890.940096616745 seconds\n",
      "Step 15900, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 896.6258842945099 seconds\n",
      "Step 16000, loss: tensor(0.0715, grad_fn=<SubBackward0>), time elapsed: 902.146240234375 seconds\n",
      "Step 16100, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 907.9746460914612 seconds\n",
      "Step 16200, loss: tensor(0.0719, grad_fn=<SubBackward0>), time elapsed: 913.5090169906616 seconds\n",
      "Step 16300, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 919.0538387298584 seconds\n",
      "Step 16400, loss: tensor(0.0701, grad_fn=<SubBackward0>), time elapsed: 924.7681548595428 seconds\n",
      "Step 16500, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 930.2952003479004 seconds\n",
      "Step 16600, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 936.3065390586853 seconds\n",
      "Step 16700, loss: tensor(0.0703, grad_fn=<SubBackward0>), time elapsed: 941.843498468399 seconds\n",
      "Step 16800, loss: tensor(0.0708, grad_fn=<SubBackward0>), time elapsed: 947.3911111354828 seconds\n",
      "Step 16900, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 953.111980676651 seconds\n",
      "Step 17000, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 958.6562175750732 seconds\n",
      "Step 17100, loss: tensor(0.0682, grad_fn=<SubBackward0>), time elapsed: 964.35626745224 seconds\n",
      "Step 17200, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 969.9033029079437 seconds\n",
      "Step 17300, loss: tensor(0.0701, grad_fn=<SubBackward0>), time elapsed: 975.5986108779907 seconds\n",
      "Step 17400, loss: tensor(0.0712, grad_fn=<SubBackward0>), time elapsed: 981.1729204654694 seconds\n",
      "Step 17500, loss: tensor(0.0676, grad_fn=<SubBackward0>), time elapsed: 986.7095324993134 seconds\n",
      "Step 17600, loss: tensor(0.0673, grad_fn=<SubBackward0>), time elapsed: 992.4306647777557 seconds\n",
      "Step 17700, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 997.9745721817017 seconds\n",
      "Step 17800, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 1004.3386538028717 seconds\n",
      "Step 17900, loss: tensor(0.0678, grad_fn=<SubBackward0>), time elapsed: 1009.9074931144714 seconds\n",
      "Step 18000, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 1015.4733119010925 seconds\n",
      "Step 18100, loss: tensor(0.0671, grad_fn=<SubBackward0>), time elapsed: 1021.1605265140533 seconds\n",
      "Step 18200, loss: tensor(0.0721, grad_fn=<SubBackward0>), time elapsed: 1026.6853835582733 seconds\n",
      "Step 18300, loss: tensor(0.0676, grad_fn=<SubBackward0>), time elapsed: 1032.3889744281769 seconds\n",
      "Step 18400, loss: tensor(0.0677, grad_fn=<SubBackward0>), time elapsed: 1037.9504313468933 seconds\n",
      "Step 18500, loss: tensor(0.0689, grad_fn=<SubBackward0>), time elapsed: 1043.6925644874573 seconds\n",
      "Step 18600, loss: tensor(0.0678, grad_fn=<SubBackward0>), time elapsed: 1049.242736339569 seconds\n",
      "Step 18700, loss: tensor(0.0673, grad_fn=<SubBackward0>), time elapsed: 1054.8013832569122 seconds\n",
      "Step 18800, loss: tensor(0.0666, grad_fn=<SubBackward0>), time elapsed: 1060.5497028827667 seconds\n",
      "Step 18900, loss: tensor(0.0662, grad_fn=<SubBackward0>), time elapsed: 1066.1200411319733 seconds\n",
      "Step 19000, loss: tensor(0.0651, grad_fn=<SubBackward0>), time elapsed: 1071.9036588668823 seconds\n",
      "Step 19100, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 1077.4908678531647 seconds\n",
      "Step 19200, loss: tensor(0.0624, grad_fn=<SubBackward0>), time elapsed: 1083.0265316963196 seconds\n",
      "Step 19300, loss: tensor(0.0677, grad_fn=<SubBackward0>), time elapsed: 1088.764208316803 seconds\n",
      "Step 19400, loss: tensor(0.0648, grad_fn=<SubBackward0>), time elapsed: 1094.319179058075 seconds\n",
      "Step 19500, loss: tensor(0.0657, grad_fn=<SubBackward0>), time elapsed: 1100.0799469947815 seconds\n",
      "Step 19600, loss: tensor(0.0656, grad_fn=<SubBackward0>), time elapsed: 1105.6524164676666 seconds\n",
      "Step 19700, loss: tensor(0.0643, grad_fn=<SubBackward0>), time elapsed: 1111.1856527328491 seconds\n",
      "Step 19800, loss: tensor(0.0649, grad_fn=<SubBackward0>), time elapsed: 1116.9231343269348 seconds\n",
      "Step 19900, loss: tensor(0.0681, grad_fn=<SubBackward0>), time elapsed: 1122.4857308864594 seconds\n",
      "Step 20000, loss: tensor(0.0674, grad_fn=<SubBackward0>), time elapsed: 1128.1913888454437 seconds\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(0.9006, grad_fn=<SubBackward0>), time elapsed: 0.0589601993560791 seconds\n",
      "Step 100, loss: tensor(0.8347, grad_fn=<SubBackward0>), time elapsed: 5.685437202453613 seconds\n",
      "Step 200, loss: tensor(0.7449, grad_fn=<SubBackward0>), time elapsed: 11.38449478149414 seconds\n",
      "Step 300, loss: tensor(0.6671, grad_fn=<SubBackward0>), time elapsed: 16.866544723510742 seconds\n",
      "Step 400, loss: tensor(0.6072, grad_fn=<SubBackward0>), time elapsed: 22.361700296401978 seconds\n",
      "Step 500, loss: tensor(0.5596, grad_fn=<SubBackward0>), time elapsed: 27.980072259902954 seconds\n",
      "Step 600, loss: tensor(0.5040, grad_fn=<SubBackward0>), time elapsed: 33.497756242752075 seconds\n",
      "Step 700, loss: tensor(0.4467, grad_fn=<SubBackward0>), time elapsed: 39.14647674560547 seconds\n",
      "Step 800, loss: tensor(0.3918, grad_fn=<SubBackward0>), time elapsed: 44.674232006073 seconds\n",
      "Step 900, loss: tensor(0.3541, grad_fn=<SubBackward0>), time elapsed: 50.32330775260925 seconds\n",
      "Step 1000, loss: tensor(0.3122, grad_fn=<SubBackward0>), time elapsed: 55.838579177856445 seconds\n",
      "Step 1100, loss: tensor(0.2707, grad_fn=<SubBackward0>), time elapsed: 61.36057424545288 seconds\n",
      "Step 1200, loss: tensor(0.2358, grad_fn=<SubBackward0>), time elapsed: 66.9978895187378 seconds\n",
      "Step 1300, loss: tensor(0.1987, grad_fn=<SubBackward0>), time elapsed: 72.52506995201111 seconds\n",
      "Step 1400, loss: tensor(0.1686, grad_fn=<SubBackward0>), time elapsed: 78.16832375526428 seconds\n",
      "Step 1500, loss: tensor(0.1536, grad_fn=<SubBackward0>), time elapsed: 83.67292618751526 seconds\n",
      "Step 1600, loss: tensor(0.1420, grad_fn=<SubBackward0>), time elapsed: 89.31725788116455 seconds\n",
      "Step 1700, loss: tensor(0.1375, grad_fn=<SubBackward0>), time elapsed: 94.81732845306396 seconds\n",
      "Step 1800, loss: tensor(0.1332, grad_fn=<SubBackward0>), time elapsed: 100.335458278656 seconds\n",
      "Step 1900, loss: tensor(0.1317, grad_fn=<SubBackward0>), time elapsed: 105.996906042099 seconds\n",
      "Step 2000, loss: tensor(0.1309, grad_fn=<SubBackward0>), time elapsed: 111.50765299797058 seconds\n",
      "Step 2100, loss: tensor(0.1293, grad_fn=<SubBackward0>), time elapsed: 117.1560845375061 seconds\n",
      "Step 2200, loss: tensor(0.1289, grad_fn=<SubBackward0>), time elapsed: 122.65601682662964 seconds\n",
      "Step 2300, loss: tensor(0.1282, grad_fn=<SubBackward0>), time elapsed: 128.15339517593384 seconds\n",
      "Step 2400, loss: tensor(0.1289, grad_fn=<SubBackward0>), time elapsed: 133.81958031654358 seconds\n",
      "Step 2500, loss: tensor(0.1280, grad_fn=<SubBackward0>), time elapsed: 139.32093024253845 seconds\n",
      "Step 2600, loss: tensor(0.1274, grad_fn=<SubBackward0>), time elapsed: 144.96576595306396 seconds\n",
      "Step 2700, loss: tensor(0.1275, grad_fn=<SubBackward0>), time elapsed: 150.4640429019928 seconds\n",
      "Step 2800, loss: tensor(0.1273, grad_fn=<SubBackward0>), time elapsed: 156.1130726337433 seconds\n",
      "Step 2900, loss: tensor(0.1274, grad_fn=<SubBackward0>), time elapsed: 161.59652972221375 seconds\n",
      "Step 3000, loss: tensor(0.1271, grad_fn=<SubBackward0>), time elapsed: 167.133873462677 seconds\n",
      "Step 3100, loss: tensor(0.1265, grad_fn=<SubBackward0>), time elapsed: 172.77470302581787 seconds\n",
      "Step 3200, loss: tensor(0.1265, grad_fn=<SubBackward0>), time elapsed: 178.2645947933197 seconds\n",
      "Step 3300, loss: tensor(0.1267, grad_fn=<SubBackward0>), time elapsed: 183.89729237556458 seconds\n",
      "Step 3400, loss: tensor(0.1261, grad_fn=<SubBackward0>), time elapsed: 189.41304278373718 seconds\n",
      "Step 3500, loss: tensor(0.1261, grad_fn=<SubBackward0>), time elapsed: 195.11370730400085 seconds\n",
      "Step 3600, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 200.6266894340515 seconds\n",
      "Step 3700, loss: tensor(0.1263, grad_fn=<SubBackward0>), time elapsed: 206.1461329460144 seconds\n",
      "Step 3800, loss: tensor(0.1263, grad_fn=<SubBackward0>), time elapsed: 211.77829670906067 seconds\n",
      "Step 3900, loss: tensor(0.1261, grad_fn=<SubBackward0>), time elapsed: 217.29269361495972 seconds\n",
      "Step 4000, loss: tensor(0.1260, grad_fn=<SubBackward0>), time elapsed: 222.93043160438538 seconds\n",
      "Step 4100, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 228.47283720970154 seconds\n",
      "Step 4200, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 233.98174214363098 seconds\n",
      "Step 4300, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 239.63052678108215 seconds\n",
      "Step 4400, loss: tensor(0.1255, grad_fn=<SubBackward0>), time elapsed: 245.14523363113403 seconds\n",
      "Step 4500, loss: tensor(0.1255, grad_fn=<SubBackward0>), time elapsed: 250.79987907409668 seconds\n",
      "Step 4600, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 256.33619022369385 seconds\n",
      "Step 4700, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 261.9691514968872 seconds\n",
      "Step 4800, loss: tensor(0.1254, grad_fn=<SubBackward0>), time elapsed: 267.46748304367065 seconds\n",
      "Step 4900, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 272.9828522205353 seconds\n",
      "Step 5000, loss: tensor(0.1255, grad_fn=<SubBackward0>), time elapsed: 278.62629985809326 seconds\n",
      "Step 5100, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 284.1362340450287 seconds\n",
      "Step 5200, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 289.79220962524414 seconds\n",
      "Step 5300, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 295.30193758010864 seconds\n",
      "Step 5400, loss: tensor(0.1253, grad_fn=<SubBackward0>), time elapsed: 300.9367835521698 seconds\n",
      "Step 5500, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 306.42805552482605 seconds\n",
      "Step 5600, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 311.9183497428894 seconds\n",
      "Step 5700, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 317.6050179004669 seconds\n",
      "Step 5800, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 323.11590027809143 seconds\n",
      "Step 5900, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 328.7794120311737 seconds\n",
      "Step 6000, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 334.2915029525757 seconds\n",
      "Step 6100, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 339.8208341598511 seconds\n",
      "Step 6200, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 345.4687931537628 seconds\n",
      "Step 6300, loss: tensor(0.1250, grad_fn=<SubBackward0>), time elapsed: 351.0031487941742 seconds\n",
      "Step 6400, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 356.61629819869995 seconds\n",
      "Step 6500, loss: tensor(0.1249, grad_fn=<SubBackward0>), time elapsed: 362.1283931732178 seconds\n",
      "Step 6600, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 367.79641008377075 seconds\n",
      "Step 6700, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 373.30381083488464 seconds\n",
      "Step 6800, loss: tensor(0.1246, grad_fn=<SubBackward0>), time elapsed: 378.8469820022583 seconds\n",
      "Step 6900, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 384.4952335357666 seconds\n",
      "Step 7000, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 390.01288890838623 seconds\n",
      "Step 7100, loss: tensor(0.1248, grad_fn=<SubBackward0>), time elapsed: 395.65164589881897 seconds\n",
      "Step 7200, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 401.136745929718 seconds\n",
      "Step 7300, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 406.7719361782074 seconds\n",
      "Step 7400, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 412.2852351665497 seconds\n",
      "Step 7500, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 417.79533195495605 seconds\n",
      "Step 7600, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 423.46334505081177 seconds\n",
      "Step 7700, loss: tensor(0.1242, grad_fn=<SubBackward0>), time elapsed: 428.9919717311859 seconds\n",
      "Step 7800, loss: tensor(0.1240, grad_fn=<SubBackward0>), time elapsed: 434.6622791290283 seconds\n",
      "Step 7900, loss: tensor(0.1240, grad_fn=<SubBackward0>), time elapsed: 440.21053767204285 seconds\n",
      "Step 8000, loss: tensor(0.1237, grad_fn=<SubBackward0>), time elapsed: 445.7121877670288 seconds\n",
      "Step 8100, loss: tensor(0.1236, grad_fn=<SubBackward0>), time elapsed: 451.38631534576416 seconds\n",
      "Step 8200, loss: tensor(0.1234, grad_fn=<SubBackward0>), time elapsed: 457.09366035461426 seconds\n",
      "Step 8300, loss: tensor(0.1233, grad_fn=<SubBackward0>), time elapsed: 462.72738432884216 seconds\n",
      "Step 8400, loss: tensor(0.1232, grad_fn=<SubBackward0>), time elapsed: 468.70948791503906 seconds\n",
      "Step 8500, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 474.36397981643677 seconds\n",
      "Step 8600, loss: tensor(0.1230, grad_fn=<SubBackward0>), time elapsed: 479.8702247142792 seconds\n",
      "Step 8700, loss: tensor(0.1228, grad_fn=<SubBackward0>), time elapsed: 485.38734459877014 seconds\n",
      "Step 8800, loss: tensor(0.1228, grad_fn=<SubBackward0>), time elapsed: 491.03122305870056 seconds\n",
      "Step 8900, loss: tensor(0.1226, grad_fn=<SubBackward0>), time elapsed: 496.57352924346924 seconds\n",
      "Step 9000, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 502.2739214897156 seconds\n",
      "Step 9100, loss: tensor(0.1224, grad_fn=<SubBackward0>), time elapsed: 507.83039140701294 seconds\n",
      "Step 9200, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 513.4098234176636 seconds\n",
      "Step 9300, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 519.2541375160217 seconds\n",
      "Step 9400, loss: tensor(0.1224, grad_fn=<SubBackward0>), time elapsed: 524.8406705856323 seconds\n",
      "Step 9500, loss: tensor(0.1223, grad_fn=<SubBackward0>), time elapsed: 530.5392289161682 seconds\n",
      "Step 9600, loss: tensor(0.1222, grad_fn=<SubBackward0>), time elapsed: 536.095682144165 seconds\n",
      "Step 9700, loss: tensor(0.1221, grad_fn=<SubBackward0>), time elapsed: 541.8161988258362 seconds\n",
      "Step 9800, loss: tensor(0.1220, grad_fn=<SubBackward0>), time elapsed: 548.6547391414642 seconds\n",
      "Step 9900, loss: tensor(0.1216, grad_fn=<SubBackward0>), time elapsed: 554.169615983963 seconds\n",
      "Step 10000, loss: tensor(0.1215, grad_fn=<SubBackward0>), time elapsed: 559.8807461261749 seconds\n",
      "Step 10100, loss: tensor(0.1214, grad_fn=<SubBackward0>), time elapsed: 565.400110244751 seconds\n",
      "Step 10200, loss: tensor(0.1210, grad_fn=<SubBackward0>), time elapsed: 571.0764985084534 seconds\n",
      "Step 10300, loss: tensor(0.1210, grad_fn=<SubBackward0>), time elapsed: 576.6062614917755 seconds\n",
      "Step 10400, loss: tensor(0.1200, grad_fn=<SubBackward0>), time elapsed: 582.1375465393066 seconds\n",
      "Step 10500, loss: tensor(0.1184, grad_fn=<SubBackward0>), time elapsed: 588.3636810779572 seconds\n",
      "Step 10600, loss: tensor(0.1122, grad_fn=<SubBackward0>), time elapsed: 593.9109020233154 seconds\n",
      "Step 10700, loss: tensor(0.1035, grad_fn=<SubBackward0>), time elapsed: 599.5703639984131 seconds\n",
      "Step 10800, loss: tensor(0.0983, grad_fn=<SubBackward0>), time elapsed: 605.0879509449005 seconds\n",
      "Step 10900, loss: tensor(0.0971, grad_fn=<SubBackward0>), time elapsed: 610.7476110458374 seconds\n",
      "Step 11000, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 616.2652924060822 seconds\n",
      "Step 11100, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 621.8000500202179 seconds\n",
      "Step 11200, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 627.4642395973206 seconds\n",
      "Step 11300, loss: tensor(0.0864, grad_fn=<SubBackward0>), time elapsed: 632.9707005023956 seconds\n",
      "Step 11400, loss: tensor(0.0834, grad_fn=<SubBackward0>), time elapsed: 638.6702828407288 seconds\n",
      "Step 11500, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 644.1937870979309 seconds\n",
      "Step 11600, loss: tensor(0.0815, grad_fn=<SubBackward0>), time elapsed: 649.7442600727081 seconds\n",
      "Step 11700, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 655.4151754379272 seconds\n",
      "Step 11800, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 660.9543333053589 seconds\n",
      "Step 11900, loss: tensor(0.0793, grad_fn=<SubBackward0>), time elapsed: 666.6457672119141 seconds\n",
      "Step 12000, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 672.1971890926361 seconds\n",
      "Step 12100, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 677.8685023784637 seconds\n",
      "Step 12200, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 683.4238965511322 seconds\n",
      "Step 12300, loss: tensor(0.0785, grad_fn=<SubBackward0>), time elapsed: 688.9389963150024 seconds\n",
      "Step 12400, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 694.6184952259064 seconds\n",
      "Step 12500, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 700.1529939174652 seconds\n",
      "Step 12600, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 705.8291256427765 seconds\n",
      "Step 12700, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 711.3538262844086 seconds\n",
      "Step 12800, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 716.8952696323395 seconds\n",
      "Step 12900, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 723.9145405292511 seconds\n",
      "Step 13000, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 729.4602553844452 seconds\n",
      "Step 13100, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 735.1461985111237 seconds\n",
      "Step 13200, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 740.658447265625 seconds\n",
      "Step 13300, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 746.3378534317017 seconds\n",
      "Step 13400, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 751.9048082828522 seconds\n",
      "Step 13500, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 757.4377663135529 seconds\n",
      "Step 13600, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 763.1254546642303 seconds\n",
      "Step 13700, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 768.6440889835358 seconds\n",
      "Step 13800, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 774.3513581752777 seconds\n",
      "Step 13900, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 779.8813421726227 seconds\n",
      "Step 14000, loss: tensor(0.0730, grad_fn=<SubBackward0>), time elapsed: 785.4156968593597 seconds\n",
      "Step 14100, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 791.0927317142487 seconds\n",
      "Step 14200, loss: tensor(0.0725, grad_fn=<SubBackward0>), time elapsed: 796.6431844234467 seconds\n",
      "Step 14300, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 802.3137023448944 seconds\n",
      "Step 14400, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 807.8585379123688 seconds\n",
      "Step 14500, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 813.5299725532532 seconds\n",
      "Step 14600, loss: tensor(0.0724, grad_fn=<SubBackward0>), time elapsed: 819.0455651283264 seconds\n",
      "Step 14700, loss: tensor(0.0724, grad_fn=<SubBackward0>), time elapsed: 824.5815489292145 seconds\n",
      "Step 14800, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 830.2527358531952 seconds\n",
      "Step 14900, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 835.831887960434 seconds\n",
      "Step 15000, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 841.5136342048645 seconds\n",
      "Step 15100, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 847.0333671569824 seconds\n",
      "Step 15200, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 852.5594940185547 seconds\n",
      "Step 15300, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 858.2485692501068 seconds\n",
      "Step 15400, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 863.7578725814819 seconds\n",
      "Step 15500, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 869.442877292633 seconds\n",
      "Step 15600, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 874.9418082237244 seconds\n",
      "Step 15700, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 880.6845374107361 seconds\n",
      "Step 15800, loss: tensor(0.0714, grad_fn=<SubBackward0>), time elapsed: 886.2071025371552 seconds\n",
      "Step 15900, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 891.7472791671753 seconds\n",
      "Step 16000, loss: tensor(0.0726, grad_fn=<SubBackward0>), time elapsed: 897.4452726840973 seconds\n",
      "Step 16100, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 902.9996609687805 seconds\n",
      "Step 16200, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 908.6851408481598 seconds\n",
      "Step 16300, loss: tensor(0.0717, grad_fn=<SubBackward0>), time elapsed: 914.2064793109894 seconds\n",
      "Step 16400, loss: tensor(0.0709, grad_fn=<SubBackward0>), time elapsed: 919.7341768741608 seconds\n",
      "Step 16500, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 925.4613420963287 seconds\n",
      "Step 16600, loss: tensor(0.0717, grad_fn=<SubBackward0>), time elapsed: 931.0260698795319 seconds\n",
      "Step 16700, loss: tensor(0.0723, grad_fn=<SubBackward0>), time elapsed: 936.7593252658844 seconds\n",
      "Step 16800, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 942.3120663166046 seconds\n",
      "Step 16900, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 947.8682975769043 seconds\n",
      "Step 17000, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 953.5881226062775 seconds\n",
      "Step 17100, loss: tensor(0.0674, grad_fn=<SubBackward0>), time elapsed: 959.3082129955292 seconds\n",
      "Step 17200, loss: tensor(0.0696, grad_fn=<SubBackward0>), time elapsed: 965.0532808303833 seconds\n",
      "Step 17300, loss: tensor(0.0690, grad_fn=<SubBackward0>), time elapsed: 970.6271409988403 seconds\n",
      "Step 17400, loss: tensor(0.0703, grad_fn=<SubBackward0>), time elapsed: 977.4132900238037 seconds\n",
      "Step 17500, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 982.9929776191711 seconds\n",
      "Step 17600, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 988.5647337436676 seconds\n",
      "Step 17700, loss: tensor(0.0710, grad_fn=<SubBackward0>), time elapsed: 994.2817220687866 seconds\n",
      "Step 17800, loss: tensor(0.0702, grad_fn=<SubBackward0>), time elapsed: 999.8199117183685 seconds\n",
      "Step 17900, loss: tensor(0.0711, grad_fn=<SubBackward0>), time elapsed: 1005.5386884212494 seconds\n",
      "Step 18000, loss: tensor(0.0705, grad_fn=<SubBackward0>), time elapsed: 1011.085953950882 seconds\n",
      "Step 18100, loss: tensor(0.0704, grad_fn=<SubBackward0>), time elapsed: 1016.6277616024017 seconds\n",
      "Step 18200, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 1022.3486104011536 seconds\n",
      "Step 18300, loss: tensor(0.0702, grad_fn=<SubBackward0>), time elapsed: 1027.9026205539703 seconds\n",
      "Step 18400, loss: tensor(0.0679, grad_fn=<SubBackward0>), time elapsed: 1033.6269624233246 seconds\n",
      "Step 18500, loss: tensor(0.0694, grad_fn=<SubBackward0>), time elapsed: 1039.1865587234497 seconds\n",
      "Step 18600, loss: tensor(0.0698, grad_fn=<SubBackward0>), time elapsed: 1044.8931646347046 seconds\n",
      "Step 18700, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 1050.4910371303558 seconds\n",
      "Step 18800, loss: tensor(0.0704, grad_fn=<SubBackward0>), time elapsed: 1056.0720779895782 seconds\n",
      "Step 18900, loss: tensor(0.0703, grad_fn=<SubBackward0>), time elapsed: 1061.8197619915009 seconds\n",
      "Step 19000, loss: tensor(0.0698, grad_fn=<SubBackward0>), time elapsed: 1067.42076253891 seconds\n",
      "Step 19100, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 1073.1322212219238 seconds\n",
      "Step 19200, loss: tensor(0.0685, grad_fn=<SubBackward0>), time elapsed: 1078.6815793514252 seconds\n",
      "Step 19300, loss: tensor(0.0684, grad_fn=<SubBackward0>), time elapsed: 1084.2178654670715 seconds\n",
      "Step 19400, loss: tensor(0.0688, grad_fn=<SubBackward0>), time elapsed: 1089.947408914566 seconds\n",
      "Step 19500, loss: tensor(0.0706, grad_fn=<SubBackward0>), time elapsed: 1095.5418405532837 seconds\n",
      "Step 19600, loss: tensor(0.0702, grad_fn=<SubBackward0>), time elapsed: 1101.639924287796 seconds\n",
      "Step 19700, loss: tensor(0.0711, grad_fn=<SubBackward0>), time elapsed: 1107.1912875175476 seconds\n",
      "Step 19800, loss: tensor(0.0693, grad_fn=<SubBackward0>), time elapsed: 1112.7883338928223 seconds\n",
      "Step 19900, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 1118.4788446426392 seconds\n",
      "Step 20000, loss: tensor(0.0684, grad_fn=<SubBackward0>), time elapsed: 1124.0350768566132 seconds\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(1.0626, grad_fn=<SubBackward0>), time elapsed: 0.06366205215454102 seconds\n",
      "Step 100, loss: tensor(0.9619, grad_fn=<SubBackward0>), time elapsed: 5.856937646865845 seconds\n",
      "Step 200, loss: tensor(0.8342, grad_fn=<SubBackward0>), time elapsed: 11.411599397659302 seconds\n",
      "Step 300, loss: tensor(0.7113, grad_fn=<SubBackward0>), time elapsed: 17.04464101791382 seconds\n",
      "Step 400, loss: tensor(0.6038, grad_fn=<SubBackward0>), time elapsed: 22.575085639953613 seconds\n",
      "Step 500, loss: tensor(0.5203, grad_fn=<SubBackward0>), time elapsed: 28.08791184425354 seconds\n",
      "Step 600, loss: tensor(0.4396, grad_fn=<SubBackward0>), time elapsed: 33.69665718078613 seconds\n",
      "Step 700, loss: tensor(0.3817, grad_fn=<SubBackward0>), time elapsed: 39.20098900794983 seconds\n",
      "Step 800, loss: tensor(0.3364, grad_fn=<SubBackward0>), time elapsed: 44.82703495025635 seconds\n",
      "Step 900, loss: tensor(0.2882, grad_fn=<SubBackward0>), time elapsed: 50.357731103897095 seconds\n",
      "Step 1000, loss: tensor(0.2341, grad_fn=<SubBackward0>), time elapsed: 55.98743271827698 seconds\n",
      "Step 1100, loss: tensor(0.2076, grad_fn=<SubBackward0>), time elapsed: 61.47940421104431 seconds\n",
      "Step 1200, loss: tensor(0.1865, grad_fn=<SubBackward0>), time elapsed: 66.97989964485168 seconds\n",
      "Step 1300, loss: tensor(0.1693, grad_fn=<SubBackward0>), time elapsed: 72.60607314109802 seconds\n",
      "Step 1400, loss: tensor(0.1535, grad_fn=<SubBackward0>), time elapsed: 78.10436916351318 seconds\n",
      "Step 1500, loss: tensor(0.1438, grad_fn=<SubBackward0>), time elapsed: 83.75465273857117 seconds\n",
      "Step 1600, loss: tensor(0.1310, grad_fn=<SubBackward0>), time elapsed: 89.24928283691406 seconds\n",
      "Step 1700, loss: tensor(0.1279, grad_fn=<SubBackward0>), time elapsed: 94.87662768363953 seconds\n",
      "Step 1800, loss: tensor(0.1153, grad_fn=<SubBackward0>), time elapsed: 100.38036894798279 seconds\n",
      "Step 1900, loss: tensor(0.1126, grad_fn=<SubBackward0>), time elapsed: 105.88838601112366 seconds\n",
      "Step 2000, loss: tensor(0.1119, grad_fn=<SubBackward0>), time elapsed: 111.56647634506226 seconds\n",
      "Step 2100, loss: tensor(0.1085, grad_fn=<SubBackward0>), time elapsed: 117.06910133361816 seconds\n",
      "Step 2200, loss: tensor(0.1026, grad_fn=<SubBackward0>), time elapsed: 122.71047902107239 seconds\n",
      "Step 2300, loss: tensor(0.1028, grad_fn=<SubBackward0>), time elapsed: 128.21481943130493 seconds\n",
      "Step 2400, loss: tensor(0.0948, grad_fn=<SubBackward0>), time elapsed: 133.8704056739807 seconds\n",
      "Step 2500, loss: tensor(0.0955, grad_fn=<SubBackward0>), time elapsed: 139.3709762096405 seconds\n",
      "Step 2600, loss: tensor(0.0928, grad_fn=<SubBackward0>), time elapsed: 144.91666746139526 seconds\n",
      "Step 2700, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 150.60518884658813 seconds\n",
      "Step 2800, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 156.13659811019897 seconds\n",
      "Step 2900, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 161.77836179733276 seconds\n",
      "Step 3000, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 167.29023385047913 seconds\n",
      "Step 3100, loss: tensor(0.0854, grad_fn=<SubBackward0>), time elapsed: 172.81529355049133 seconds\n",
      "Step 3200, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 178.4465720653534 seconds\n",
      "Step 3300, loss: tensor(0.0853, grad_fn=<SubBackward0>), time elapsed: 183.94220399856567 seconds\n",
      "Step 3400, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 189.59465646743774 seconds\n",
      "Step 3500, loss: tensor(0.0828, grad_fn=<SubBackward0>), time elapsed: 195.1121735572815 seconds\n",
      "Step 3600, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 200.76309466362 seconds\n",
      "Step 3700, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 206.274995803833 seconds\n",
      "Step 3800, loss: tensor(0.0796, grad_fn=<SubBackward0>), time elapsed: 211.77875804901123 seconds\n",
      "Step 3900, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 217.4238748550415 seconds\n",
      "Step 4000, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 222.9165494441986 seconds\n",
      "Step 4100, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 228.56310725212097 seconds\n",
      "Step 4200, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 234.08530378341675 seconds\n",
      "Step 4300, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 239.74097442626953 seconds\n",
      "Step 4400, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 245.24835348129272 seconds\n",
      "Step 4500, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 250.7830512523651 seconds\n",
      "Step 4600, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 256.4469828605652 seconds\n",
      "Step 4700, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 261.9753084182739 seconds\n",
      "Step 4800, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 267.62809896469116 seconds\n",
      "Step 4900, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 274.2188847064972 seconds\n",
      "Step 5000, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 280.2834663391113 seconds\n",
      "Step 5100, loss: tensor(0.0722, grad_fn=<SubBackward0>), time elapsed: 285.96069383621216 seconds\n",
      "Step 5200, loss: tensor(0.0716, grad_fn=<SubBackward0>), time elapsed: 291.5083785057068 seconds\n",
      "Step 5300, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 297.1924498081207 seconds\n",
      "Step 5400, loss: tensor(0.0705, grad_fn=<SubBackward0>), time elapsed: 302.708922624588 seconds\n",
      "Step 5500, loss: tensor(0.0686, grad_fn=<SubBackward0>), time elapsed: 308.37460803985596 seconds\n",
      "Step 5600, loss: tensor(0.0691, grad_fn=<SubBackward0>), time elapsed: 314.07137966156006 seconds\n",
      "Step 5700, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 319.5717968940735 seconds\n",
      "Step 5800, loss: tensor(0.0674, grad_fn=<SubBackward0>), time elapsed: 325.2573926448822 seconds\n",
      "Step 5900, loss: tensor(0.0694, grad_fn=<SubBackward0>), time elapsed: 330.7734658718109 seconds\n",
      "Step 6000, loss: tensor(0.0660, grad_fn=<SubBackward0>), time elapsed: 336.4769811630249 seconds\n",
      "Step 6100, loss: tensor(0.0656, grad_fn=<SubBackward0>), time elapsed: 342.21183943748474 seconds\n",
      "Step 6200, loss: tensor(0.0646, grad_fn=<SubBackward0>), time elapsed: 347.86743330955505 seconds\n",
      "Step 6300, loss: tensor(0.0646, grad_fn=<SubBackward0>), time elapsed: 353.395299911499 seconds\n",
      "Step 6400, loss: tensor(0.0657, grad_fn=<SubBackward0>), time elapsed: 358.9256327152252 seconds\n",
      "Step 6500, loss: tensor(0.0642, grad_fn=<SubBackward0>), time elapsed: 364.60861325263977 seconds\n",
      "Step 6600, loss: tensor(0.0645, grad_fn=<SubBackward0>), time elapsed: 370.14488339424133 seconds\n",
      "Step 6700, loss: tensor(0.0651, grad_fn=<SubBackward0>), time elapsed: 375.83851528167725 seconds\n",
      "Step 6800, loss: tensor(0.0622, grad_fn=<SubBackward0>), time elapsed: 381.4244980812073 seconds\n",
      "Step 6900, loss: tensor(0.0619, grad_fn=<SubBackward0>), time elapsed: 387.1333546638489 seconds\n",
      "Step 7000, loss: tensor(0.0641, grad_fn=<SubBackward0>), time elapsed: 393.29481649398804 seconds\n",
      "Step 7100, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 398.809024810791 seconds\n",
      "Step 7200, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 404.46404457092285 seconds\n",
      "Step 7300, loss: tensor(0.0610, grad_fn=<SubBackward0>), time elapsed: 409.9781596660614 seconds\n",
      "Step 7400, loss: tensor(0.0622, grad_fn=<SubBackward0>), time elapsed: 415.69148349761963 seconds\n",
      "Step 7500, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 421.2323360443115 seconds\n",
      "Step 7600, loss: tensor(0.0627, grad_fn=<SubBackward0>), time elapsed: 426.77100563049316 seconds\n",
      "Step 7700, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 432.56025195121765 seconds\n",
      "Step 7800, loss: tensor(0.0637, grad_fn=<SubBackward0>), time elapsed: 438.29197359085083 seconds\n",
      "Step 7900, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 443.95631980895996 seconds\n",
      "Step 8000, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 449.4931983947754 seconds\n",
      "Step 8100, loss: tensor(0.0627, grad_fn=<SubBackward0>), time elapsed: 455.19588804244995 seconds\n",
      "Step 8200, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 460.74261808395386 seconds\n",
      "Step 8300, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 466.25044417381287 seconds\n",
      "Step 8400, loss: tensor(0.0621, grad_fn=<SubBackward0>), time elapsed: 471.9205038547516 seconds\n",
      "Step 8500, loss: tensor(0.0615, grad_fn=<SubBackward0>), time elapsed: 477.75867533683777 seconds\n",
      "Step 8600, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 483.4395709037781 seconds\n",
      "Step 8700, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 489.0100202560425 seconds\n",
      "Step 8800, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 494.53993225097656 seconds\n",
      "Step 8900, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 500.6031105518341 seconds\n",
      "Step 9000, loss: tensor(0.0602, grad_fn=<SubBackward0>), time elapsed: 506.1539583206177 seconds\n",
      "Step 9100, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 511.8027799129486 seconds\n",
      "Step 9200, loss: tensor(0.0592, grad_fn=<SubBackward0>), time elapsed: 517.3128833770752 seconds\n",
      "Step 9300, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 523.0170311927795 seconds\n",
      "Step 9400, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 528.5270364284515 seconds\n",
      "Step 9500, loss: tensor(0.0616, grad_fn=<SubBackward0>), time elapsed: 534.0412495136261 seconds\n",
      "Step 9600, loss: tensor(0.0600, grad_fn=<SubBackward0>), time elapsed: 539.7478048801422 seconds\n",
      "Step 9700, loss: tensor(0.0606, grad_fn=<SubBackward0>), time elapsed: 546.0374310016632 seconds\n",
      "Step 9800, loss: tensor(0.0612, grad_fn=<SubBackward0>), time elapsed: 551.7143807411194 seconds\n",
      "Step 9900, loss: tensor(0.0609, grad_fn=<SubBackward0>), time elapsed: 557.2383823394775 seconds\n",
      "Step 10000, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 562.759290933609 seconds\n",
      "Step 10100, loss: tensor(0.0576, grad_fn=<SubBackward0>), time elapsed: 568.4417607784271 seconds\n",
      "Step 10200, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 573.9622147083282 seconds\n",
      "Step 10300, loss: tensor(0.0620, grad_fn=<SubBackward0>), time elapsed: 579.6428940296173 seconds\n",
      "Step 10400, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 585.1805417537689 seconds\n",
      "Step 10500, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 590.870744228363 seconds\n",
      "Step 10600, loss: tensor(0.0600, grad_fn=<SubBackward0>), time elapsed: 596.4013757705688 seconds\n",
      "Step 10700, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 601.9948146343231 seconds\n",
      "Step 10800, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 607.7336113452911 seconds\n",
      "Step 10900, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 613.6002259254456 seconds\n",
      "Step 11000, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 619.340677022934 seconds\n",
      "Step 11100, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 624.9154813289642 seconds\n",
      "Step 11200, loss: tensor(0.0603, grad_fn=<SubBackward0>), time elapsed: 630.4546291828156 seconds\n",
      "Step 11300, loss: tensor(0.0604, grad_fn=<SubBackward0>), time elapsed: 636.1885275840759 seconds\n",
      "Step 11400, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 642.4418745040894 seconds\n",
      "Step 11500, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 648.1489112377167 seconds\n",
      "Step 11600, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 653.654951095581 seconds\n",
      "Step 11700, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 659.3500738143921 seconds\n",
      "Step 11800, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 664.8673875331879 seconds\n",
      "Step 11900, loss: tensor(0.0591, grad_fn=<SubBackward0>), time elapsed: 670.3685851097107 seconds\n",
      "Step 12000, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 676.0357239246368 seconds\n",
      "Step 12100, loss: tensor(0.0583, grad_fn=<SubBackward0>), time elapsed: 681.5463383197784 seconds\n",
      "Step 12200, loss: tensor(0.0575, grad_fn=<SubBackward0>), time elapsed: 687.2151391506195 seconds\n",
      "Step 12300, loss: tensor(0.0572, grad_fn=<SubBackward0>), time elapsed: 692.783043384552 seconds\n",
      "Step 12400, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 698.3612751960754 seconds\n",
      "Step 12500, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 704.74485206604 seconds\n",
      "Step 12600, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 710.275310754776 seconds\n",
      "Step 12700, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 715.9433703422546 seconds\n",
      "Step 12800, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 721.4931735992432 seconds\n",
      "Step 12900, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 727.1709191799164 seconds\n",
      "Step 13000, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 732.6950812339783 seconds\n",
      "Step 13100, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 738.2257797718048 seconds\n",
      "Step 13200, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 743.9028792381287 seconds\n",
      "Step 13300, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 749.4643921852112 seconds\n",
      "Step 13400, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 755.2758159637451 seconds\n",
      "Step 13500, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 760.7962436676025 seconds\n",
      "Step 13600, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 766.9473214149475 seconds\n",
      "Step 13700, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 772.6497004032135 seconds\n",
      "Step 13800, loss: tensor(0.0582, grad_fn=<SubBackward0>), time elapsed: 778.1859471797943 seconds\n",
      "Step 13900, loss: tensor(0.0593, grad_fn=<SubBackward0>), time elapsed: 783.8866443634033 seconds\n",
      "Step 14000, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 789.4202265739441 seconds\n",
      "Step 14100, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 795.0998258590698 seconds\n",
      "Step 14200, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 800.6420660018921 seconds\n",
      "Step 14300, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 806.1751415729523 seconds\n",
      "Step 14400, loss: tensor(0.0568, grad_fn=<SubBackward0>), time elapsed: 811.8887858390808 seconds\n",
      "Step 14500, loss: tensor(0.0606, grad_fn=<SubBackward0>), time elapsed: 817.433794260025 seconds\n",
      "Step 14600, loss: tensor(0.0593, grad_fn=<SubBackward0>), time elapsed: 823.135577917099 seconds\n",
      "Step 14700, loss: tensor(0.0592, grad_fn=<SubBackward0>), time elapsed: 828.6737494468689 seconds\n",
      "Step 14800, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 834.1995713710785 seconds\n",
      "Step 14900, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 839.882609128952 seconds\n",
      "Step 15000, loss: tensor(0.0572, grad_fn=<SubBackward0>), time elapsed: 845.4597582817078 seconds\n",
      "Step 15100, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 851.1474885940552 seconds\n",
      "Step 15200, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 856.6972243785858 seconds\n",
      "Step 15300, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 862.4240198135376 seconds\n",
      "Step 15400, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 867.9753406047821 seconds\n",
      "Step 15500, loss: tensor(0.0581, grad_fn=<SubBackward0>), time elapsed: 873.5395841598511 seconds\n",
      "Step 15600, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 879.2479798793793 seconds\n",
      "Step 15700, loss: tensor(0.0592, grad_fn=<SubBackward0>), time elapsed: 885.6220850944519 seconds\n",
      "Step 15800, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 891.329377412796 seconds\n",
      "Step 15900, loss: tensor(0.0579, grad_fn=<SubBackward0>), time elapsed: 896.8506269454956 seconds\n",
      "Step 16000, loss: tensor(0.0593, grad_fn=<SubBackward0>), time elapsed: 902.4009482860565 seconds\n",
      "Step 16100, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 908.0831904411316 seconds\n",
      "Step 16200, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 913.6168220043182 seconds\n",
      "Step 16300, loss: tensor(0.0590, grad_fn=<SubBackward0>), time elapsed: 919.3100171089172 seconds\n",
      "Step 16400, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 924.8614883422852 seconds\n",
      "Step 16500, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 931.3418016433716 seconds\n",
      "Step 16600, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 936.8931241035461 seconds\n",
      "Step 16700, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 942.4242446422577 seconds\n",
      "Step 16800, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 948.0972962379456 seconds\n",
      "Step 16900, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 953.6799824237823 seconds\n",
      "Step 17000, loss: tensor(0.0563, grad_fn=<SubBackward0>), time elapsed: 959.3921272754669 seconds\n",
      "Step 17100, loss: tensor(0.0557, grad_fn=<SubBackward0>), time elapsed: 964.9774451255798 seconds\n",
      "Step 17200, loss: tensor(0.0589, grad_fn=<SubBackward0>), time elapsed: 970.5394623279572 seconds\n",
      "Step 17300, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 976.2372596263885 seconds\n",
      "Step 17400, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 981.7772860527039 seconds\n",
      "Step 17500, loss: tensor(0.0573, grad_fn=<SubBackward0>), time elapsed: 987.517288684845 seconds\n",
      "Step 17600, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 993.8112921714783 seconds\n",
      "Step 17700, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 999.3656537532806 seconds\n",
      "Step 17800, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 1005.0421707630157 seconds\n",
      "Step 17900, loss: tensor(0.0582, grad_fn=<SubBackward0>), time elapsed: 1010.5778560638428 seconds\n",
      "Step 18000, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 1016.2797529697418 seconds\n",
      "Step 18100, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 1021.8152439594269 seconds\n",
      "Step 18200, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 1027.5337193012238 seconds\n",
      "Step 18300, loss: tensor(0.0583, grad_fn=<SubBackward0>), time elapsed: 1033.0899305343628 seconds\n",
      "Step 18400, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 1038.6342799663544 seconds\n",
      "Step 18500, loss: tensor(0.0591, grad_fn=<SubBackward0>), time elapsed: 1044.3804759979248 seconds\n",
      "Step 18600, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 1049.9277963638306 seconds\n",
      "Step 18700, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 1055.8949303627014 seconds\n",
      "Step 18800, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 1061.4664688110352 seconds\n",
      "Step 18900, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 1067.0357646942139 seconds\n",
      "Step 19000, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 1072.9355251789093 seconds\n",
      "Step 19100, loss: tensor(0.0565, grad_fn=<SubBackward0>), time elapsed: 1079.3397300243378 seconds\n",
      "Step 19200, loss: tensor(0.0559, grad_fn=<SubBackward0>), time elapsed: 1085.1037938594818 seconds\n",
      "Step 19300, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 1090.6914727687836 seconds\n",
      "Step 19400, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 1096.2862575054169 seconds\n",
      "Step 19500, loss: tensor(0.0573, grad_fn=<SubBackward0>), time elapsed: 1101.9996011257172 seconds\n",
      "Step 19600, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 1107.5420277118683 seconds\n",
      "Step 19700, loss: tensor(0.0576, grad_fn=<SubBackward0>), time elapsed: 1113.2342340946198 seconds\n",
      "Step 19800, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 1118.8107323646545 seconds\n",
      "Step 19900, loss: tensor(0.0588, grad_fn=<SubBackward0>), time elapsed: 1124.5297067165375 seconds\n",
      "Step 20000, loss: tensor(0.0585, grad_fn=<SubBackward0>), time elapsed: 1130.118370771408 seconds\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "Step 0, loss: tensor(1.1024, grad_fn=<SubBackward0>), time elapsed: 0.06127309799194336 seconds\n",
      "Step 100, loss: tensor(0.9126, grad_fn=<SubBackward0>), time elapsed: 5.6715171337127686 seconds\n",
      "Step 200, loss: tensor(0.7467, grad_fn=<SubBackward0>), time elapsed: 11.347835779190063 seconds\n",
      "Step 300, loss: tensor(0.6280, grad_fn=<SubBackward0>), time elapsed: 16.84639048576355 seconds\n",
      "Step 400, loss: tensor(0.5410, grad_fn=<SubBackward0>), time elapsed: 22.477572679519653 seconds\n",
      "Step 500, loss: tensor(0.4670, grad_fn=<SubBackward0>), time elapsed: 27.987683057785034 seconds\n",
      "Step 600, loss: tensor(0.4107, grad_fn=<SubBackward0>), time elapsed: 33.626829385757446 seconds\n",
      "Step 700, loss: tensor(0.3521, grad_fn=<SubBackward0>), time elapsed: 39.13586783409119 seconds\n",
      "Step 800, loss: tensor(0.3067, grad_fn=<SubBackward0>), time elapsed: 44.66277599334717 seconds\n",
      "Step 900, loss: tensor(0.2615, grad_fn=<SubBackward0>), time elapsed: 50.305768728256226 seconds\n",
      "Step 1000, loss: tensor(0.2271, grad_fn=<SubBackward0>), time elapsed: 55.83565640449524 seconds\n",
      "Step 1100, loss: tensor(0.1972, grad_fn=<SubBackward0>), time elapsed: 61.46279859542847 seconds\n",
      "Step 1200, loss: tensor(0.1750, grad_fn=<SubBackward0>), time elapsed: 66.94369196891785 seconds\n",
      "Step 1300, loss: tensor(0.1598, grad_fn=<SubBackward0>), time elapsed: 72.5898232460022 seconds\n",
      "Step 1400, loss: tensor(0.1461, grad_fn=<SubBackward0>), time elapsed: 78.0912024974823 seconds\n",
      "Step 1500, loss: tensor(0.1366, grad_fn=<SubBackward0>), time elapsed: 83.63745903968811 seconds\n",
      "Step 1600, loss: tensor(0.1291, grad_fn=<SubBackward0>), time elapsed: 89.30206680297852 seconds\n",
      "Step 1700, loss: tensor(0.1229, grad_fn=<SubBackward0>), time elapsed: 94.81974005699158 seconds\n",
      "Step 1800, loss: tensor(0.1180, grad_fn=<SubBackward0>), time elapsed: 100.48179030418396 seconds\n",
      "Step 1900, loss: tensor(0.1143, grad_fn=<SubBackward0>), time elapsed: 105.9915189743042 seconds\n",
      "Step 2000, loss: tensor(0.1073, grad_fn=<SubBackward0>), time elapsed: 111.54001212120056 seconds\n",
      "Step 2100, loss: tensor(0.1048, grad_fn=<SubBackward0>), time elapsed: 117.1845166683197 seconds\n",
      "Step 2200, loss: tensor(0.1032, grad_fn=<SubBackward0>), time elapsed: 122.70169949531555 seconds\n",
      "Step 2300, loss: tensor(0.0996, grad_fn=<SubBackward0>), time elapsed: 128.340900182724 seconds\n",
      "Step 2400, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 133.87253427505493 seconds\n",
      "Step 2500, loss: tensor(0.0942, grad_fn=<SubBackward0>), time elapsed: 139.52904963493347 seconds\n",
      "Step 2600, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 145.0523386001587 seconds\n",
      "Step 2700, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 150.55467414855957 seconds\n",
      "Step 2800, loss: tensor(0.0886, grad_fn=<SubBackward0>), time elapsed: 156.1949806213379 seconds\n",
      "Step 2900, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 161.69279026985168 seconds\n",
      "Step 3000, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 167.33424973487854 seconds\n",
      "Step 3100, loss: tensor(0.0854, grad_fn=<SubBackward0>), time elapsed: 172.84259819984436 seconds\n",
      "Step 3200, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 178.50521302223206 seconds\n",
      "Step 3300, loss: tensor(0.0829, grad_fn=<SubBackward0>), time elapsed: 184.0066740512848 seconds\n",
      "Step 3400, loss: tensor(0.0816, grad_fn=<SubBackward0>), time elapsed: 189.51614022254944 seconds\n",
      "Step 3500, loss: tensor(0.0813, grad_fn=<SubBackward0>), time elapsed: 195.14990997314453 seconds\n",
      "Step 3600, loss: tensor(0.0817, grad_fn=<SubBackward0>), time elapsed: 200.6685700416565 seconds\n",
      "Step 3700, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 206.32434606552124 seconds\n",
      "Step 3800, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 211.8214201927185 seconds\n",
      "Step 3900, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 217.45604133605957 seconds\n",
      "Step 4000, loss: tensor(0.0777, grad_fn=<SubBackward0>), time elapsed: 222.9657747745514 seconds\n",
      "Step 4100, loss: tensor(0.0788, grad_fn=<SubBackward0>), time elapsed: 228.48394179344177 seconds\n",
      "Step 4200, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 234.14177942276 seconds\n",
      "Step 4300, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 239.64850735664368 seconds\n",
      "Step 4400, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 245.29300618171692 seconds\n",
      "Step 4500, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 250.8073811531067 seconds\n",
      "Step 4600, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 256.3065378665924 seconds\n",
      "Step 4700, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 262.434285402298 seconds\n",
      "Step 4800, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 268.062415599823 seconds\n",
      "Step 4900, loss: tensor(0.0726, grad_fn=<SubBackward0>), time elapsed: 274.22010350227356 seconds\n",
      "Step 5000, loss: tensor(0.0717, grad_fn=<SubBackward0>), time elapsed: 279.7360954284668 seconds\n",
      "Step 5100, loss: tensor(0.0714, grad_fn=<SubBackward0>), time elapsed: 285.4201512336731 seconds\n",
      "Step 5200, loss: tensor(0.0716, grad_fn=<SubBackward0>), time elapsed: 290.9356656074524 seconds\n",
      "Step 5300, loss: tensor(0.0707, grad_fn=<SubBackward0>), time elapsed: 296.4886209964752 seconds\n",
      "Step 5400, loss: tensor(0.0686, grad_fn=<SubBackward0>), time elapsed: 303.0358622074127 seconds\n",
      "Step 5500, loss: tensor(0.0689, grad_fn=<SubBackward0>), time elapsed: 308.5718150138855 seconds\n",
      "Step 5600, loss: tensor(0.0671, grad_fn=<SubBackward0>), time elapsed: 314.27351546287537 seconds\n",
      "Step 5700, loss: tensor(0.0676, grad_fn=<SubBackward0>), time elapsed: 319.78784441947937 seconds\n",
      "Step 5800, loss: tensor(0.0658, grad_fn=<SubBackward0>), time elapsed: 325.44705033302307 seconds\n",
      "Step 5900, loss: tensor(0.0655, grad_fn=<SubBackward0>), time elapsed: 330.9605493545532 seconds\n",
      "Step 6000, loss: tensor(0.0668, grad_fn=<SubBackward0>), time elapsed: 336.4807574748993 seconds\n",
      "Step 6100, loss: tensor(0.0656, grad_fn=<SubBackward0>), time elapsed: 342.1211955547333 seconds\n",
      "Step 6200, loss: tensor(0.0672, grad_fn=<SubBackward0>), time elapsed: 347.6421332359314 seconds\n",
      "Step 6300, loss: tensor(0.0634, grad_fn=<SubBackward0>), time elapsed: 353.3016428947449 seconds\n",
      "Step 6400, loss: tensor(0.0636, grad_fn=<SubBackward0>), time elapsed: 358.83563590049744 seconds\n",
      "Step 6500, loss: tensor(0.0617, grad_fn=<SubBackward0>), time elapsed: 364.34699964523315 seconds\n",
      "Step 6600, loss: tensor(0.0620, grad_fn=<SubBackward0>), time elapsed: 369.99903297424316 seconds\n",
      "Step 6700, loss: tensor(0.0624, grad_fn=<SubBackward0>), time elapsed: 375.51307559013367 seconds\n",
      "Step 6800, loss: tensor(0.0625, grad_fn=<SubBackward0>), time elapsed: 381.15696573257446 seconds\n",
      "Step 6900, loss: tensor(0.0622, grad_fn=<SubBackward0>), time elapsed: 386.69809913635254 seconds\n",
      "Step 7000, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 392.34743213653564 seconds\n",
      "Step 7100, loss: tensor(0.0605, grad_fn=<SubBackward0>), time elapsed: 397.8651988506317 seconds\n",
      "Step 7200, loss: tensor(0.0609, grad_fn=<SubBackward0>), time elapsed: 403.3893039226532 seconds\n",
      "Step 7300, loss: tensor(0.0599, grad_fn=<SubBackward0>), time elapsed: 409.057213306427 seconds\n",
      "Step 7400, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 414.5716829299927 seconds\n",
      "Step 7500, loss: tensor(0.0608, grad_fn=<SubBackward0>), time elapsed: 420.2739887237549 seconds\n",
      "Step 7600, loss: tensor(0.0581, grad_fn=<SubBackward0>), time elapsed: 425.7739963531494 seconds\n",
      "Step 7700, loss: tensor(0.0585, grad_fn=<SubBackward0>), time elapsed: 431.4463906288147 seconds\n",
      "Step 7800, loss: tensor(0.0584, grad_fn=<SubBackward0>), time elapsed: 436.9662232398987 seconds\n",
      "Step 7900, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 442.50547456741333 seconds\n",
      "Step 8000, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 448.2317056655884 seconds\n",
      "Step 8100, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 453.76620507240295 seconds\n",
      "Step 8200, loss: tensor(0.0594, grad_fn=<SubBackward0>), time elapsed: 459.47711181640625 seconds\n",
      "Step 8300, loss: tensor(0.0577, grad_fn=<SubBackward0>), time elapsed: 465.02250123023987 seconds\n",
      "Step 8400, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 470.55354714393616 seconds\n",
      "Step 8500, loss: tensor(0.0575, grad_fn=<SubBackward0>), time elapsed: 476.2201509475708 seconds\n",
      "Step 8600, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 481.766459941864 seconds\n",
      "Step 8700, loss: tensor(0.0580, grad_fn=<SubBackward0>), time elapsed: 487.4225935935974 seconds\n",
      "Step 8800, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 492.94405603408813 seconds\n",
      "Step 8900, loss: tensor(0.0570, grad_fn=<SubBackward0>), time elapsed: 498.74072337150574 seconds\n",
      "Step 9000, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 504.276424407959 seconds\n",
      "Step 9100, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 509.8367202281952 seconds\n",
      "Step 9200, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 515.4841959476471 seconds\n",
      "Step 9300, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 521.5908606052399 seconds\n",
      "Step 9400, loss: tensor(0.0567, grad_fn=<SubBackward0>), time elapsed: 527.2744948863983 seconds\n",
      "Step 9500, loss: tensor(0.0572, grad_fn=<SubBackward0>), time elapsed: 532.7897870540619 seconds\n",
      "Step 9600, loss: tensor(0.0575, grad_fn=<SubBackward0>), time elapsed: 538.3292503356934 seconds\n",
      "Step 9700, loss: tensor(0.0551, grad_fn=<SubBackward0>), time elapsed: 544.0246722698212 seconds\n",
      "Step 9800, loss: tensor(0.0564, grad_fn=<SubBackward0>), time elapsed: 549.6206266880035 seconds\n",
      "Step 9900, loss: tensor(0.0556, grad_fn=<SubBackward0>), time elapsed: 555.288565158844 seconds\n",
      "Step 10000, loss: tensor(0.0564, grad_fn=<SubBackward0>), time elapsed: 560.8042471408844 seconds\n",
      "Step 10100, loss: tensor(0.0549, grad_fn=<SubBackward0>), time elapsed: 566.4587273597717 seconds\n",
      "Step 10200, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 571.9870393276215 seconds\n",
      "Step 10300, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 577.504052400589 seconds\n",
      "Step 10400, loss: tensor(0.0560, grad_fn=<SubBackward0>), time elapsed: 583.1558156013489 seconds\n",
      "Step 10500, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 588.6835012435913 seconds\n",
      "Step 10600, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 594.3674554824829 seconds\n",
      "Step 10700, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 599.9322688579559 seconds\n",
      "Step 10800, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 605.4943957328796 seconds\n",
      "Step 10900, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 611.1824054718018 seconds\n",
      "Step 11000, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 616.7371206283569 seconds\n",
      "Step 11100, loss: tensor(0.0548, grad_fn=<SubBackward0>), time elapsed: 622.4104290008545 seconds\n",
      "Step 11200, loss: tensor(0.0540, grad_fn=<SubBackward0>), time elapsed: 627.9580652713776 seconds\n",
      "Step 11300, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 634.0456011295319 seconds\n",
      "Step 11400, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 639.5622549057007 seconds\n",
      "Step 11500, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 645.0807678699493 seconds\n",
      "Step 11600, loss: tensor(0.0550, grad_fn=<SubBackward0>), time elapsed: 650.7491993904114 seconds\n",
      "Step 11700, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 656.2686066627502 seconds\n",
      "Step 11800, loss: tensor(0.0543, grad_fn=<SubBackward0>), time elapsed: 662.051059961319 seconds\n",
      "Step 11900, loss: tensor(0.0553, grad_fn=<SubBackward0>), time elapsed: 667.6526017189026 seconds\n",
      "Step 12000, loss: tensor(0.0542, grad_fn=<SubBackward0>), time elapsed: 673.2235827445984 seconds\n",
      "Step 12100, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 678.8814959526062 seconds\n",
      "Step 12200, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 685.0388987064362 seconds\n",
      "Step 12300, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 690.731103181839 seconds\n",
      "Step 12400, loss: tensor(0.0535, grad_fn=<SubBackward0>), time elapsed: 696.2600948810577 seconds\n",
      "Step 12500, loss: tensor(0.0533, grad_fn=<SubBackward0>), time elapsed: 701.9402132034302 seconds\n",
      "Step 12600, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 707.472647190094 seconds\n",
      "Step 12700, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 712.9872524738312 seconds\n",
      "Step 12800, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 718.6666285991669 seconds\n",
      "Step 12900, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 724.2233757972717 seconds\n",
      "Step 13000, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 729.8973054885864 seconds\n",
      "Step 13100, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 735.4387547969818 seconds\n",
      "Step 13200, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 740.9814183712006 seconds\n",
      "Step 13300, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 747.0603196620941 seconds\n",
      "Step 13400, loss: tensor(0.0540, grad_fn=<SubBackward0>), time elapsed: 752.6677746772766 seconds\n",
      "Step 13500, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 758.4015424251556 seconds\n",
      "Step 13600, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 763.960177898407 seconds\n",
      "Step 13700, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 770.199179649353 seconds\n",
      "Step 13800, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 775.753497838974 seconds\n",
      "Step 13900, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 781.8729887008667 seconds\n",
      "Step 14000, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 787.5797619819641 seconds\n",
      "Step 14100, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 793.0921356678009 seconds\n",
      "Step 14200, loss: tensor(0.0536, grad_fn=<SubBackward0>), time elapsed: 798.7618281841278 seconds\n",
      "Step 14300, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 804.266491651535 seconds\n",
      "Step 14400, loss: tensor(0.0535, grad_fn=<SubBackward0>), time elapsed: 809.7927303314209 seconds\n",
      "Step 14500, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 815.4841501712799 seconds\n",
      "Step 14600, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 821.0158307552338 seconds\n",
      "Step 14700, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 826.6965725421906 seconds\n",
      "Step 14800, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 832.2199156284332 seconds\n",
      "Step 14900, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 837.8842566013336 seconds\n",
      "Step 15000, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 843.4446849822998 seconds\n",
      "Step 15100, loss: tensor(0.0538, grad_fn=<SubBackward0>), time elapsed: 848.989381313324 seconds\n",
      "Step 15200, loss: tensor(0.0544, grad_fn=<SubBackward0>), time elapsed: 854.6723608970642 seconds\n",
      "Step 15300, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 860.1939985752106 seconds\n",
      "Step 15400, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 865.9038319587708 seconds\n",
      "Step 15500, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 871.4453122615814 seconds\n",
      "Step 15600, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 877.0386221408844 seconds\n",
      "Step 15700, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 882.7336685657501 seconds\n",
      "Step 15800, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 888.2780206203461 seconds\n",
      "Step 15900, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 894.0019116401672 seconds\n",
      "Step 16000, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 899.5530455112457 seconds\n",
      "Step 16100, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 905.2950778007507 seconds\n",
      "Step 16200, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 910.8432850837708 seconds\n",
      "Step 16300, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 916.7256052494049 seconds\n",
      "Step 16400, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 922.4151880741119 seconds\n",
      "Step 16500, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 927.952260017395 seconds\n",
      "Step 16600, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 933.6293203830719 seconds\n",
      "Step 16700, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 939.1675901412964 seconds\n",
      "Step 16800, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 944.6946909427643 seconds\n",
      "Step 16900, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 950.3833408355713 seconds\n",
      "Step 17000, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 955.9246695041656 seconds\n",
      "Step 17100, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 961.6282420158386 seconds\n",
      "Step 17200, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 967.2000916004181 seconds\n",
      "Step 17300, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 972.8307156562805 seconds\n",
      "Step 17400, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 978.6204776763916 seconds\n",
      "Step 17500, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 984.1652226448059 seconds\n",
      "Step 17600, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 990.2793476581573 seconds\n",
      "Step 17700, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 995.8477613925934 seconds\n",
      "Step 17800, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1001.5437209606171 seconds\n",
      "Step 17900, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 1007.0915729999542 seconds\n",
      "Step 18000, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 1012.6376776695251 seconds\n",
      "Step 18100, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 1018.3344006538391 seconds\n",
      "Step 18200, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1023.8804130554199 seconds\n",
      "Step 18300, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 1029.6142644882202 seconds\n",
      "Step 18400, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 1035.16548037529 seconds\n",
      "Step 18500, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 1040.836682319641 seconds\n",
      "Step 18600, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 1046.6226258277893 seconds\n",
      "Step 18700, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1052.1882600784302 seconds\n",
      "Step 18800, loss: tensor(0.0491, grad_fn=<SubBackward0>), time elapsed: 1058.910239458084 seconds\n",
      "Step 18900, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 1064.4784977436066 seconds\n",
      "Step 19000, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 1070.0485138893127 seconds\n",
      "Step 19100, loss: tensor(0.0490, grad_fn=<SubBackward0>), time elapsed: 1075.7678608894348 seconds\n",
      "Step 19200, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 1081.3343198299408 seconds\n",
      "Step 19300, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 1087.0792722702026 seconds\n",
      "Step 19400, loss: tensor(0.0468, grad_fn=<SubBackward0>), time elapsed: 1092.6563303470612 seconds\n",
      "Step 19500, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 1098.3458564281464 seconds\n",
      "Step 19600, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1103.8634746074677 seconds\n",
      "Step 19700, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1109.3799471855164 seconds\n",
      "Step 19800, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 1115.063797712326 seconds\n",
      "Step 19900, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 1120.667457818985 seconds\n",
      "Step 20000, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 1126.3942999839783 seconds\n"
     ]
    }
   ],
   "source": [
    "#train on diffused data\n",
    "n, na = 4, 1 # number of data and ancilla qubits\n",
    "T = 20 # number of diffusion steps\n",
    "L = 6 # layer of backward PQC\n",
    "Ndata = 2000 # number of data in the training data set\n",
    "epochs = 20001\n",
    "\n",
    " # number of training epochs\n",
    "\n",
    "#gen random samples at t = T\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_T = diffModel.HaarSampleGeneration(Ndata, seed = 22)\n",
    "\n",
    "#load diffusion process\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "model.set_diffusionSet(states_diff)\n",
    "\n",
    "params_total = []\n",
    "loss_hist_total = []\n",
    "\n",
    "for t in range(T - 1, -1, -1):\n",
    "    params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "    for tt in range(t+1, T):\n",
    "        print(tt)\n",
    "        params_tot[tt] = np.load('paramsandloss_squaremodel/params_t%d.npy'%tt)\n",
    "    \n",
    "    params, loss_hist = Training_t(model, t, inputs_T, params_tot, Ndata, epochs)\n",
    "\n",
    "    np.save('paramsandloss_squaremodel/params_t%d'%t, params.detach().numpy())\n",
    "    np.save('paramsandloss_squaremodel/loss_t%d'%t, loss_hist.detach().numpy())\n",
    "    \n",
    "    #params_total = np.append(params_total, params, axis = 1)\n",
    "    #loss_hist_total = np.append(loss_hist_total, loss_hist, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save total parameters\n",
    "n, na = 4, 1\n",
    "T = 20\n",
    "L = 6\n",
    "Ndata = 2000\n",
    "epochs = 20000 + 1\n",
    "\n",
    "params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "loss_tot = np.zeros((T, epochs))\n",
    "f0_tot = np.zeros((T, epochs))\n",
    "\n",
    "for t in range(T):\n",
    "    params_tot[t] = np.load('paramsandloss_squaremodel/params_t%d.npy'%t)\n",
    "    loss_tot[t] = np.load('paramsandloss_squaremodel/loss_t%d.npy'%t)\n",
    "    \n",
    "\n",
    "np.save(\"params_total_2000Ndata_20kEpochs\", params_tot)\n",
    "np.save(\"loss_tot_2000Ndata_20kEpochs\", loss_tot)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create random 16 pixel data\n",
    "Ndata = 2000\n",
    "n = 4\n",
    "random_images = np.zeros((Ndata, 4, 4))\n",
    "amplitude_vals = np.zeros(Ndata)\n",
    "\n",
    "for i in range(0, Ndata):\n",
    "    random_images[i] = np.random.rand(4,4)\n",
    "    amplitude_vals[i] = np.sum(random_images[i] ** 2) ** 0.5\n",
    "\n",
    "plt.imshow(random_images[1], cmap = 'grey', interpolation = 'nearest')\n",
    "np.save(\"random_images\", random_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amplitude encode random image data into qubits\n",
    "random_images_qubits = np.zeros((Ndata, 2**n)) + 1j * np.zeros((Ndata, 2**n))\n",
    "for i in range(0, Ndata):\n",
    "    random_images_qubits[i] = np.ravel(random_images[i] / amplitude_vals[i] + 0j)\n",
    "\n",
    "#print(random_images_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run trained model on random image data\n",
    "n, na = 4, 1\n",
    "T = 20\n",
    "L = 6\n",
    "Ndata = 2000\n",
    "\n",
    "params_tot = np.load('params_total_2000Ndata_20kEpochs.npy')\n",
    "\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_te = diffModel.HaarSampleGeneration(Ndata, seed=22)\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "\n",
    "\n",
    "#data_te = model.backDataGeneration(test_data_T20, params_tot, Ndata)[:, :, :2**n].numpy()\n",
    "data_te = model.backDataGeneration(torch.from_numpy(random_images_qubits), params_tot, Ndata)[:, :, :2**n].numpy()\n",
    "\n",
    "np.save(\"test_backwardsgen\", data_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from backwards_gen decode the qubit state to get the nxn image\n",
    "backwards_gen = np.load('test_backwardsgen.npy')\n",
    "dim = 4#int((final_output_flattened.size) ** 0.5)\n",
    "final_output_nxn = np.zeros((T + 1, Ndata, dim, dim))\n",
    "\n",
    "for z in range(0, 21):\n",
    "    final_output_flattened = backwards_gen[z]\n",
    "    final_output_flattened = np.abs(final_output_flattened)\n",
    "    multiplier = np.max(final_output_flattened)\n",
    "    for i in range(0, np.size(amplitude_vals)):\n",
    "        final_output_flattened[:][i] *= amplitude_vals[i]\n",
    "    #final_output_flattened = final_output_flattened.flatten()\n",
    "\n",
    "    for i in range(0, dim):\n",
    "        for j in range(0, dim):\n",
    "            for nth_data in range(0, Ndata):\n",
    "                final_output_nxn[z][nth_data][i][j] = final_output_flattened[nth_data][(i*dim) + j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.24994205e-03+3.61188268e-03j  7.34723406e-03+1.77129661e-03j\n",
      "   4.56541590e-03+3.85116111e-03j  5.74028306e-03+8.77854880e-04j\n",
      "   7.30061531e-03+5.36405249e-04j -2.14001001e-03+6.79478340e-04j\n",
      "   4.30809945e-04-4.03131964e-03j  9.82984994e-03+5.48126234e-04j\n",
      "   1.13124223e-02+9.92742134e-04j -3.29119997e-04+5.63828391e-04j\n",
      "   2.13921288e-04+3.00176995e-04j  6.58961106e-03+3.63063446e-04j\n",
      "   4.68253857e-03+2.23202747e-03j  6.74372073e-03+4.40751668e-03j\n",
      "   6.05876651e-03+4.00977861e-03j  9.26413573e-03+3.62730259e-03j]\n",
      " [-5.00253914e-03+1.92943425e-03j -6.15850650e-03+2.55892030e-03j\n",
      "  -5.54458285e-03+2.73469067e-03j -7.17123877e-03+2.73746485e-03j\n",
      "  -6.16558315e-03+2.88178609e-03j -1.58322824e-03+3.13267275e-03j\n",
      "   1.56174286e-03-1.20912550e-03j -7.53058912e-03+4.15771594e-03j\n",
      "  -6.19709771e-03+2.83790380e-03j -1.13724871e-03+2.60582287e-03j\n",
      "   1.08028005e-03-1.66269182e-03j -7.47529883e-03+4.59579518e-03j\n",
      "  -5.67423180e-03+5.10643527e-04j -6.65187137e-03+1.89351931e-03j\n",
      "  -5.18915430e-03+2.57897517e-03j -6.00492535e-03+4.31176135e-03j]\n",
      " [-3.30932741e-03-5.05299424e-04j -5.44542028e-03-2.88232043e-03j\n",
      "  -5.19692153e-03-3.11401882e-03j -4.81464714e-03-7.00590992e-03j\n",
      "  -7.45670032e-03-1.78092252e-03j  1.46922539e-04+2.39614444e-03j\n",
      "  -5.69837110e-04-8.01793067e-04j -3.82704218e-03-2.83622509e-03j\n",
      "  -4.37839702e-03-2.60895235e-03j -1.42874673e-03+7.23367906e-04j\n",
      "   1.21360691e-03+1.13950367e-03j -4.14525159e-03-4.52012243e-03j\n",
      "  -8.91654287e-03-3.13334796e-03j -4.53867810e-03-1.57691503e-03j\n",
      "  -4.44670068e-03-1.53954630e-03j -4.26906720e-03-2.49162986e-04j]\n",
      " [ 9.31676535e-04-3.81698669e-03j  1.78649556e-03-3.83273629e-03j\n",
      "   3.61902709e-03-1.66965544e-03j -8.76820937e-04-2.13422487e-03j\n",
      "   4.88245068e-03-5.52939018e-03j  1.42107578e-03-5.84684021e-04j\n",
      "  -1.91774056e-03+2.88310018e-03j -1.32878416e-03-3.76286265e-03j\n",
      "   3.44400620e-03-1.30283835e-04j  4.03907476e-03-1.81527110e-03j\n",
      "   1.41758908e-04+8.55666178e-04j  2.69131979e-05-3.44367512e-03j\n",
      "   7.36418413e-03-4.40374669e-03j  8.92775424e-04-4.46908362e-03j\n",
      "   2.73904437e-03-4.17594099e-03j -3.88690387e-03-2.68723443e-03j]\n",
      " [ 1.24938635e-03+5.32352692e-03j -6.34804135e-03+6.74433773e-03j\n",
      "  -1.65030535e-04+1.19520603e-02j -5.96676115e-03+6.13671541e-03j\n",
      "  -2.85169482e-03+5.15372399e-03j  1.02689303e-03-9.62925260e-04j\n",
      "   1.42984802e-03-5.22420323e-03j  1.06725609e-03+8.87499191e-03j\n",
      "  -1.76298362e-03+4.89285495e-03j -5.06171782e-04+1.31423050e-03j\n",
      "  -2.58495123e-03-7.46959937e-04j -2.63579190e-03+8.26772675e-03j\n",
      "  -1.19614983e-02+9.94846970e-03j -1.03969488e-03+7.52966944e-03j\n",
      "   3.33889329e-04+6.64168363e-03j  6.12535467e-03+5.44944033e-03j]\n",
      " [-3.44682462e-03+1.68121140e-03j  3.81799182e-03-9.30651464e-03j\n",
      "   1.30575651e-03-3.95147596e-03j  4.06362256e-03-9.57859308e-03j\n",
      "  -4.26447066e-03-1.23479674e-02j -3.66872665e-03-4.45768842e-03j\n",
      "  -8.17578810e-04+1.59988922e-04j -1.50808180e-03-3.89589695e-03j\n",
      "   3.85391759e-03-8.55603814e-03j -3.69244226e-05+2.38609035e-03j\n",
      "   3.70776484e-04-2.61358626e-04j  4.92765661e-03-8.23821966e-03j\n",
      "   9.08530597e-03-6.97701378e-03j  7.09076703e-04-9.18400660e-03j\n",
      "   9.31268383e-04+1.12213625e-03j -1.14730550e-02+3.70446098e-04j]\n",
      " [ 2.87417206e-03+7.80331017e-03j -7.29021663e-03+1.03155908e-03j\n",
      "  -6.27083704e-03+1.02074433e-03j  2.72433186e-04-1.56775094e-03j\n",
      "  -8.41964129e-03-1.89108553e-03j -5.41639281e-03-4.49377717e-03j\n",
      "   2.15198772e-04-5.39767090e-03j -2.84171407e-03-5.57501428e-03j\n",
      "  -5.69314836e-03+7.66164798e-04j -3.19117308e-03-1.02613913e-02j\n",
      "   1.47062528e-03+4.02292702e-03j -1.31175471e-02-8.83679278e-03j\n",
      "  -3.90339363e-03-5.98864630e-03j -2.63439538e-03-7.78164715e-03j\n",
      "  -8.47361889e-03+7.04497143e-05j -1.12323323e-03+1.54926060e-02j]\n",
      " [-7.74019491e-03-9.01034288e-03j  9.46420059e-03-5.41275227e-03j\n",
      "   1.21177618e-04+3.05087352e-03j -1.26095186e-03+4.16180678e-03j\n",
      "   4.52884194e-03-3.57072742e-04j  3.84286453e-04-1.23642222e-03j\n",
      "   4.04199032e-04+1.27271481e-03j  1.41402241e-04+4.72197682e-03j\n",
      "   7.55147310e-03+1.87413569e-03j  2.64902576e-03-2.88859289e-03j\n",
      "  -9.18236328e-04-5.96920203e-04j -2.44894088e-03+1.50132121e-03j\n",
      "   1.86774496e-03+3.32707143e-03j -5.00470074e-03+9.47913807e-03j\n",
      "   8.66353419e-03-1.17104081e-02j  7.84905441e-03-5.71449101e-03j]\n",
      " [ 6.60790084e-03+1.31909014e-03j  1.01760123e-02-7.54164439e-03j\n",
      "  -9.66494158e-03+4.34319163e-03j -5.99324812e-05+1.91759132e-03j\n",
      "   9.61129647e-03-9.60647408e-03j  3.09229316e-03+3.50333820e-03j\n",
      "  -1.29807713e-02+1.06348749e-02j  7.11784232e-03-6.35060389e-03j\n",
      "  -1.27204403e-03+6.05365261e-03j  4.47942270e-03-8.16830216e-05j\n",
      "   1.07651381e-02-1.86242943e-03j  4.44606459e-03-2.15260615e-03j\n",
      "   1.14746462e-03-1.94770936e-03j -5.72570413e-03-1.01059154e-02j\n",
      "  -1.70264896e-02+4.80942614e-03j -1.26195361e-03+3.19367653e-04j]\n",
      " [ 9.64251161e-03-4.28615324e-03j -6.79004658e-03-8.65393132e-03j\n",
      "  -8.67321435e-03-1.36795510e-02j  1.34905044e-03-3.85792204e-03j\n",
      "   7.40860822e-03+1.58513784e-02j -1.32001285e-03+2.00025109e-03j\n",
      "  -5.40519599e-04+8.36279336e-03j  2.43103621e-03+3.96335078e-03j\n",
      "  -2.02274881e-03+6.44143485e-03j -7.78473401e-03+4.96919733e-03j\n",
      "  -1.27758374e-02+8.62138066e-03j  3.44266975e-03-4.14608233e-03j\n",
      "  -1.50531419e-02+1.61571591e-03j -7.31646921e-03+1.28589496e-02j\n",
      "   3.46329575e-03+1.79445613e-02j  4.67293011e-03+1.02285426e-02j]\n",
      " [ 6.06596423e-03-6.14963612e-03j  3.99295660e-03+2.48736124e-02j\n",
      "  -1.27528114e-02+3.99073539e-03j  1.45114213e-02+9.18549765e-03j\n",
      "  -7.04033487e-03+9.42840893e-03j -1.00809271e-02-9.30093322e-03j\n",
      "   7.64342071e-03+6.68445369e-03j  1.91812254e-02+1.25267655e-02j\n",
      "  -1.94335207e-02-1.47474110e-02j  2.20863003e-04-1.60653442e-02j\n",
      "   5.67585742e-03-2.86349677e-03j -9.24212486e-03+4.11328301e-03j\n",
      "   3.98063101e-03-2.32149102e-03j  6.83053222e-04-3.05078514e-02j\n",
      "  -2.91946373e-04-3.25475493e-03j  2.69303098e-02+4.67348000e-04j]\n",
      " [-6.49190322e-03-1.49931973e-02j  2.74613183e-02-1.67014394e-02j\n",
      "   2.59594456e-03-3.98269761e-03j -2.01418567e-02+9.20608174e-03j\n",
      "   2.03804802e-02-1.61783230e-02j -2.79068784e-03-1.96885038e-02j\n",
      "   2.26755869e-02+8.73305555e-03j -2.99952459e-03-4.39394265e-03j\n",
      "   3.16847931e-03+8.47858563e-03j  2.83536147e-02-2.14063637e-02j\n",
      "   7.78887374e-03+7.77634094e-03j  1.78472716e-02-2.54495274e-02j\n",
      "  -1.92832928e-02+1.00051090e-02j  2.34198030e-02-1.75695103e-02j\n",
      "   5.41443611e-03-1.61522124e-02j  5.98171027e-03-1.27834845e-02j]\n",
      " [ 7.38049904e-03-2.07601059e-02j  2.42746025e-02+9.51256603e-03j\n",
      "   2.59026885e-02-3.75167131e-02j  1.35061722e-02+1.11838933e-02j\n",
      "  -1.40188187e-02-6.08328264e-03j -8.72372091e-03+3.30226831e-02j\n",
      "   8.99047870e-03+5.63373649e-03j -1.54251922e-02-1.56969894e-02j\n",
      "  -3.23861418e-03-2.64318241e-03j -2.92270966e-02-4.11867797e-02j\n",
      "  -2.67003721e-04+1.83369741e-02j  5.84864116e-04-1.78697817e-02j\n",
      "  -1.34900236e-03-3.77576910e-02j  4.57121991e-03+2.93757510e-03j\n",
      "   4.39638784e-03+2.02547312e-02j  1.47525156e-02+6.01769518e-03j]\n",
      " [-1.18795875e-02-2.46299207e-02j  6.67934120e-03+1.65273435e-02j\n",
      "   5.93238277e-03-4.51543927e-03j -3.43591645e-02-5.39836809e-02j\n",
      "   2.94801779e-02+5.12660332e-02j  3.31043191e-02+2.98756491e-02j\n",
      "   1.03417877e-02-1.26652438e-02j -5.50214425e-02+3.85611989e-02j\n",
      "   8.76270700e-03+6.10098708e-03j -4.54021655e-02-7.33538866e-02j\n",
      "  -4.81875874e-02+1.89694874e-02j  1.74368508e-02-1.30877513e-02j\n",
      "   2.23698691e-02-1.38823828e-02j -4.31646444e-02+2.46159844e-02j\n",
      "  -7.18477815e-02+4.52181213e-02j  1.12321405e-02-3.45489196e-02j]\n",
      " [-2.04471573e-02-1.65670365e-02j  6.58083474e-03-5.74079528e-03j\n",
      "  -1.04958033e-02-1.17754629e-02j  1.13258855e-02-8.45666081e-02j\n",
      "  -1.04221953e-02-6.99041039e-03j -2.02584080e-02-1.93399079e-02j\n",
      "  -1.03778415e-03+3.27607878e-02j  2.35575903e-02-1.96706373e-02j\n",
      "   5.64534543e-03-2.75495276e-02j -2.97945216e-02-3.75134572e-02j\n",
      "  -8.52787271e-02+9.60989594e-02j  7.23744407e-02-2.56310292e-02j\n",
      "  -8.05204175e-03+1.33001124e-02j -5.52277528e-02+4.13722582e-02j\n",
      "  -5.68960756e-02-1.51474876e-02j  5.32041080e-02+3.16979922e-02j]\n",
      " [-4.21977453e-02+1.31341759e-02j  5.20559847e-02+4.97171329e-03j\n",
      "   4.75625023e-02+4.26392118e-03j -2.17788648e-02+4.00652774e-02j\n",
      "   5.86762913e-02+2.92415591e-03j -7.78557211e-02-8.30280259e-02j\n",
      "   4.04001474e-02-1.49879232e-02j -4.68809381e-02-5.19386195e-02j\n",
      "  -2.10304791e-03-6.09240029e-03j  2.42730938e-02-4.61457074e-02j\n",
      "   5.95844537e-02-5.70840649e-02j -6.08361475e-02-6.08516857e-02j\n",
      "   1.92268137e-02-3.12118717e-02j -3.35398465e-02-1.56868249e-02j\n",
      "   3.40413712e-02+2.71007586e-02j -2.67730057e-02-6.52958229e-02j]\n",
      " [-1.42877307e-02-3.24438475e-02j  8.09308793e-03+3.48538086e-02j\n",
      "  -1.22168520e-02-5.19924499e-02j  6.86183572e-02-7.29714930e-02j\n",
      "   1.24157388e-02+5.88151366e-02j -3.30074355e-02-5.85395284e-02j\n",
      "   9.77460854e-03-7.36852214e-02j  1.51515286e-02+2.65168771e-02j\n",
      "  -2.88518928e-02-4.18892801e-02j -5.40885422e-03+2.50310879e-02j\n",
      "  -2.01918893e-02-2.50304416e-02j  1.56514764e-01-6.34405687e-02j\n",
      "  -2.72338893e-02+3.50964330e-02j  7.67189339e-02-2.30775550e-02j\n",
      "   5.00444919e-02+9.26837698e-03j  9.77293253e-02-2.81700846e-02j]\n",
      " [-7.69690722e-02-1.18015790e-02j -3.46964821e-02-9.53755155e-03j\n",
      "  -1.58702061e-01+5.69370873e-02j -2.02500485e-02+3.45134996e-02j\n",
      "  -4.28387336e-02+1.37945311e-03j  2.01411871e-03+6.16708733e-02j\n",
      "  -2.16112062e-01+1.21137984e-01j  8.93342867e-02-3.43841724e-02j\n",
      "  -6.09813258e-02+3.80626395e-02j  7.54090771e-02-3.81914638e-02j\n",
      "  -2.71670818e-02+1.00607418e-01j -4.44328599e-02+1.25612095e-01j\n",
      "  -2.16556005e-02+3.58641326e-01j  1.02480920e-02+1.66173652e-01j\n",
      "   1.59428328e-01+1.74560726e-01j  7.15179890e-02+2.58012712e-02j]\n",
      " [ 8.22050646e-02-1.50761098e-01j  4.43744101e-02+1.39685273e-01j\n",
      "  -6.31187037e-02+2.52986830e-02j -5.31043485e-02-1.06922448e-01j\n",
      "   9.81996581e-03-1.64758891e-01j -5.45206433e-03-8.69442821e-02j\n",
      "  -9.82670337e-02-6.49994016e-02j  2.10575715e-01+1.50498480e-01j\n",
      "  -4.64765541e-03-1.90604880e-01j -1.83203481e-02+2.16668487e-01j\n",
      "  -4.05237786e-02+1.91520438e-01j -1.13666832e-01+8.48722532e-02j\n",
      "  -5.99413291e-02+1.45161927e-01j -1.11498386e-01-2.72960141e-02j\n",
      "   1.10447422e-01+3.11785173e-02j -4.47663143e-02+6.32130727e-03j]\n",
      " [ 2.31460020e-01-1.54098466e-01j  6.87731281e-02-7.84677714e-02j\n",
      "   1.20580904e-01-1.98904395e-01j -9.92621556e-02-5.44239320e-02j\n",
      "   4.69883569e-02-1.53407544e-01j -1.41343027e-01+5.45858927e-02j\n",
      "   1.68207623e-02+2.73198192e-03j -1.02001682e-01-7.09903762e-02j\n",
      "   7.53468648e-02-2.12842688e-01j  6.36234973e-03+1.14391550e-01j\n",
      "   8.85069296e-02-1.48446903e-01j  2.71039568e-02+5.59065901e-02j\n",
      "   1.74992353e-01+1.50626257e-01j -3.10899969e-03+4.22838051e-03j\n",
      "  -8.21037441e-02+5.13758585e-02j  8.91962871e-02-1.27584770e-01j]\n",
      " [ 2.18461633e-01+0.00000000e+00j  2.13710442e-01+0.00000000e+00j\n",
      "   2.19629958e-01+0.00000000e+00j  2.12317720e-01+0.00000000e+00j\n",
      "   2.19098419e-01+0.00000000e+00j  2.16836005e-01+0.00000000e+00j\n",
      "   2.22233579e-01+0.00000000e+00j  2.16905355e-01+0.00000000e+00j\n",
      "   2.19100311e-01+0.00000000e+00j  2.17276916e-01+0.00000000e+00j\n",
      "   2.15979114e-01+0.00000000e+00j  2.17635840e-01+0.00000000e+00j\n",
      "   2.15716094e-01+0.00000000e+00j  2.16161430e-01+0.00000000e+00j\n",
      "   2.15789542e-01+0.00000000e+00j  2.18145922e-01+0.00000000e+00j]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'T')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Will change to showing loss instead of mean square error\n",
    "#Xout[T][Ndata][2**n]\n",
    "#backwards_gen[T][Ndata][2**n]\n",
    "mse_calc = np.zeros((21, 16))\n",
    "avg_backwards_vector = np.sum(backwards_gen, axis = 1)/Ndata\n",
    "print(avg_backwards_vector)\n",
    "\n",
    "orig_vals = np.sum((np.load('training_data.npy')), axis = 0) / 2000\n",
    "#print(orig_vals)\n",
    "for i in range(0, 21):\n",
    "    temp1 = 0\n",
    "    for j in range(0, 2**n):\n",
    "        temp1 += (np.abs(orig_vals[j] - avg_backwards_vector[i][j]))**2\n",
    "    #print(temp1)\n",
    "    mse_calc[i] = temp1/ 16\n",
    "\n",
    "#mse_calc = np.abs(np.sum(mse_calc, axis = 1)) / 16\n",
    "\n",
    "plt.plot(range(0, 21), mse_calc)\n",
    "plt.title(\"Mean Square Error between Forward and Backward Diffusion\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.xlabel(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize average output of the model\n",
    "#final_output_nxn[T][Ndata][4][4]\n",
    "#print_this_1 is average output of model\n",
    "#fig, axs = plt.subplots(1, 1, figsize = (10, 20))\n",
    "num1 = np.max(np.abs(avg_backwards_vector))\n",
    "print_this = np.abs(avg_backwards_vector) /num1\n",
    "print_this_1 = np.zeros((4,4))\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 4):\n",
    "        print_this_1[i][j] = print_this[0][4*i + j]\n",
    "\n",
    "\n",
    "for z in range(0, 1):\n",
    "    #print(final_output_nxn[z])\n",
    "    plt.imshow(\n",
    "        #final_output_nxn[0][z]\n",
    "        print_this_1, cmap = 'grey', interpolation = 'nearest')\n",
    "    plt.title('Average Output at T = %d'%0)\n",
    "\n",
    "#plt.suptitle('Backwards Diffusion At T = 0')\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
