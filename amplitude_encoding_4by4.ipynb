{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please first ``pip install -U qiskit`` to enable related functionality in translation module\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import unitary_group\n",
    "from skimage.measure import block_reduce\n",
    "from opt_einsum import contract\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from src.QDDPM_torch import DiffusionModel, QDDPM, naturalDistance\n",
    "#import src.ImageEncode as ie\n",
    "rc('text', usetex=False)\n",
    "rc('axes', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Torch will use CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Torch will use CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training(values: np.array, n_train: int, scale: float, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    n = values.size\n",
    "    noise = abs(np.random.randn(n_train,n))+0j*np.random.randn(n_train,n) \n",
    "    print(noise.shape)\n",
    "    print(values.shape)\n",
    "    states = (noise*scale) + values\n",
    "    states/=np.tile(np.linalg.norm(states, axis=1).reshape(1,n_train), (n,1)).T\n",
    "    return states\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#will not work with mixed state probabilities\n",
    "#   ***This cell is not being used right now***\n",
    "from PIL import Image\n",
    "scale = 40\n",
    "img = Image.open('images.png')\n",
    "img_greyscale = img.convert(\"L\")\n",
    "img_resized = img_greyscale.resize((scale, scale))\n",
    "\n",
    "img_array = np.array(img_resized, ndmin = 2)\n",
    "img_resized.save('dog_resized_greyscale.jpg')\n",
    "\n",
    "collapsed_array = np.zeros(img_array.size)\n",
    "\n",
    "for i in range(0, img_array.size):\n",
    "    collapsed_array[i] = img_array[int(i/scale)][i%scale]\n",
    "\n",
    "test_data = np.zeros((int(img_array.size / 4), 4)) + 1j * np.zeros((int(img_array.size / 4), 4))\n",
    "temp_normalize = max(collapsed_array) * 4\n",
    "for i in range(0, int(img_array.size / 4)):\n",
    "    test_data[i] = collapsed_array[4*i : 4*i+4] / temp_normalize + 1j * np.zeros(4)\n",
    "\n",
    "plt.imshow(img_array, cmap='grey',interpolation = 'nearest')\n",
    "\n",
    "# Find where NaN values are\n",
    "print(np.isnan(test_data).any())\n",
    "\n",
    "# Get the indices of the NaN values\n",
    "#nan_indices = np.where(img_array)\n",
    "\n",
    "#print(nan_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Square 4 by 4 to add noise and turn into training data\n",
    "temp_test = np.array([[1, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1], [1, 1, 1, 1]])\n",
    "plt.imshow(temp_test, cmap='grey',interpolation = 'nearest')\n",
    "temp_test = temp_test.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 16)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "#generate training data for diffusion\n",
    "n = 4\n",
    "T = 25\n",
    "Ndata = 3000\n",
    "\n",
    "diff_hs = torch.from_numpy(np.linspace(0.5, 4., T))\n",
    "\n",
    "model_diff = DiffusionModel(n, T, Ndata)\n",
    "\n",
    "X = torch.from_numpy(generate_training(temp_test, Ndata, 0.01))\n",
    "\n",
    "np.save('training_data', np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2800x1400 with 50 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize training data before they are forward diffused\n",
    "training_temp = np.abs(np.array(X))\n",
    "multiplier = np.max(training_temp)\n",
    "training_temp /= multiplier\n",
    "fig, axs = plt.subplots(5, 10, figsize = (28, 14))\n",
    "for index_1 in range(0, 50):\n",
    "    training_images = training_temp[index_1]\n",
    "\n",
    "    picture_training  = np.zeros((4, 4))\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            picture_training[i][j] = training_images[i*4 + j]\n",
    "\n",
    "    axs[int(index_1 / 10)][index_1 % 10].imshow(picture_training, cmap='grey',interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "#diffuse training data\n",
    "Xout = np.zeros((T+1, Ndata, 2**n), dtype = np.complex64)\n",
    "Xout[0] = X.numpy()\n",
    "\n",
    "for t in range(1, T+1):\n",
    "    Xout[t] = model_diff.set_diffusionData_t(t, X, diff_hs[:t], seed = t).numpy()\n",
    "    print(t)\n",
    "\n",
    "np.save(\"states_diff\", Xout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         1.         ... 1.         0.99999988 1.        ]\n",
      " [0.95379411 0.94283223 0.95989051 ... 0.96230815 0.93162313 0.94562588]\n",
      " [0.93342679 0.94968429 0.80878709 ... 0.84761103 0.76824997 0.73139394]\n",
      " ...\n",
      " [0.03011609 0.10262213 0.04946043 ... 0.09197472 0.0871414  0.150233  ]\n",
      " [0.05082321 0.06157269 0.08411071 ... 0.16418842 0.0980667  0.00735945]\n",
      " [0.08787546 0.0059782  0.01721599 ... 0.04829054 0.00670319 0.0607175 ]]\n"
     ]
    }
   ],
   "source": [
    "#calculate fidelity of forward diffusion\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "fidelity = np.zeros((T + 1, Ndata))\n",
    "for i in range(0, T + 1):\n",
    "    for j in range(0, Ndata):\n",
    "        #different calculations for mixed state fidelity\n",
    "        fidelity[i][j] = np.abs(np.vdot(states_diff[i][j], states_diff[0][j])) ** 2\n",
    "\n",
    "\n",
    "fidelity_mean = np.mean(fidelity, axis = 1)\n",
    "print(fidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAG2CAYAAADY5Dp/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABMOElEQVR4nO3deXxU1d3H8e9MCCQkSAIJaAg7VBZlFaygCLVotbLJ4lKLuLRVa6v1cX/EVh8Vwbo+1WpdEBE3dgUrggYU8AkW2aPIDgEkCSFsSUjI3OeP6yS5M1lmJjNzJ5nP+/Wa19xzc8+5v0Bu5pdzzz3HYRiGIQAAAISN0+4AAAAAog0JGAAAQJiRgAEAAIQZCRgAAECYkYABAACEGQkYAABAmJGAAQAAhBkJGAAAQJiRgAEAAIQZCRgAAECYNbI7APjGMAx9++23Wr9+vXJyciRJrVu3Vu/evdWvXz85HA6bIwQAAL4iAZO0f/9+rVmzRpmZmVqzZo3+85//6Pjx4+Vfb9++vXbv3m1LbKWlpXrhhRf0/PPPa//+/VUek56errvuukt//vOfFRsbG+YIAQCAvxzRuhj3qlWr9MwzzygzM1MHDhyo8Vi7ErB9+/Zp1KhRWrdunU/H9+/fXwsXLlSbNm1CHBkAAKiLqB0D9s0332j+/Pm1Jl92ycnJ0bBhw7ySr/j4ePXs2VPdu3dXXFyc5Wtr167VsGHDlJeXF85QAQCAn6I2AatJYmKi3SFo0qRJ2rFjR3k5Li5Ozz//vPLy8rR582ZlZWUpLy9Pzz77rCUR27Ztm2666SY7QgYAAD6K+jFgzZo1U//+/TVgwAANHDhQAwYM0K5duzRs2DDbYvrss8/073//u7wcGxurJUuWaMiQIZbjEhIS9Je//EX9+vXT8OHDVVpaKkn6+OOPlZGRYev3AAAAqhe1Y8B27NihU6dOqVu3bnI6rR2By5cvtyQv4R4Ddv7552vNmjXl5cmTJ+uxxx6rsc7kyZP1+OOPl5cHDRqkVatWhSxGAAAQuKhNwGpiZwK2adMm9erVq7yckJCggwcPqlmzZjXWO378uM466yydPHmyfF9WVpa6d+8eslgBAEBgGAMWYRYuXGgpT5gwodbkSzJvpY4fP96yb8GCBcEMDQAABAkJWIRZvHixpXzppZf6XHf48OGW8qJFi4ISEwAACC4SsAhiGIY2btxo2Tdo0CCf6w8ePNhS3rBhg7jDDABA5CEBiyB79uxRYWFheTkhIUHt2rXzuX779u3VtGnT8vLJkye1b9++oMYIAADqLuqnoYgkW7dutZTbtm3rdxtt27a1tLN169Yqk7jTp09r27ZtkqT8/HxJUlJSktcToTVJSUnxOz4AACKRP5OYu1wuFRQUSJJatGghSeratasaNfI9rSIBiyDuRbbd0tPT/W6jTZs2lgTMs023bdu2qUePHn63DwAAvPk78wC3ICPIiRMnLOWEhAS/2/Cs49kmAACwHwlYBPFMljzXevRFfHx8jW0CAAD7kYBFkOLiYku5cePGfrfRpEkTS7moqKhOMQEAgOBjDFgE8ezxKikp8buNU6dO1dimm3vQYGUrV66scn91UlJSpA8/lO64w/cA09Ol6dOl3r19rwNEmby8PK8xmllZWTz4AgTA1+vJn0H4+fn5uvDCCy37/Pn8lEjAIkpiYqKl7Nkj5gvPHi/PNt2qetrxZz/7mVJTU/074c03S5MnSwUFki9zjmVnS5ddJt15p/T441KlaTMAVC8lJcX/6xNAlaq6nvy5vnJzc732+TOLgMQtyIjimSxVXtfRV551qkvAgiYuTpoxQ5JkOBxVH+NwmK/27c2yyyUtWSL58bguAAANCQlYBGnVqpWlnJ2d7Xcb+/fvr7HNkBgxQlqwQMYZZ0iSyn7a7X5XUpK0cKG0bZv05JNSfLz0r39JAYxxAwCgISABiyBnn322pRzILPaedbp161anmHw2cqQOb9qk6yUtkJTx0/uxl16SDhwwk7TYWOnBB6V9+ySPZZO0aZP0xhu+3cYEAKCeIwGLIO3bt7dMI3Hy5Ent2bPH5/pVLWUUyGz6AYuL0yxJ4yT94qf3U+PHm7cpK2vZ0louK5N+9zvpllukYcOkH36o+FpxsTRzpjR2rDR0qPk+c6a5HwCAeooELII4HA716tXLsm/16tU+11+1apWl3KtXLzmqG5cVSZYtkzIzze0VK6RevcwB+nPnSmlp0sSJ0oIF5tcWLDDLaWnSxx/bGTUAAAEjAYswV155paW8dOlSn+t6HjtixIigxBRyl10mffqp1LGjWT51ynyyctw48+lKyRy4X/m9oEAaNUr66KNwRwsAQJ2RgEWYkSNHWsqzZ8/2aTb748ePa/bs2ZZ9o0aNCmpsIXXZZeY4sHvvlSo/ylvdmDD3/kmTuB0JAKh3SMAiTK9evTRgwIDy8okTJzRt2rRa602bNs0yBcXPf/7z+rfYdkKCNG2a9Le/+Xa8YUhHjkhz5oQ0LAAAgo0ELMQcDofltXz58lrrPPbYY5byU089pS+//LLa41esWKGpU6da9j3++OMBxRsR1q+39oLVxOmU5s8PaTgAAARbVM+EuWrVqirXStywYYOlXFxcrGXLllXZRlpaWtB7mn71q1/p0ksv1WeffSZJKi0t1WWXXaannnpKv/vd79T0p9njT548qddee00PPvigSktLy+tfccUVuuSSS4IaU1gdPlwx1qs2LpeUnx/aeAAACDKHYUTvxEsdOnTwa5qHqtxwww166623qv2651OIGRkZGjp0aK3tHjp0SBdccIF27dpl2R8fH69OnTrJMAzt3LnTa7mizp076+uvv651SYXc3FyvSVpzcnIiY6mTsWPNpx19ScKcTmn0aPOJSQAAwiAYn6HcgoxQrVu3VkZGhnp7LFpdVFSkLVu2KCsryyv56tOnjzIyMiIjiaqL0aP96wEbMyak4QAAEGwkYBGsffv2WrNmjaZOnaq0tLRqj0tLS9O0adOUmZkZ3olXQ2X8eCk52Vw/sjbJyeZ0FQAA1CNRfQuyPnG5XFq7dq02bNignJwcSeY6j3369FG/fv38XoU9om9BSuYkq+5pNGr6EX3lFekPfwhPTAAAKDifoSRgUSriEzDJnGR10iRzqgmn07zd6H5369tXWr3ae7kjAABChDFgaNhGjjQX8p450xwXNnSo+f7aa1LXruYx69ZJd99tY5AAAPiPHrAoVS96wGqycaN0/vkVs+C//7509dX2xgQAiAr0gCF69eol/eMfFeVbbpG2bbMvHgAA/EAChvrrppuk3/7W3D5xwnx6soqJdQEAiDQkYKi/HA7p5Zelbt3M8oYN0j332BsTAAA+IAFD/ZaYKM2eLcXHSx07SjfeaHdEAADUKqrXgkQDcc455rxh/ftLSUl2RwMAQK1IwNAw1OfFxwEAUYdbkGiYTp+W5s+3OwoAAKpEAoaG58ABs0fsqqukd96xOxoAALyQgKHhyciQvvzS3L71Vun77+2NBwAADyRgaHh+85uKpyFPnjTnBysstDcmAAAqIQFDw/SPf0g9e5rbmzdLf/qTvfEAAFAJCRgapqZNzfnBEhLM8ptvSm+/bW9MAAD8hAQMDVf37tIrr1SUb7tNysqyLx4AAH5CAoaG7frrzYW6JXMc2Pjx5rgwAABsxESsaPhefFHKzJQ2bTJ7wKZMkc4+W1qwQDp8WGrZUho92kzO4uLsjhYAEAUchmEYdgeB8MvNzVWrVq0s+3JycpSammpTRCG2dat03nnSRRdJX38tFRRITqfkclW8JydLM2ZII0bYHS0AIIIF4zOUW5CIDmefLb3wgvTpp9LRo+Y+l8v6XlAgjRolffSRLSECAKIHCRiiQ3GxdM895nZ1nb7u/ZMmmccDABAiJGCIDrNnS0eOVJ98uRmGedycOeGJCwAQlUjAEB0WLDDHevnC6WQhbwBASJGAITocPlwx1qs2LpeUnx/aeAAAUY0EDNGhZUv/esBatAhtPACAqEYChugwerR/PWBjxoQ0HABAdCMBQ3QYP96c58vhqPk4h8M8bty48MQFAIhKJGCIDnFx5iSrUu1J2IwZzIgPAAgpEjBEjxEjzKchk5LMclVjwv7nf5gJHwAQciRgiC4jR0oHDkgzZ5rjwoYOlQYMqPj6F1/YFRkAIIqwGDeiT1ycdP315kuSysqk7t2lbdukH36QcnOlhromJgAgIpCAATEx0tSpUl6eNHGi1KSJ3REBABo4EjBAYtoJAEBYMQYMAAAgzEjAgKrs2CHt2mV3FACABooEDKgsJ0f67W+ln/1Meughu6MBADRQJGBAZYmJ0pIl5nJEH3wgbd1qd0QAgAaIBAyorGlT6b/+y9w2DGnKFHvjAQA0SCRggKfbbzfXg5Skd95hLBgAIOhIwABPzZpJd91lbpeVmXOEAQAQRCRgQFX+9CczEZOk6dOl7Gx74wEANCgkYEBVkpPNJEySSkqkp5+2Nx4AQINCAgZU5667zEH5kvSvf0mHDtkaDgCg4SABA6qTmirdequ5XVwsvfaavfEAABoMEjCgJvfcI/XpI739tvTAA3ZHAwBoIFiMG6jJWWdJ334rORx2RwIAaEDoAQNqQ/IFAAgyEjDAX0eP2h0BAKCeIwEDfLV+vTR+vNS1q3TihN3RAADqMRIwwFfPPCPNmSPl5kqvvmp3NACAeowEDPDVQw9VjAd7+mmpqMjeeAAA9RYJGOCr7t2lcePM7UOHpDfesDceAEC9RQIG+OO//7tie+pU6dQp+2IBANRbJGCAP3r3lkaMMLezs80JWgEA8BMJGOCvhx+u2J4yRTp92r5YAAD1EgkY4K+BA6VLLzW3d+2S3n3X3ngAAPUOCRgQiMq9YE8+KZWV2RcLAKDeYS1IIBAXXSRdfLF05IiZjLFcEQDADyRgQKDmzpVatCD5AgD4jQQMCFTLlnZHAACopxgDBgAAEGb0gAHBsGKFdOedUmKi1KiR2Ts2erS5eHdcnN3RAQAiDAkYUFdPPGF9KlKSnE5p3jwzKZsxo2LyVgAAxC1IoG4++kiaPNl7v8tlvhcUSKNGmccBAPATEjAgUMXF0qRJNR9jGOb7pEnm8QAAiAQMCNzs2eY8YO4kqzqGYR43Z0544gIARDwSMCBQCxaYY7184XRK8+eHNBwAQP1BAgYE6vDhirFetXG5pPz80MYDAKg3SMCAQLVs6V8PWIsWoY0HAFBvkIABgRo92r8esDFjQhoOAKD+IAEDAjV+vJScXPtakA6Hedy4ceGJCwAQ8UjAgEDFxZmTrErVJ2Hu/TNmMCM+AKAcCRhQFyNGmE9DJiWZZfeYMPd7UpK0cCEz4QMALFiKCKirkSOlAwfMeb7mzzefdmzRwhzzNW4cPV8AAC8kYEAwxMVJ119vvjzt3i116BDuiAAAEYxbkECozJol9esndekiHTxodzQAgAhCAgaEyvffS+vWSWVl0syZdkcDAIggJGBAqFReqHv69NrXjAQARA0SMCBUOneWhgwxt7//XsrMtDceAEDEIAEDQummmyq233zTvjgAABGFBAwIpXHjpMREc/v996XCQnvjAQBEBBIwIJQSEqQJE8zt48elefPsjQcAEBFIwIBQu/HGim1uQwIARAIGhN7gwVLXruZ2Roa0a5e98QAAbEcCBoSaw2H2gsXGSlddJZ06ZXdEAACbsRQREA633ir97ndSSordkQAAIgAJGBAOycl2RwAAiCDcggQAAAgzEjAg3E6dkmbPlo4etTsSAIBNSMCAcJo7V0pLM+cG++ADu6MBANiEBAwIp06dpPx8c5s5wQAgapGAAeHUt6/Up4+5nZkpffedreEAAOxBAgaEW+WZ8adPty8OAIBtSMCAcLvuOnNSVkl6+22ptNTeeAAAYUcCBoRbSoo0cqS5feiQ9Omn9sYDAAg7EjDADjfdVLHNbUgAiDokYIAdLr1UOussc/vjj6WcHHvjAQCEFQkYYIdGjaSJE83t06elWbPsjQcAEFYkYIBdbrzRnJT1wQcrxoQBAKICi3EDdjn7bGnfPsnJ30EAEG34zQ/YieQLAKISv/0BAADCjAQMiAT79kmPPy598ondkQAAwoAxYIDdNmww14g0DGn4cOmKK+yOCAAQYvSAAXY791ypQwdze9kyae9eW8MBAIQeCRhgN6ezYoFuwzDXhwQANGgkYEAkuOEGyeEwt6dPl1wue+MBAIQUCRgQCdq1ky65xNzeuVP66it74wEAhBQJGBApWKAbAKIGCRgQKUaPlpo3N7dnz5aOH7c1HABA6JCAAZEiPl669lpzu7DQTMIAAA0SCRgQSSrfhnz/ffviAACEFBOxApHkvPOkSZOkiy+Wxo2zOxoAQIiQgAGRxOFgAD4ARAFuQQIAAIQZCRgQ6QzD7ggAAEFGAgZEIpdLWrJEGjzYXCdy6FBp7Fhp5kypuNju6AAAdcQYMCASLVhgDsJ3937t3WuuGTlvnnTnndKMGdKIEbaGCAAIXEQkYPv371dWVpb27Nmj48ePq6ioSPHx8WrWrJnatWunnj17qk2bNnaHCYTHRx9Zky839/qQBQXSqFFmkjZyZLijAwAEgW0J2I8//qgXXnhBc+bM0c6dO2s9vmPHjho/frz+/Oc/66yzzgpDhIANiovNaShqYhjm05KTJkkHDkhxceGIDAAQRLaMAXvppZfUtWtXTZs2TTt37pRhGLW+du3apWnTpqlr16763//9XzvCBkJv9mzpyJHaB94bhnncnDnhiQsAEFRh7wF7/PHH9de//lXGTx8wTZs21c9//nP16NFDbdu2VbNmzdSkSROdOnVKJ06c0N69e5WVlaX/+7//U2FhoQoLC3XXXXepoKBAkydPDnp8O3bs0Jo1a5Sdna2SkhIlJyerW7duGjRokOLoaUCoLVhgjvVy326sidMpzZ8vXX99yMMCAASZEUZr1641GjVqZDgcDqNVq1bG66+/bhQVFflUt6ioyHj99deN1q1bGw6Hw2jUqJGxdu3aoMU2f/58o1+/foakKl+JiYnGHXfcYeTm5gbtnNW5+OKLq43Dl9f06dNrPUdOTo5XvZycnJB/b6jFxRcbhtm/5dtr6FC7IwaAqBOMz9Cw3oJ85ZVXVFZWprS0NK1du1Y333yzz71KcXFxuvnmm/XNN9/orLPOksvl0iuvvFLnmE6dOqXrr79eY8aM0bffflvtcSdOnNA//vEP9ejRQ19++WWdzwtUqWVLs2fLF06n1KJFaOMBAIREWBOwjIwMORwOPfDAA0pPTw+ojbZt2+rBBx+UYRj64osv6hSPy+XS1VdfrVmzZln2x8TEqGPHjurTp4+aN29u+Vpubq4uv/xyff3113U6N1Cl0aN9u/0omceNGRPScAAAoRHWMWAHDhyQJJ1//vl1asdd/+DBg3Vq5+mnn9bChQst+2699VZNnjxZaWlpkswkbeHChbrrrru0d+9eSVJhYaEmTJigzZs3eyVoobB06VK/ju/Zs2eIIkHIjR9vzvNVUFDzQHyHQ0pKYsFuAKinwpqAxcfHq7i4WCdOnKhTO+76dRkUf/jwYT3xxBOWfVOmTNEDDzxg2ed0OjVmzBgNHDhQF154oXbv3i1Jys7O1rPPPqtHH3004Bh89ctf/jLk50CEiIszJ1kdNcpMsqpKwhwO833GDKagAIB6Kqy3IDt16iRJmlPHR+dnz54tSercuXPAbUybNk3Hjx8vLw8ZMkT3339/tce3adNGr7/+umXfc889p8OHDwccA1ClESPMpyGTksyye0yY+z0pSVq4kJnwAaAeC2sCNnr0aBmGoVdffVVvvPFGQG28/vrrevXVV+VwODQmwPEvLpdL06dPt+z729/+Joe7Z6Eal1xyiS666KLy8vHjx/Xhhx8GFANQo5EjzUlWZ840x4UNHWq+z5xp7if5AoB6LawJ2B//+Ee1adNGhmHo97//vS6++GK988475WPDqnPgwAHNmjVLQ4cO1R/+8AcZhqG0tDT98Y9/DCiO1atXKzc3t7zcqVMnDR061Ke6N998s6W8YMGCgGIAahUXZ87xNXeulJFhvl9/vbk/L8/u6AAAdRDWMWDNmzfX3LlzdeWVVyovL08rV67UypUrJUlnnHGG2rZtq8TERDVu3FglJSU6ceKE9u3bp2PHjpW3YRiGWrRooblz5+qMM84IKI7FixdbysOHD6+196vysZUtX75cJ0+eVEJCQkCxAH6ZPl165RVpxw5p/36pSRO7IwIABCDsSxENHDhQmZmZGjlypGWpoaNHj2rLli3KzMzUV199pczMTG3ZskVHjx61HDdixAhlZmZq4MCBAcewfv16S3nQoEE+101LS1OHDh3KyyUlJcrKygo4FsAvS5dKa9ZIhw9LHn9IAADqD1sW4+7YsaMWLFig7777TnPmzNGKFSu0ZcsWHTp0yOvYVq1aqWfPnrr44os1btw49ejRo87n/+677yxlf9vs0aNH+dOQ7vYGDBhQ57hqcvToUe3Zs0cFBQVKTExUy5YtlZ6erpiYmJCeFxFm0iTpvffM7bfekq66ys5oAAAB8isBmzt3rgYMGKB27doF5eTdu3fX5MmTy9d0LCkp0fHjx1VUVKT4+Hg1a9ZMjRs3Dsq53IqKisrn83Jr27atX214Hr9169Y6x1WTvn37auPGjXJ5TNCZmJiowYMHa+zYsZo4caKa1PF2VJ6f44pSU1PrdD4E4JJLpDZtzNuPn3wiHToktW5td1QAUO9VHhteG38/L6viVwI2fvx4ORwOpaamavHixerfv3+dA6iscePGatmyZVDb9JSXl1e+ELgkxcbGqlWrVn610aZNG0s5JycnKLFVx/OWqduJEye0ZMkSLVmyRI888ohefPFFjR8/PuDz+NsTaNQ0UShCIyZGmjhRmjJFKiuTZs2S7r7b7qgAoN7zNxeoK7/HgBmGodzcXB09ejQU8YSc5ySwTZs29XkAvpvngPu6TiwbDD/++KMmTJige++91+5QEGo33FCxPX16zTPmAwAikt8JmL/JSqTxTJYCmU0/Pj6+xjaDIS4uTiNGjNDLL7+s1atXKycnp/wW7Y4dO/TOO+/o17/+tdf/x9///nc99dRTQY8HEeTss6ULLjC3N2+W1q2zNx4AgN9COgh/w4YNevPNNzVgwACdd9556tatWyhP55Pi4mJLOZAxZp5jrYqKiuoUk6e7775bgwcPrvJ2bGxsrBITE9WpUyf95je/0cqVK3XNNddo//795cc89NBDuvzyy9W7d++gxoUIMmmS5F4Q/q23pH797IwGAOCnkCZgeXl5+t///V85HA45HA6dPn06lKfziWePV0lJid9tnDp1qsY262rkyJE+H3vhhRdq+fLluuCCC8oHBRqGoYcfflgff/yxX+fNyspSSkqKX3VgkwkTzEW7i4vNcWBPP82cYABQB/6M587Ly6vzrAxhmYYikgZrJyYmWsqePWK+8Ozx8mwz3Lp06aKnn35aN954Y/m+Tz75RPn5+WrRooXP7aSkpPBkY32RlCSNGSN9+KF04YXmvGBpaXZHBQD1Vrg//8I+EavdPJOlwsJCvxPEkydP1timHSZOnGj54XG5XFq2bJmNESHknnjCnI5i4UKSLwCoZ6IuAUtJSbEMXC8tLfV7GonK462k8D+6WhWn0+m1nmWo5yeDzTp2ZA4wAKinoi4Bi4+P95pI1nNi1tp4Hh8JDxdI3hPE+jOpHAAACJ+oS8Ak74TJ37UcPZcyipQELDY21lIuLS21KRKEXWmp9NlnzAkGAPVEVCZgffr0sZRXr17tc92DBw9a1oGMjY0NyvqUwfDjjz9aygyojxIvvWQuT3TZZdK339odDQDAB1GZgF155ZWW8rJly3weiP/ZZ59ZysOGDYuIQfiStHLlSkvZ3zUuUU81biy5bze/9ZatoQAAfBOVCdigQYMs813t3LlTy5cv96nuG2+8YSmPGjUqmKEFbMWKFdqxY4dl3yWXXGJTNAirCRMk91x0774recxTBwCIPAEnYJ9++qkyMzODPgt8ODidTk2aNMmy79FHH621F+zzzz/XV199VV5u1qyZJkyYEIoQ/XLy5En9+c9/tuw799xz1alTJ5siQlg1by5ddZW5nZ8vLVpkbzwAgFoFnIA988wzGjRokM444wx169ZNV199tZ588kl98sknXtM0RKL777/fcutwxYoVmjp1arXH79+/X7fccotl35133lnrzPHuVQDcr9p62u68804dOHCg9m/gJ3l5eRo5cqQ2btxo2f/oo4/63AYagMp/UHAbEgAin+EHh8NhOJ1Ow+l0Gg6Hw/Jy73e/UlJSjHPPPdfy9Ujz5JNPGpIsr9tuu83Yv39/+TFlZWXG/PnzjXbt2lmOS0tLM44cOVLrOTzbz8jIqPX4Jk2aGKNHjzbeeecdY9euXVUet3fvXmPatGnGmWee6XWO0aNH1xpXTk6OV72cnJxa6yFCnT5tGOnphiEZRkyMYRw8aHdEANBgBeMz1GEYvj+37nRaO8wqT2j6UzJX5dcNw5DD4VBycrJ69+6tPn36qG/fvurTp4+6d++umJgYX0MIKpfLpVGjRmmRxy2bmJgYtW/fXs2bN9euXbtUUFBg+Xp8fLyWLl2qwYMH13oOz3+jjIwMrwlTazpeks444wydddZZat68uUpLS3Xo0KFqe8kuuugiLVmyRPHx8TXGlZub6zWBbE5ODk9O1mf//d/Sk0+a23//u/Rf/2VvPADQQAXjM9SvBKygoEDr16/XunXryl9bt271WmS7qiRCqkjEKmvSpIl69OhRnpD16dNHvXv3DtuThcXFxbrxxhv1/vvv+3R8y5YtNWfOnBqTqMqCkYD5wul06p577tHjjz/uNR9YVUjAGqAffpDOPtvcPuccaeNGKcCfJwBA9cKegFXl1KlT2rRpU3lCtn79em3cuFGFhYXWE/nYW+be7tSpU3lC1qdPH11xxRV1CbNWc+fO1eOPP67169dX+fWEhATdcMMN+utf/+rX0kP+JmCvvfaavvjiC61atUr79u2rtf0zzzxTV199te644w516dLF57hIwBqowYMl97x2//mP1L+/vfEAQAMUEQlYVQzD0NatWy1J2bp163T48GHryX1MyhwOh1cvW6hs375dmZmZ2r9/v0pKSpSUlKTu3btr8ODBinM/6h8mhw8f1nfffac9e/YoNzdXJ0+eVExMjJKTk5WSkqK+ffsG/KQjCVgD9dpr0iOPSL/9rXTHHZLHslsAgLqL2ASsOtnZ2ZaEbN26ddqzZ481oCqSMofDobKysnCFGRVIwBqoU6ekmBipUSO7IwGABisYn6Fh/S2dnp6u9PR0jRgxonxfQUGBV1L2/fffk3ABgWjSxO4IAAA+sP3P5KSkJA0bNkzDhg0r3+c5rmzdunU2RggAABBctidgVWnSpInOO+88nXfeeXaHAtRfBw5IM2dKN9wgnXmm3dEAACqJyAQMQB299ZZ0882Sy2WOCbvnHrsjAgBUEpWLcQMN3uDBZvIlmclY+J61AQD4gAQMaIi6djWTMEnaskVau9beeAAAFiRgQEPFAt0AELFIwICGavx4yb0m6LvvmnOEAQAiAgkY0FA1by5ddZW5feSI9PHH9sYDAChHAgY0ZNyGBICIRAIGNGTDhklt25rbn34qHTxobzwAAEkkYEDDFhMjTZxobpeVSbNm2RsPAEASE7ECDd8NN0gZGdKNN5oD8wEAtiMBAxq6rl2lVavsjgIAUAm3IAEAAMKMBAwAACDMSMCAaGEY0ldfmYt0f/SR3dEAQFRjDBgQLVasMKelkKRDh6SRI+2NBwCiGD1gQLS46CIpPd3cXrxYGjRIGjtWmjlTKi62NzYAiDIkYEC0WLxYysurKH/9tbRggTlPWFoaSxUBQBiRgAHR4KOPpNGjvRfkdrnM94ICadQoxoYBQJiQgAENXXFxxZqQhlH1Me79kyZxOxIAwoAEDGjoZs+WjhypPvlyMwzzuDlzwhMXAEQxEjCgoVuwQHL6eKk7ndL8+SENBwBAAgY0fIcPV4z1qo3LJeXnhzYeAAAJGNDgtWzpXw9YixahjQcAQAIGNHijR/vXAzZmTEjDAQCQgAEN3/jxUnKy5HDUfJzDYR43blx44gKAKEYCBjR0cXHSjBnmdnVJmHv/jBnm8QCAkCIBA6LBiBHm05BJSWbZPSbM/Z6UJD36qPSLX9gQHABEHxbjBqLFyJHSgQPmPF/z55tPO7ZoIfXuLS1aJD3yiJSYKP3lL3ZHCgANnsMwapudEQ1Rbm6uWrVqZdmXk5Oj1NRUmyKCbTZvls4919w+6yxp505uQwJADYLxGcotSCDanXOOdNVV5vbBg9Ibb9gbDwBEARIwANLDD1dsP/WU96LdAICgIgEDIPXtK115pbmdnV3x1CQAICRIwACYJk+u2J4yRSottS8WAGjgSMAAmAYOlC67zNzevVt65x1bwwGAhowEDECFyr1gTz4pnT5tXywA0ICRgAGoMHhwxWSs27dLX3xhbzwA0EAxESsAq8mTpSZNzPcLLrA7GgBokEjAAFgNHWq+AAAhwy1IAACAMCMBA1Azw5AOHbI7CgBoUEjAAFTNMKS5c6V+/aTLLzfLAICgIAEDUL0nn5TWr5fWrZM++cTuaACgwSABA1A1h8O6RuT//A+9YAAQJCRgAKo3apR07rnmdmamtGyZvfEAQANBAgagek6ntRfsscfoBQOAICABA1CzsWOlbt3M7ZUrpRUr7I0HABoAEjAANYuJkf77vyvK//M/9sUCAA0ECRiA2l1zjdSli7n9xRfS6tX2xgMA9RwJGIDaNWokPfRQRZleMACoExIwAL65/nqpUydzTNiUKXZHAwD1GotxA/BNbKy0YYOUmGh3JABQ79EDBsB3JF8AEBQkYAACx5xgABAQEjAA/jt9WnrnHemcc6TNm+2OBgDqHcaAITTynzVftYnrJ6V/ZN2XPVIq/rb2ui3uNl9uZcelXd19iy99oRTXv6J8YpH0462113MmSp2+t+7LuVc69l7tdRN/LZ35qnXf7vOk0z/WXjd1mtT8uoryqa3SvktqrydJHb6RGp1VUS74l5T3WO31Gv9MaveFdd+B30iFK6STJ6X0AuklSYUDpe0trMcl/U5K+at13/Z03+I96x0pYWhF+eRy6eD1vtXtkm0t5z0qFbxWe72mF0tps6z79v5CKvmh9ropj0hJv68onz4o7R5Qez1Javu51OTsivLRd6Xc+2qv1+hMqcN/rPt+/IN0YnHtdc+4Vmr1tHXfzm6S60Ttdc98RUq8sqJcvFbKHlV7PUnq+J0U06yizO8Ibw3pd0RtIuV3hI1IwBAaZcek0/trP+502yr25fpWt+yYxw7Dt3qSZJRYy64i3+o6m3nvKzviY7z53vtO/+hbXaPQs6If32uZtew64eP32tx7X1meWbeJpDPdO6v4tys76l3X53hPeZd9reup7KiP/zd53vtOH/KtrmfiYpT5Ee9pj7qFdfhe8338Xo9UEcYByXW89rquImvZKPEjXo/b1fyOqOK4BvQ7ojaR8jvCRiRgCI2YM6RGbWo/rlFq1ft8qRtzhscOh2/1JMnR2Fp2xvtW11nFIPSYZB/jbeG9r9GZ3vuq4mjqWdGP7zXGWnYm+vh/09p7X0xKRd3jx6WjP33ANW0qtUiudFwVv5h9jreJd9nXup5imvv4f5Piva9Ra8lVxYeEJ8+fCUeMH/F6/Ap2NPXx/6aKn5uYFj5+r8ne+xql+dYD5oy3lh2N/fheHR5x8DvC+7gG9juiJpHyO8JGDsNgFG00ys3NVatWrSz7cnJylJpaxS87oCrHj0sdOkj5+eZyRVu3Sp072x0VAIRcMD5DGYQPIDDNmkl33WVul5VJN91kTtI6dKj5PnOmVFxsZ4QAELHoAYtS9IAhKAoKpDZtpMKfxp84nZLLVfGenCzNmCGNGGFrmAAQTPSAAbDXl19WJF+SmXRVfi8okEaNkj76yKsqAEQzEjAAgSkuliZNkhyO6o9xd7BPmsTtSACohAQMQGBmz5aOHKl9NnzDMI+bMyc8cQFAPUACBiAwCxaYY7184XRK8+eHNBwAqE9IwAAE5vDhirFetXG5zOkqAACSSMAABKplS/96wFpUMckkAEQpEjAAgRk92r8esDFjQhoOANQnJGAAAjN+vDnPV01PQbolJ0vjxoU+JgCoJ0jAAAQmLs6cZFWqPQl78EHzeACAJBIwAHUxYoT5NGRSkll2jwnzHBv23HPSwYPhjAwAIhoJGIC6GTlSOnDAXPtx9GhzLcjRo6Xp06WLLjKPOXjQvAVZUmJjoAAQOVgLMkqxFiTCIidH6t9fys42y7fdJr38sr0xAUAdsRYkgMjWqpU0b57UpIlZ/uc/pTfftDcmAIgAJGAAQmvAAOmVVyrK//Vf0tGj9sUDABGABAxA6E2aJN1+u9SunfT551Lz5nZHBAC2IgEDEB7PPSetXSv162d3JABgOxIwAOHRuLGUkmJ3FAAQEUjAANjj9GlzPNjMmXZHAgBh18juAABEoeJi6de/lr74wpwhv2dPbk0CiCr0gAEIv7g4qUsXc7u42FyoOy/P3pgAIIxIwADY48UXpZ//3Nzeu1e6+mrztiQARAESMAD2aNJEmjtXOvNMs/zFF9IDD9gbEwCECQkYAPukpUlz5kiNfhqO+swz0nvv2RsTAIQBCRgAew0eLL3wQkX55pulDRvsiwcAwoAEDID9brtNuvFGc7uoyByUf+CAOUXF2LHS0KHm+8yZ5qB9AKjnmIYCgP0cDunll6VNm6T//Me8Jdmjh7lmpNMpuVzm+7x50p13SjNmSCNG2B01AASMHjAAkSEuzkywRo6Utm+Xjh0z97tc1veCAmnUKOmjj2wJEwCCgQQMQORITZW++srcNoyqj3HvnzSJ25EA6i0SMACRY/Zs6ciR6pMvN8Mwj5szJzxxAUCQkYABiBwLFphjvXzhdErz54c0HAAIFRIwAJHj8OGKsV61cbmk/PzQxgMAIUICBiBytGzpew+YwyElJ4c2HgAIERIwAJFj9Gjfe8AMQ/r2W2nx4trHjAFAhCEBAxA5xo83e7UcDt+O37NHuvJKc4JWAKhHSMAARI64OHOSVan6JMzhMF+dOpnls86Sxo0LT3wAECQkYAAiy4gR5tOQSUlm2T0mzP2elCQtXGhO1jp3rvT881LTptY2/vUvad0677aLi1neCEBEcBgGgyeiUW5urlq1amXZl5OTo9TUVJsiAjwUF5vzfM2fbz7t2KKFuUbkuHFmT1l1du+WfvYzqbTUPPaxx6Tu3c2Z8ydNMucPq7y8kctl3vZkeSMAPgrGZygJWJQiAUOD9Ze/mL1ibk6nNGSItGKFWa7qV577dueCBeZSSP4oLjYnkF2wwJxGo2VL82GC8eNrThQB1FskYAgYCRgarOJi8xbkk09Khw75Xs/hMG9vHjjge+JErxoQlYLxGcoYMAANS1yc9Oc/Szt2SE895T0+rDr+Lm/00UdmT1dBgVlm0XAAfmhkdwAAEBIJCdL990urVkmLFvk+V9iUKeb4sTZtpPR08/2MM6xPZRYXmz1fUs2Lhjsc5nH+9Kq52+e2JtCgkYABaNiOHfNvotasLOmmm6z7EhPNRKxNG+mii6TOnc3estpU7lW7/nrfzl/dbc1586Q77+S2JtBAcAsSQMPmz/JG1TlxQtq6VfriC3N6C38XDY+E25pMwQFEFHrAADRso0ebvUe+uusuqUsXaf9+85WdXfF+8qTZC5aV5d+i4QsXmrczX3jBTHzcSkvNBKhZs9De1qRXDYg4JGAAGrbx480ko6Cg5luR7qcgp0ypOrExDPN25unT0u9/X5HI+Gr/fnNcWmXffCMNHiy1amWOMwvFbU13r5pbdb1qTMEBhBUJGICGzb280ahRZpJV0zxgM2ZUnzg4HFLz5ua2v71qnTtLR4+a75Vt326+5+SYL185ndJLL0m5ueYEte5Xy5bme3KyFBtLrxoQwZgHLEoxDxiiTjDn7CoultLSfO9Vqy6xmT/fnDR2+3bzGH+kpEh5edV/vVkzqUmTmo/xNHOm/71qwZ7YVgpNz1o0t4mgYyJWBIwEDFEp0OWNqvLxx2avmlRzErJwoW+J3ahR5nQZvtzWdDrN2H1Jrqrr9atKfLzUs6d05plS69bmeLhzzqn4+qlT5m3Ys8+ue/JZnVBMbhvNbUr1J1EMVfIZgnaD8hlqICrl5OQYkiyvnJwcu8MC6peFCw0jOdkwJMNwOq3vycmG8dFHvrf19ttmPV9fjzxiGO+9Zxj/+IdhPPaYYdx1l2FMnGgYv/61YVxwgWGcfbZhxMb616bna8UKa4yLFvnfxsyZ/v17Ohzmq6q23F9buJA2/Wk3WD+j9a3NELYbjM9QErAoRQIGBElRkZlkXHWVYQwdar7PnGnu97ed5OTqP4ArfxAnJ/vW/lVXVXzY+PJq0sR6/Nat1vZef92/5MvpNGMwDMOYN88wnnvOfP/PfwwjJ8cwXK7Qfv/R3KZh1J9EMZTJZyjaNYLzGcogfACoi7g4c9yUrxOt1tROMB4WqMzfhwVef1269lrz1uaPP0odOli/3rKleVvRPU9ZbVwu81avO+aFC61fj4uT2rWT2rc3b2/68xTorFnS1VebZZfLfG/WTIqJqTh21qzAnyz98UfrJL7uj+2FC/1rc+pU6Ve/qoixcrzduplPwM6e7V+b779f8XBFdULxAEZ9aTOU7QYRY8CiFGPAgAgViQ8LVDZ2rDmWxtexaqNHS3PnSn37SuvX+xZ3oDZskHr1qiifd560dq1vdSvHKpmJ6PvvBz1Ei3ffNc/jz7+p2/795v+t29tvS//8p7n2aUKC+VRtZqbv7V12mfmUbmmpVFJivrtf7nLnztIrr/jeZtu2FRMhT59u/b/JyJD+9jczzu+/973NmTPNRHv9evMhk8aNzVfl7caNzSXInnvOv3b9+CMqGJ+h9IABQCQZOdJMhILxsIDdvWoulxm3JL38srlA+p490t69Fa89e8wJboPB8/s7ccL3upV768LFHe/hw/4lX5L3nHK7d0v/93+Bx7JkSe3HZGf7N//dvn3mS5KKiqxfy8mRvvzSvxidTvOaSEqS3nzTv7q+tFvXXmw/kYABQKQJ1m1NyewtW7Cg+l61pCT/etX8ndh23DizfMEF5suTYZhtjRtn9or4elMmNdXsUXE6zXM5HOaanZW1amUuIeUL95OlbhddZM6l5m7b/T1lZJhJoy8cDqlTJ+nKKyvaqRxv9+7mce5eIl8Tm5YtzSdWKzt1yre6dVFY6F+i6HSa/4bun7fK/E043XXy881evmCyI/kWtyCjFrcggSgTyVNwSOYtoIkTfY/Bl1tG0dZmWZnZ01RYaLa3dKnvt4qHDJH+/nczYYqNNW/jubfd5RtuMG+R+3v7uSoul7mqxIQJ5s+TP23+7W9mb1xJiZl4lpR4b7/1lvTdd74l9LXFWgWmoUDAeAoSQJ0E+/H++vJ0YX1p099pTXyZLqS+tBnKdn8SjM9QZ/WpGQAA1XCPVZs50+w9GDrUfJ8509zv74Sh7vFqUkUPmid/x6tFc5vjx5sPbVTXXuV2k5MrbhU3hDZD2W4QcQvSw44dO7RmzRplZ2erpKREycnJ6tatmwYNGqQ4G5eBMAxD3377rdavX6+cn9aMa926tXr37q1+/frJUdsPmQduQQKISPVlhvn60GYobhXXlzZD2a64BRlU8+fPN/r16+fVpeh+JSYmGnfccYeRm5sb1rhKSkqMp59+2mjTpk21saWnpxt///vfjZKSEp/bDcUtSG5rAsER9ddSsCa3pc36M2t9CGfCL2ve3DAk4/RPtxvd73bPhB/1PWCnTp3SzTffrFmzZvl0fGpqqubMmaMhQ4aEODJp3759GjVqlNatW+fT8f3799fChQvVpk2bWo8NRQ8YvWpAcHAtIaiC+QBGfWtTUu6+ffpLu3YaI6mFpHxJw196SWfcdJOta0FGdQLmcrl01VVXaaHH7MwxMTFq166dmjdvrl27duno0aOWrzdt2lTLli3TBVU9Uh0kOTk5GjRokHbs2GHZHx8fr06dOsnlcmnXrl0qLi62fL1r165avXq1UlJSamyfBAyIXFxLQPBE6uddVA/Cf/rpp72Sr1tvvVV79+7Vzp07tW7dOuXn52vevHlq165d+TGFhYWaMGGCV2IWTJMmTbIkX3FxcXr++eeVl5enzZs3KysrS3l5eXr22WctY9O2bdumm266KWRxAQCAuovaBOzw4cN64oknLPumTJmif/7zn0qrtLyD0+nUmDFjtHr1anWotC5adna2nn322ZDE9tlnn+nf//53eTk2NlZLlizRnXfeqaaVJqBLSEjQX/7yF3366aeKjY0t3//xxx8rIyMjJLEBAIC6i9oEbNq0aTp+/Hh5eciQIbr//vurPb5NmzZ6/fXXLfuee+45HT58OOixTZ482VJ+4IEHahxzdvHFF3vF/vDDDwc9LgAAEBxRmYC5XC5Nnz7dsu9vf/tbrVM5XHLJJbrooovKy8ePH9eHH34Y1Ng2bdqkNWvWlJcTEhJ077331lrvvvvuU0KltcFWr16t7777LqixAQCA4IjKBGz16tXKzc0tL3fq1ElDhw71qe7NN99sKS9YsCCIkclrTNqECRPUrFmzWus1a9ZM48ePt+wLdmwAACA4ojIBW7x4saU8fPhwnycyHT58uKW8fPlynTx5MmSxXXrppT7X9Yxt0aJFQYkJAAAEV1QmYOvXr7eUBw0a5HPdtLQ0y2D8kpISZWVlBSUuwzC0cePGgGMbPHiwpbxhwwZF8SwjAABErKhMwDzHRvXo0cOv+p7HB2us1Z49e1RYWFheTkhIsEx/UZv27dtbnpI8efKk9u3bF5TYAABA8DSyO4BwKyoq0t69ey372rZt61cbnsdv3bq1znFV1Y6/cbnrVG5n69atVSZxLpfLa98PP/ygvLw8n8/lOdlrVXX9aQ+AiWsJCB5fryd/rrH8/HyvfVV9rtYk6hKwvLw8y2252NhYr9lsa+O51I97cey68mwnPT3d7zbatGljScCqi62qH54LL7zQ7/PVxt/eRQBV41oCgicU11N+fr5at27t8/FRdwvyxIkTlnLTpk19HoDvVnm6h6raDJRnO57n8UWoYgMAAMET9QlYXAALccbHx9fYZqAiOTYAABA8UZeAeS5e3bhxY7/baNKkiaVcVFRUp5jcIjk2AAAQPFE3BsyzV6mkpMTvNk6dOlVjm4EKZ2xdu3Ytnz7DPR4sKSlJTqfvObnnIHwAAOorfwbhu1wuFRQUSJJatGghyfxc9UfUJWCJiYmWsmevky88e5U82wxUOGNr1KiRunfv7nf7AAA0RKmpqWE9X9TdgvRMSAoLC/2erNRz5vtQJWCBzLAfqtgAAEDwRF0ClpKSYnnqsbS01O9pJPbv328p+zuNRXU828nOzva7jVDFBgAAgifqbkHGx8erXbt22rNnT/m+vXv3+jV3h+dErt26dQtKbGeffbalHMgs9p51ghWbL3bs2KE1a9YoOztbJSUlSk5OVrdu3TRo0KCgjZMD4K24uFirV6/W999/ryNHjqhx48ZKT0/X+eefr06dOtkdHhAwwzC0e/dubdq0SdnZ2SooKFCTJk2UnJysrl27asCAAUH/fDl+/LhWrVqlH374QceOHVN8fLzat2+vQYMGKS0tLXgnMqLQZZddZkgqf7311lt+1e/QoYOlfmZmZlDicrlcRnx8vKXt3bt3+1x/9+7dlroJCQmGy+UKSmw1mT9/vtGvXz/LuSu/EhMTjTvuuMPIzc0NeSxAJMjOzjbmzZtn3H///cawYcOMZs2aWa6J9u3bB+U8OTk5xh//+EcjISGh2uuvf//+xoIFC4JyPiAc8vPzjTfffNOYMGGCkZKSUu3PtiQjNjbWGD16tLF8+fI6n3fnzp3G9ddfbzRu3LjKczkcDmPo0KHGihUrgvBdGkZUJmD333+/5R/197//vc91Dxw44PWff/z48aDFdv7551vaf/fdd32uO2vWLEvdCy64IGhxVaW4uNj4zW9+U+PFUfmVmpoatB9cINKsXLnSGDNmjJGWllbrtRCMBCwjI6PWD6fKr4kTJxqnTp2q+zcKhNDtt99ebQLky8/40aNHAzrvBx98YDRt2tSn8zgcDuP++++vcwdH1I0Bk6Qrr7zSUl62bJnPA/E/++wzS3nYsGFBHejuGdvSpUt9rut57IgRI4ISU1VcLpeuvvpqzZo1y7I/JiZGHTt2VJ8+fdS8eXPL13Jzc3X55Zfr66+/DllcgF2++eYbzZ8/XwcOHAj5uVauXKkrrrjC67H5pKQk9e3bVx06dFBMTIzla2+//bauvfZavx86AsIpMzOzyimYYmJilJ6erv79+6tXr15eny+S+TM+fPhwvycgnz17tq699loVFhZa9qempqpfv35KT0+3jB03DENTp07V3Xff7dd5vNQpfaunysrKvP5y/OKLL3yqe9FFF1nqvfTSS0GNbcOGDV6373zpYTt27JjXbYgtW7YENbbKnnrqKa+/Cm699VZj//795ceUlZUZ8+bNM9q1a2c5Lj093SgoKAhZbIAdnnvuuRpvw1cu16UHLD8/36uXrX379saCBQssf5Hv27fP+MMf/uAVyzPPPBOE7xYIjf79+5f/rCYlJRm33367sXjxYuPYsWOW406fPm1kZGR4fSZLMsaOHevz+bZv3+712dm7d2+vnOD77783rrrqKq9zzZ07N+DvNSoTMMMwjHvuucfyj3jxxRfX2p24bNkyS51mzZqFZFzTgAEDLOeZPHlyrXUefvhhS52f//znQY/LLS8vz2tMy5QpU6o9Pjs722vc3COPPBKy+AA7uBOwZs2aGUOHDjXuvfdeY/bs2cbu3buNjIyMoCVgDz74oKWtjh07Wv7w8fTEE09Yjm/evLmRn58f8PmBUOrfv7/RoUMH4/XXXzcKCwtrPf706dPG73//e6/EyNdOlWuvvdZSb8CAAdXexnS5XF7n6ty5s1FaWurX9+gWtQlYbm6u11+l/iYRDz/8cK3n8fyhyMjIqLXOv//9b0ud2NjYGsdOLV++3IiNjbXUWbZsWa3nCdR9991nOdeQIUMCSl7z8vJCFiMQbtu3bze2bNlilJWVeX0tWAlYTk6O1++t2q51l8tlDBkyxFLnoYceCuj8QKgtWrTI77GKp0+fNs477zzLz/h1111Xa73NmzcbTqezvE7jxo2NrKysGusUFRUZXbt2tZzrX//6l1/xukVtAmYYhvHkk096JUi33Xab1220+fPne91GS0tLM44cOVLrOQJJwAzDMC699FJLvbi4OOP55583Tp48WX7MiRMnjOeee86Ii4uzHHvFFVf4+0/hs7KyMiM1NTWgvzQ8u4pffvnlkMUJRJJgJWAvvvii1x8/vvj8888t9c4888ywPCENhMuHH35o+Rlv2bJlrXXuvvtuS52JEyf6dK433njDUm/gwIEBxRzVCVhZWZlx5ZVXeiVJMTExRqdOnYy+ffsaSUlJXl+Pj483Vq5c6dM5Ak3AfvzxR6Njx45Vnrtnz55Gjx49vBIvd3doTk5OHf5VavbVV19ZztepUyeff5G/9dZblrqXXnppyOIEIkmwErBLLrnE0s6MGTN8qudyubx+n6xevTqgGIBIdPDgQa/Pw8odFlXp0qWL5Xhfn9I/ceKEZdyYw+GocRhAdaLyKUg3p9Op2bNn65prrrHsLysr086dO7Vu3bryxTbdWrZsqU8++USDBw8OaWytW7dWRkaGevfubdlfVFSkLVu2KCsry2utyD59+igjIyOk61ktXrzYUh4+fLjl6ZCaDB8+3FJevnx5QMstAdHoxIkT+vLLLy37Lr30Up/qOhwO/fKXv7TsW7RoUdBiA+yWnJzste/o0aPVHr9161Zt3769vJyQkKBBgwb5dC7PYw3D8Pps9EVUJ2CSFBcXp/fee09z5sxRnz59qj0uISFBt99+u7KysjR06NCwxNa+fXutWbNGU6dOrXH23bS0NE2bNk2ZmZlq27ZtSGNav369pezrD6xkxtmhQ4fycklJibKysoIUGdCwbdmyRaWlpeXljh076swzz/S5vucfjZ7XMlCfeS7DJ5kdJtXx/PkfOHCgGjXyfXGgYFxPUbcUUXXGjh2rsWPHavv27crMzNT+/ftVUlKipKQkde/eXYMHDw5ouQOjjnPuNG7cWPfdd5/uuecerV27Vhs2bChfu7JVq1bq06eP+vXrJ6czPLn0d999Zyn36NHDr/o9evTQ7t27Le0NGDAgGKEBDVowrr2a2gPqs6+++spSbt++vRo3blzt8ZFwPZGAeejSpYu6dOlidxhenE6nBgwYYGuyUlRU5LUOpr89bp7Hb926tc5xAdHA81qp67W3Z88eFRcXs04rGoQ333zTUr7iiitqPD7Y11Mgn2VRfwsSvsvLy7P06MXGxqpVq1Z+tdGmTRtL2d2bB6BmntdKenq6X/Vbt25tucXicrl0+PDhoMQG2OmTTz7xGh85adKkGuvU9Xry/CzLzc31q75EAgY/eC7v0LRpU58H4LslJCTU2CaAqnleK57XUm0cDofi4+NrbBOob/Lz8/WHP/zBsm/06NEaOHBgjfXqej15Hl9aWqpTp0751QYJGHzm+QMbyK0LPgCAwHD9AVYul0vXX3+9srOzy/c1b95cL774Yq1163o9eV5LVbVZGxIw+Mxz2ouaBjhWp0mTJpZyUVFRnWICogXXH2B177336t///rdl36uvvurTeK66Xk+e15Lk//VEAgafef6FUNWK9bXx7KJlADDgG64/oMKLL76oZ5991rLvvvvu09VXX+1T/bpeT1XdbvT3eiIBg88SExMtZc+/IHzh+ReCZ5sAqsb1B5jeffdd3XXXXZZ9kyZN0lNPPeVzG3W9nqrq7fL3eiIBg888f7gKCwv9nufMc+Z7PgAA33heK/6uImEYBgkY6r1FixbphhtusHz2XHXVVXr99df9eiisrteT5/GNGjWiBwyhk5KSYvkBLy0t9XsaCc/Ziv2dxgKIVp7XSuWBx744dOiQTp8+XV52Op1KSUkJSmxAOGRkZGj8+PGWn+Phw4frvffeU0xMjF9t1fV68vwsC2QJQBIw+Cw+Pl7t2rWz7POcmLU2nsd369atznEB0eDss8+2lOt67bVv354xYKg3MjMzNXLkSMutwkGDBmn+/PkBPZAS7OspkM8yEjD4xfOHzN+1HD2XayABA3zDtYdotXHjRl1++eWWaR769u2rTz75xO/5u9wi4XoiAYNfPBcsX716tc91Dx48aFkHMjY21u/1t4Bo1bNnT8XGxpaXd+/erYMHD/pcf9WqVZay57UMRKKtW7dq+PDhOnLkSPm+7t27a8mSJWrevHnA7Xr+/H/zzTeWW5u1Ccb1RAIGv1x55ZWW8rJly3weiP/ZZ59ZysOGDWMQMOCjZs2aaciQIZZ9S5cu9amuYRhatmyZZd+IESOCFhsQCnv27NEvf/lLy1jjjh07aunSpQGNuaqsW7du6ty5c3n55MmTPnconDx5Ul9//XV52eFweH02+oIEDH4ZNGiQZeDuzp07tXz5cp/qvvHGG5byqFGjghka0OCNHDnSUva8pqqTkZGhXbt2lZdbt26t888/P6ixAcF08OBBXXLJJZbB8W3atNHnn3/utQ5joAK9nj744APL7dDzzjtPaWlpfp+fBAx+cTqdXoucPvroo7X2gn3++ef66quvysvNmjXThAkTQhEi0GBdc801ljEvX375pb744osa6xiGoUcffdSy78Ybb5TTya9/RKb8/HwNHz5cO3bsKN+XmpqqpUuXqmPHjkE7z0033WR5sv/999/3Gtvlqbi42Gu+sZtvvjmg83MFwm/333+/5dbhihUrNHXq1GqP379/v2655RbLvjvvvJNH4AE/tWrVSnfccYdl3y233KIDBw5UW2fKlCn68ssvy8vNmzfXvffeG7IYgbo4fvy4fvWrX2nLli3l+5KSkvTZZ5+pe/fuQT3XOeecY+kIKCkp0Q033KBjx45VebxhGLrrrru0bdu28n2dOnXSTTfdFND5HYa/M2kCMn+pP/TQQ5Z9t912mx5++OHyrliXy6WPPvpId955p+WR3bS0NG3ZskVJSUnhDBkIuVWrVlU5Q/aGDRt0zz33lJdbt26td955p8o20tLSanw4JT8/Xz179tSPP/5Yvq99+/Z68cUXNWLEiPK/6LOzs/X444/r1VdftdSfNm0aCRgi1rBhw7yGtTz22GO64IIL/G6rf//+Sk5OrvGY7du3q3fv3iosLCzf17t3bz3//PMaOnRo+b4ffvhBDz74oObNm2ep/+GHH2r8+PF+xyaRgCFALpdLo0aN0qJFiyz7Y2Ji1L59ezVv3ly7du1SQUGB5evx8fFaunSpBg8eHMZogfDo0KGD9uzZU6c2brjhBr311ls1HvPll1/qsssu81o+JSkpSR07dlRBQYH27t2rsrIyy9dHjRql+fPn+zVjOBBOwfzZzMjIsCRR1Xn//fd13XXXeQ2lSU1NVbt27ZSTk6Ps7Gyvr//pT3/Siy++GHB83IJEQJxOp2bPnq1rrrnGsr+srEw7d+7UunXrvJKvli1b6pNPPiH5AupoyJAhWrx4sVq0aGHZX1BQoHXr1mnXrl1eydd1112nDz74gOQL8HDNNddo1qxZio+Pt+zPzc3V2rVrtW/fPq/k65577tELL7xQp/OSgCFgcXFxeu+99zRnzpwa50BJSEjQ7bffrqysLJ/+GgFQu1/84hfKysrSbbfdpqZNm1Z7XN++fTV37lzNmjVLTZo0CWOEQP1x7bXXavPmzbruuuss8+15GjJkiJYvX66nn366zn/McAsSQbN9+3ZlZmZq//79KikpUVJSkrp3767Bgwez5AkQQkVFRVq9erW+++47FRQUqHHjxmrTpo3OP/98denSxe7wgHrl2LFjWrlypbZt26bjx48rLi5O7dq10+DBg4M2BYZEAgYAABB23IIEAAAIMxIwAACAMCMBAwAACDMSMAAAgDAjAQMAAAgzEjAAAIAwIwEDAAAIMxIwAACAMCMBAwAACDMSMAAAgDAjAQMAAAgzEjAAAIAwIwEDAAAIMxIwAACAMCMBAwAACDMSMAAAgDAjAQOACDR16lQ5HI7y19KlS+0OCUAQkYABQATasGGDpdyrVy+bIgEQCiRgABCBNm7cWL6dmpqq1q1b2xgNgGAjAQOACHPq1Clt3bq1vHzuuefaGA2AUCABA4AIk5WVpdOnT5eXScCAhocEDAAiTOXbjxIJGNAQkYABQIQhAQMaPodhGIbdQQBAtOvevbu+//57v+s99NBDeuKJJ0IQEYBQogcMAGxWVFSkbdu2BVSX6SmA+okEDABstnnzZpWVlQVUl9uTQP3ELUgAsFlubm75xKubNm3S3XffXf613/72t5o4cWK1dYcNG6aYmJiQxwgguBrZHQAARLvU1FT98pe/lCT98MMPlq+NHDmy/GsAGg5uQQJABPn2228t5f79+9sUCYBQIgEDgAiydu3a8u0WLVqoY8eONkYDIFRIwAAgQpSUlGjLli3l5X79+tkYDYBQIgEDgAixadMmlZaWlpe5/Qg0XCRgABAhGP8FRA8SMACIEJ4JGLcggYaLBAwAIkTlBCwpKUmdO3e2MRoAoUQCBgAR4PTp05ZFuOn9Aho2EjAAiABZWVkqLi4uLzP+C2jYSMAAIAIwAB+ILiRgABAB1q9fbyn37dvXnkAAhAUJGABEgO+//758u3HjxgzABxo4EjAAiAA5OTnl240bN1ZMTIyN0QAINRIwAIgA8fHx5dsnTpzQ6tWrbYwGQKg1sjsAAIDUq1cvS9I1atQo3XbbbTr33HOVnJxcvj8mJkbDhg2zI0QAQeQwDMOwOwgAiHbffvutzjvvPNX2K7lHjx6WBbsB1E/cggSACNCvXz+99NJLio2NrfE4no4EGgYSMACIELfddps2btyou+++W/3791dSUpLXYPw+ffrYExyAoOIWJAAAQJjRAwYAABBmJGAAAABhRgIGAAAQZiRgAAAAYUYCBgAAEGYkYAAAAGFGAgYAABBmJGAAAABhRgIGAAAQZiRgAAAAYUYCBgAAEGYkYAAAAGFGAgYAABBmJGAAAABhRgIGAAAQZiRgAAAAYUYCBgAAEGb/D/Gr+tHtShcVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the fidelity decay in the diffusion process\n",
    "n = 2\n",
    "T = 20\n",
    "Ndata = 600\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "indices = np.random.permutation(Ndata)\n",
    "\n",
    "ax.plot(range(T+1), fidelity_mean, 'o--', markersize=8, lw=2, c='r')\n",
    "ax.plot(range(T+1), 0.25*np.ones(T+1), '--', lw=2, c='gold')\n",
    "ax.set_ylabel(r'$F_0$', fontsize=30)\n",
    "ax.set_xlabel(r'$t$', fontsize=30)\n",
    "ax.set_ylim(0,1)\n",
    "ax.tick_params(direction='in', length=10, width=3, top='on', right='on', labelsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_t(model, t, inputs_T, params_tot, Ndata, epochs):\n",
    "    '''\n",
    "    the trianing for the backward PQC at step t\n",
    "    input_tplus1: the output from step t+1, as the role of input at step t\n",
    "    Args:\n",
    "    model: the QDDPM model\n",
    "    t: the diffusion step\n",
    "    inputs_T: the input data at step t=T\n",
    "    params_tot: collection of PQC parameters before step t\n",
    "    Ndata: number of samples in dataset\n",
    "    epochs: the number of iterations\n",
    "    '''\n",
    "    input_tplus1 = model.prepareInput_t(inputs_T, params_tot, t, Ndata) # prepare input\n",
    "    states_diff = model.states_diff\n",
    "    loss_hist = [] # record of training history\n",
    "\n",
    "    # initialize parameters\n",
    "    np.random.seed()\n",
    "    params_t = torch.tensor(np.random.normal(size=2 * model.n_tot * model.L), requires_grad=True)\n",
    "    # set optimizer and learning rate decay\n",
    "    optimizer = torch.optim.Adam([params_t], lr=0.0005)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for step in range(epochs):\n",
    "        indices = np.random.choice(states_diff.shape[1], size=Ndata, replace=False)\n",
    "        true_data = states_diff[t, indices]\n",
    "\n",
    "        output_t = model.backwardOutput_t(input_tplus1, params_t)\n",
    "        loss = naturalDistance(output_t, true_data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_hist.append(loss) # record the current loss\n",
    "        \n",
    "        if step%100 == 0:\n",
    "            loss_value = loss_hist[-1]\n",
    "            print(\"Step %s, loss: %s, time elapsed: %s seconds\"%(step, loss_value, time.time() - t0))\n",
    "\n",
    "    return params_t, torch.stack(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: tensor(0.0205, grad_fn=<SubBackward0>), time elapsed: 0.15011215209960938 seconds\n",
      "Step 100, loss: tensor(0.0137, grad_fn=<SubBackward0>), time elapsed: 9.706285238265991 seconds\n",
      "Step 200, loss: tensor(0.0083, grad_fn=<SubBackward0>), time elapsed: 19.15778875350952 seconds\n",
      "Step 300, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 28.424359798431396 seconds\n",
      "Step 400, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 37.882956743240356 seconds\n",
      "Step 500, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 47.172815799713135 seconds\n",
      "Step 600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 56.47387146949768 seconds\n",
      "Step 700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 65.90840101242065 seconds\n",
      "Step 800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 75.20707058906555 seconds\n",
      "Step 900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 84.50103974342346 seconds\n",
      "Step 1000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 93.9702410697937 seconds\n",
      "Step 1100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 103.26173305511475 seconds\n",
      "Step 1200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 112.58059883117676 seconds\n",
      "Step 1300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 122.02238965034485 seconds\n",
      "Step 1400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 131.25474548339844 seconds\n",
      "Step 1500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 140.5183229446411 seconds\n",
      "Step 1600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 149.94139432907104 seconds\n",
      "Step 1700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 159.2215974330902 seconds\n",
      "Step 1800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 168.50809979438782 seconds\n",
      "Step 1900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 177.905127286911 seconds\n",
      "Step 2000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 187.17300844192505 seconds\n",
      "Step 2100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 196.57004308700562 seconds\n",
      "Step 2200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 205.85428476333618 seconds\n",
      "Step 2300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 215.11387944221497 seconds\n",
      "Step 2400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 224.52108812332153 seconds\n",
      "Step 2500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 233.79528999328613 seconds\n",
      "Step 2600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 243.0357301235199 seconds\n",
      "Step 2700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 252.4517960548401 seconds\n",
      "Step 2800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 261.78598260879517 seconds\n",
      "Step 2900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 271.0327546596527 seconds\n",
      "Step 3000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 280.42878580093384 seconds\n",
      "Step 3100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 289.69362330436707 seconds\n",
      "Step 3200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 298.9946942329407 seconds\n",
      "Step 3300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 308.3959090709686 seconds\n",
      "Step 3400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 317.64260387420654 seconds\n",
      "Step 3500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 326.90406799316406 seconds\n",
      "Step 3600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 336.37009859085083 seconds\n",
      "Step 3700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 345.6626660823822 seconds\n",
      "Step 3800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 355.0847191810608 seconds\n",
      "Step 3900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 364.3150818347931 seconds\n",
      "Step 4000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 373.54509115219116 seconds\n",
      "Step 4100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 382.93684244155884 seconds\n",
      "Step 4200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 392.16395473480225 seconds\n",
      "Step 4300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 401.4066672325134 seconds\n",
      "Step 4400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 410.8245644569397 seconds\n",
      "Step 4500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 420.0231397151947 seconds\n",
      "Step 4600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 429.1291997432709 seconds\n",
      "Step 4700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 438.38211011886597 seconds\n",
      "Step 4800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 447.50994420051575 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 456.5696783065796 seconds\n",
      "Step 5000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 465.79327178001404 seconds\n",
      "Step 5100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 474.8828094005585 seconds\n",
      "Step 5200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 483.95922088623047 seconds\n",
      "Step 5300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 493.19241404533386 seconds\n",
      "Step 5400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 502.2646050453186 seconds\n",
      "Step 5500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 511.50395584106445 seconds\n",
      "Step 5600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 520.5700557231903 seconds\n",
      "Step 5700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 529.66703748703 seconds\n",
      "Step 5800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 538.9510576725006 seconds\n",
      "Step 5900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 548.0400078296661 seconds\n",
      "Step 6000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 557.1232135295868 seconds\n",
      "Step 6100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 566.3907120227814 seconds\n",
      "Step 6200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 575.4954454898834 seconds\n",
      "Step 6300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 584.6033046245575 seconds\n",
      "Step 6400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 593.8459877967834 seconds\n",
      "Step 6500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 602.9422266483307 seconds\n",
      "Step 6600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 612.0191316604614 seconds\n",
      "Step 6700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 621.2759945392609 seconds\n",
      "Step 6800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 630.410472869873 seconds\n",
      "Step 6900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 639.508620262146 seconds\n",
      "Step 7000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 648.7783374786377 seconds\n",
      "Step 7100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 657.8901875019073 seconds\n",
      "Step 7200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 667.1587917804718 seconds\n",
      "Step 7300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 676.266872882843 seconds\n",
      "Step 7400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 685.3965406417847 seconds\n",
      "Step 7500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 694.6854150295258 seconds\n",
      "Step 7600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 703.7818973064423 seconds\n",
      "Step 7700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 712.8723888397217 seconds\n",
      "Step 7800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 722.1386988162994 seconds\n",
      "Step 7900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 731.2366824150085 seconds\n",
      "Step 8000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 740.3232822418213 seconds\n",
      "Step 8100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 749.5972900390625 seconds\n",
      "Step 8200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 758.7094705104828 seconds\n",
      "Step 8300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 767.8129680156708 seconds\n",
      "Step 8400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 777.0609838962555 seconds\n",
      "Step 8500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 786.1665000915527 seconds\n",
      "Step 8600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 795.2586088180542 seconds\n",
      "Step 8700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 804.5190074443817 seconds\n",
      "Step 8800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 813.6294658184052 seconds\n",
      "Step 8900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 822.690839767456 seconds\n",
      "Step 9000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 831.9546148777008 seconds\n",
      "Step 9100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 841.0508089065552 seconds\n",
      "Step 9200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 850.2975194454193 seconds\n",
      "Step 9300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 859.3758728504181 seconds\n",
      "Step 9400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 868.4584321975708 seconds\n",
      "Step 9500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 877.7105832099915 seconds\n",
      "Step 9600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 886.7928187847137 seconds\n",
      "Step 9700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 895.8557391166687 seconds\n",
      "Step 9800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 905.119291305542 seconds\n",
      "Step 9900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 914.2134163379669 seconds\n",
      "Step 10000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 923.306809425354 seconds\n",
      "Step 10100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 932.5978994369507 seconds\n",
      "Step 10200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 941.7189831733704 seconds\n",
      "Step 10300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 950.8190305233002 seconds\n",
      "Step 10400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 960.0807373523712 seconds\n",
      "Step 10500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 969.1850888729095 seconds\n",
      "Step 10600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 978.2598960399628 seconds\n",
      "Step 10700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 987.503999710083 seconds\n",
      "Step 10800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 996.6379308700562 seconds\n",
      "Step 10900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1005.7412786483765 seconds\n",
      "Step 11000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1014.9987840652466 seconds\n",
      "Step 11100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1024.1125860214233 seconds\n",
      "Step 11200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1033.3853750228882 seconds\n",
      "Step 11300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1042.4804015159607 seconds\n",
      "Step 11400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1051.576416015625 seconds\n",
      "Step 11500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1060.8627915382385 seconds\n",
      "Step 11600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1069.9505167007446 seconds\n",
      "Step 11700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1079.0095520019531 seconds\n",
      "Step 11800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1088.2816534042358 seconds\n",
      "Step 11900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1097.3637068271637 seconds\n",
      "Step 12000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1106.4403750896454 seconds\n",
      "Step 12100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1115.7063772678375 seconds\n",
      "Step 12200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1124.799656867981 seconds\n",
      "Step 12300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1133.8696246147156 seconds\n",
      "Step 12400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1143.1003630161285 seconds\n",
      "Step 12500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1152.210931301117 seconds\n",
      "Step 12600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1161.2994606494904 seconds\n",
      "Step 12700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1170.5130803585052 seconds\n",
      "Step 12800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1179.5908770561218 seconds\n",
      "Step 12900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1188.6673316955566 seconds\n",
      "Step 13000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1197.928162574768 seconds\n",
      "Step 13100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1207.0241463184357 seconds\n",
      "Step 13200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1216.2924709320068 seconds\n",
      "Step 13300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1225.389402627945 seconds\n",
      "Step 13400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1234.5095310211182 seconds\n",
      "Step 13500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1243.8315443992615 seconds\n",
      "Step 13600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1252.9550075531006 seconds\n",
      "Step 13700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1262.06170296669 seconds\n",
      "Step 13800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1271.3368666172028 seconds\n",
      "Step 13900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1280.4036195278168 seconds\n",
      "Step 14000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1289.467694759369 seconds\n",
      "Step 14100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1298.7277719974518 seconds\n",
      "Step 14200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1307.8238773345947 seconds\n",
      "Step 14300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1316.9083359241486 seconds\n",
      "Step 14400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1326.1982204914093 seconds\n",
      "Step 14500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1335.335375547409 seconds\n",
      "Step 14600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1344.444976568222 seconds\n",
      "Step 14700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1353.7321004867554 seconds\n",
      "Step 14800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1362.864687204361 seconds\n",
      "Step 14900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1371.9658510684967 seconds\n",
      "Step 15000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1381.2387371063232 seconds\n",
      "Step 15100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1390.351196527481 seconds\n",
      "Step 15200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1399.4169473648071 seconds\n",
      "Step 15300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1408.6624128818512 seconds\n",
      "Step 15400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1417.7435111999512 seconds\n",
      "Step 15500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1426.8379199504852 seconds\n",
      "Step 15600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1436.080785036087 seconds\n",
      "Step 15700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1445.1694128513336 seconds\n",
      "Step 15800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1454.4407391548157 seconds\n",
      "Step 15900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1463.5476224422455 seconds\n",
      "Step 16000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1472.6473660469055 seconds\n",
      "Step 16100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1481.9428884983063 seconds\n",
      "Step 16200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1491.0234496593475 seconds\n",
      "Step 16300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1500.0880589485168 seconds\n",
      "Step 16400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1509.3316860198975 seconds\n",
      "Step 16500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1518.4347145557404 seconds\n",
      "Step 16600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1527.5183551311493 seconds\n",
      "Step 16700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1536.77503657341 seconds\n",
      "Step 16800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1545.911201953888 seconds\n",
      "Step 16900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1555.0042507648468 seconds\n",
      "Step 17000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1564.263824224472 seconds\n",
      "Step 17100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1573.3764352798462 seconds\n",
      "Step 17200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1582.4449644088745 seconds\n",
      "Step 17300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1591.7058579921722 seconds\n",
      "Step 17400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1600.8133480548859 seconds\n",
      "Step 17500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1609.9253566265106 seconds\n",
      "Step 17600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1619.1907465457916 seconds\n",
      "Step 17700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1628.279929637909 seconds\n",
      "Step 17800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1637.3977706432343 seconds\n",
      "Step 17900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1646.6641464233398 seconds\n",
      "Step 18000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1655.7591605186462 seconds\n",
      "Step 18100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1665.049523115158 seconds\n",
      "Step 18200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1674.1603682041168 seconds\n",
      "Step 18300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1683.268717288971 seconds\n",
      "Step 18400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1692.552214384079 seconds\n",
      "Step 18500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1701.6590132713318 seconds\n",
      "Step 18600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1710.7638223171234 seconds\n",
      "Step 18700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1720.049092054367 seconds\n",
      "Step 18800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1729.1570982933044 seconds\n",
      "Step 18900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1738.2326593399048 seconds\n",
      "Step 19000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1747.504916191101 seconds\n",
      "Step 19100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1756.5920243263245 seconds\n",
      "Step 19200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1765.6754696369171 seconds\n",
      "Step 19300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1774.9400537014008 seconds\n",
      "Step 19400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1784.0442383289337 seconds\n",
      "Step 19500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1793.1642699241638 seconds\n",
      "Step 19600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1802.4210698604584 seconds\n",
      "Step 19700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1811.504629611969 seconds\n",
      "Step 19800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1820.6124317646027 seconds\n",
      "Step 19900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1829.8831181526184 seconds\n",
      "Step 20000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1838.9983041286469 seconds\n",
      "Step 20100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1848.0914812088013 seconds\n",
      "Step 20200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1857.3490784168243 seconds\n",
      "Step 20300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1866.4303781986237 seconds\n",
      "Step 20400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1875.5112211704254 seconds\n",
      "Step 20500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1884.7784559726715 seconds\n",
      "Step 20600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1893.8471496105194 seconds\n",
      "Step 20700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1903.1154251098633 seconds\n",
      "Step 20800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1912.2145206928253 seconds\n",
      "Step 20900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1921.3500826358795 seconds\n",
      "Step 21000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1930.671793460846 seconds\n",
      "Step 21100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1939.8422491550446 seconds\n",
      "Step 21200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1948.9918036460876 seconds\n",
      "Step 21300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1958.3235530853271 seconds\n",
      "Step 21400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1967.4347200393677 seconds\n",
      "Step 21500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1976.5460243225098 seconds\n",
      "Step 21600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1985.8112969398499 seconds\n",
      "Step 21700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1994.940157175064 seconds\n",
      "Step 21800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2004.0519218444824 seconds\n",
      "Step 21900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2013.316279888153 seconds\n",
      "Step 22000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2022.4326865673065 seconds\n",
      "Step 22100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2031.5497188568115 seconds\n",
      "Step 22200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2040.8244931697845 seconds\n",
      "Step 22300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2049.935711622238 seconds\n",
      "Step 22400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2059.038008928299 seconds\n",
      "Step 22500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2068.59162068367 seconds\n",
      "Step 22600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2077.7302033901215 seconds\n",
      "Step 22700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2086.836650133133 seconds\n",
      "Step 22800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2096.1552352905273 seconds\n",
      "Step 22900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2105.2714564800262 seconds\n",
      "Step 23000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2114.3825812339783 seconds\n",
      "Step 23100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2123.6882979869843 seconds\n",
      "Step 23200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2132.7909042835236 seconds\n",
      "Step 23300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2141.9463572502136 seconds\n",
      "Step 23400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2151.2501425743103 seconds\n",
      "Step 23500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2160.380534887314 seconds\n",
      "Step 23600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2169.6842589378357 seconds\n",
      "Step 23700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2178.779392719269 seconds\n",
      "Step 23800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2187.9142332077026 seconds\n",
      "Step 23900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2197.2604825496674 seconds\n",
      "Step 24000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2206.400583744049 seconds\n",
      "Step 24100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2215.5739142894745 seconds\n",
      "Step 24200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2224.899198293686 seconds\n",
      "Step 24300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2234.00502038002 seconds\n",
      "Step 24400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2243.1280212402344 seconds\n",
      "Step 24500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2252.451043367386 seconds\n",
      "Step 24600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2261.593677043915 seconds\n",
      "Step 24700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2270.769973754883 seconds\n",
      "Step 24800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2280.138540506363 seconds\n",
      "Step 24900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2289.6864104270935 seconds\n",
      "Step 25000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2298.800060033798 seconds\n",
      "Step 25100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2308.1373596191406 seconds\n",
      "Step 25200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2317.220257997513 seconds\n",
      "Step 25300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2326.2800335884094 seconds\n",
      "Step 25400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2335.5246543884277 seconds\n",
      "Step 25500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2344.643495082855 seconds\n",
      "Step 25600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2353.74653506279 seconds\n",
      "Step 25700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2363.0262360572815 seconds\n",
      "Step 25800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2372.1350622177124 seconds\n",
      "Step 25900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2381.679702281952 seconds\n",
      "Step 26000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2391.0079765319824 seconds\n",
      "Step 26100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2400.1276593208313 seconds\n",
      "Step 26200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2409.2240438461304 seconds\n",
      "Step 26300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2418.530168056488 seconds\n",
      "Step 26400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2427.621597290039 seconds\n",
      "Step 26500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2436.730008840561 seconds\n",
      "Step 26600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2446.0094294548035 seconds\n",
      "Step 26700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2455.1015634536743 seconds\n",
      "Step 26800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2464.389860868454 seconds\n",
      "Step 26900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2473.4651465415955 seconds\n",
      "Step 27000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2482.5675086975098 seconds\n",
      "Step 27100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2491.872688293457 seconds\n",
      "Step 27200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2500.9699840545654 seconds\n",
      "Step 27300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2510.093029499054 seconds\n",
      "Step 27400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2519.4402232170105 seconds\n",
      "Step 27500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2528.5515830516815 seconds\n",
      "Step 27600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2537.671630859375 seconds\n",
      "Step 27700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2546.985777616501 seconds\n",
      "Step 27800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2556.1160502433777 seconds\n",
      "Step 27900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2565.237546682358 seconds\n",
      "Step 28000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2574.559835910797 seconds\n",
      "Step 28100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2584.0598134994507 seconds\n",
      "Step 28200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2593.161558866501 seconds\n",
      "Step 28300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2602.4547970294952 seconds\n",
      "Step 28400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2611.575598716736 seconds\n",
      "Step 28500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2620.6720337867737 seconds\n",
      "Step 28600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2629.9598491191864 seconds\n",
      "Step 28700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2639.0562875270844 seconds\n",
      "Step 28800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2648.1506135463715 seconds\n",
      "Step 28900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2657.4374022483826 seconds\n",
      "Step 29000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2667.0382204055786 seconds\n",
      "Step 29100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2676.1740975379944 seconds\n",
      "Step 29200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2685.486665725708 seconds\n",
      "Step 29300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2694.5928387641907 seconds\n",
      "Step 29400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2703.7240540981293 seconds\n",
      "Step 29500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2713.0213174819946 seconds\n",
      "Step 29600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2722.1137001514435 seconds\n",
      "Step 29700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2731.2170650959015 seconds\n",
      "Step 29800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2740.51504278183 seconds\n",
      "Step 29900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2749.625326395035 seconds\n",
      "Step 30000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2758.7015945911407 seconds\n",
      "Step 30100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2768.001419067383 seconds\n",
      "Step 30200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2777.120775461197 seconds\n",
      "Step 30300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2786.2335493564606 seconds\n",
      "Step 30400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2795.5592353343964 seconds\n",
      "Step 30500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2804.669285297394 seconds\n",
      "Step 30600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2813.981633901596 seconds\n",
      "Step 30700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2823.0673582553864 seconds\n",
      "Step 30800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2832.211178302765 seconds\n",
      "Step 30900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2841.540637731552 seconds\n",
      "Step 31000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2850.6310715675354 seconds\n",
      "Step 31100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2859.7524569034576 seconds\n",
      "Step 31200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2869.0457689762115 seconds\n",
      "Step 31300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2878.1231286525726 seconds\n",
      "Step 31400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2887.214242696762 seconds\n",
      "Step 31500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2896.5061750411987 seconds\n",
      "Step 31600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2905.6522545814514 seconds\n",
      "Step 31700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2914.778966188431 seconds\n",
      "Step 31800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2924.1222245693207 seconds\n",
      "Step 31900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2933.25759267807 seconds\n",
      "Step 32000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2942.3753051757812 seconds\n",
      "Step 32100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2951.715662717819 seconds\n",
      "Step 32200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2961.2548413276672 seconds\n",
      "Step 32300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2970.338593482971 seconds\n",
      "Step 32400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2979.6599390506744 seconds\n",
      "Step 32500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2988.746096611023 seconds\n",
      "Step 32600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2997.83397936821 seconds\n",
      "Step 32700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3007.119938135147 seconds\n",
      "Step 32800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3016.2743244171143 seconds\n",
      "Step 32900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3025.3593199253082 seconds\n",
      "Step 33000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3034.6604783535004 seconds\n",
      "Step 33100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3043.7910149097443 seconds\n",
      "Step 33200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3052.9291133880615 seconds\n",
      "Step 33300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3062.5301921367645 seconds\n",
      "Step 33400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3071.654237508774 seconds\n",
      "Step 33500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3080.74644780159 seconds\n",
      "Step 33600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3090.02693939209 seconds\n",
      "Step 33700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3099.147514104843 seconds\n",
      "Step 33800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3108.2910871505737 seconds\n",
      "Step 33900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3117.938953399658 seconds\n",
      "Step 34000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3127.033188343048 seconds\n",
      "Step 34100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3136.158690214157 seconds\n",
      "Step 34200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3145.472587108612 seconds\n",
      "Step 34300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3154.5915410518646 seconds\n",
      "Step 34400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3163.736864089966 seconds\n",
      "Step 34500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3173.4981548786163 seconds\n",
      "Step 34600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3182.583684206009 seconds\n",
      "Step 34700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3191.8856506347656 seconds\n",
      "Step 34800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3200.9967427253723 seconds\n",
      "Step 34900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3210.0982444286346 seconds\n",
      "Step 35000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3219.388431787491 seconds\n",
      "24\n",
      "Step 0, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 0.09886765480041504 seconds\n",
      "Step 100, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 9.192646741867065 seconds\n",
      "Step 200, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 18.284358263015747 seconds\n",
      "Step 300, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 27.59253215789795 seconds\n",
      "Step 400, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 36.71610069274902 seconds\n",
      "Step 500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 45.87041616439819 seconds\n",
      "Step 600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 55.20488500595093 seconds\n",
      "Step 700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 64.97939014434814 seconds\n",
      "Step 800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 74.06251811981201 seconds\n",
      "Step 900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 83.32586646080017 seconds\n",
      "Step 1000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 92.46771907806396 seconds\n",
      "Step 1100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 101.57280945777893 seconds\n",
      "Step 1200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 110.82515668869019 seconds\n",
      "Step 1300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 119.95246720314026 seconds\n",
      "Step 1400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 129.06741404533386 seconds\n",
      "Step 1500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 138.3421812057495 seconds\n",
      "Step 1600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 147.48010110855103 seconds\n",
      "Step 1700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 156.78101205825806 seconds\n",
      "Step 1800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 165.85872745513916 seconds\n",
      "Step 1900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 174.98568534851074 seconds\n",
      "Step 2000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 184.28434109687805 seconds\n",
      "Step 2100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 193.40589261054993 seconds\n",
      "Step 2200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 202.53404808044434 seconds\n",
      "Step 2300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 211.8347749710083 seconds\n",
      "Step 2400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 220.96520900726318 seconds\n",
      "Step 2500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 230.07823610305786 seconds\n",
      "Step 2600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 239.3757667541504 seconds\n",
      "Step 2700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 248.51531744003296 seconds\n",
      "Step 2800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 257.6310176849365 seconds\n",
      "Step 2900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 266.8677444458008 seconds\n",
      "Step 3000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 275.9795904159546 seconds\n",
      "Step 3100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 285.0605466365814 seconds\n",
      "Step 3200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 294.35684847831726 seconds\n",
      "Step 3300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 303.56097388267517 seconds\n",
      "Step 3400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 312.88516068458557 seconds\n",
      "Step 3500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 322.49359703063965 seconds\n",
      "Step 3600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 331.618022441864 seconds\n",
      "Step 3700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 340.90512919425964 seconds\n",
      "Step 3800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 350.00141072273254 seconds\n",
      "Step 3900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 359.0864779949188 seconds\n",
      "Step 4000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 368.3359498977661 seconds\n",
      "Step 4100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 377.40399622917175 seconds\n",
      "Step 4200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 386.4919157028198 seconds\n",
      "Step 4300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 395.7328553199768 seconds\n",
      "Step 4400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 404.8376507759094 seconds\n",
      "Step 4500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 414.47470116615295 seconds\n",
      "Step 4600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 423.7522306442261 seconds\n",
      "Step 4700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 432.90491938591003 seconds\n",
      "Step 4800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 442.012601852417 seconds\n",
      "Step 4900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 451.722186088562 seconds\n",
      "Step 5000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 462.0038242340088 seconds\n",
      "Step 5100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 472.00894498825073 seconds\n",
      "Step 5200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 481.7503514289856 seconds\n",
      "Step 5300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 491.05226826667786 seconds\n",
      "Step 5400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 500.4658408164978 seconds\n",
      "Step 5500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 509.70082235336304 seconds\n",
      "Step 5600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 518.940413236618 seconds\n",
      "Step 5700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 528.2969672679901 seconds\n",
      "Step 5800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 537.4835801124573 seconds\n",
      "Step 5900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 546.6860251426697 seconds\n",
      "Step 6000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 556.0709137916565 seconds\n",
      "Step 6100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 565.2442548274994 seconds\n",
      "Step 6200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 574.5020790100098 seconds\n",
      "Step 6300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 584.1726803779602 seconds\n",
      "Step 6400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 594.2473204135895 seconds\n",
      "Step 6500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 603.5923120975494 seconds\n",
      "Step 6600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 613.0135691165924 seconds\n",
      "Step 6700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 622.1313536167145 seconds\n",
      "Step 6800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 631.2475113868713 seconds\n",
      "Step 6900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 640.5423099994659 seconds\n",
      "Step 7000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 649.6597044467926 seconds\n",
      "Step 7100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 658.9397795200348 seconds\n",
      "Step 7200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 668.0639629364014 seconds\n",
      "Step 7300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 677.2052702903748 seconds\n",
      "Step 7400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 686.503321647644 seconds\n",
      "Step 7500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 695.6224865913391 seconds\n",
      "Step 7600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 704.7481408119202 seconds\n",
      "Step 7700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 714.4402225017548 seconds\n",
      "Step 7800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 723.5442621707916 seconds\n",
      "Step 7900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 732.7207722663879 seconds\n",
      "Step 8000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 742.014598608017 seconds\n",
      "Step 8100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 751.8226916790009 seconds\n",
      "Step 8200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 760.890572309494 seconds\n",
      "Step 8300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 770.1690375804901 seconds\n",
      "Step 8400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 779.2865929603577 seconds\n",
      "Step 8500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 788.35795545578 seconds\n",
      "Step 8600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 797.6395409107208 seconds\n",
      "Step 8700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 806.7333765029907 seconds\n",
      "Step 8800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 815.9813747406006 seconds\n",
      "Step 8900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 825.0555512905121 seconds\n",
      "Step 9000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 834.1915154457092 seconds\n",
      "Step 9100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 843.5021831989288 seconds\n",
      "Step 9200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 852.6238896846771 seconds\n",
      "Step 9300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 861.7583539485931 seconds\n",
      "Step 9400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 871.0844905376434 seconds\n",
      "Step 9500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 880.1894888877869 seconds\n",
      "Step 9600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 889.3298096656799 seconds\n",
      "Step 9700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 898.616685628891 seconds\n",
      "Step 9800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 907.7023272514343 seconds\n",
      "Step 9900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 916.7907688617706 seconds\n",
      "Step 10000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 926.085723400116 seconds\n",
      "Step 10100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 935.1865861415863 seconds\n",
      "Step 10200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 944.2936398983002 seconds\n",
      "Step 10300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 953.6066348552704 seconds\n",
      "Step 10400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 962.732762336731 seconds\n",
      "Step 10500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 971.8443627357483 seconds\n",
      "Step 10600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 981.220431804657 seconds\n",
      "Step 10700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 990.5719139575958 seconds\n",
      "Step 10800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 999.9879364967346 seconds\n",
      "Step 10900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1009.0869207382202 seconds\n",
      "Step 11000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1018.2413623332977 seconds\n",
      "Step 11100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1027.5077464580536 seconds\n",
      "Step 11200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1036.7253346443176 seconds\n",
      "Step 11300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1045.8665671348572 seconds\n",
      "Step 11400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1055.1686308383942 seconds\n",
      "Step 11500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1064.369021654129 seconds\n",
      "Step 11600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1073.5522019863129 seconds\n",
      "Step 11700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1083.6807124614716 seconds\n",
      "Step 11800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1092.7797133922577 seconds\n",
      "Step 11900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1101.8816587924957 seconds\n",
      "Step 12000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1111.1348731517792 seconds\n",
      "Step 12100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1120.2220616340637 seconds\n",
      "Step 12200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1129.309520483017 seconds\n",
      "Step 12300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1138.569186925888 seconds\n",
      "Step 12400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1147.6424429416656 seconds\n",
      "Step 12500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1156.7929365634918 seconds\n",
      "Step 12600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1166.1059370040894 seconds\n",
      "Step 12700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1175.2327539920807 seconds\n",
      "Step 12800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1184.354466676712 seconds\n",
      "Step 12900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1194.4742152690887 seconds\n",
      "Step 13000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1203.6019072532654 seconds\n",
      "Step 13100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1212.919413805008 seconds\n",
      "Step 13200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1222.007668018341 seconds\n",
      "Step 13300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1231.1418602466583 seconds\n",
      "Step 13400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1240.4134459495544 seconds\n",
      "Step 13500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1249.4922864437103 seconds\n",
      "Step 13600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1258.6385099887848 seconds\n",
      "Step 13700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1267.9450061321259 seconds\n",
      "Step 13800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1277.0709767341614 seconds\n",
      "Step 13900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1286.2294023036957 seconds\n",
      "Step 14000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1295.6645064353943 seconds\n",
      "Step 14100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1305.086009502411 seconds\n",
      "Step 14200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1314.2315924167633 seconds\n",
      "Step 14300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1323.5294618606567 seconds\n",
      "Step 14400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1332.672315120697 seconds\n",
      "Step 14500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1341.8201174736023 seconds\n",
      "Step 14600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1351.831393480301 seconds\n",
      "Step 14700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1361.0027952194214 seconds\n",
      "Step 14800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1370.149034500122 seconds\n",
      "Step 14900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1379.4254171848297 seconds\n",
      "Step 15000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1388.5330038070679 seconds\n",
      "Step 15100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1397.6384162902832 seconds\n",
      "Step 15200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1406.9606506824493 seconds\n",
      "Step 15300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1416.1800918579102 seconds\n",
      "Step 15400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1426.0391011238098 seconds\n",
      "Step 15500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1435.1358096599579 seconds\n",
      "Step 15600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1444.268626689911 seconds\n",
      "Step 15700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1453.6000308990479 seconds\n",
      "Step 15800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1463.4061679840088 seconds\n",
      "Step 15900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1472.5276455879211 seconds\n",
      "Step 16000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1481.8118433952332 seconds\n",
      "Step 16100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1490.9165720939636 seconds\n",
      "Step 16200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1500.0467236042023 seconds\n",
      "Step 16300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1509.3354132175446 seconds\n",
      "Step 16400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1518.4434051513672 seconds\n",
      "Step 16500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1527.540458202362 seconds\n",
      "Step 16600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1536.8167941570282 seconds\n",
      "Step 16700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1545.9344296455383 seconds\n",
      "Step 16800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1555.0327467918396 seconds\n",
      "Step 16900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1564.3407304286957 seconds\n",
      "Step 17000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1573.458021402359 seconds\n",
      "Step 17100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1582.5994176864624 seconds\n",
      "Step 17200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1591.9399926662445 seconds\n",
      "Step 17300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1601.0961277484894 seconds\n",
      "Step 17400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1610.2273592948914 seconds\n",
      "Step 17500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1619.5263323783875 seconds\n",
      "Step 17600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1628.6723217964172 seconds\n",
      "Step 17700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1637.9876897335052 seconds\n",
      "Step 17800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1647.096082687378 seconds\n",
      "Step 17900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1656.4958488941193 seconds\n",
      "Step 18000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1665.820925951004 seconds\n",
      "Step 18100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1674.9298403263092 seconds\n",
      "Step 18200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1684.133991241455 seconds\n",
      "Step 18300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1694.1279363632202 seconds\n",
      "Step 18400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1703.25856924057 seconds\n",
      "Step 18500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1712.357720375061 seconds\n",
      "Step 18600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1721.7309424877167 seconds\n",
      "Step 18700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1730.9327456951141 seconds\n",
      "Step 18800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1740.0421504974365 seconds\n",
      "Step 18900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1749.3332352638245 seconds\n",
      "Step 19000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1758.4482958316803 seconds\n",
      "Step 19100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1768.212262392044 seconds\n",
      "Step 19200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1777.531983613968 seconds\n",
      "Step 19300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1786.7715022563934 seconds\n",
      "Step 19400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1795.8724579811096 seconds\n",
      "Step 19500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1805.1654751300812 seconds\n",
      "Step 19600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1814.3050661087036 seconds\n",
      "Step 19700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1823.4862377643585 seconds\n",
      "Step 19800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1833.1529772281647 seconds\n",
      "Step 19900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1842.3250391483307 seconds\n",
      "Step 20000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1851.4064872264862 seconds\n",
      "Step 20100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1860.764937877655 seconds\n",
      "Step 20200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1870.228184223175 seconds\n",
      "Step 20300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1880.0637624263763 seconds\n",
      "Step 20400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1889.1528089046478 seconds\n",
      "Step 20500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1898.2844243049622 seconds\n",
      "Step 20600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1907.548930644989 seconds\n",
      "Step 20700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1917.4621710777283 seconds\n",
      "Step 20800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1926.5870568752289 seconds\n",
      "Step 20900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1935.925184249878 seconds\n",
      "Step 21000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1945.0695178508759 seconds\n",
      "Step 21100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1954.1887640953064 seconds\n",
      "Step 21200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1963.6315858364105 seconds\n",
      "Step 21300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1972.7492365837097 seconds\n",
      "Step 21400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1981.979983329773 seconds\n",
      "Step 21500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1991.3334500789642 seconds\n",
      "Step 21600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2000.613079547882 seconds\n",
      "Step 21700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2009.8095335960388 seconds\n",
      "Step 21800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2019.1421475410461 seconds\n",
      "Step 21900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2028.9477124214172 seconds\n",
      "Step 22000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2038.0628814697266 seconds\n",
      "Step 22100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2047.4652619361877 seconds\n",
      "Step 22200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2056.7146055698395 seconds\n",
      "Step 22300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2065.830434322357 seconds\n",
      "Step 22400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2075.122100830078 seconds\n",
      "Step 22500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2084.2711946964264 seconds\n",
      "Step 22600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2093.904043197632 seconds\n",
      "Step 22700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2103.2163066864014 seconds\n",
      "Step 22800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2112.3332710266113 seconds\n",
      "Step 22900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2121.6074731349945 seconds\n",
      "Step 23000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2130.9249691963196 seconds\n",
      "Step 23100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2140.016379594803 seconds\n",
      "Step 23200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2149.347558736801 seconds\n",
      "Step 23300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2159.633958339691 seconds\n",
      "Step 23400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2168.736529111862 seconds\n",
      "Step 23500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2178.08341050148 seconds\n",
      "Step 23600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2187.1973748207092 seconds\n",
      "Step 23700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2196.291211605072 seconds\n",
      "Step 23800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2205.5856931209564 seconds\n",
      "Step 23900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2214.712342739105 seconds\n",
      "Step 24000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2223.8175055980682 seconds\n",
      "Step 24100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2233.112492799759 seconds\n",
      "Step 24200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2242.232665538788 seconds\n",
      "Step 24300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2251.383509874344 seconds\n",
      "Step 24400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2260.719702243805 seconds\n",
      "Step 24500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2269.8904087543488 seconds\n",
      "Step 24600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2279.0150055885315 seconds\n",
      "Step 24700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2288.3214559555054 seconds\n",
      "Step 24800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2297.4454317092896 seconds\n",
      "Step 24900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2306.54825091362 seconds\n",
      "Step 25000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2315.8427183628082 seconds\n",
      "Step 25100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2324.9727931022644 seconds\n",
      "Step 25200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2334.087695837021 seconds\n",
      "Step 25300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2343.399425268173 seconds\n",
      "Step 25400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2352.5269691944122 seconds\n",
      "Step 25500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2361.642268180847 seconds\n",
      "Step 25600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2370.9381120204926 seconds\n",
      "Step 25700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2380.692976474762 seconds\n",
      "Step 25800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2389.8156814575195 seconds\n",
      "Step 25900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2399.0929775238037 seconds\n",
      "Step 26000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2408.3134019374847 seconds\n",
      "Step 26100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2417.4456148147583 seconds\n",
      "Step 26200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2426.745936155319 seconds\n",
      "Step 26300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2435.874460697174 seconds\n",
      "Step 26400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2445.6144077777863 seconds\n",
      "Step 26500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2454.7509763240814 seconds\n",
      "Step 26600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2463.8755824565887 seconds\n",
      "Step 26700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2473.232198238373 seconds\n",
      "Step 26800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2482.905270576477 seconds\n",
      "Step 26900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2491.9835181236267 seconds\n",
      "Step 27000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2501.2675812244415 seconds\n",
      "Step 27100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2510.4268991947174 seconds\n",
      "Step 27200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2519.5403542518616 seconds\n",
      "Step 27300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2528.8539187908173 seconds\n",
      "Step 27400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2537.9762864112854 seconds\n",
      "Step 27500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2547.215917110443 seconds\n",
      "Step 27600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2557.144785642624 seconds\n",
      "Step 27700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2566.2408125400543 seconds\n",
      "Step 27800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2575.376095056534 seconds\n",
      "Step 27900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2584.6932349205017 seconds\n",
      "Step 28000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2593.8217833042145 seconds\n",
      "Step 28100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2602.948075056076 seconds\n",
      "Step 28200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2612.19802236557 seconds\n",
      "Step 28300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2621.6659557819366 seconds\n",
      "Step 28400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2631.263028383255 seconds\n",
      "Step 28500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2640.581806898117 seconds\n",
      "Step 28600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2649.7085480690002 seconds\n",
      "Step 28700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2658.8554842472076 seconds\n",
      "Step 28800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2668.2581980228424 seconds\n",
      "Step 28900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2677.881012916565 seconds\n",
      "Step 29000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2687.005107164383 seconds\n",
      "Step 29100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2696.352380514145 seconds\n",
      "Step 29200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2705.4455654621124 seconds\n",
      "Step 29300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2714.9131071567535 seconds\n",
      "Step 29400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2724.234228372574 seconds\n",
      "Step 29500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2733.363672018051 seconds\n",
      "Step 29600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2742.5099749565125 seconds\n",
      "Step 29700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2752.0174860954285 seconds\n",
      "Step 29800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2761.2152767181396 seconds\n",
      "Step 29900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2770.3338901996613 seconds\n",
      "Step 30000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2780.8083848953247 seconds\n",
      "Step 30100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2789.921145439148 seconds\n",
      "Step 30200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2799.307953596115 seconds\n",
      "Step 30300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2808.510096311569 seconds\n",
      "Step 30400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2817.6335821151733 seconds\n",
      "Step 30500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2826.948014497757 seconds\n",
      "Step 30600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2836.0744721889496 seconds\n",
      "Step 30700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2845.385401248932 seconds\n",
      "Step 30800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2855.2001638412476 seconds\n",
      "Step 30900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2864.305981874466 seconds\n",
      "Step 31000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2873.424042701721 seconds\n",
      "Step 31100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2882.7674338817596 seconds\n",
      "Step 31200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2891.8916895389557 seconds\n",
      "Step 31300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2901.1920397281647 seconds\n",
      "Step 31400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2910.9682173728943 seconds\n",
      "Step 31500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2920.065557718277 seconds\n",
      "Step 31600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2929.1634805202484 seconds\n",
      "Step 31700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2938.495156764984 seconds\n",
      "Step 31800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2947.8665115833282 seconds\n",
      "Step 31900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2957.0739591121674 seconds\n",
      "Step 32000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2966.410976409912 seconds\n",
      "Step 32100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2976.3762600421906 seconds\n",
      "Step 32200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2985.482108592987 seconds\n",
      "Step 32300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2994.776511669159 seconds\n",
      "Step 32400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3003.902866601944 seconds\n",
      "Step 32500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3013.008134126663 seconds\n",
      "Step 32600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3022.3152027130127 seconds\n",
      "Step 32700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3031.4338965415955 seconds\n",
      "Step 32800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3040.558271408081 seconds\n",
      "Step 32900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3049.880286693573 seconds\n",
      "Step 33000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3059.005380153656 seconds\n",
      "Step 33100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3068.100970029831 seconds\n",
      "Step 33200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3077.37366938591 seconds\n",
      "Step 33300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3086.4762160778046 seconds\n",
      "Step 33400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3095.5828952789307 seconds\n",
      "Step 33500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3104.9255821704865 seconds\n",
      "Step 33600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3114.07027387619 seconds\n",
      "Step 33700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3123.202610731125 seconds\n",
      "Step 33800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3132.5166878700256 seconds\n",
      "Step 33900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3141.652403354645 seconds\n",
      "Step 34000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3151.543763399124 seconds\n",
      "Step 34100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3160.828394651413 seconds\n",
      "Step 34200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3169.965082883835 seconds\n",
      "Step 34300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3179.3390884399414 seconds\n",
      "Step 34400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3188.4817390441895 seconds\n",
      "Step 34500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3197.645537853241 seconds\n",
      "Step 34600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3207.711865901947 seconds\n",
      "Step 34700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3216.810693502426 seconds\n",
      "Step 34800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3226.043936729431 seconds\n",
      "Step 34900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3235.343742132187 seconds\n",
      "Step 35000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3244.4432423114777 seconds\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0098, grad_fn=<SubBackward0>), time elapsed: 0.1470801830291748 seconds\n",
      "Step 100, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 9.466786861419678 seconds\n",
      "Step 200, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 19.09433364868164 seconds\n",
      "Step 300, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 28.174793243408203 seconds\n",
      "Step 400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 37.27523398399353 seconds\n",
      "Step 500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 46.511223793029785 seconds\n",
      "Step 600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 55.6141083240509 seconds\n",
      "Step 700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 64.72411179542542 seconds\n",
      "Step 800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 73.98309755325317 seconds\n",
      "Step 900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 83.08779215812683 seconds\n",
      "Step 1000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 92.1700873374939 seconds\n",
      "Step 1100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 101.41820049285889 seconds\n",
      "Step 1200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 110.51040029525757 seconds\n",
      "Step 1300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 119.74593758583069 seconds\n",
      "Step 1400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 128.8226761817932 seconds\n",
      "Step 1500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 137.88574290275574 seconds\n",
      "Step 1600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 147.13544964790344 seconds\n",
      "Step 1700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 156.23268032073975 seconds\n",
      "Step 1800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 165.3211007118225 seconds\n",
      "Step 1900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 174.58796644210815 seconds\n",
      "Step 2000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 183.69570350646973 seconds\n",
      "Step 2100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 192.80944800376892 seconds\n",
      "Step 2200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 202.04659748077393 seconds\n",
      "Step 2300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 211.14385628700256 seconds\n",
      "Step 2400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 220.21815061569214 seconds\n",
      "Step 2500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 229.43232202529907 seconds\n",
      "Step 2600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 238.5190691947937 seconds\n",
      "Step 2700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 247.6518988609314 seconds\n",
      "Step 2800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 256.9206955432892 seconds\n",
      "Step 2900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 266.04036140441895 seconds\n",
      "Step 3000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 275.34355187416077 seconds\n",
      "Step 3100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 284.4784848690033 seconds\n",
      "Step 3200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 293.5899624824524 seconds\n",
      "Step 3300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 302.86950039863586 seconds\n",
      "Step 3400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 311.96710991859436 seconds\n",
      "Step 3500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 321.0673632621765 seconds\n",
      "Step 3600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 330.78885293006897 seconds\n",
      "Step 3700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 339.8788459300995 seconds\n",
      "Step 3800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 348.9692175388336 seconds\n",
      "Step 3900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 358.2025809288025 seconds\n",
      "Step 4000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 367.51900482177734 seconds\n",
      "Step 4100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 376.6316523551941 seconds\n",
      "Step 4200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 385.9182457923889 seconds\n",
      "Step 4300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 395.05227398872375 seconds\n",
      "Step 4400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 404.1884205341339 seconds\n",
      "Step 4500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 414.04898858070374 seconds\n",
      "Step 4600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 423.201167345047 seconds\n",
      "Step 4700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 432.43362188339233 seconds\n",
      "Step 4800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 441.4933476448059 seconds\n",
      "Step 4900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 450.65154123306274 seconds\n",
      "Step 5000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 460.6317071914673 seconds\n",
      "Step 5100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 469.71193766593933 seconds\n",
      "Step 5200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 478.93070220947266 seconds\n",
      "Step 5300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 488.28382539749146 seconds\n",
      "Step 5400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 497.38660168647766 seconds\n",
      "Step 5500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 506.5117495059967 seconds\n",
      "Step 5600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 515.7934350967407 seconds\n",
      "Step 5700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 525.0188522338867 seconds\n",
      "Step 5800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 534.3814795017242 seconds\n",
      "Step 5900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 543.7431967258453 seconds\n",
      "Step 6000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 553.5423488616943 seconds\n",
      "Step 6100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 562.6416673660278 seconds\n",
      "Step 6200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 571.8747754096985 seconds\n",
      "Step 6300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 580.9963793754578 seconds\n",
      "Step 6400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 590.0931859016418 seconds\n",
      "Step 6500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 599.3418552875519 seconds\n",
      "Step 6600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 608.4494254589081 seconds\n",
      "Step 6700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 618.1681847572327 seconds\n",
      "Step 6800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 627.3648958206177 seconds\n",
      "Step 6900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 636.447815656662 seconds\n",
      "Step 7000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 645.7283129692078 seconds\n",
      "Step 7100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 654.8339040279388 seconds\n",
      "Step 7200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 664.6587812900543 seconds\n",
      "Step 7300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 673.9713423252106 seconds\n",
      "Step 7400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 684.475549697876 seconds\n",
      "Step 7500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 693.5829169750214 seconds\n",
      "Step 7600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 702.8717331886292 seconds\n",
      "Step 7700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 711.98308634758 seconds\n",
      "Step 7800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 721.0993399620056 seconds\n",
      "Step 7900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 730.3460476398468 seconds\n",
      "Step 8000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 739.5658466815948 seconds\n",
      "Step 8100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 748.6479089260101 seconds\n",
      "Step 8200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 757.8998811244965 seconds\n",
      "Step 8300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 767.0176346302032 seconds\n",
      "Step 8400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 776.3152270317078 seconds\n",
      "Step 8500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 785.9251770973206 seconds\n",
      "Step 8600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 795.1521556377411 seconds\n",
      "Step 8700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 804.4840350151062 seconds\n",
      "Step 8800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 813.5912852287292 seconds\n",
      "Step 8900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 823.2571811676025 seconds\n",
      "Step 9000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 832.6564548015594 seconds\n",
      "Step 9100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 841.7586643695831 seconds\n",
      "Step 9200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 850.8790056705475 seconds\n",
      "Step 9300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 860.8892171382904 seconds\n",
      "Step 9400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 869.9918847084045 seconds\n",
      "Step 9500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 879.0729343891144 seconds\n",
      "Step 9600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 888.3279900550842 seconds\n",
      "Step 9700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 897.4250812530518 seconds\n",
      "Step 9800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 906.5406773090363 seconds\n",
      "Step 9900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 915.8702461719513 seconds\n",
      "Step 10000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 925.046314239502 seconds\n",
      "Step 10100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 934.1908469200134 seconds\n",
      "Step 10200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 943.5365595817566 seconds\n",
      "Step 10300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 952.6314370632172 seconds\n",
      "Step 10400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 961.9365820884705 seconds\n",
      "Step 10500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 971.0307219028473 seconds\n",
      "Step 10600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 980.1666264533997 seconds\n",
      "Step 10700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 990.1439874172211 seconds\n",
      "Step 10800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 999.237028837204 seconds\n",
      "Step 10900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1008.3523802757263 seconds\n",
      "Step 11000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1017.6631767749786 seconds\n",
      "Step 11100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1026.7761838436127 seconds\n",
      "Step 11200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1035.9108574390411 seconds\n",
      "Step 11300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1045.2295167446136 seconds\n",
      "Step 11400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1054.3583178520203 seconds\n",
      "Step 11500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1063.679673910141 seconds\n",
      "Step 11600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1073.2251408100128 seconds\n",
      "Step 11700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1082.6975600719452 seconds\n",
      "Step 11800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1091.908228635788 seconds\n",
      "Step 11900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1101.2182087898254 seconds\n",
      "Step 12000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1110.3545010089874 seconds\n",
      "Step 12100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1119.5782597064972 seconds\n",
      "Step 12200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1129.4274835586548 seconds\n",
      "Step 12300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1138.6218829154968 seconds\n",
      "Step 12400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1147.9915008544922 seconds\n",
      "Step 12500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1157.1185331344604 seconds\n",
      "Step 12600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1166.2462403774261 seconds\n",
      "Step 12700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1175.534027338028 seconds\n",
      "Step 12800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1185.4011976718903 seconds\n",
      "Step 12900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1194.4981820583344 seconds\n",
      "Step 13000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1204.0155622959137 seconds\n",
      "Step 13100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1213.1161584854126 seconds\n",
      "Step 13200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1222.2259376049042 seconds\n",
      "Step 13300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1231.5571024417877 seconds\n",
      "Step 13400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1240.950117111206 seconds\n",
      "Step 13500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1250.5731341838837 seconds\n",
      "Step 13600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1259.8796730041504 seconds\n",
      "Step 13700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1269.0031633377075 seconds\n",
      "Step 13800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1278.098769903183 seconds\n",
      "Step 13900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1287.531441450119 seconds\n",
      "Step 14000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1296.862940788269 seconds\n",
      "Step 14100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1306.0735864639282 seconds\n",
      "Step 14200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1315.7286248207092 seconds\n",
      "Step 14300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1324.8523275852203 seconds\n",
      "Step 14400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1334.0699579715729 seconds\n",
      "Step 14500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1343.3656451702118 seconds\n",
      "Step 14600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1352.8056433200836 seconds\n",
      "Step 14700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1362.1620335578918 seconds\n",
      "Step 14800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1371.370027065277 seconds\n",
      "Step 14900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1381.291090965271 seconds\n",
      "Step 15000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1390.5305132865906 seconds\n",
      "Step 15100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1399.7171728610992 seconds\n",
      "Step 15200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1408.8261301517487 seconds\n",
      "Step 15300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1418.1315977573395 seconds\n",
      "Step 15400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1427.2603840827942 seconds\n",
      "Step 15500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1437.0181488990784 seconds\n",
      "Step 15600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1446.319556236267 seconds\n",
      "Step 15700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1455.4879133701324 seconds\n",
      "Step 15800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1464.603040933609 seconds\n",
      "Step 15900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1473.9347960948944 seconds\n",
      "Step 16000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1483.0299010276794 seconds\n",
      "Step 16100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1492.4517941474915 seconds\n",
      "Step 16200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1501.8708403110504 seconds\n",
      "Step 16300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1511.180606842041 seconds\n",
      "Step 16400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1520.83185338974 seconds\n",
      "Step 16500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1530.157747745514 seconds\n",
      "Step 16600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1539.293517112732 seconds\n",
      "Step 16700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1548.4328105449677 seconds\n",
      "Step 16800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1557.7608499526978 seconds\n",
      "Step 16900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1566.9257848262787 seconds\n",
      "Step 17000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1576.395644903183 seconds\n",
      "Step 17100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1585.7355842590332 seconds\n",
      "Step 17200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1594.8407649993896 seconds\n",
      "Step 17300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1605.71870470047 seconds\n",
      "Step 17400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1614.8186905384064 seconds\n",
      "Step 17500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1623.9245309829712 seconds\n",
      "Step 17600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1633.1827635765076 seconds\n",
      "Step 17700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1642.2690753936768 seconds\n",
      "Step 17800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1651.4006476402283 seconds\n",
      "Step 17900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1660.7003061771393 seconds\n",
      "Step 18000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1669.800945520401 seconds\n",
      "Step 18100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1678.9122850894928 seconds\n",
      "Step 18200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1688.238029241562 seconds\n",
      "Step 18300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1697.3545081615448 seconds\n",
      "Step 18400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1706.4712946414948 seconds\n",
      "Step 18500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1715.7750940322876 seconds\n",
      "Step 18600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1724.865454673767 seconds\n",
      "Step 18700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1733.984563589096 seconds\n",
      "Step 18800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1743.3101224899292 seconds\n",
      "Step 18900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1752.4305720329285 seconds\n",
      "Step 19000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1761.5616924762726 seconds\n",
      "Step 19100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1770.9117858409882 seconds\n",
      "Step 19200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1780.0498032569885 seconds\n",
      "Step 19300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1789.1751658916473 seconds\n",
      "Step 19400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1798.483907699585 seconds\n",
      "Step 19500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1807.8762362003326 seconds\n",
      "Step 19600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1817.1056373119354 seconds\n",
      "Step 19700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1826.4771144390106 seconds\n",
      "Step 19800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1836.11496925354 seconds\n",
      "Step 19900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1845.5172681808472 seconds\n",
      "Step 20000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1854.6170415878296 seconds\n",
      "Step 20100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1863.7320239543915 seconds\n",
      "Step 20200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1873.4693512916565 seconds\n",
      "Step 20300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1882.9242079257965 seconds\n",
      "Step 20400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1892.06525349617 seconds\n",
      "Step 20500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1901.608657836914 seconds\n",
      "Step 20600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1911.0204148292542 seconds\n",
      "Step 20700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1920.2487432956696 seconds\n",
      "Step 20800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1929.5990862846375 seconds\n",
      "Step 20900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1938.85014295578 seconds\n",
      "Step 21000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1948.8026864528656 seconds\n",
      "Step 21100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1958.1007978916168 seconds\n",
      "Step 21200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1967.292350769043 seconds\n",
      "Step 21300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1976.3827619552612 seconds\n",
      "Step 21400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1985.6618068218231 seconds\n",
      "Step 21500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1994.7861139774323 seconds\n",
      "Step 21600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2004.1931719779968 seconds\n",
      "Step 21700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2013.5484240055084 seconds\n",
      "Step 21800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2023.5211358070374 seconds\n",
      "Step 21900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2032.6310997009277 seconds\n",
      "Step 22000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2041.924058675766 seconds\n",
      "Step 22100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2051.0306208133698 seconds\n",
      "Step 22200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2060.219744205475 seconds\n",
      "Step 22300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2069.554974079132 seconds\n",
      "Step 22400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2079.3005537986755 seconds\n",
      "Step 22500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2088.900506734848 seconds\n",
      "Step 22600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2098.1746208667755 seconds\n",
      "Step 22700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2107.431569337845 seconds\n",
      "Step 22800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2117.1865298748016 seconds\n",
      "Step 22900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2126.4568309783936 seconds\n",
      "Step 23000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2135.580151319504 seconds\n",
      "Step 23100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2145.914258480072 seconds\n",
      "Step 23200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2155.0399222373962 seconds\n",
      "Step 23300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2164.142075061798 seconds\n",
      "Step 23400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2173.4497966766357 seconds\n",
      "Step 23500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2182.553713083267 seconds\n",
      "Step 23600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2191.6491746902466 seconds\n",
      "Step 23700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2200.9333283901215 seconds\n",
      "Step 23800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2210.048741579056 seconds\n",
      "Step 23900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2219.1523962020874 seconds\n",
      "Step 24000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2228.4225676059723 seconds\n",
      "Step 24100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2237.543737888336 seconds\n",
      "Step 24200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2246.6334660053253 seconds\n",
      "Step 24300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2255.8933284282684 seconds\n",
      "Step 24400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2264.993179798126 seconds\n",
      "Step 24500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2274.1211478710175 seconds\n",
      "Step 24600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2283.4230337142944 seconds\n",
      "Step 24700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2292.5207595825195 seconds\n",
      "Step 24800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2301.6711390018463 seconds\n",
      "Step 24900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2310.9661571979523 seconds\n",
      "Step 25000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2320.0759382247925 seconds\n",
      "Step 25100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2329.1848323345184 seconds\n",
      "Step 25200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2338.5105402469635 seconds\n",
      "Step 25300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2347.6198942661285 seconds\n",
      "Step 25400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2356.9596059322357 seconds\n",
      "Step 25500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2366.0839653015137 seconds\n",
      "Step 25600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2375.181928396225 seconds\n",
      "Step 25700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2384.503278493881 seconds\n",
      "Step 25800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2393.741417646408 seconds\n",
      "Step 25900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2402.9528391361237 seconds\n",
      "Step 26000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2412.9196333885193 seconds\n",
      "Step 26100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2422.0451192855835 seconds\n",
      "Step 26200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2431.147851228714 seconds\n",
      "Step 26300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2440.4387726783752 seconds\n",
      "Step 26400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2449.5362651348114 seconds\n",
      "Step 26500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2458.616772413254 seconds\n",
      "Step 26600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2467.895545244217 seconds\n",
      "Step 26700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2477.0039446353912 seconds\n",
      "Step 26800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2486.1150467395782 seconds\n",
      "Step 26900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2495.4412937164307 seconds\n",
      "Step 27000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2504.5693662166595 seconds\n",
      "Step 27100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2513.715259552002 seconds\n",
      "Step 27200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2523.0670189857483 seconds\n",
      "Step 27300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2532.2703001499176 seconds\n",
      "Step 27400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2541.4464643001556 seconds\n",
      "Step 27500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2551.331104516983 seconds\n",
      "Step 27600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2560.4572792053223 seconds\n",
      "Step 27700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2569.566056728363 seconds\n",
      "Step 27800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2578.8375005722046 seconds\n",
      "Step 27900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2587.9652450084686 seconds\n",
      "Step 28000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2597.139393091202 seconds\n",
      "Step 28100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2606.4394521713257 seconds\n",
      "Step 28200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2615.5307400226593 seconds\n",
      "Step 28300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2624.9397497177124 seconds\n",
      "Step 28400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2634.271394968033 seconds\n",
      "Step 28500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2643.3771896362305 seconds\n",
      "Step 28600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2652.4882731437683 seconds\n",
      "Step 28700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2661.826259613037 seconds\n",
      "Step 28800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2671.33429646492 seconds\n",
      "Step 28900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2680.6537642478943 seconds\n",
      "Step 29000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2689.8855810165405 seconds\n",
      "Step 29100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2699.028846502304 seconds\n",
      "Step 29200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2708.377299308777 seconds\n",
      "Step 29300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2717.4717314243317 seconds\n",
      "Step 29400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2727.409392595291 seconds\n",
      "Step 29500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2736.718686103821 seconds\n",
      "Step 29600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2745.7975533008575 seconds\n",
      "Step 29700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2754.9047107696533 seconds\n",
      "Step 29800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2764.1924810409546 seconds\n",
      "Step 29900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2773.4768068790436 seconds\n",
      "Step 30000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2782.604630470276 seconds\n",
      "Step 30100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2791.9988691806793 seconds\n",
      "Step 30200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2801.122334241867 seconds\n",
      "Step 30300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2810.2661712169647 seconds\n",
      "Step 30400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2820.538226366043 seconds\n",
      "Step 30500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2829.6626749038696 seconds\n",
      "Step 30600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2838.758871793747 seconds\n",
      "Step 30700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2848.075732946396 seconds\n",
      "Step 30800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2857.1672768592834 seconds\n",
      "Step 30900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2866.273216485977 seconds\n",
      "Step 31000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2875.6016964912415 seconds\n",
      "Step 31100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2884.701377391815 seconds\n",
      "Step 31200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2893.813702583313 seconds\n",
      "Step 31300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2903.1700718402863 seconds\n",
      "Step 31400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2912.3625161647797 seconds\n",
      "Step 31500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2921.5174152851105 seconds\n",
      "Step 31600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2930.886703491211 seconds\n",
      "Step 31700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2940.0570273399353 seconds\n",
      "Step 31800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2949.1983721256256 seconds\n",
      "Step 31900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2958.52023601532 seconds\n",
      "Step 32000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2967.681833744049 seconds\n",
      "Step 32100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2976.8432796001434 seconds\n",
      "Step 32200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2986.1825363636017 seconds\n",
      "Step 32300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2995.3393914699554 seconds\n",
      "Step 32400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3004.5027043819427 seconds\n",
      "Step 32500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3013.861461162567 seconds\n",
      "Step 32600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3023.2173385620117 seconds\n",
      "Step 32700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3032.343703508377 seconds\n",
      "Step 32800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3041.698484182358 seconds\n",
      "Step 32900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3050.787919521332 seconds\n",
      "Step 33000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3060.1146907806396 seconds\n",
      "Step 33100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3069.2327075004578 seconds\n",
      "Step 33200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3078.6728847026825 seconds\n",
      "Step 33300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3088.0168673992157 seconds\n",
      "Step 33400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3097.1360223293304 seconds\n",
      "Step 33500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3106.3428609371185 seconds\n",
      "Step 33600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3115.6720430850983 seconds\n",
      "Step 33700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3125.020664691925 seconds\n",
      "Step 33800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3134.193795442581 seconds\n",
      "Step 33900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3143.546480178833 seconds\n",
      "Step 34000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3152.7250831127167 seconds\n",
      "Step 34100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3161.892541885376 seconds\n",
      "Step 34200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3171.4688532352448 seconds\n",
      "Step 34300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3180.6766533851624 seconds\n",
      "Step 34400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3189.7963070869446 seconds\n",
      "Step 34500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3199.377121448517 seconds\n",
      "Step 34600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3208.9265942573547 seconds\n",
      "Step 34700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3218.103112220764 seconds\n",
      "Step 34800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3227.463788509369 seconds\n",
      "Step 34900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3236.619339942932 seconds\n",
      "Step 35000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3245.7848892211914 seconds\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 0.14234614372253418 seconds\n",
      "Step 100, loss: tensor(0.0125, grad_fn=<SubBackward0>), time elapsed: 9.884027004241943 seconds\n",
      "Step 200, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 18.998228073120117 seconds\n",
      "Step 300, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 28.10262942314148 seconds\n",
      "Step 400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 37.33982062339783 seconds\n",
      "Step 500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 46.448513984680176 seconds\n",
      "Step 600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 55.523902893066406 seconds\n",
      "Step 700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 64.74310827255249 seconds\n",
      "Step 800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 73.84872436523438 seconds\n",
      "Step 900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 83.0776424407959 seconds\n",
      "Step 1000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 92.15062832832336 seconds\n",
      "Step 1100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 101.24729871749878 seconds\n",
      "Step 1200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 110.47852420806885 seconds\n",
      "Step 1300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 119.54620838165283 seconds\n",
      "Step 1400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 128.62131476402283 seconds\n",
      "Step 1500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 137.890038728714 seconds\n",
      "Step 1600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 146.98252201080322 seconds\n",
      "Step 1700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 156.06420040130615 seconds\n",
      "Step 1800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 165.32157707214355 seconds\n",
      "Step 1900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 174.41454672813416 seconds\n",
      "Step 2000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 183.49329471588135 seconds\n",
      "Step 2100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 192.7524619102478 seconds\n",
      "Step 2200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 201.8520119190216 seconds\n",
      "Step 2300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 210.95945358276367 seconds\n",
      "Step 2400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 220.22224307060242 seconds\n",
      "Step 2500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 229.33977007865906 seconds\n",
      "Step 2600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 238.57307481765747 seconds\n",
      "Step 2700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 247.65022563934326 seconds\n",
      "Step 2800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 256.7643094062805 seconds\n",
      "Step 2900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 266.0115125179291 seconds\n",
      "Step 3000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 275.09913063049316 seconds\n",
      "Step 3100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 284.208247423172 seconds\n",
      "Step 3200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 293.48108100891113 seconds\n",
      "Step 3300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 302.59554743766785 seconds\n",
      "Step 3400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 311.7268168926239 seconds\n",
      "Step 3500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 321.03926610946655 seconds\n",
      "Step 3600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 330.1393795013428 seconds\n",
      "Step 3700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 339.2334372997284 seconds\n",
      "Step 3800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 348.5295481681824 seconds\n",
      "Step 3900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 357.64856576919556 seconds\n",
      "Step 4000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 366.7531805038452 seconds\n",
      "Step 4100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 376.00717210769653 seconds\n",
      "Step 4200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 385.11380648612976 seconds\n",
      "Step 4300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 394.35788583755493 seconds\n",
      "Step 4400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 403.44137835502625 seconds\n",
      "Step 4500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 412.5798408985138 seconds\n",
      "Step 4600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 421.86279106140137 seconds\n",
      "Step 4700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 430.9832832813263 seconds\n",
      "Step 4800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 440.1128897666931 seconds\n",
      "Step 4900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 449.400363445282 seconds\n",
      "Step 5000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 458.4766252040863 seconds\n",
      "Step 5100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 467.59306287765503 seconds\n",
      "Step 5200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 476.8920955657959 seconds\n",
      "Step 5300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 485.9983561038971 seconds\n",
      "Step 5400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 495.11885833740234 seconds\n",
      "Step 5500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 505.032306432724 seconds\n",
      "Step 5600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 514.1336131095886 seconds\n",
      "Step 5700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 523.2269642353058 seconds\n",
      "Step 5800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 532.5071260929108 seconds\n",
      "Step 5900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 541.6134624481201 seconds\n",
      "Step 6000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 550.8901214599609 seconds\n",
      "Step 6100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 559.9923181533813 seconds\n",
      "Step 6200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 569.0844171047211 seconds\n",
      "Step 6300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 578.341705083847 seconds\n",
      "Step 6400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 587.4503610134125 seconds\n",
      "Step 6500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 596.5856280326843 seconds\n",
      "Step 6600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 606.9388284683228 seconds\n",
      "Step 6700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 616.0209562778473 seconds\n",
      "Step 6800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 625.1207201480865 seconds\n",
      "Step 6900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 634.3761205673218 seconds\n",
      "Step 7000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 643.4904639720917 seconds\n",
      "Step 7100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 652.6255235671997 seconds\n",
      "Step 7200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 661.9031059741974 seconds\n",
      "Step 7300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 671.0097007751465 seconds\n",
      "Step 7400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 680.192950963974 seconds\n",
      "Step 7500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 689.4706592559814 seconds\n",
      "Step 7600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 698.5811259746552 seconds\n",
      "Step 7700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 707.6734094619751 seconds\n",
      "Step 7800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 716.9704821109772 seconds\n",
      "Step 7900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 726.0709452629089 seconds\n",
      "Step 8000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 735.3407406806946 seconds\n",
      "Step 8100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 744.4412484169006 seconds\n",
      "Step 8200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 753.5263562202454 seconds\n",
      "Step 8300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 762.7962863445282 seconds\n",
      "Step 8400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 771.891152381897 seconds\n",
      "Step 8500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 781.0095477104187 seconds\n",
      "Step 8600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 790.3138835430145 seconds\n",
      "Step 8700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 799.4400289058685 seconds\n",
      "Step 8800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 808.5794031620026 seconds\n",
      "Step 8900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 817.8909542560577 seconds\n",
      "Step 9000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 827.0268881320953 seconds\n",
      "Step 9100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 836.1867835521698 seconds\n",
      "Step 9200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 845.4964218139648 seconds\n",
      "Step 9300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 854.6611340045929 seconds\n",
      "Step 9400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 863.7910158634186 seconds\n",
      "Step 9500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 873.0826756954193 seconds\n",
      "Step 9600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 882.1614677906036 seconds\n",
      "Step 9700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 891.2628409862518 seconds\n",
      "Step 9800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 900.6033756732941 seconds\n",
      "Step 9900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 909.7577888965607 seconds\n",
      "Step 10000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 919.0941424369812 seconds\n",
      "Step 10100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 928.2679471969604 seconds\n",
      "Step 10200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 937.4326100349426 seconds\n",
      "Step 10300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 947.1435706615448 seconds\n",
      "Step 10400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 956.2684516906738 seconds\n",
      "Step 10500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 965.4114782810211 seconds\n",
      "Step 10600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 974.666264295578 seconds\n",
      "Step 10700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 983.7525715827942 seconds\n",
      "Step 10800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 992.8606426715851 seconds\n",
      "Step 10900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1002.1504809856415 seconds\n",
      "Step 11000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1012.4936137199402 seconds\n",
      "Step 11100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1021.6180889606476 seconds\n",
      "Step 11200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1030.914844751358 seconds\n",
      "Step 11300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1040.0262734889984 seconds\n",
      "Step 11400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1049.1421749591827 seconds\n",
      "Step 11500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1058.4068384170532 seconds\n",
      "Step 11600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1067.5140669345856 seconds\n",
      "Step 11700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1076.8032324314117 seconds\n",
      "Step 11800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1085.9205257892609 seconds\n",
      "Step 11900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1095.0300207138062 seconds\n",
      "Step 12000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1104.30113697052 seconds\n",
      "Step 12100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1113.4269843101501 seconds\n",
      "Step 12200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1122.5400846004486 seconds\n",
      "Step 12300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1131.8373444080353 seconds\n",
      "Step 12400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1140.9737722873688 seconds\n",
      "Step 12500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1150.0862262248993 seconds\n",
      "Step 12600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1159.3984587192535 seconds\n",
      "Step 12700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1169.301748752594 seconds\n",
      "Step 12800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1178.5757267475128 seconds\n",
      "Step 12900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1187.8401274681091 seconds\n",
      "Step 13000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1196.9297313690186 seconds\n",
      "Step 13100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1206.038333415985 seconds\n",
      "Step 13200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1215.2842545509338 seconds\n",
      "Step 13300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1224.3725273609161 seconds\n",
      "Step 13400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1233.5148148536682 seconds\n",
      "Step 13500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1242.8067107200623 seconds\n",
      "Step 13600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1251.9120371341705 seconds\n",
      "Step 13700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1261.0271227359772 seconds\n",
      "Step 13800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1270.3146064281464 seconds\n",
      "Step 13900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1279.4380435943604 seconds\n",
      "Step 14000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1288.533217906952 seconds\n",
      "Step 14100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1297.8081929683685 seconds\n",
      "Step 14200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1306.889881849289 seconds\n",
      "Step 14300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1316.1547739505768 seconds\n",
      "Step 14400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1325.3009073734283 seconds\n",
      "Step 14500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1334.4190773963928 seconds\n",
      "Step 14600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1343.7119874954224 seconds\n",
      "Step 14700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1352.8358688354492 seconds\n",
      "Step 14800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1361.9873096942902 seconds\n",
      "Step 14900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1371.3000702857971 seconds\n",
      "Step 15000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1380.4153428077698 seconds\n",
      "Step 15100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1389.5890839099884 seconds\n",
      "Step 15200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1398.9113314151764 seconds\n",
      "Step 15300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1408.0521442890167 seconds\n",
      "Step 15400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1417.1774578094482 seconds\n",
      "Step 15500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1426.4682116508484 seconds\n",
      "Step 15600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1435.9101059436798 seconds\n",
      "Step 15700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1445.0115644931793 seconds\n",
      "Step 15800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1454.3097319602966 seconds\n",
      "Step 15900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1463.3995819091797 seconds\n",
      "Step 16000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1472.4923813343048 seconds\n",
      "Step 16100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1481.8110551834106 seconds\n",
      "Step 16200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1490.908570766449 seconds\n",
      "Step 16300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1500.031858921051 seconds\n",
      "Step 16400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1509.4524128437042 seconds\n",
      "Step 16500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1518.8312311172485 seconds\n",
      "Step 16600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1528.111665725708 seconds\n",
      "Step 16700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1537.1606268882751 seconds\n",
      "Step 16800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1546.2352712154388 seconds\n",
      "Step 16900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1555.4838948249817 seconds\n",
      "Step 17000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1565.1540801525116 seconds\n",
      "Step 17100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1574.2743396759033 seconds\n",
      "Step 17200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1583.5584700107574 seconds\n",
      "Step 17300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1592.6613953113556 seconds\n",
      "Step 17400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1601.7794423103333 seconds\n",
      "Step 17500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1611.0520482063293 seconds\n",
      "Step 17600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1620.1489675045013 seconds\n",
      "Step 17700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1629.2675716876984 seconds\n",
      "Step 17800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1638.5513911247253 seconds\n",
      "Step 17900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1647.6459209918976 seconds\n",
      "Step 18000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1656.7443943023682 seconds\n",
      "Step 18100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1666.0595471858978 seconds\n",
      "Step 18200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1675.1878445148468 seconds\n",
      "Step 18300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1684.3492176532745 seconds\n",
      "Step 18400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1693.6679918766022 seconds\n",
      "Step 18500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1702.8171486854553 seconds\n",
      "Step 18600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1712.6064462661743 seconds\n",
      "Step 18700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1721.9356536865234 seconds\n",
      "Step 18800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1731.0507068634033 seconds\n",
      "Step 18900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1740.1621263027191 seconds\n",
      "Step 19000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1749.420250415802 seconds\n",
      "Step 19100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1758.519344329834 seconds\n",
      "Step 19200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1767.8599636554718 seconds\n",
      "Step 19300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1776.9423022270203 seconds\n",
      "Step 19400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1786.0665917396545 seconds\n",
      "Step 19500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1795.3572268486023 seconds\n",
      "Step 19600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1804.4561748504639 seconds\n",
      "Step 19700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1813.6592807769775 seconds\n",
      "Step 19800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1823.0096461772919 seconds\n",
      "Step 19900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1833.6758358478546 seconds\n",
      "Step 20000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1842.7913105487823 seconds\n",
      "Step 20100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1852.1041691303253 seconds\n",
      "Step 20200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1861.2960736751556 seconds\n",
      "Step 20300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1870.454128742218 seconds\n",
      "Step 20400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1879.756408214569 seconds\n",
      "Step 20500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1888.8696048259735 seconds\n",
      "Step 20600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1897.9926662445068 seconds\n",
      "Step 20700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1907.6723546981812 seconds\n",
      "Step 20800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1916.7837252616882 seconds\n",
      "Step 20900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1925.8723454475403 seconds\n",
      "Step 21000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1935.1481049060822 seconds\n",
      "Step 21100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1944.2407250404358 seconds\n",
      "Step 21200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1953.3464226722717 seconds\n",
      "Step 21300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1962.6135613918304 seconds\n",
      "Step 21400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1971.7336812019348 seconds\n",
      "Step 21500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1980.8454928398132 seconds\n",
      "Step 21600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1990.138364315033 seconds\n",
      "Step 21700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1999.2732203006744 seconds\n",
      "Step 21800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2008.5936176776886 seconds\n",
      "Step 21900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2017.7079663276672 seconds\n",
      "Step 22000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2026.8382709026337 seconds\n",
      "Step 22100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2036.1207475662231 seconds\n",
      "Step 22200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2045.2162356376648 seconds\n",
      "Step 22300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2054.331488132477 seconds\n",
      "Step 22400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2063.637321949005 seconds\n",
      "Step 22500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2072.7441596984863 seconds\n",
      "Step 22600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2081.8787825107574 seconds\n",
      "Step 22700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2091.21590590477 seconds\n",
      "Step 22800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2100.3474996089935 seconds\n",
      "Step 22900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2109.5383477211 seconds\n",
      "Step 23000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2118.8897793293 seconds\n",
      "Step 23100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2128.0498130321503 seconds\n",
      "Step 23200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2137.234037399292 seconds\n",
      "Step 23300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2146.8454084396362 seconds\n",
      "Step 23400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2156.1269042491913 seconds\n",
      "Step 23500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2165.2723178863525 seconds\n",
      "Step 23600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2174.5977697372437 seconds\n",
      "Step 23700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2183.741458415985 seconds\n",
      "Step 23800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2192.8703310489655 seconds\n",
      "Step 23900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2202.3709149360657 seconds\n",
      "Step 24000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2212.028410434723 seconds\n",
      "Step 24100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2221.121917247772 seconds\n",
      "Step 24200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2230.404500722885 seconds\n",
      "Step 24300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2239.51016664505 seconds\n",
      "Step 24400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2248.719653606415 seconds\n",
      "Step 24500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2258.0048620700836 seconds\n",
      "Step 24600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2267.1627900600433 seconds\n",
      "Step 24700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2276.2849678993225 seconds\n",
      "Step 24800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2286.2185332775116 seconds\n",
      "Step 24900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2295.370326757431 seconds\n",
      "Step 25000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2304.7406675815582 seconds\n",
      "Step 25100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2313.9825003147125 seconds\n",
      "Step 25200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2323.068871498108 seconds\n",
      "Step 25300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2332.3772954940796 seconds\n",
      "Step 25400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2341.490723133087 seconds\n",
      "Step 25500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2351.373883962631 seconds\n",
      "Step 25600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2360.6927003860474 seconds\n",
      "Step 25700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2369.8213670253754 seconds\n",
      "Step 25800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2378.9408349990845 seconds\n",
      "Step 25900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2388.4543538093567 seconds\n",
      "Step 26000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2397.590692281723 seconds\n",
      "Step 26100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2406.8166530132294 seconds\n",
      "Step 26200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2416.1884326934814 seconds\n",
      "Step 26300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2425.83588886261 seconds\n",
      "Step 26400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2434.9609811306 seconds\n",
      "Step 26500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2444.285135984421 seconds\n",
      "Step 26600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2453.3882615566254 seconds\n",
      "Step 26700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2463.047080516815 seconds\n",
      "Step 26800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2472.5370948314667 seconds\n",
      "Step 26900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2481.7951810359955 seconds\n",
      "Step 27000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2491.0491077899933 seconds\n",
      "Step 27100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2500.3474793434143 seconds\n",
      "Step 27200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2509.4332888126373 seconds\n",
      "Step 27300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2518.5527312755585 seconds\n",
      "Step 27400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2528.0857026576996 seconds\n",
      "Step 27500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2537.2824580669403 seconds\n",
      "Step 27600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2547.2613344192505 seconds\n",
      "Step 27700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2556.9628715515137 seconds\n",
      "Step 27800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2566.0552344322205 seconds\n",
      "Step 27900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2575.147976398468 seconds\n",
      "Step 28000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2584.4825303554535 seconds\n",
      "Step 28100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2593.5956132411957 seconds\n",
      "Step 28200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2603.229356765747 seconds\n",
      "Step 28300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2612.762449502945 seconds\n",
      "Step 28400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2621.8886952400208 seconds\n",
      "Step 28500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2631.3270058631897 seconds\n",
      "Step 28600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2641.0235266685486 seconds\n",
      "Step 28700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2650.1804168224335 seconds\n",
      "Step 28800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2659.5325338840485 seconds\n",
      "Step 28900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2668.6945865154266 seconds\n",
      "Step 29000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2677.8462343215942 seconds\n",
      "Step 29100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2687.55828332901 seconds\n",
      "Step 29200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2696.737072467804 seconds\n",
      "Step 29300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2705.8613545894623 seconds\n",
      "Step 29400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2715.1564195156097 seconds\n",
      "Step 29500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2724.803920984268 seconds\n",
      "Step 29600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2733.9315025806427 seconds\n",
      "Step 29700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2743.2723710536957 seconds\n",
      "Step 29800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2752.4181323051453 seconds\n",
      "Step 29900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2761.559019088745 seconds\n",
      "Step 30000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2770.8983883857727 seconds\n",
      "Step 30100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2780.010799407959 seconds\n",
      "Step 30200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2789.1369123458862 seconds\n",
      "Step 30300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2798.486927509308 seconds\n",
      "Step 30400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2807.615880012512 seconds\n",
      "Step 30500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2816.763249397278 seconds\n",
      "Step 30600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2826.117427587509 seconds\n",
      "Step 30700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2835.256996154785 seconds\n",
      "Step 30800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2844.427688598633 seconds\n",
      "Step 30900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2853.8192987442017 seconds\n",
      "Step 31000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2862.995297193527 seconds\n",
      "Step 31100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2872.1509721279144 seconds\n",
      "Step 31200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2881.6417100429535 seconds\n",
      "Step 31300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2891.357542514801 seconds\n",
      "Step 31400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2900.48907494545 seconds\n",
      "Step 31500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2909.8892815113068 seconds\n",
      "Step 31600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2919.0425074100494 seconds\n",
      "Step 31700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2928.186062335968 seconds\n",
      "Step 31800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2938.3227796554565 seconds\n",
      "Step 31900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2947.6107835769653 seconds\n",
      "Step 32000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2956.8434932231903 seconds\n",
      "Step 32100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2966.188950061798 seconds\n",
      "Step 32200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2975.350925207138 seconds\n",
      "Step 32300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2985.502986431122 seconds\n",
      "Step 32400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2994.719661474228 seconds\n",
      "Step 32500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3004.0234167575836 seconds\n",
      "Step 32600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3013.400015115738 seconds\n",
      "Step 32700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3022.529332637787 seconds\n",
      "Step 32800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3031.684009075165 seconds\n",
      "Step 32900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3041.931337594986 seconds\n",
      "Step 33000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3051.069309234619 seconds\n",
      "Step 33100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3060.2097771167755 seconds\n",
      "Step 33200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3069.5807740688324 seconds\n",
      "Step 33300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3079.234696149826 seconds\n",
      "Step 33400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3088.3719515800476 seconds\n",
      "Step 33500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3097.728806257248 seconds\n",
      "Step 33600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3106.8742859363556 seconds\n",
      "Step 33700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3116.0254344940186 seconds\n",
      "Step 33800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3125.4049015045166 seconds\n",
      "Step 33900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3134.563132286072 seconds\n",
      "Step 34000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3143.9977807998657 seconds\n",
      "Step 34100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3153.4811220169067 seconds\n",
      "Step 34200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3164.532579421997 seconds\n",
      "Step 34300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3173.674646139145 seconds\n",
      "Step 34400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3183.127102613449 seconds\n",
      "Step 34500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3192.275826215744 seconds\n",
      "Step 34600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3201.4960803985596 seconds\n",
      "Step 34700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3210.8387048244476 seconds\n",
      "Step 34800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3220.5852987766266 seconds\n",
      "Step 34900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3229.7235469818115 seconds\n",
      "Step 35000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3239.0741617679596 seconds\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0248, grad_fn=<SubBackward0>), time elapsed: 0.15448355674743652 seconds\n",
      "Step 100, loss: tensor(0.0176, grad_fn=<SubBackward0>), time elapsed: 9.473515510559082 seconds\n",
      "Step 200, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 18.579792976379395 seconds\n",
      "Step 300, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 28.399324893951416 seconds\n",
      "Step 400, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 37.49053168296814 seconds\n",
      "Step 500, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 46.58261728286743 seconds\n",
      "Step 600, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 55.80810904502869 seconds\n",
      "Step 700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 64.88115525245667 seconds\n",
      "Step 800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 74.1494083404541 seconds\n",
      "Step 900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 83.217369556427 seconds\n",
      "Step 1000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 92.30450463294983 seconds\n",
      "Step 1100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 101.57204246520996 seconds\n",
      "Step 1200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 110.66838812828064 seconds\n",
      "Step 1300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 119.75927639007568 seconds\n",
      "Step 1400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 129.0073139667511 seconds\n",
      "Step 1500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 138.10785627365112 seconds\n",
      "Step 1600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 147.18740677833557 seconds\n",
      "Step 1700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 156.4209852218628 seconds\n",
      "Step 1800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 165.5450723171234 seconds\n",
      "Step 1900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 174.63665056228638 seconds\n",
      "Step 2000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 183.88157510757446 seconds\n",
      "Step 2100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 192.98108911514282 seconds\n",
      "Step 2200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 202.0443000793457 seconds\n",
      "Step 2300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 211.29194045066833 seconds\n",
      "Step 2400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 220.3942539691925 seconds\n",
      "Step 2500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 229.68867087364197 seconds\n",
      "Step 2600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 238.78853297233582 seconds\n",
      "Step 2700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 247.88536715507507 seconds\n",
      "Step 2800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 257.1586320400238 seconds\n",
      "Step 2900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 266.26271748542786 seconds\n",
      "Step 3000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 275.3840456008911 seconds\n",
      "Step 3100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 284.6753988265991 seconds\n",
      "Step 3200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 293.77306056022644 seconds\n",
      "Step 3300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 302.88905119895935 seconds\n",
      "Step 3400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 312.2114312648773 seconds\n",
      "Step 3500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 321.3333568572998 seconds\n",
      "Step 3600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 330.57030034065247 seconds\n",
      "Step 3700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 339.8745503425598 seconds\n",
      "Step 3800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 349.94744420051575 seconds\n",
      "Step 3900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 359.03730058670044 seconds\n",
      "Step 4000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 368.28634786605835 seconds\n",
      "Step 4100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 377.63105869293213 seconds\n",
      "Step 4200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 386.87778544425964 seconds\n",
      "Step 4300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 395.9969289302826 seconds\n",
      "Step 4400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 405.1065731048584 seconds\n",
      "Step 4500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 414.41832542419434 seconds\n",
      "Step 4600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 423.9132356643677 seconds\n",
      "Step 4700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 433.1383607387543 seconds\n",
      "Step 4800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 442.4338312149048 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 451.5931088924408 seconds\n",
      "Step 5000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 461.5225772857666 seconds\n",
      "Step 5100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 470.80759596824646 seconds\n",
      "Step 5200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 479.91028094291687 seconds\n",
      "Step 5300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 489.1472969055176 seconds\n",
      "Step 5400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 498.42720651626587 seconds\n",
      "Step 5500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 508.3601176738739 seconds\n",
      "Step 5600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 517.4811933040619 seconds\n",
      "Step 5700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 526.7411122322083 seconds\n",
      "Step 5800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 535.8727917671204 seconds\n",
      "Step 5900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 545.1693067550659 seconds\n",
      "Step 6000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 554.2714340686798 seconds\n",
      "Step 6100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 563.4075312614441 seconds\n",
      "Step 6200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 572.7238898277283 seconds\n",
      "Step 6300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 583.1610922813416 seconds\n",
      "Step 6400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 592.2777280807495 seconds\n",
      "Step 6500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 601.5361747741699 seconds\n",
      "Step 6600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 610.6312580108643 seconds\n",
      "Step 6700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 619.7770392894745 seconds\n",
      "Step 6800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 629.1640224456787 seconds\n",
      "Step 6900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 638.2878820896149 seconds\n",
      "Step 7000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 647.3859927654266 seconds\n",
      "Step 7100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 656.6843490600586 seconds\n",
      "Step 7200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 666.2812786102295 seconds\n",
      "Step 7300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 675.4543936252594 seconds\n",
      "Step 7400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 685.6343171596527 seconds\n",
      "Step 7500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 694.7587103843689 seconds\n",
      "Step 7600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 703.8394167423248 seconds\n",
      "Step 7700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 713.2065646648407 seconds\n",
      "Step 7800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 722.3470766544342 seconds\n",
      "Step 7900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 731.6287825107574 seconds\n",
      "Step 8000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 740.7487077713013 seconds\n",
      "Step 8100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 749.9590017795563 seconds\n",
      "Step 8200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 760.0469787120819 seconds\n",
      "Step 8300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 769.1908855438232 seconds\n",
      "Step 8400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 778.3011591434479 seconds\n",
      "Step 8500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 787.591982126236 seconds\n",
      "Step 8600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 797.3004493713379 seconds\n",
      "Step 8700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 806.4230062961578 seconds\n",
      "Step 8800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 815.6973783969879 seconds\n",
      "Step 8900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 824.7518632411957 seconds\n",
      "Step 9000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 834.0305109024048 seconds\n",
      "Step 9100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 843.3506445884705 seconds\n",
      "Step 9200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 852.5461945533752 seconds\n",
      "Step 9300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 861.6818792819977 seconds\n",
      "Step 9400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 871.000363111496 seconds\n",
      "Step 9500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 880.1453430652618 seconds\n",
      "Step 9600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 889.7545645236969 seconds\n",
      "Step 9700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 899.0622227191925 seconds\n",
      "Step 9800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 908.2676794528961 seconds\n",
      "Step 9900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 917.5438406467438 seconds\n",
      "Step 10000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 926.6533932685852 seconds\n",
      "Step 10100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 935.7441737651825 seconds\n",
      "Step 10200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 945.6072840690613 seconds\n",
      "Step 10300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 954.7171950340271 seconds\n",
      "Step 10400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 963.9076070785522 seconds\n",
      "Step 10500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 973.1959052085876 seconds\n",
      "Step 10600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 982.3026101589203 seconds\n",
      "Step 10700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 991.4240252971649 seconds\n",
      "Step 10800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1000.7280864715576 seconds\n",
      "Step 10900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1009.8656408786774 seconds\n",
      "Step 11000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1019.0174412727356 seconds\n",
      "Step 11100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1028.2906670570374 seconds\n",
      "Step 11200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1037.4004936218262 seconds\n",
      "Step 11300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1046.5168328285217 seconds\n",
      "Step 11400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1055.7995150089264 seconds\n",
      "Step 11500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1064.967736005783 seconds\n",
      "Step 11600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1074.2845220565796 seconds\n",
      "Step 11700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1083.4195342063904 seconds\n",
      "Step 11800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1092.562992811203 seconds\n",
      "Step 11900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1101.887731552124 seconds\n",
      "Step 12000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1111.0163941383362 seconds\n",
      "Step 12100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1120.1184251308441 seconds\n",
      "Step 12200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1129.3881833553314 seconds\n",
      "Step 12300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1138.5200102329254 seconds\n",
      "Step 12400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1147.8641312122345 seconds\n",
      "Step 12500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1157.3123829364777 seconds\n",
      "Step 12600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1166.6046786308289 seconds\n",
      "Step 12700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1176.355369091034 seconds\n",
      "Step 12800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1185.7446353435516 seconds\n",
      "Step 12900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1194.8722212314606 seconds\n",
      "Step 13000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1204.0335700511932 seconds\n",
      "Step 13100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1213.353212594986 seconds\n",
      "Step 13200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1223.3123416900635 seconds\n",
      "Step 13300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1232.4785614013672 seconds\n",
      "Step 13400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1241.7814083099365 seconds\n",
      "Step 13500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1251.014030456543 seconds\n",
      "Step 13600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1260.2369632720947 seconds\n",
      "Step 13700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1269.5349044799805 seconds\n",
      "Step 13800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1278.6349742412567 seconds\n",
      "Step 13900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1287.7422420978546 seconds\n",
      "Step 14000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1297.640207529068 seconds\n",
      "Step 14100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1306.778032541275 seconds\n",
      "Step 14200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1316.3175718784332 seconds\n",
      "Step 14300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1325.4421110153198 seconds\n",
      "Step 14400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1334.5512745380402 seconds\n",
      "Step 14500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1343.8100152015686 seconds\n",
      "Step 14600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1353.0503571033478 seconds\n",
      "Step 14700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1362.735958814621 seconds\n",
      "Step 14800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1372.0237395763397 seconds\n",
      "Step 14900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1381.1401164531708 seconds\n",
      "Step 15000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1390.2576804161072 seconds\n",
      "Step 15100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1399.8383519649506 seconds\n",
      "Step 15200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1409.8332343101501 seconds\n",
      "Step 15300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1418.9398403167725 seconds\n",
      "Step 15400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1428.259479522705 seconds\n",
      "Step 15500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1437.7557022571564 seconds\n",
      "Step 15600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1446.91259431839 seconds\n",
      "Step 15700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1456.2460548877716 seconds\n",
      "Step 15800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1465.4049234390259 seconds\n",
      "Step 15900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1474.5659217834473 seconds\n",
      "Step 16000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1484.1133892536163 seconds\n",
      "Step 16100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1493.2868468761444 seconds\n",
      "Step 16200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1503.4268887043 seconds\n",
      "Step 16300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1512.7574801445007 seconds\n",
      "Step 16400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1521.870141029358 seconds\n",
      "Step 16500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1531.1772091388702 seconds\n",
      "Step 16600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1540.3563928604126 seconds\n",
      "Step 16700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1549.574134349823 seconds\n",
      "Step 16800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1558.9274339675903 seconds\n",
      "Step 16900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1568.0655670166016 seconds\n",
      "Step 17000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1577.2042121887207 seconds\n",
      "Step 17100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1586.5447630882263 seconds\n",
      "Step 17200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1596.035980463028 seconds\n",
      "Step 17300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1605.227064371109 seconds\n",
      "Step 17400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1614.5712838172913 seconds\n",
      "Step 17500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1623.7418689727783 seconds\n",
      "Step 17600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1632.9155082702637 seconds\n",
      "Step 17700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1642.8506882190704 seconds\n",
      "Step 17800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1651.986007452011 seconds\n",
      "Step 17900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1661.1161713600159 seconds\n",
      "Step 18000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1670.549798488617 seconds\n",
      "Step 18100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1679.6766893863678 seconds\n",
      "Step 18200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1688.8105685710907 seconds\n",
      "Step 18300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1698.151763677597 seconds\n",
      "Step 18400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1708.0014553070068 seconds\n",
      "Step 18500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1717.403042793274 seconds\n",
      "Step 18600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1726.741022348404 seconds\n",
      "Step 18700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1736.1735379695892 seconds\n",
      "Step 18800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1746.2072134017944 seconds\n",
      "Step 18900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1755.5214385986328 seconds\n",
      "Step 19000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1764.63946890831 seconds\n",
      "Step 19100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1774.1695086956024 seconds\n",
      "Step 19200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1783.2920060157776 seconds\n",
      "Step 19300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1792.3999655246735 seconds\n",
      "Step 19400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1801.7046902179718 seconds\n",
      "Step 19500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1811.501425743103 seconds\n",
      "Step 19600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1820.6091022491455 seconds\n",
      "Step 19700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1829.8784537315369 seconds\n",
      "Step 19800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1839.037765264511 seconds\n",
      "Step 19900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1848.1847176551819 seconds\n",
      "Step 20000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1857.512585401535 seconds\n",
      "Step 20100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1866.6498291492462 seconds\n",
      "Step 20200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1875.814080953598 seconds\n",
      "Step 20300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1885.1377511024475 seconds\n",
      "Step 20400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1894.2560391426086 seconds\n",
      "Step 20500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1903.3857078552246 seconds\n",
      "Step 20600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1912.7075107097626 seconds\n",
      "Step 20700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1921.8378472328186 seconds\n",
      "Step 20800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1930.9519000053406 seconds\n",
      "Step 20900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1940.3141152858734 seconds\n",
      "Step 21000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1949.467132806778 seconds\n",
      "Step 21100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1958.5897691249847 seconds\n",
      "Step 21200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1967.931643486023 seconds\n",
      "Step 21300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1977.4471497535706 seconds\n",
      "Step 21400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1986.6583619117737 seconds\n",
      "Step 21500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 1995.9704489707947 seconds\n",
      "Step 21600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2005.177371263504 seconds\n",
      "Step 21700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2015.620658159256 seconds\n",
      "Step 21800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2024.7875180244446 seconds\n",
      "Step 21900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2033.9191324710846 seconds\n",
      "Step 22000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2043.21244931221 seconds\n",
      "Step 22100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2052.3124310970306 seconds\n",
      "Step 22200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2061.4416196346283 seconds\n",
      "Step 22300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2070.755729198456 seconds\n",
      "Step 22400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2079.8601739406586 seconds\n",
      "Step 22500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2088.98334980011 seconds\n",
      "Step 22600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2098.2699263095856 seconds\n",
      "Step 22700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2107.3640818595886 seconds\n",
      "Step 22800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2116.4584896564484 seconds\n",
      "Step 22900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2125.780147075653 seconds\n",
      "Step 23000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2134.9142746925354 seconds\n",
      "Step 23100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2144.0535259246826 seconds\n",
      "Step 23200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2153.396058320999 seconds\n",
      "Step 23300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2162.5341796875 seconds\n",
      "Step 23400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2171.6915016174316 seconds\n",
      "Step 23500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2181.615763902664 seconds\n",
      "Step 23600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2190.701601266861 seconds\n",
      "Step 23700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2199.795789718628 seconds\n",
      "Step 23800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2209.1023228168488 seconds\n",
      "Step 23900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2218.193059206009 seconds\n",
      "Step 24000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2227.2861020565033 seconds\n",
      "Step 24100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2236.8126957416534 seconds\n",
      "Step 24200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2245.9327318668365 seconds\n",
      "Step 24300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2255.076873064041 seconds\n",
      "Step 24400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2264.4257044792175 seconds\n",
      "Step 24500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2273.5795838832855 seconds\n",
      "Step 24600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2283.081088066101 seconds\n",
      "Step 24700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2292.376049518585 seconds\n",
      "Step 24800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2301.4830005168915 seconds\n",
      "Step 24900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2310.7789504528046 seconds\n",
      "Step 25000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2319.881949901581 seconds\n",
      "Step 25100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2329.0073573589325 seconds\n",
      "Step 25200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2338.391505241394 seconds\n",
      "Step 25300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2347.515714406967 seconds\n",
      "Step 25400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2356.7047848701477 seconds\n",
      "Step 25500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2366.049442052841 seconds\n",
      "Step 25600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2375.1939067840576 seconds\n",
      "Step 25700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2384.33997297287 seconds\n",
      "Step 25800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2394.3189375400543 seconds\n",
      "Step 25900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2403.426922559738 seconds\n",
      "Step 26000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2412.549206495285 seconds\n",
      "Step 26100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2421.8892543315887 seconds\n",
      "Step 26200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2431.0137465000153 seconds\n",
      "Step 26300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2440.1371948719025 seconds\n",
      "Step 26400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2449.508610010147 seconds\n",
      "Step 26500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2458.6563608646393 seconds\n",
      "Step 26600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2467.797439813614 seconds\n",
      "Step 26700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2477.1132225990295 seconds\n",
      "Step 26800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2486.9595618247986 seconds\n",
      "Step 26900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2496.064391374588 seconds\n",
      "Step 27000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2505.3585662841797 seconds\n",
      "Step 27100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2514.4587416648865 seconds\n",
      "Step 27200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2523.566896677017 seconds\n",
      "Step 27300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2532.8415117263794 seconds\n",
      "Step 27400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2542.0767312049866 seconds\n",
      "Step 27500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2551.214009284973 seconds\n",
      "Step 27600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2560.5534224510193 seconds\n",
      "Step 27700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2569.690730571747 seconds\n",
      "Step 27800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2578.841372013092 seconds\n",
      "Step 27900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2588.998065471649 seconds\n",
      "Step 28000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2598.1131088733673 seconds\n",
      "Step 28100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2607.262861251831 seconds\n",
      "Step 28200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2616.606127023697 seconds\n",
      "Step 28300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2625.698502779007 seconds\n",
      "Step 28400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2635.01761674881 seconds\n",
      "Step 28500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2644.169104576111 seconds\n",
      "Step 28600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2653.3050212860107 seconds\n",
      "Step 28700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2662.654674768448 seconds\n",
      "Step 28800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2671.7911388874054 seconds\n",
      "Step 28900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2680.920756816864 seconds\n",
      "Step 29000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2690.2574100494385 seconds\n",
      "Step 29100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2699.4012110233307 seconds\n",
      "Step 29200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2708.5331840515137 seconds\n",
      "Step 29300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2717.8763213157654 seconds\n",
      "Step 29400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2726.991244316101 seconds\n",
      "Step 29500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2736.1738193035126 seconds\n",
      "Step 29600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2745.5388021469116 seconds\n",
      "Step 29700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2754.6729321479797 seconds\n",
      "Step 29800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2763.849018096924 seconds\n",
      "Step 29900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2773.9876670837402 seconds\n",
      "Step 30000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2783.1017711162567 seconds\n",
      "Step 30100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2792.2512533664703 seconds\n",
      "Step 30200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2801.5549902915955 seconds\n",
      "Step 30300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2810.6694073677063 seconds\n",
      "Step 30400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2819.822376012802 seconds\n",
      "Step 30500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2829.4437341690063 seconds\n",
      "Step 30600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2838.529050350189 seconds\n",
      "Step 30700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2847.651793718338 seconds\n",
      "Step 30800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2856.980514526367 seconds\n",
      "Step 30900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2866.0981814861298 seconds\n",
      "Step 31000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2875.260683298111 seconds\n",
      "Step 31100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2884.589230775833 seconds\n",
      "Step 31200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2893.716588973999 seconds\n",
      "Step 31300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2902.869459629059 seconds\n",
      "Step 31400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2912.2577364444733 seconds\n",
      "Step 31500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2921.7952485084534 seconds\n",
      "Step 31600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2930.899482488632 seconds\n",
      "Step 31700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2940.2057015895844 seconds\n",
      "Step 31800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2949.3355383872986 seconds\n",
      "Step 31900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2958.434643507004 seconds\n",
      "Step 32000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2967.748001098633 seconds\n",
      "Step 32100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2976.9815270900726 seconds\n",
      "Step 32200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2986.313863992691 seconds\n",
      "Step 32300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2995.437888145447 seconds\n",
      "Step 32400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3004.5899591445923 seconds\n",
      "Step 32500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3014.3982632160187 seconds\n",
      "Step 32600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3023.5190658569336 seconds\n",
      "Step 32700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3032.6707994937897 seconds\n",
      "Step 32800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3042.0508272647858 seconds\n",
      "Step 32900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3051.1772861480713 seconds\n",
      "Step 33000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3060.3217358589172 seconds\n",
      "Step 33100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3069.6853547096252 seconds\n",
      "Step 33200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3078.819077014923 seconds\n",
      "Step 33300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3087.9694492816925 seconds\n",
      "Step 33400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3097.3383576869965 seconds\n",
      "Step 33500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3107.7973222732544 seconds\n",
      "Step 33600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3116.951003074646 seconds\n",
      "Step 33700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3126.3285660743713 seconds\n",
      "Step 33800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3135.4647777080536 seconds\n",
      "Step 33900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3144.5893080234528 seconds\n",
      "Step 34000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3153.891236782074 seconds\n",
      "Step 34100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3163.006531238556 seconds\n",
      "Step 34200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3172.1249952316284 seconds\n",
      "Step 34300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3181.4510345458984 seconds\n",
      "Step 34400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3190.6173434257507 seconds\n",
      "Step 34500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3199.7627437114716 seconds\n",
      "Step 34600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3209.111226797104 seconds\n",
      "Step 34700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3218.3149139881134 seconds\n",
      "Step 34800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3227.503625392914 seconds\n",
      "Step 34900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3236.8923037052155 seconds\n",
      "Step 35000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3246.0425107479095 seconds\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0130, grad_fn=<SubBackward0>), time elapsed: 0.1447436809539795 seconds\n",
      "Step 100, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 9.315183639526367 seconds\n",
      "Step 200, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 19.013249397277832 seconds\n",
      "Step 300, loss: tensor(0.0028, grad_fn=<SubBackward0>), time elapsed: 28.086777925491333 seconds\n",
      "Step 400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 37.167187452316284 seconds\n",
      "Step 500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 46.420597314834595 seconds\n",
      "Step 600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 55.508445262908936 seconds\n",
      "Step 700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 64.7549192905426 seconds\n",
      "Step 800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 73.8625180721283 seconds\n",
      "Step 900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 82.973708152771 seconds\n",
      "Step 1000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 92.23338198661804 seconds\n",
      "Step 1100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 101.33797001838684 seconds\n",
      "Step 1200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 110.4557056427002 seconds\n",
      "Step 1300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 119.69657874107361 seconds\n",
      "Step 1400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 128.76646494865417 seconds\n",
      "Step 1500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 137.86550951004028 seconds\n",
      "Step 1600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 147.0928885936737 seconds\n",
      "Step 1700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 156.1931688785553 seconds\n",
      "Step 1800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 165.31159114837646 seconds\n",
      "Step 1900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 174.56002759933472 seconds\n",
      "Step 2000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 183.66419076919556 seconds\n",
      "Step 2100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 192.77326130867004 seconds\n",
      "Step 2200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 202.05719113349915 seconds\n",
      "Step 2300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 211.16279125213623 seconds\n",
      "Step 2400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 220.40225911140442 seconds\n",
      "Step 2500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 229.48761796951294 seconds\n",
      "Step 2600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 238.59069061279297 seconds\n",
      "Step 2700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 247.87454748153687 seconds\n",
      "Step 2800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 257.00437784194946 seconds\n",
      "Step 2900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 266.128214597702 seconds\n",
      "Step 3000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 275.41359877586365 seconds\n",
      "Step 3100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 284.53262186050415 seconds\n",
      "Step 3200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 293.6727066040039 seconds\n",
      "Step 3300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 302.95233845710754 seconds\n",
      "Step 3400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 312.08381152153015 seconds\n",
      "Step 3500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 321.18459248542786 seconds\n",
      "Step 3600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 330.4430820941925 seconds\n",
      "Step 3700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 339.52682757377625 seconds\n",
      "Step 3800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 349.60519433021545 seconds\n",
      "Step 3900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 358.85629630088806 seconds\n",
      "Step 4000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 367.9641978740692 seconds\n",
      "Step 4100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 377.2472937107086 seconds\n",
      "Step 4200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 386.33097791671753 seconds\n",
      "Step 4300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 395.42997312545776 seconds\n",
      "Step 4400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 404.6924421787262 seconds\n",
      "Step 4500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 413.8155708312988 seconds\n",
      "Step 4600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 422.91346073150635 seconds\n",
      "Step 4700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 432.165593624115 seconds\n",
      "Step 4800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 441.28962802886963 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 450.3788983821869 seconds\n",
      "Step 5000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 459.62419843673706 seconds\n",
      "Step 5100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 468.7380201816559 seconds\n",
      "Step 5200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 477.8210172653198 seconds\n",
      "Step 5300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 487.07885360717773 seconds\n",
      "Step 5400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 496.19304966926575 seconds\n",
      "Step 5500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 505.304181098938 seconds\n",
      "Step 5600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 514.5681326389313 seconds\n",
      "Step 5700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 523.6923549175262 seconds\n",
      "Step 5800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 532.9988975524902 seconds\n",
      "Step 5900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 542.1242189407349 seconds\n",
      "Step 6000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 551.6858155727386 seconds\n",
      "Step 6100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 560.9780604839325 seconds\n",
      "Step 6200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 570.0475113391876 seconds\n",
      "Step 6300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 579.1281952857971 seconds\n",
      "Step 6400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 588.3857612609863 seconds\n",
      "Step 6500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 597.503734588623 seconds\n",
      "Step 6600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 606.6380844116211 seconds\n",
      "Step 6700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 615.8919303417206 seconds\n",
      "Step 6800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 625.0136315822601 seconds\n",
      "Step 6900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 634.1645135879517 seconds\n",
      "Step 7000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 644.0312979221344 seconds\n",
      "Step 7100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 653.1614558696747 seconds\n",
      "Step 7200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 662.2594029903412 seconds\n",
      "Step 7300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 671.538407087326 seconds\n",
      "Step 7400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 680.6239969730377 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 689.7299785614014 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 699.0213875770569 seconds\n",
      "Step 7700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 708.1412608623505 seconds\n",
      "Step 7800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 717.4365746974945 seconds\n",
      "Step 7900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 727.5262730121613 seconds\n",
      "Step 8000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 736.6419751644135 seconds\n",
      "Step 8100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 745.9527180194855 seconds\n",
      "Step 8200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 755.0325968265533 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 764.1044199466705 seconds\n",
      "Step 8400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 773.3596031665802 seconds\n",
      "Step 8500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 782.4539804458618 seconds\n",
      "Step 8600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 791.5312960147858 seconds\n",
      "Step 8700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 800.79381108284 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 809.9006521701813 seconds\n",
      "Step 8900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 819.0046136379242 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 828.295560836792 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 837.4657654762268 seconds\n",
      "Step 9200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 846.589277267456 seconds\n",
      "Step 9300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 855.8812479972839 seconds\n",
      "Step 9400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 864.9770619869232 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 874.2902500629425 seconds\n",
      "Step 9600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 883.3851718902588 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 892.5040280818939 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 901.8121256828308 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 910.9178140163422 seconds\n",
      "Step 10000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 920.04523229599 seconds\n",
      "Step 10100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 929.3413622379303 seconds\n",
      "Step 10200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 938.4386944770813 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 947.5712132453918 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 956.8712787628174 seconds\n",
      "Step 10500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 966.569365978241 seconds\n",
      "Step 10600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 975.6543669700623 seconds\n",
      "Step 10700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 984.9145123958588 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 994.0172407627106 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1003.1176545619965 seconds\n",
      "Step 11000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1012.3578906059265 seconds\n",
      "Step 11100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1021.4858708381653 seconds\n",
      "Step 11200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1030.6592359542847 seconds\n",
      "Step 11300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1039.9439446926117 seconds\n",
      "Step 11400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1049.114366531372 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1058.444257736206 seconds\n",
      "Step 11600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1067.5528111457825 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1076.6560022830963 seconds\n",
      "Step 11800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1087.095311164856 seconds\n",
      "Step 11900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1096.18057346344 seconds\n",
      "Step 12000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1105.272732257843 seconds\n",
      "Step 12100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1114.5593416690826 seconds\n",
      "Step 12200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1123.6566970348358 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1132.7916746139526 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1142.0662472248077 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1151.2262814044952 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1160.3511633872986 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1169.617956161499 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1178.746128320694 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1187.8300218582153 seconds\n",
      "Step 13000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1197.132067680359 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1206.3995785713196 seconds\n",
      "Step 13200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1215.5449197292328 seconds\n",
      "Step 13300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1224.8201434612274 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1233.965702533722 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1243.088538646698 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1252.3778545856476 seconds\n",
      "Step 13700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1261.5828924179077 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1271.4983160495758 seconds\n",
      "Step 13900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1280.5852415561676 seconds\n",
      "Step 14000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1289.684826850891 seconds\n",
      "Step 14100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1298.9813580513 seconds\n",
      "Step 14200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1308.0726873874664 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1317.1951038837433 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1326.498797416687 seconds\n",
      "Step 14500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1335.6272082328796 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1344.7592301368713 seconds\n",
      "Step 14700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1354.0686535835266 seconds\n",
      "Step 14800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1363.1917719841003 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1372.2917249202728 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1381.5813133716583 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1390.7349557876587 seconds\n",
      "Step 15200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1399.8530299663544 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1409.7589609622955 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1418.8919699192047 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1427.9976513385773 seconds\n",
      "Step 15600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1437.2668626308441 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1446.3705551624298 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1455.496619462967 seconds\n",
      "Step 15900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1464.7758615016937 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1473.9352896213531 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1483.0651865005493 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1492.3602583408356 seconds\n",
      "Step 16300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1501.4783613681793 seconds\n",
      "Step 16400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1510.999873638153 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1520.1836063861847 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1530.3096704483032 seconds\n",
      "Step 16700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1539.7683913707733 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1548.8909962177277 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1557.9998829364777 seconds\n",
      "Step 17000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1567.3117322921753 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1576.456285238266 seconds\n",
      "Step 17200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1585.6054921150208 seconds\n",
      "Step 17300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1594.8916730880737 seconds\n",
      "Step 17400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1604.0711863040924 seconds\n",
      "Step 17500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1613.2334458827972 seconds\n",
      "Step 17600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1622.5516338348389 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1631.7052319049835 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1640.8358025550842 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1650.1584630012512 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1659.297593832016 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1668.4444482326508 seconds\n",
      "Step 18200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1677.7359671592712 seconds\n",
      "Step 18300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1686.8819160461426 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1695.9940736293793 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1705.288028717041 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1714.3956248760223 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1723.714545249939 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1732.8070707321167 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1741.9170186519623 seconds\n",
      "Step 19000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1751.2355244159698 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1760.4802286624908 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1769.8624110221863 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1779.1720371246338 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1788.3687181472778 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1797.4744737148285 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1806.8081884384155 seconds\n",
      "Step 19700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1815.9651732444763 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1825.9553689956665 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1835.3709893226624 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1844.495451450348 seconds\n",
      "Step 20100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1853.583765745163 seconds\n",
      "Step 20200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1862.8574810028076 seconds\n",
      "Step 20300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1871.9824302196503 seconds\n",
      "Step 20400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1881.1273987293243 seconds\n",
      "Step 20500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1890.423745393753 seconds\n",
      "Step 20600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1899.541202545166 seconds\n",
      "Step 20700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1908.6642608642578 seconds\n",
      "Step 20800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1917.9458124637604 seconds\n",
      "Step 20900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1927.0558352470398 seconds\n",
      "Step 21000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1936.1867189407349 seconds\n",
      "Step 21100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1946.217516899109 seconds\n",
      "Step 21200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1955.7497291564941 seconds\n",
      "Step 21300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1965.0498230457306 seconds\n",
      "Step 21400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1974.1508116722107 seconds\n",
      "Step 21500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1983.2476451396942 seconds\n",
      "Step 21600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1992.5330097675323 seconds\n",
      "Step 21700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2001.677815914154 seconds\n",
      "Step 21800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2010.7781698703766 seconds\n",
      "Step 21900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2020.0516638755798 seconds\n",
      "Step 22000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2029.1568529605865 seconds\n",
      "Step 22100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2038.2753183841705 seconds\n",
      "Step 22200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2047.5938637256622 seconds\n",
      "Step 22300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2056.703636407852 seconds\n",
      "Step 22400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2065.8494820594788 seconds\n",
      "Step 22500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2075.1871526241302 seconds\n",
      "Step 22600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2084.3475682735443 seconds\n",
      "Step 22700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2093.527279615402 seconds\n",
      "Step 22800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2102.863033056259 seconds\n",
      "Step 22900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2112.0139820575714 seconds\n",
      "Step 23000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2121.164392709732 seconds\n",
      "Step 23100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2130.484147787094 seconds\n",
      "Step 23200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2139.58003282547 seconds\n",
      "Step 23300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2148.732566833496 seconds\n",
      "Step 23400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2158.057604074478 seconds\n",
      "Step 23500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2167.172118663788 seconds\n",
      "Step 23600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2176.2917428016663 seconds\n",
      "Step 23700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2185.612929582596 seconds\n",
      "Step 23800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2194.747142791748 seconds\n",
      "Step 23900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2204.3313472270966 seconds\n",
      "Step 24000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2213.643478870392 seconds\n",
      "Step 24100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2222.9667508602142 seconds\n",
      "Step 24200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2232.0773611068726 seconds\n",
      "Step 24300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2241.396868944168 seconds\n",
      "Step 24400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2251.0031566619873 seconds\n",
      "Step 24500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2260.3199384212494 seconds\n",
      "Step 24600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2269.44677400589 seconds\n",
      "Step 24700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2278.592269182205 seconds\n",
      "Step 24800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2288.3084967136383 seconds\n",
      "Step 24900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2297.4723715782166 seconds\n",
      "Step 25000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2306.5993835926056 seconds\n",
      "Step 25100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2315.937998533249 seconds\n",
      "Step 25200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2325.719930410385 seconds\n",
      "Step 25300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2334.9566407203674 seconds\n",
      "Step 25400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2344.27472281456 seconds\n",
      "Step 25500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2353.41038441658 seconds\n",
      "Step 25600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2363.37712931633 seconds\n",
      "Step 25700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2372.697461605072 seconds\n",
      "Step 25800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2381.79923415184 seconds\n",
      "Step 25900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2391.070725917816 seconds\n",
      "Step 26000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2400.3739700317383 seconds\n",
      "Step 26100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2409.526728630066 seconds\n",
      "Step 26200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2419.211755990982 seconds\n",
      "Step 26300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2428.5082767009735 seconds\n",
      "Step 26400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2437.588185787201 seconds\n",
      "Step 26500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2446.6971197128296 seconds\n",
      "Step 26600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2457.1166894435883 seconds\n",
      "Step 26700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2466.2686681747437 seconds\n",
      "Step 26800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2475.812422990799 seconds\n",
      "Step 26900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2485.791015625 seconds\n",
      "Step 27000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2495.0894260406494 seconds\n",
      "Step 27100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2504.2506351470947 seconds\n",
      "Step 27200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2513.596273422241 seconds\n",
      "Step 27300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2523.092572927475 seconds\n",
      "Step 27400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2532.3189418315887 seconds\n",
      "Step 27500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2541.6326248645782 seconds\n",
      "Step 27600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2550.763086080551 seconds\n",
      "Step 27700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2560.207094192505 seconds\n",
      "Step 27800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2570.0843427181244 seconds\n",
      "Step 27900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2579.223263025284 seconds\n",
      "Step 28000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2588.5614728927612 seconds\n",
      "Step 28100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2597.67046046257 seconds\n",
      "Step 28200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2606.7923500537872 seconds\n",
      "Step 28300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2616.1440947055817 seconds\n",
      "Step 28400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2625.9348497390747 seconds\n",
      "Step 28500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2635.040192127228 seconds\n",
      "Step 28600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2644.3461253643036 seconds\n",
      "Step 28700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2653.6499574184418 seconds\n",
      "Step 28800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2662.7498960494995 seconds\n",
      "Step 28900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2672.0936205387115 seconds\n",
      "Step 29000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2681.230532169342 seconds\n",
      "Step 29100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2690.8516347408295 seconds\n",
      "Step 29200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2700.280809402466 seconds\n",
      "Step 29300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2709.4146099090576 seconds\n",
      "Step 29400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2718.5828545093536 seconds\n",
      "Step 29500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2728.2401559352875 seconds\n",
      "Step 29600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2737.551810503006 seconds\n",
      "Step 29700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2746.6780230998993 seconds\n",
      "Step 29800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2755.997222661972 seconds\n",
      "Step 29900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2765.389930009842 seconds\n",
      "Step 30000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2774.5806560516357 seconds\n",
      "Step 30100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2783.898182630539 seconds\n",
      "Step 30200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2793.856129169464 seconds\n",
      "Step 30300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2802.984342098236 seconds\n",
      "Step 30400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2812.3289453983307 seconds\n",
      "Step 30500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2821.478295326233 seconds\n",
      "Step 30600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2830.628204345703 seconds\n",
      "Step 30700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2839.9510037899017 seconds\n",
      "Step 30800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2849.0808942317963 seconds\n",
      "Step 30900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2858.217117547989 seconds\n",
      "Step 31000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2867.5000460147858 seconds\n",
      "Step 31100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2876.5963406562805 seconds\n",
      "Step 31200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2885.716554880142 seconds\n",
      "Step 31300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2895.0301146507263 seconds\n",
      "Step 31400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2904.1520278453827 seconds\n",
      "Step 31500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2913.296590089798 seconds\n",
      "Step 31600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2923.081226348877 seconds\n",
      "Step 31700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2932.175303697586 seconds\n",
      "Step 31800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2941.47771024704 seconds\n",
      "Step 31900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2950.6106696128845 seconds\n",
      "Step 32000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2959.7403435707092 seconds\n",
      "Step 32100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2969.084563732147 seconds\n",
      "Step 32200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2979.0361771583557 seconds\n",
      "Step 32300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2988.269599199295 seconds\n",
      "Step 32400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2997.5986948013306 seconds\n",
      "Step 32500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3007.545747280121 seconds\n",
      "Step 32600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3016.642212867737 seconds\n",
      "Step 32700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3026.1037373542786 seconds\n",
      "Step 32800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3035.4280219078064 seconds\n",
      "Step 32900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3044.594069480896 seconds\n",
      "Step 33000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3053.9329500198364 seconds\n",
      "Step 33100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3063.0789132118225 seconds\n",
      "Step 33200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3073.051789522171 seconds\n",
      "Step 33300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3082.3971798419952 seconds\n",
      "Step 33400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3091.735716819763 seconds\n",
      "Step 33500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3100.8757045269012 seconds\n",
      "Step 33600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3110.199424266815 seconds\n",
      "Step 33700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3119.3462414741516 seconds\n",
      "Step 33800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3129.3094170093536 seconds\n",
      "Step 33900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3138.713918685913 seconds\n",
      "Step 34000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3147.936847925186 seconds\n",
      "Step 34100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3157.1102423667908 seconds\n",
      "Step 34200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3166.4740970134735 seconds\n",
      "Step 34300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3175.6254255771637 seconds\n",
      "Step 34400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3184.787236213684 seconds\n",
      "Step 34500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3194.831461906433 seconds\n",
      "Step 34600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3203.9655241966248 seconds\n",
      "Step 34700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3213.098771572113 seconds\n",
      "Step 34800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3222.54070019722 seconds\n",
      "Step 34900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3231.7431230545044 seconds\n",
      "Step 35000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3240.914936542511 seconds\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 0.14695453643798828 seconds\n",
      "Step 100, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 10.162761688232422 seconds\n",
      "Step 200, loss: tensor(0.0108, grad_fn=<SubBackward0>), time elapsed: 19.264664888381958 seconds\n",
      "Step 300, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 28.367674350738525 seconds\n",
      "Step 400, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 37.629907846450806 seconds\n",
      "Step 500, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 46.71914577484131 seconds\n",
      "Step 600, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 55.96649885177612 seconds\n",
      "Step 700, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 65.07759547233582 seconds\n",
      "Step 800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 74.17555975914001 seconds\n",
      "Step 900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 83.43799710273743 seconds\n",
      "Step 1000, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 92.52679133415222 seconds\n",
      "Step 1100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 101.66060757637024 seconds\n",
      "Step 1200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 110.9133505821228 seconds\n",
      "Step 1300, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 119.99663925170898 seconds\n",
      "Step 1400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 129.1198103427887 seconds\n",
      "Step 1500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 138.3542366027832 seconds\n",
      "Step 1600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 147.44865727424622 seconds\n",
      "Step 1700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 156.55790495872498 seconds\n",
      "Step 1800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 165.79302954673767 seconds\n",
      "Step 1900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 174.8902609348297 seconds\n",
      "Step 2000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 184.1563482284546 seconds\n",
      "Step 2100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 193.25519847869873 seconds\n",
      "Step 2200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 202.34790301322937 seconds\n",
      "Step 2300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 211.59732866287231 seconds\n",
      "Step 2400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 220.70855474472046 seconds\n",
      "Step 2500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 229.80882716178894 seconds\n",
      "Step 2600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 239.10676527023315 seconds\n",
      "Step 2700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 248.25179862976074 seconds\n",
      "Step 2800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 257.389771938324 seconds\n",
      "Step 2900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 266.66008853912354 seconds\n",
      "Step 3000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 275.7581400871277 seconds\n",
      "Step 3100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 284.8743703365326 seconds\n",
      "Step 3200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 294.15071868896484 seconds\n",
      "Step 3300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 303.24982380867004 seconds\n",
      "Step 3400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 312.3582799434662 seconds\n",
      "Step 3500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 321.648268699646 seconds\n",
      "Step 3600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 330.7437059879303 seconds\n",
      "Step 3700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 340.57234811782837 seconds\n",
      "Step 3800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 349.81940484046936 seconds\n",
      "Step 3900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 358.9775037765503 seconds\n",
      "Step 4000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 368.3391511440277 seconds\n",
      "Step 4100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 377.5628502368927 seconds\n",
      "Step 4200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 386.7074830532074 seconds\n",
      "Step 4300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 396.59476613998413 seconds\n",
      "Step 4400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 405.735942363739 seconds\n",
      "Step 4500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 414.919855594635 seconds\n",
      "Step 4600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 424.17903304100037 seconds\n",
      "Step 4700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 433.3150563240051 seconds\n",
      "Step 4800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 442.45079827308655 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 451.7608368396759 seconds\n",
      "Step 5000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 461.67064666748047 seconds\n",
      "Step 5100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 470.75256729125977 seconds\n",
      "Step 5200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 480.00979137420654 seconds\n",
      "Step 5300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 490.3183870315552 seconds\n",
      "Step 5400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 499.57666969299316 seconds\n",
      "Step 5500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 509.0082266330719 seconds\n",
      "Step 5600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 518.281964302063 seconds\n",
      "Step 5700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 527.7354748249054 seconds\n",
      "Step 5800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 536.9802446365356 seconds\n",
      "Step 5900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 546.2531278133392 seconds\n",
      "Step 6000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 555.7056722640991 seconds\n",
      "Step 6100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 564.9648644924164 seconds\n",
      "Step 6200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 574.2409942150116 seconds\n",
      "Step 6300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 583.6714172363281 seconds\n",
      "Step 6400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 592.9640681743622 seconds\n",
      "Step 6500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 602.2557036876678 seconds\n",
      "Step 6600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 611.695561170578 seconds\n",
      "Step 6700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 621.7390477657318 seconds\n",
      "Step 6800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 631.0151109695435 seconds\n",
      "Step 6900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 640.4539258480072 seconds\n",
      "Step 7000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 649.7674746513367 seconds\n",
      "Step 7100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 659.1185960769653 seconds\n",
      "Step 7200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 668.5823676586151 seconds\n",
      "Step 7300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 677.8892540931702 seconds\n",
      "Step 7400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 687.3347001075745 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 696.781765460968 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 706.8463287353516 seconds\n",
      "Step 7700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 716.2807157039642 seconds\n",
      "Step 7800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 725.5444190502167 seconds\n",
      "Step 7900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 734.8369898796082 seconds\n",
      "Step 8000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 744.4592370986938 seconds\n",
      "Step 8100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 753.7353456020355 seconds\n",
      "Step 8200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 762.991592168808 seconds\n",
      "Step 8300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 772.4461286067963 seconds\n",
      "Step 8400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 781.7365322113037 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 791.7821943759918 seconds\n",
      "Step 8600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 801.2385101318359 seconds\n",
      "Step 8700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 810.5137422084808 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 819.8148372173309 seconds\n",
      "Step 8900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 829.4370226860046 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 838.7275094985962 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 847.9808161258698 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 857.4153623580933 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 866.7248075008392 seconds\n",
      "Step 9400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 876.1887094974518 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 885.4573755264282 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 894.7491915225983 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 904.1711347103119 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 913.4377262592316 seconds\n",
      "Step 9900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 922.7234432697296 seconds\n",
      "Step 10000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 932.215086221695 seconds\n",
      "Step 10100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 941.5558996200562 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 950.8435399532318 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 960.2945239543915 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 969.5749309062958 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 978.8677699565887 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 988.3690621852875 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 997.6438624858856 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1006.8878130912781 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1016.3390665054321 seconds\n",
      "Step 11000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1025.6035430431366 seconds\n",
      "Step 11100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1034.8534007072449 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1044.3350443840027 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1053.6037411689758 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1063.054969549179 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1072.3331706523895 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1081.62051987648 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1091.0813155174255 seconds\n",
      "Step 11800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1100.38241147995 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1109.7335398197174 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1119.2744672298431 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1128.6131160259247 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1137.9287893772125 seconds\n",
      "Step 12300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1147.4561841487885 seconds\n",
      "Step 12400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1156.7897627353668 seconds\n",
      "Step 12500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1166.154286146164 seconds\n",
      "Step 12600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1175.6754565238953 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1185.0254402160645 seconds\n",
      "Step 12800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1194.914622783661 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1204.3794605731964 seconds\n",
      "Step 13000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1213.7597992420197 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1223.1357340812683 seconds\n",
      "Step 13200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1232.604148387909 seconds\n",
      "Step 13300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1242.0261409282684 seconds\n",
      "Step 13400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1251.348070859909 seconds\n",
      "Step 13500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1260.8627071380615 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1270.7592334747314 seconds\n",
      "Step 13700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1280.3158378601074 seconds\n",
      "Step 13800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1289.8411645889282 seconds\n",
      "Step 13900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1299.167359828949 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1308.722890138626 seconds\n",
      "Step 14100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1318.0698781013489 seconds\n",
      "Step 14200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1327.41357254982 seconds\n",
      "Step 14300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1336.9388070106506 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1346.8769629001617 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1356.1738622188568 seconds\n",
      "Step 14600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1365.6596479415894 seconds\n",
      "Step 14700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1375.131757736206 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1384.4154169559479 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1393.7392807006836 seconds\n",
      "Step 15000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1402.895688533783 seconds\n",
      "Step 15100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1412.7136240005493 seconds\n",
      "Step 15200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1422.058752298355 seconds\n",
      "Step 15300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1431.1912634372711 seconds\n",
      "Step 15400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1440.3410925865173 seconds\n",
      "Step 15500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1450.2635502815247 seconds\n",
      "Step 15600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1459.4371147155762 seconds\n",
      "Step 15700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1468.5743038654327 seconds\n",
      "Step 15800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1478.7096772193909 seconds\n",
      "Step 15900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1487.8327271938324 seconds\n",
      "Step 16000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1497.141934633255 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1506.3904068470001 seconds\n",
      "Step 16200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1515.583752155304 seconds\n",
      "Step 16300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1524.9200541973114 seconds\n",
      "Step 16400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1534.0834453105927 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1543.206871509552 seconds\n",
      "Step 16600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1552.8027257919312 seconds\n",
      "Step 16700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1562.006959438324 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1571.1136529445648 seconds\n",
      "Step 16900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1580.4390769004822 seconds\n",
      "Step 17000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1590.1782212257385 seconds\n",
      "Step 17100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1599.5352838039398 seconds\n",
      "Step 17200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1608.8743116855621 seconds\n",
      "Step 17300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1617.9907381534576 seconds\n",
      "Step 17400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1627.1377403736115 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1636.4345326423645 seconds\n",
      "Step 17600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1645.934044122696 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1655.0712747573853 seconds\n",
      "Step 17800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1664.3621757030487 seconds\n",
      "Step 17900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1673.4612519741058 seconds\n",
      "Step 18000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1682.581702709198 seconds\n",
      "Step 18100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1691.9093322753906 seconds\n",
      "Step 18200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1701.0533726215363 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1710.4117090702057 seconds\n",
      "Step 18400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1719.5764741897583 seconds\n",
      "Step 18500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1728.721892118454 seconds\n",
      "Step 18600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1738.029503583908 seconds\n",
      "Step 18700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1747.1700069904327 seconds\n",
      "Step 18800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1756.3274745941162 seconds\n",
      "Step 18900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1765.6403002738953 seconds\n",
      "Step 19000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1775.0584790706635 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1784.202448129654 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1793.536494255066 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1802.6754019260406 seconds\n",
      "Step 19400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1811.833493232727 seconds\n",
      "Step 19500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1821.1726801395416 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1830.2963392734528 seconds\n",
      "Step 19700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1839.8458805084229 seconds\n",
      "Step 19800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1849.228798866272 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1858.3614082336426 seconds\n",
      "Step 20000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1867.5523653030396 seconds\n",
      "Step 20100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1877.5897643566132 seconds\n",
      "Step 20200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1886.7610771656036 seconds\n",
      "Step 20300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1896.0598638057709 seconds\n",
      "Step 20400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1905.3804411888123 seconds\n",
      "Step 20500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1914.5439982414246 seconds\n",
      "Step 20600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1923.6802034378052 seconds\n",
      "Step 20700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1933.032719373703 seconds\n",
      "Step 20800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1942.8214206695557 seconds\n",
      "Step 20900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1952.1669261455536 seconds\n",
      "Step 21000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1961.5241844654083 seconds\n",
      "Step 21100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1970.6618752479553 seconds\n",
      "Step 21200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1979.9758450984955 seconds\n",
      "Step 21300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1989.1351804733276 seconds\n",
      "Step 21400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1998.3204622268677 seconds\n",
      "Step 21500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2008.5158021450043 seconds\n",
      "Step 21600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2017.6743392944336 seconds\n",
      "Step 21700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2026.955750465393 seconds\n",
      "Step 21800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2036.4180545806885 seconds\n",
      "Step 21900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2045.5687634944916 seconds\n",
      "Step 22000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2054.7477643489838 seconds\n",
      "Step 22100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2064.1284551620483 seconds\n",
      "Step 22200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2074.5405275821686 seconds\n",
      "Step 22300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2083.728658437729 seconds\n",
      "Step 22400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2093.1058263778687 seconds\n",
      "Step 22500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2102.2821247577667 seconds\n",
      "Step 22600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2111.4581117630005 seconds\n",
      "Step 22700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2120.8011758327484 seconds\n",
      "Step 22800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2129.9523808956146 seconds\n",
      "Step 22900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2139.0705695152283 seconds\n",
      "Step 23000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2148.3927521705627 seconds\n",
      "Step 23100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2157.575172662735 seconds\n",
      "Step 23200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2166.708000898361 seconds\n",
      "Step 23300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2176.1353318691254 seconds\n",
      "Step 23400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2185.2369520664215 seconds\n",
      "Step 23500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2194.372594833374 seconds\n",
      "Step 23600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2204.271376132965 seconds\n",
      "Step 23700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2213.442672729492 seconds\n",
      "Step 23800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2222.7732100486755 seconds\n",
      "Step 23900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2231.8644828796387 seconds\n",
      "Step 24000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2240.99746799469 seconds\n",
      "Step 24100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2250.4639980793 seconds\n",
      "Step 24200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2259.587628364563 seconds\n",
      "Step 24300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2268.7072942256927 seconds\n",
      "Step 24400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2278.3709049224854 seconds\n",
      "Step 24500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2287.913563489914 seconds\n",
      "Step 24600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2297.0278079509735 seconds\n",
      "Step 24700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2306.353853225708 seconds\n",
      "Step 24800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2315.464132785797 seconds\n",
      "Step 24900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2324.607343196869 seconds\n",
      "Step 25000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2333.968503713608 seconds\n",
      "Step 25100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2343.96347284317 seconds\n",
      "Step 25200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2353.1137142181396 seconds\n",
      "Step 25300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2362.5945949554443 seconds\n",
      "Step 25400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2371.9189398288727 seconds\n",
      "Step 25500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2381.039526939392 seconds\n",
      "Step 25600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2390.4001021385193 seconds\n",
      "Step 25700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2399.891130924225 seconds\n",
      "Step 25800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2409.040450811386 seconds\n",
      "Step 25900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2418.3826155662537 seconds\n",
      "Step 26000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2427.4832112789154 seconds\n",
      "Step 26100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2436.610385656357 seconds\n",
      "Step 26200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2445.9000146389008 seconds\n",
      "Step 26300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2455.030745983124 seconds\n",
      "Step 26400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2464.1671090126038 seconds\n",
      "Step 26500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2473.52689576149 seconds\n",
      "Step 26600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2482.6884570121765 seconds\n",
      "Step 26700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2491.7903554439545 seconds\n",
      "Step 26800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2501.106453180313 seconds\n",
      "Step 26900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2510.202684879303 seconds\n",
      "Step 27000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2519.306875228882 seconds\n",
      "Step 27100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2528.623148202896 seconds\n",
      "Step 27200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2537.76665019989 seconds\n",
      "Step 27300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2547.143524169922 seconds\n",
      "Step 27400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2556.2785906791687 seconds\n",
      "Step 27500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2565.3970425128937 seconds\n",
      "Step 27600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2574.7615699768066 seconds\n",
      "Step 27700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2584.403820991516 seconds\n",
      "Step 27800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2593.6021225452423 seconds\n",
      "Step 27900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2602.94885969162 seconds\n",
      "Step 28000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2612.066160440445 seconds\n",
      "Step 28100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2621.1789362430573 seconds\n",
      "Step 28200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2630.510509967804 seconds\n",
      "Step 28300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2639.632315158844 seconds\n",
      "Step 28400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2648.855923652649 seconds\n",
      "Step 28500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2658.190309047699 seconds\n",
      "Step 28600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2667.3724143505096 seconds\n",
      "Step 28700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2676.533714532852 seconds\n",
      "Step 28800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2685.9195280075073 seconds\n",
      "Step 28900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2696.2199382781982 seconds\n",
      "Step 29000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2705.324137210846 seconds\n",
      "Step 29100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2714.6520957946777 seconds\n",
      "Step 29200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2723.76038813591 seconds\n",
      "Step 29300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2732.856727361679 seconds\n",
      "Step 29400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2742.1291790008545 seconds\n",
      "Step 29500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2751.2240014076233 seconds\n",
      "Step 29600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2760.349029779434 seconds\n",
      "Step 29700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2769.6201417446136 seconds\n",
      "Step 29800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2778.7184092998505 seconds\n",
      "Step 29900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2787.858333349228 seconds\n",
      "Step 30000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2797.1691806316376 seconds\n",
      "Step 30100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2806.285967350006 seconds\n",
      "Step 30200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2815.424882411957 seconds\n",
      "Step 30300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2824.7774138450623 seconds\n",
      "Step 30400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2833.9051535129547 seconds\n",
      "Step 30500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2843.0421001911163 seconds\n",
      "Step 30600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2853.0182926654816 seconds\n",
      "Step 30700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2862.144815683365 seconds\n",
      "Step 30800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2871.450327396393 seconds\n",
      "Step 30900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2880.585545539856 seconds\n",
      "Step 31000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2889.699125289917 seconds\n",
      "Step 31100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2898.992970943451 seconds\n",
      "Step 31200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2908.0955786705017 seconds\n",
      "Step 31300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2917.2323410511017 seconds\n",
      "Step 31400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2926.559977054596 seconds\n",
      "Step 31500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2935.69247674942 seconds\n",
      "Step 31600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2944.845366001129 seconds\n",
      "Step 31700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2954.6150329113007 seconds\n",
      "Step 31800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2963.723954677582 seconds\n",
      "Step 31900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2972.860226392746 seconds\n",
      "Step 32000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2982.1384365558624 seconds\n",
      "Step 32100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2991.241389989853 seconds\n",
      "Step 32200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3000.373076438904 seconds\n",
      "Step 32300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3010.6121106147766 seconds\n",
      "Step 32400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3019.7085127830505 seconds\n",
      "Step 32500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3028.815618276596 seconds\n",
      "Step 32600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3038.105133295059 seconds\n",
      "Step 32700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3047.2097568511963 seconds\n",
      "Step 32800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3056.312265396118 seconds\n",
      "Step 32900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3065.672512769699 seconds\n",
      "Step 33000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3074.862905025482 seconds\n",
      "Step 33100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3083.981820821762 seconds\n",
      "Step 33200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3093.311531305313 seconds\n",
      "Step 33300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3102.4593274593353 seconds\n",
      "Step 33400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3112.786619901657 seconds\n",
      "Step 33500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3122.1432929039 seconds\n",
      "Step 33600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3131.25453209877 seconds\n",
      "Step 33700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3140.3756473064423 seconds\n",
      "Step 33800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3149.7291867733 seconds\n",
      "Step 33900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3158.8655292987823 seconds\n",
      "Step 34000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3168.002368450165 seconds\n",
      "Step 34100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3177.312841653824 seconds\n",
      "Step 34200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3186.448033094406 seconds\n",
      "Step 34300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3195.5659182071686 seconds\n",
      "Step 34400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3204.869538784027 seconds\n",
      "Step 34500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3214.025077819824 seconds\n",
      "Step 34600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3223.175344467163 seconds\n",
      "Step 34700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3232.528482198715 seconds\n",
      "Step 34800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3241.6799166202545 seconds\n",
      "Step 34900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3250.83855175972 seconds\n",
      "Step 35000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3260.189649105072 seconds\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 0.1092076301574707 seconds\n",
      "Step 100, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 9.357279300689697 seconds\n",
      "Step 200, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 18.924135446548462 seconds\n",
      "Step 300, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 28.014943838119507 seconds\n",
      "Step 400, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 37.107173442840576 seconds\n",
      "Step 500, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 46.34972333908081 seconds\n",
      "Step 600, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 55.46283578872681 seconds\n",
      "Step 700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 64.5587956905365 seconds\n",
      "Step 800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 73.77316308021545 seconds\n",
      "Step 900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 82.8527443408966 seconds\n",
      "Step 1000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 91.90790438652039 seconds\n",
      "Step 1100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 101.15775871276855 seconds\n",
      "Step 1200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 110.24887084960938 seconds\n",
      "Step 1300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 119.32743000984192 seconds\n",
      "Step 1400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 128.55556964874268 seconds\n",
      "Step 1500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 137.65838956832886 seconds\n",
      "Step 1600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 146.9363808631897 seconds\n",
      "Step 1700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 156.0154504776001 seconds\n",
      "Step 1800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 165.12969851493835 seconds\n",
      "Step 1900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 174.4252917766571 seconds\n",
      "Step 2000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 183.51327896118164 seconds\n",
      "Step 2100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 192.62735652923584 seconds\n",
      "Step 2200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 201.90907979011536 seconds\n",
      "Step 2300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 210.9926884174347 seconds\n",
      "Step 2400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 220.10033082962036 seconds\n",
      "Step 2500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 229.36547899246216 seconds\n",
      "Step 2600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 238.4907522201538 seconds\n",
      "Step 2700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 247.58381509780884 seconds\n",
      "Step 2800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 256.8315305709839 seconds\n",
      "Step 2900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 265.9574704170227 seconds\n",
      "Step 3000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 275.04421877861023 seconds\n",
      "Step 3100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 284.30544996261597 seconds\n",
      "Step 3200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 293.4271605014801 seconds\n",
      "Step 3300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 302.69504404067993 seconds\n",
      "Step 3400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 311.77880859375 seconds\n",
      "Step 3500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 320.8870224952698 seconds\n",
      "Step 3600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 330.16126799583435 seconds\n",
      "Step 3700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 339.26241731643677 seconds\n",
      "Step 3800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 348.39088129997253 seconds\n",
      "Step 3900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 357.6844720840454 seconds\n",
      "Step 4000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 366.7947647571564 seconds\n",
      "Step 4100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 375.880254983902 seconds\n",
      "Step 4200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 385.16386556625366 seconds\n",
      "Step 4300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 394.24899888038635 seconds\n",
      "Step 4400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 403.3652858734131 seconds\n",
      "Step 4500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 412.66010093688965 seconds\n",
      "Step 4600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 421.7930715084076 seconds\n",
      "Step 4700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 430.91507148742676 seconds\n",
      "Step 4800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 440.21032786369324 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 449.36989092826843 seconds\n",
      "Step 5000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 458.67907071113586 seconds\n",
      "Step 5100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 467.7994921207428 seconds\n",
      "Step 5200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 477.20470118522644 seconds\n",
      "Step 5300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 486.53832030296326 seconds\n",
      "Step 5400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 495.6512861251831 seconds\n",
      "Step 5500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 504.7305426597595 seconds\n",
      "Step 5600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 513.9884445667267 seconds\n",
      "Step 5700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 523.1528055667877 seconds\n",
      "Step 5800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 532.2668421268463 seconds\n",
      "Step 5900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 542.124330997467 seconds\n",
      "Step 6000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 551.2421061992645 seconds\n",
      "Step 6100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 560.3620319366455 seconds\n",
      "Step 6200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 569.654009103775 seconds\n",
      "Step 6300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 578.7478621006012 seconds\n",
      "Step 6400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 587.8788986206055 seconds\n",
      "Step 6500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 597.236447095871 seconds\n",
      "Step 6600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 606.3375039100647 seconds\n",
      "Step 6700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 615.4225759506226 seconds\n",
      "Step 6800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 624.690248966217 seconds\n",
      "Step 6900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 633.7937054634094 seconds\n",
      "Step 7000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 643.0863058567047 seconds\n",
      "Step 7100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 653.0432405471802 seconds\n",
      "Step 7200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 662.1710357666016 seconds\n",
      "Step 7300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 671.4364264011383 seconds\n",
      "Step 7400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 680.5269231796265 seconds\n",
      "Step 7500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 689.656590461731 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 698.9146466255188 seconds\n",
      "Step 7700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 708.0128390789032 seconds\n",
      "Step 7800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 717.1101207733154 seconds\n",
      "Step 7900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 726.3837375640869 seconds\n",
      "Step 8000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 735.4806311130524 seconds\n",
      "Step 8100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 744.5993230342865 seconds\n",
      "Step 8200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 753.9138705730438 seconds\n",
      "Step 8300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 763.6486992835999 seconds\n",
      "Step 8400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 772.7592384815216 seconds\n",
      "Step 8500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 782.0625236034393 seconds\n",
      "Step 8600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 791.1647644042969 seconds\n",
      "Step 8700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 800.4224274158478 seconds\n",
      "Step 8800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 809.5067284107208 seconds\n",
      "Step 8900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 818.6220071315765 seconds\n",
      "Step 9000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 827.8937859535217 seconds\n",
      "Step 9100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 836.972062587738 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 846.0694200992584 seconds\n",
      "Step 9300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 855.3362076282501 seconds\n",
      "Step 9400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 864.4545323848724 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 873.5875782966614 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 882.9158110618591 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 892.2728495597839 seconds\n",
      "Step 9800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 901.461434841156 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 910.7500083446503 seconds\n",
      "Step 10000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 919.8537724018097 seconds\n",
      "Step 10100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 928.9373207092285 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 938.2302784919739 seconds\n",
      "Step 10300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 948.2501473426819 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 957.3953506946564 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 966.6943633556366 seconds\n",
      "Step 10600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 975.7704734802246 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 985.0368940830231 seconds\n",
      "Step 10800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 994.0988392829895 seconds\n",
      "Step 10900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1003.2055132389069 seconds\n",
      "Step 11000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1012.4676198959351 seconds\n",
      "Step 11100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1021.5794222354889 seconds\n",
      "Step 11200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1030.7010011672974 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1039.9655592441559 seconds\n",
      "Step 11400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1049.070304632187 seconds\n",
      "Step 11500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1058.212583065033 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1067.5058062076569 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1076.6212983131409 seconds\n",
      "Step 11800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1085.7338955402374 seconds\n",
      "Step 11900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1095.0395340919495 seconds\n",
      "Step 12000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1104.1797823905945 seconds\n",
      "Step 12100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1113.304127216339 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1122.608366727829 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1131.7257738113403 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1140.8497698307037 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1150.151049375534 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1159.2860763072968 seconds\n",
      "Step 12700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1168.6087894439697 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1177.6966423988342 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1186.8176090717316 seconds\n",
      "Step 13000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1196.126813173294 seconds\n",
      "Step 13100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1205.2554776668549 seconds\n",
      "Step 13200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1214.9658060073853 seconds\n",
      "Step 13300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1224.2537305355072 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1233.3463790416718 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1242.4590990543365 seconds\n",
      "Step 13600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1251.7018308639526 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1260.789894580841 seconds\n",
      "Step 13800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1269.8570969104767 seconds\n",
      "Step 13900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1279.1174619197845 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1288.2287242412567 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1297.3551452159882 seconds\n",
      "Step 14200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1306.646378993988 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1315.7919614315033 seconds\n",
      "Step 14400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1324.946841955185 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1334.3689403533936 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1344.4478015899658 seconds\n",
      "Step 14700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1353.5374448299408 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1362.8371307849884 seconds\n",
      "Step 14900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1371.9451444149017 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1381.2213411331177 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1390.3062312602997 seconds\n",
      "Step 15200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1399.4396646022797 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1408.7104465961456 seconds\n",
      "Step 15400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1417.805989265442 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1426.9450979232788 seconds\n",
      "Step 15600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1436.2243371009827 seconds\n",
      "Step 15700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1445.286643743515 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1454.4705255031586 seconds\n",
      "Step 15900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1463.7785968780518 seconds\n",
      "Step 16000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1472.8988010883331 seconds\n",
      "Step 16100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1482.0022888183594 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1491.909071445465 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1501.0357353687286 seconds\n",
      "Step 16400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1510.1308510303497 seconds\n",
      "Step 16500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1519.4294919967651 seconds\n",
      "Step 16600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1528.5321762561798 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1537.6278491020203 seconds\n",
      "Step 16800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1546.936710357666 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1556.0427646636963 seconds\n",
      "Step 17000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1565.1466076374054 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1574.4815373420715 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1583.6273291110992 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1592.754879951477 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1602.0698244571686 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1611.2195649147034 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1621.0142381191254 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1630.8800239562988 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1639.9996614456177 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1649.2738962173462 seconds\n",
      "Step 18000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1658.3596975803375 seconds\n",
      "Step 18100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1667.4667484760284 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1676.7342133522034 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1685.8232266902924 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1694.9346385002136 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1704.2317821979523 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1713.3504045009613 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1722.4784915447235 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1731.7897334098816 seconds\n",
      "Step 18900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1740.8771271705627 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1749.9699921607971 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1759.2517232894897 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1768.345591545105 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1777.4327945709229 seconds\n",
      "Step 19400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1786.7064807415009 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1795.8615124225616 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1804.9769711494446 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1814.2803633213043 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1823.436193704605 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1832.5820829868317 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1841.9288864135742 seconds\n",
      "Step 20100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1851.0873413085938 seconds\n",
      "Step 20200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1860.4024925231934 seconds\n",
      "Step 20300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1869.4957044124603 seconds\n",
      "Step 20400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1878.6217908859253 seconds\n",
      "Step 20500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1887.9564254283905 seconds\n",
      "Step 20600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1897.0829439163208 seconds\n",
      "Step 20700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1906.2129442691803 seconds\n",
      "Step 20800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1915.540339231491 seconds\n",
      "Step 20900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1924.6740102767944 seconds\n",
      "Step 21000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1933.8333435058594 seconds\n",
      "Step 21100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1943.2101168632507 seconds\n",
      "Step 21200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1952.3501963615417 seconds\n",
      "Step 21300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1961.535499572754 seconds\n",
      "Step 21400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1971.953851699829 seconds\n",
      "Step 21500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1981.0918893814087 seconds\n",
      "Step 21600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1990.2084746360779 seconds\n",
      "Step 21700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1999.5071573257446 seconds\n",
      "Step 21800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2008.6718633174896 seconds\n",
      "Step 21900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2017.811908006668 seconds\n",
      "Step 22000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2027.1231770515442 seconds\n",
      "Step 22100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2036.2753598690033 seconds\n",
      "Step 22200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2045.4196734428406 seconds\n",
      "Step 22300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2054.7301921844482 seconds\n",
      "Step 22400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2063.8630969524384 seconds\n",
      "Step 22500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2073.060964822769 seconds\n",
      "Step 22600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2082.3740565776825 seconds\n",
      "Step 22700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2091.5218851566315 seconds\n",
      "Step 22800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2100.907168149948 seconds\n",
      "Step 22900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2110.0324323177338 seconds\n",
      "Step 23000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2119.1801064014435 seconds\n",
      "Step 23100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2128.563141345978 seconds\n",
      "Step 23200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2138.442597389221 seconds\n",
      "Step 23300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2147.5675847530365 seconds\n",
      "Step 23400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2156.929555416107 seconds\n",
      "Step 23500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2166.048988342285 seconds\n",
      "Step 23600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2175.196977853775 seconds\n",
      "Step 23700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2184.652384996414 seconds\n",
      "Step 23800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2193.7797706127167 seconds\n",
      "Step 23900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2202.915074110031 seconds\n",
      "Step 24000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2212.227337360382 seconds\n",
      "Step 24100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2222.017546892166 seconds\n",
      "Step 24200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2231.127969264984 seconds\n",
      "Step 24300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2240.4345581531525 seconds\n",
      "Step 24400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2249.7190492153168 seconds\n",
      "Step 24500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2258.8728284835815 seconds\n",
      "Step 24600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2268.1779754161835 seconds\n",
      "Step 24700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2277.2918853759766 seconds\n",
      "Step 24800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2286.913437128067 seconds\n",
      "Step 24900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2296.253021001816 seconds\n",
      "Step 25000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2305.5874211788177 seconds\n",
      "Step 25100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2314.7358984947205 seconds\n",
      "Step 25200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2324.071334838867 seconds\n",
      "Step 25300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2333.188726425171 seconds\n",
      "Step 25400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2342.642731666565 seconds\n",
      "Step 25500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2352.5557055473328 seconds\n",
      "Step 25600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2361.665382862091 seconds\n",
      "Step 25700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2371.00324010849 seconds\n",
      "Step 25800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2380.128797531128 seconds\n",
      "Step 25900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2389.278914451599 seconds\n",
      "Step 26000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2399.508301258087 seconds\n",
      "Step 26100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2408.6356959342957 seconds\n",
      "Step 26200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2417.7483472824097 seconds\n",
      "Step 26300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2427.0463049411774 seconds\n",
      "Step 26400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2436.1617465019226 seconds\n",
      "Step 26500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2445.279866695404 seconds\n",
      "Step 26600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2454.6040234565735 seconds\n",
      "Step 26700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2463.7329993247986 seconds\n",
      "Step 26800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2472.8876049518585 seconds\n",
      "Step 26900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2482.207177877426 seconds\n",
      "Step 27000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2491.3350098133087 seconds\n",
      "Step 27100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2500.464054584503 seconds\n",
      "Step 27200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2509.796637058258 seconds\n",
      "Step 27300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2518.9012784957886 seconds\n",
      "Step 27400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2528.038385629654 seconds\n",
      "Step 27500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2538.1591601371765 seconds\n",
      "Step 27600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2547.27370429039 seconds\n",
      "Step 27700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2556.400309562683 seconds\n",
      "Step 27800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2565.730903863907 seconds\n",
      "Step 27900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2574.8905878067017 seconds\n",
      "Step 28000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2584.048620223999 seconds\n",
      "Step 28100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2593.4244661331177 seconds\n",
      "Step 28200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2602.5990555286407 seconds\n",
      "Step 28300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2612.8220508098602 seconds\n",
      "Step 28400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2622.248955488205 seconds\n",
      "Step 28500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2631.3839473724365 seconds\n",
      "Step 28600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2640.521192073822 seconds\n",
      "Step 28700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2649.8661935329437 seconds\n",
      "Step 28800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2659.0492775440216 seconds\n",
      "Step 28900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2668.2445225715637 seconds\n",
      "Step 29000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2677.574371814728 seconds\n",
      "Step 29100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2686.6823394298553 seconds\n",
      "Step 29200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2696.099452495575 seconds\n",
      "Step 29300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2705.9775240421295 seconds\n",
      "Step 29400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2715.0852954387665 seconds\n",
      "Step 29500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2724.396109342575 seconds\n",
      "Step 29600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2733.4961819648743 seconds\n",
      "Step 29700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2742.607218503952 seconds\n",
      "Step 29800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2751.9504873752594 seconds\n",
      "Step 29900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2761.0877730846405 seconds\n",
      "Step 30000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2770.250082731247 seconds\n",
      "Step 30100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2780.3253247737885 seconds\n",
      "Step 30200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2789.6332125663757 seconds\n",
      "Step 30300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2798.745316505432 seconds\n",
      "Step 30400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2808.0571427345276 seconds\n",
      "Step 30500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2817.1754438877106 seconds\n",
      "Step 30600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2827.108386993408 seconds\n",
      "Step 30700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2836.4288651943207 seconds\n",
      "Step 30800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2845.640844345093 seconds\n",
      "Step 30900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2854.8454020023346 seconds\n",
      "Step 31000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2864.1869390010834 seconds\n",
      "Step 31100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2873.3166711330414 seconds\n",
      "Step 31200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2882.4599623680115 seconds\n",
      "Step 31300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2892.802798986435 seconds\n",
      "Step 31400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2901.90456700325 seconds\n",
      "Step 31500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2911.0213656425476 seconds\n",
      "Step 31600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2920.3533928394318 seconds\n",
      "Step 31700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2929.5561258792877 seconds\n",
      "Step 31800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2938.7704133987427 seconds\n",
      "Step 31900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2948.1070427894592 seconds\n",
      "Step 32000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2957.250460624695 seconds\n",
      "Step 32100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2966.956328392029 seconds\n",
      "Step 32200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2976.2760813236237 seconds\n",
      "Step 32300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2985.413573026657 seconds\n",
      "Step 32400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 2994.537378311157 seconds\n",
      "Step 32500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3003.8755688667297 seconds\n",
      "Step 32600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3013.4245476722717 seconds\n",
      "Step 32700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3022.6363139152527 seconds\n",
      "Step 32800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3031.9573192596436 seconds\n",
      "Step 32900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3041.090507745743 seconds\n",
      "Step 33000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3051.0647959709167 seconds\n",
      "Step 33100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3060.3994567394257 seconds\n",
      "Step 33200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3069.5252561569214 seconds\n",
      "Step 33300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3078.924150943756 seconds\n",
      "Step 33400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3088.2022716999054 seconds\n",
      "Step 33500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3097.334160089493 seconds\n",
      "Step 33600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3106.7066144943237 seconds\n",
      "Step 33700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3115.825540781021 seconds\n",
      "Step 33800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3125.645540714264 seconds\n",
      "Step 33900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3135.127332687378 seconds\n",
      "Step 34000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3144.314510822296 seconds\n",
      "Step 34100, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3153.403584241867 seconds\n",
      "Step 34200, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3162.735202550888 seconds\n",
      "Step 34300, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3171.871596097946 seconds\n",
      "Step 34400, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3181.69345331192 seconds\n",
      "Step 34500, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3191.1481103897095 seconds\n",
      "Step 34600, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3200.3326292037964 seconds\n",
      "Step 34700, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3209.54456281662 seconds\n",
      "Step 34800, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3220.9991915225983 seconds\n",
      "Step 34900, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3230.2068860530853 seconds\n",
      "Step 35000, loss: tensor(0.0002, grad_fn=<SubBackward0>), time elapsed: 3239.476802110672 seconds\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0136, grad_fn=<SubBackward0>), time elapsed: 0.15081548690795898 seconds\n",
      "Step 100, loss: tensor(0.0089, grad_fn=<SubBackward0>), time elapsed: 10.08132266998291 seconds\n",
      "Step 200, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 19.388700246810913 seconds\n",
      "Step 300, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 28.58782649040222 seconds\n",
      "Step 400, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 37.81959819793701 seconds\n",
      "Step 500, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 46.90515398979187 seconds\n",
      "Step 600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 56.015204668045044 seconds\n",
      "Step 700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 65.26955771446228 seconds\n",
      "Step 800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 74.37045693397522 seconds\n",
      "Step 900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 83.6406557559967 seconds\n",
      "Step 1000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 92.89055275917053 seconds\n",
      "Step 1100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 101.984792470932 seconds\n",
      "Step 1200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 111.23350214958191 seconds\n",
      "Step 1300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 120.33086967468262 seconds\n",
      "Step 1400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 129.4165267944336 seconds\n",
      "Step 1500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 138.63920521736145 seconds\n",
      "Step 1600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 147.71860671043396 seconds\n",
      "Step 1700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 156.7971420288086 seconds\n",
      "Step 1800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 166.0264356136322 seconds\n",
      "Step 1900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 175.12105703353882 seconds\n",
      "Step 2000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 184.21580052375793 seconds\n",
      "Step 2100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 193.4749846458435 seconds\n",
      "Step 2200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 202.5786485671997 seconds\n",
      "Step 2300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 211.71768712997437 seconds\n",
      "Step 2400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 220.96927118301392 seconds\n",
      "Step 2500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 230.07423448562622 seconds\n",
      "Step 2600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 239.39307403564453 seconds\n",
      "Step 2700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 248.4952735900879 seconds\n",
      "Step 2800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 257.6142225265503 seconds\n",
      "Step 2900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 266.8997802734375 seconds\n",
      "Step 3000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 276.0538866519928 seconds\n",
      "Step 3100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 285.17359685897827 seconds\n",
      "Step 3200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 294.459255695343 seconds\n",
      "Step 3300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 303.59224915504456 seconds\n",
      "Step 3400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 312.71755504608154 seconds\n",
      "Step 3500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 322.02490305900574 seconds\n",
      "Step 3600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 331.15530157089233 seconds\n",
      "Step 3700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 340.28821873664856 seconds\n",
      "Step 3800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 349.6133363246918 seconds\n",
      "Step 3900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 359.90233874320984 seconds\n",
      "Step 4000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 368.993843793869 seconds\n",
      "Step 4100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 378.26465582847595 seconds\n",
      "Step 4200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 387.3667845726013 seconds\n",
      "Step 4300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 396.63616156578064 seconds\n",
      "Step 4400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 405.7068500518799 seconds\n",
      "Step 4500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 414.8659589290619 seconds\n",
      "Step 4600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 424.10996317863464 seconds\n",
      "Step 4700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 433.1892101764679 seconds\n",
      "Step 4800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 442.2665832042694 seconds\n",
      "Step 4900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 451.9517607688904 seconds\n",
      "Step 5000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 461.12500166893005 seconds\n",
      "Step 5100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 470.273432970047 seconds\n",
      "Step 5200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 479.53564977645874 seconds\n",
      "Step 5300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 488.67598962783813 seconds\n",
      "Step 5400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 497.8156359195709 seconds\n",
      "Step 5500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 507.7078905105591 seconds\n",
      "Step 5600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 516.8466939926147 seconds\n",
      "Step 5700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 526.2228109836578 seconds\n",
      "Step 5800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 535.5068628787994 seconds\n",
      "Step 5900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 544.6597383022308 seconds\n",
      "Step 6000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 553.9659209251404 seconds\n",
      "Step 6100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 563.0890581607819 seconds\n",
      "Step 6200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 572.2362525463104 seconds\n",
      "Step 6300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 582.1366353034973 seconds\n",
      "Step 6400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 591.2384135723114 seconds\n",
      "Step 6500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 600.512681722641 seconds\n",
      "Step 6600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 609.8214139938354 seconds\n",
      "Step 6700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 618.9380550384521 seconds\n",
      "Step 6800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 628.0829691886902 seconds\n",
      "Step 6900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 637.8500545024872 seconds\n",
      "Step 7000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 647.0289947986603 seconds\n",
      "Step 7100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 656.1883985996246 seconds\n",
      "Step 7200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 665.4419739246368 seconds\n",
      "Step 7300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 674.5577132701874 seconds\n",
      "Step 7400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 684.3795487880707 seconds\n",
      "Step 7500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 693.645679473877 seconds\n",
      "Step 7600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 702.7635924816132 seconds\n",
      "Step 7700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 712.0348310470581 seconds\n",
      "Step 7800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 721.3105590343475 seconds\n",
      "Step 7900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 730.4662652015686 seconds\n",
      "Step 8000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 739.8075475692749 seconds\n",
      "Step 8100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 748.9248614311218 seconds\n",
      "Step 8200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 758.4072563648224 seconds\n",
      "Step 8300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 767.810213804245 seconds\n",
      "Step 8400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 776.9206092357635 seconds\n",
      "Step 8500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 786.6357672214508 seconds\n",
      "Step 8600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 795.9718196392059 seconds\n",
      "Step 8700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 805.074052810669 seconds\n",
      "Step 8800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 814.1679739952087 seconds\n",
      "Step 8900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 823.4545104503632 seconds\n",
      "Step 9000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 832.9527952671051 seconds\n",
      "Step 9100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 842.2840268611908 seconds\n",
      "Step 9200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 851.6945371627808 seconds\n",
      "Step 9300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 860.8851437568665 seconds\n",
      "Step 9400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 869.9724178314209 seconds\n",
      "Step 9500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 879.2494878768921 seconds\n",
      "Step 9600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 888.3745074272156 seconds\n",
      "Step 9700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 897.7825019359589 seconds\n",
      "Step 9800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 907.1325476169586 seconds\n",
      "Step 9900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 916.2412467002869 seconds\n",
      "Step 10000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 925.5351383686066 seconds\n",
      "Step 10100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 935.4446840286255 seconds\n",
      "Step 10200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 944.6046063899994 seconds\n",
      "Step 10300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 953.8968489170074 seconds\n",
      "Step 10400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 962.9840891361237 seconds\n",
      "Step 10500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 972.1074001789093 seconds\n",
      "Step 10600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 981.3759329319 seconds\n",
      "Step 10700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 990.4922535419464 seconds\n",
      "Step 10800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 999.6083698272705 seconds\n",
      "Step 10900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1008.9466950893402 seconds\n",
      "Step 11000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1018.0795106887817 seconds\n",
      "Step 11100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1027.208724975586 seconds\n",
      "Step 11200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1036.553776025772 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1045.6971464157104 seconds\n",
      "Step 11400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1054.8261940479279 seconds\n",
      "Step 11500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1064.141590833664 seconds\n",
      "Step 11600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1073.286434173584 seconds\n",
      "Step 11700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1082.5903823375702 seconds\n",
      "Step 11800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1091.7421560287476 seconds\n",
      "Step 11900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1100.8951396942139 seconds\n",
      "Step 12000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1110.1999785900116 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1119.329225063324 seconds\n",
      "Step 12200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1128.4698929786682 seconds\n",
      "Step 12300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1137.958696603775 seconds\n",
      "Step 12400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1147.1282427310944 seconds\n",
      "Step 12500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1156.944295167923 seconds\n",
      "Step 12600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1166.2238247394562 seconds\n",
      "Step 12700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1175.3365676403046 seconds\n",
      "Step 12800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1184.4580788612366 seconds\n",
      "Step 12900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1193.7529060840607 seconds\n",
      "Step 13000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1202.91246342659 seconds\n",
      "Step 13100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1212.8441047668457 seconds\n",
      "Step 13200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1222.1700596809387 seconds\n",
      "Step 13300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1231.2775082588196 seconds\n",
      "Step 13400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1240.40598487854 seconds\n",
      "Step 13500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1249.8404812812805 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1258.9777629375458 seconds\n",
      "Step 13700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1268.099366426468 seconds\n",
      "Step 13800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1277.4203898906708 seconds\n",
      "Step 13900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1286.9577450752258 seconds\n",
      "Step 14000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1296.139099597931 seconds\n",
      "Step 14100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1305.4360723495483 seconds\n",
      "Step 14200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1314.593377828598 seconds\n",
      "Step 14300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1323.931478023529 seconds\n",
      "Step 14400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1333.6713054180145 seconds\n",
      "Step 14500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1342.8834035396576 seconds\n",
      "Step 14600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1352.1508238315582 seconds\n",
      "Step 14700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1361.2748034000397 seconds\n",
      "Step 14800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1370.430325269699 seconds\n",
      "Step 14900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1379.749172925949 seconds\n",
      "Step 15000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1389.1101412773132 seconds\n",
      "Step 15100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1399.833416223526 seconds\n",
      "Step 15200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1409.1376042366028 seconds\n",
      "Step 15300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1418.2433400154114 seconds\n",
      "Step 15400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1427.34645819664 seconds\n",
      "Step 15500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1436.625886440277 seconds\n",
      "Step 15600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1445.7334642410278 seconds\n",
      "Step 15700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1454.8529443740845 seconds\n",
      "Step 15800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1464.1455328464508 seconds\n",
      "Step 15900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1473.265953540802 seconds\n",
      "Step 16000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1482.400804758072 seconds\n",
      "Step 16100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1491.7150521278381 seconds\n",
      "Step 16200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1500.81201338768 seconds\n",
      "Step 16300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1509.9288001060486 seconds\n",
      "Step 16400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1519.2740824222565 seconds\n",
      "Step 16500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1528.395902633667 seconds\n",
      "Step 16600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1538.3916914463043 seconds\n",
      "Step 16700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1547.4968280792236 seconds\n",
      "Step 16800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1556.6614880561829 seconds\n",
      "Step 16900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1566.0985543727875 seconds\n",
      "Step 17000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1575.2332925796509 seconds\n",
      "Step 17100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1584.3605802059174 seconds\n",
      "Step 17200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1593.999695777893 seconds\n",
      "Step 17300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1603.076817035675 seconds\n",
      "Step 17400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1612.1965579986572 seconds\n",
      "Step 17500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1621.5088031291962 seconds\n",
      "Step 17600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1631.6557574272156 seconds\n",
      "Step 17700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1640.75967669487 seconds\n",
      "Step 17800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1650.049851179123 seconds\n",
      "Step 17900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1659.2767083644867 seconds\n",
      "Step 18000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1668.5050041675568 seconds\n",
      "Step 18100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1677.8000211715698 seconds\n",
      "Step 18200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1687.004386663437 seconds\n",
      "Step 18300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1696.3634793758392 seconds\n",
      "Step 18400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1705.9698083400726 seconds\n",
      "Step 18500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1715.099133014679 seconds\n",
      "Step 18600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1724.201456785202 seconds\n",
      "Step 18700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1733.5583200454712 seconds\n",
      "Step 18800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1742.8191375732422 seconds\n",
      "Step 18900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1752.3697340488434 seconds\n",
      "Step 19000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1761.6621811389923 seconds\n",
      "Step 19100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1770.8113930225372 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1780.1282484531403 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1790.0139651298523 seconds\n",
      "Step 19400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1799.1519529819489 seconds\n",
      "Step 19500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1808.5586857795715 seconds\n",
      "Step 19600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1817.7580194473267 seconds\n",
      "Step 19700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1826.8943865299225 seconds\n",
      "Step 19800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1836.2111999988556 seconds\n",
      "Step 19900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1845.3261942863464 seconds\n",
      "Step 20000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1856.2478811740875 seconds\n",
      "Step 20100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1865.60125041008 seconds\n",
      "Step 20200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1874.7146933078766 seconds\n",
      "Step 20300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1883.8143451213837 seconds\n",
      "Step 20400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1893.1271126270294 seconds\n",
      "Step 20500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1902.2315497398376 seconds\n",
      "Step 20600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1911.3619420528412 seconds\n",
      "Step 20700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1920.667803287506 seconds\n",
      "Step 20800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1929.8132026195526 seconds\n",
      "Step 20900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1938.9541058540344 seconds\n",
      "Step 21000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1948.2937180995941 seconds\n",
      "Step 21100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1957.45689868927 seconds\n",
      "Step 21200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1966.602842092514 seconds\n",
      "Step 21300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1975.935446023941 seconds\n",
      "Step 21400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1985.0935351848602 seconds\n",
      "Step 21500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1994.2595760822296 seconds\n",
      "Step 21600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2003.613299369812 seconds\n",
      "Step 21700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2012.7499985694885 seconds\n",
      "Step 21800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2022.0945928096771 seconds\n",
      "Step 21900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2031.1637079715729 seconds\n",
      "Step 22000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2040.2749781608582 seconds\n",
      "Step 22100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2049.626776456833 seconds\n",
      "Step 22200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2058.9838461875916 seconds\n",
      "Step 22300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2068.0872678756714 seconds\n",
      "Step 22400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2077.475842475891 seconds\n",
      "Step 22500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2086.562632083893 seconds\n",
      "Step 22600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2095.696608066559 seconds\n",
      "Step 22700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2105.0620539188385 seconds\n",
      "Step 22800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2114.216783761978 seconds\n",
      "Step 22900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2123.90287232399 seconds\n",
      "Step 23000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2133.2459678649902 seconds\n",
      "Step 23100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2142.3825907707214 seconds\n",
      "Step 23200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2151.5092883110046 seconds\n",
      "Step 23300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2160.8302001953125 seconds\n",
      "Step 23400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2170.032521009445 seconds\n",
      "Step 23500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2179.1694033145905 seconds\n",
      "Step 23600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2188.4900426864624 seconds\n",
      "Step 23700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2198.419900417328 seconds\n",
      "Step 23800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2207.5486645698547 seconds\n",
      "Step 23900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2216.8774445056915 seconds\n",
      "Step 24000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2226.02498960495 seconds\n",
      "Step 24100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2235.1497554779053 seconds\n",
      "Step 24200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2244.4701709747314 seconds\n",
      "Step 24300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2253.5975589752197 seconds\n",
      "Step 24400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2262.7603142261505 seconds\n",
      "Step 24500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2272.090404033661 seconds\n",
      "Step 24600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2281.9857563972473 seconds\n",
      "Step 24700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2291.13697886467 seconds\n",
      "Step 24800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2300.459320306778 seconds\n",
      "Step 24900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2309.796257019043 seconds\n",
      "Step 25000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2319.134251356125 seconds\n",
      "Step 25100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2328.2863278388977 seconds\n",
      "Step 25200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2337.4168100357056 seconds\n",
      "Step 25300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2346.8083074092865 seconds\n",
      "Step 25400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2356.0536155700684 seconds\n",
      "Step 25500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2366.2496733665466 seconds\n",
      "Step 25600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2375.619211912155 seconds\n",
      "Step 25700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2384.7967178821564 seconds\n",
      "Step 25800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2393.9614255428314 seconds\n",
      "Step 25900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2403.3216319084167 seconds\n",
      "Step 26000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2412.4856708049774 seconds\n",
      "Step 26100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2421.653636932373 seconds\n",
      "Step 26200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2431.0203053951263 seconds\n",
      "Step 26300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2440.235873937607 seconds\n",
      "Step 26400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2449.996411561966 seconds\n",
      "Step 26500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2459.3095619678497 seconds\n",
      "Step 26600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2468.4441940784454 seconds\n",
      "Step 26700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2477.5704069137573 seconds\n",
      "Step 26800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2486.8638603687286 seconds\n",
      "Step 26900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2495.9817361831665 seconds\n",
      "Step 27000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2505.0845873355865 seconds\n",
      "Step 27100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2514.3427572250366 seconds\n",
      "Step 27200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2523.667266368866 seconds\n",
      "Step 27300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2533.159888267517 seconds\n",
      "Step 27400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2542.924707889557 seconds\n",
      "Step 27500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2552.4938509464264 seconds\n",
      "Step 27600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2561.8486819267273 seconds\n",
      "Step 27700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2571.3407866954803 seconds\n",
      "Step 27800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2580.645983695984 seconds\n",
      "Step 27900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2589.9677698612213 seconds\n",
      "Step 28000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2599.49831700325 seconds\n",
      "Step 28100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2608.8137578964233 seconds\n",
      "Step 28200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2618.1440873146057 seconds\n",
      "Step 28300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2627.664317846298 seconds\n",
      "Step 28400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2636.982084274292 seconds\n",
      "Step 28500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2646.5043790340424 seconds\n",
      "Step 28600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2655.8219833374023 seconds\n",
      "Step 28700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2665.6819772720337 seconds\n",
      "Step 28800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2675.1861832141876 seconds\n",
      "Step 28900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2684.4843769073486 seconds\n",
      "Step 29000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2693.7646582126617 seconds\n",
      "Step 29100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2703.280513048172 seconds\n",
      "Step 29200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2712.577403783798 seconds\n",
      "Step 29300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2721.9016144275665 seconds\n",
      "Step 29400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2731.4391906261444 seconds\n",
      "Step 29500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2740.7498531341553 seconds\n",
      "Step 29600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2750.0551388263702 seconds\n",
      "Step 29700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2759.579036951065 seconds\n",
      "Step 29800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2768.8852705955505 seconds\n",
      "Step 29900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2778.2387895584106 seconds\n",
      "Step 30000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2787.7431626319885 seconds\n",
      "Step 30100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2797.0233986377716 seconds\n",
      "Step 30200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2806.996188879013 seconds\n",
      "Step 30300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2816.478505373001 seconds\n",
      "Step 30400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2825.765014886856 seconds\n",
      "Step 30500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2835.077503681183 seconds\n",
      "Step 30600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2844.61435174942 seconds\n",
      "Step 30700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2853.9395835399628 seconds\n",
      "Step 30800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2863.3156836032867 seconds\n",
      "Step 30900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2873.1484229564667 seconds\n",
      "Step 31000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2882.4610290527344 seconds\n",
      "Step 31100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2891.780820131302 seconds\n",
      "Step 31200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2901.382216691971 seconds\n",
      "Step 31300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2910.990496635437 seconds\n",
      "Step 31400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2920.3754348754883 seconds\n",
      "Step 31500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2929.898597240448 seconds\n",
      "Step 31600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2939.2038662433624 seconds\n",
      "Step 31700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2948.510397672653 seconds\n",
      "Step 31800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2958.0094134807587 seconds\n",
      "Step 31900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2967.325107574463 seconds\n",
      "Step 32000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2976.6596324443817 seconds\n",
      "Step 32100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2986.127490758896 seconds\n",
      "Step 32200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2995.4244129657745 seconds\n",
      "Step 32300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3004.9300513267517 seconds\n",
      "Step 32400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3014.285185813904 seconds\n",
      "Step 32500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3023.916199207306 seconds\n",
      "Step 32600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3033.5005674362183 seconds\n",
      "Step 32700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3042.822439432144 seconds\n",
      "Step 32800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3052.177836894989 seconds\n",
      "Step 32900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3061.723824739456 seconds\n",
      "Step 33000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3071.0554599761963 seconds\n",
      "Step 33100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3080.3981161117554 seconds\n",
      "Step 33200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3089.8826220035553 seconds\n",
      "Step 33300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3099.1982889175415 seconds\n",
      "Step 33400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3108.5014905929565 seconds\n",
      "Step 33500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3118.0236427783966 seconds\n",
      "Step 33600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3127.324728488922 seconds\n",
      "Step 33700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3136.6216411590576 seconds\n",
      "Step 33800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3146.1760852336884 seconds\n",
      "Step 33900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3155.4980375766754 seconds\n",
      "Step 34000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3164.825216293335 seconds\n",
      "Step 34100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3174.3723995685577 seconds\n",
      "Step 34200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3183.7026908397675 seconds\n",
      "Step 34300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3193.0183522701263 seconds\n",
      "Step 34400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3202.5511243343353 seconds\n",
      "Step 34500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3211.83970952034 seconds\n",
      "Step 34600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3221.1239445209503 seconds\n",
      "Step 34700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3230.6505110263824 seconds\n",
      "Step 34800, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3240.0100243091583 seconds\n",
      "Step 34900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3249.352541208267 seconds\n",
      "Step 35000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3258.905818939209 seconds\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 0.17299199104309082 seconds\n",
      "Step 100, loss: tensor(0.0093, grad_fn=<SubBackward0>), time elapsed: 10.057613134384155 seconds\n",
      "Step 200, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 19.57182216644287 seconds\n",
      "Step 300, loss: tensor(0.0037, grad_fn=<SubBackward0>), time elapsed: 29.346513032913208 seconds\n",
      "Step 400, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 38.652703046798706 seconds\n",
      "Step 500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 47.95572328567505 seconds\n",
      "Step 600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 57.41221809387207 seconds\n",
      "Step 700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 66.7551474571228 seconds\n",
      "Step 800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 76.23130011558533 seconds\n",
      "Step 900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 85.56884527206421 seconds\n",
      "Step 1000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 94.9283332824707 seconds\n",
      "Step 1100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 104.43070769309998 seconds\n",
      "Step 1200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 113.7715756893158 seconds\n",
      "Step 1300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 123.38635110855103 seconds\n",
      "Step 1400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 132.97213506698608 seconds\n",
      "Step 1500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 142.27014684677124 seconds\n",
      "Step 1600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 151.53262090682983 seconds\n",
      "Step 1700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 161.00891399383545 seconds\n",
      "Step 1800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 170.32676100730896 seconds\n",
      "Step 1900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 179.59024143218994 seconds\n",
      "Step 2000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 189.04981899261475 seconds\n",
      "Step 2100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 198.3707253932953 seconds\n",
      "Step 2200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 207.70831036567688 seconds\n",
      "Step 2300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 217.1492657661438 seconds\n",
      "Step 2400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 226.45646262168884 seconds\n",
      "Step 2500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 235.8889582157135 seconds\n",
      "Step 2600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 245.1733672618866 seconds\n",
      "Step 2700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 254.52867102622986 seconds\n",
      "Step 2800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 264.01636838912964 seconds\n",
      "Step 2900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 273.3102111816406 seconds\n",
      "Step 3000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 282.63215374946594 seconds\n",
      "Step 3100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 292.07504892349243 seconds\n",
      "Step 3200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 301.357394695282 seconds\n",
      "Step 3300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 310.689759016037 seconds\n",
      "Step 3400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 320.1857750415802 seconds\n",
      "Step 3500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 329.5296850204468 seconds\n",
      "Step 3600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 338.8350327014923 seconds\n",
      "Step 3700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 348.2831001281738 seconds\n",
      "Step 3800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 357.58837270736694 seconds\n",
      "Step 3900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 366.89145493507385 seconds\n",
      "Step 4000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 376.37299609184265 seconds\n",
      "Step 4100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 385.7168643474579 seconds\n",
      "Step 4200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 395.20565915107727 seconds\n",
      "Step 4300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 404.49321365356445 seconds\n",
      "Step 4400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 413.8056993484497 seconds\n",
      "Step 4500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 423.3727250099182 seconds\n",
      "Step 4600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 432.7092499732971 seconds\n",
      "Step 4700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 442.02911591529846 seconds\n",
      "Step 4800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 451.47409653663635 seconds\n",
      "Step 4900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 460.7480905056 seconds\n",
      "Step 5000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 470.03040885925293 seconds\n",
      "Step 5100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 479.4568204879761 seconds\n",
      "Step 5200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 488.75665855407715 seconds\n",
      "Step 5300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 498.0296311378479 seconds\n",
      "Step 5400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 507.4891941547394 seconds\n",
      "Step 5500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 516.7902989387512 seconds\n",
      "Step 5600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 526.091385602951 seconds\n",
      "Step 5700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 535.5677225589752 seconds\n",
      "Step 5800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 544.8772504329681 seconds\n",
      "Step 5900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 554.3742146492004 seconds\n",
      "Step 6000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 563.7295770645142 seconds\n",
      "Step 6100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 573.089013338089 seconds\n",
      "Step 6200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 582.5652768611908 seconds\n",
      "Step 6300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 591.8903231620789 seconds\n",
      "Step 6400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 601.17791056633 seconds\n",
      "Step 6500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 610.6480846405029 seconds\n",
      "Step 6600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 619.9854431152344 seconds\n",
      "Step 6700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 629.2730271816254 seconds\n",
      "Step 6800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 638.7182223796844 seconds\n",
      "Step 6900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 648.0317287445068 seconds\n",
      "Step 7000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 657.3556463718414 seconds\n",
      "Step 7100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 666.8527789115906 seconds\n",
      "Step 7200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 676.2101850509644 seconds\n",
      "Step 7300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 685.5268485546112 seconds\n",
      "Step 7400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 695.0292522907257 seconds\n",
      "Step 7500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 704.3542680740356 seconds\n",
      "Step 7600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 713.8545763492584 seconds\n",
      "Step 7700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 723.1451423168182 seconds\n",
      "Step 7800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 732.4535994529724 seconds\n",
      "Step 7900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 741.9476482868195 seconds\n",
      "Step 8000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 751.2237722873688 seconds\n",
      "Step 8100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 760.5289607048035 seconds\n",
      "Step 8200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 770.0302612781525 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 779.3648595809937 seconds\n",
      "Step 8400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 788.6668798923492 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 798.1587595939636 seconds\n",
      "Step 8600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 807.4932947158813 seconds\n",
      "Step 8700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 816.8318490982056 seconds\n",
      "Step 8800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 826.31667137146 seconds\n",
      "Step 8900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 835.671783208847 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 845.0176491737366 seconds\n",
      "Step 9100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 854.507443189621 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 863.8503353595734 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 873.1681988239288 seconds\n",
      "Step 9400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 882.6570980548859 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 891.991352558136 seconds\n",
      "Step 9600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 901.4581949710846 seconds\n",
      "Step 9700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 910.7188651561737 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 920.0322630405426 seconds\n",
      "Step 9900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 929.5358045101166 seconds\n",
      "Step 10000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 938.855495929718 seconds\n",
      "Step 10100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 948.1609308719635 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 957.6622357368469 seconds\n",
      "Step 10300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 966.971508026123 seconds\n",
      "Step 10400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 976.2643249034882 seconds\n",
      "Step 10500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 985.7249040603638 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 995.0290999412537 seconds\n",
      "Step 10700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1004.3470664024353 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1013.8554437160492 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1023.2579092979431 seconds\n",
      "Step 11000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1032.6317763328552 seconds\n",
      "Step 11100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1042.0830488204956 seconds\n",
      "Step 11200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1051.3904144763947 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1060.6727721691132 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1070.1261146068573 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1079.4189722537994 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1088.8950743675232 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1098.20658826828 seconds\n",
      "Step 11800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1107.5306103229523 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1117.0068163871765 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1126.3090562820435 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1135.6361801624298 seconds\n",
      "Step 12200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1145.1020903587341 seconds\n",
      "Step 12300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1154.406350851059 seconds\n",
      "Step 12400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1163.729231595993 seconds\n",
      "Step 12500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1173.2325975894928 seconds\n",
      "Step 12600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1182.5316033363342 seconds\n",
      "Step 12700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1191.828600883484 seconds\n",
      "Step 12800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1201.3184933662415 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1210.5850393772125 seconds\n",
      "Step 13000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1219.8612711429596 seconds\n",
      "Step 13100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1229.3584275245667 seconds\n",
      "Step 13200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1238.6687531471252 seconds\n",
      "Step 13300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1247.960575580597 seconds\n",
      "Step 13400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1257.4577584266663 seconds\n",
      "Step 13500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1266.7680578231812 seconds\n",
      "Step 13600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1276.0889270305634 seconds\n",
      "Step 13700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1285.561194896698 seconds\n",
      "Step 13800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1294.8840689659119 seconds\n",
      "Step 13900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1304.3696558475494 seconds\n",
      "Step 14000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1313.6311655044556 seconds\n",
      "Step 14100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1322.9574527740479 seconds\n",
      "Step 14200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1332.4293587207794 seconds\n",
      "Step 14300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1341.7179234027863 seconds\n",
      "Step 14400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1351.0179331302643 seconds\n",
      "Step 14500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1360.4621000289917 seconds\n",
      "Step 14600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1369.745261669159 seconds\n",
      "Step 14700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1379.0724568367004 seconds\n",
      "Step 14800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1388.5320448875427 seconds\n",
      "Step 14900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1397.822744846344 seconds\n",
      "Step 15000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1407.13006234169 seconds\n",
      "Step 15100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1416.6250789165497 seconds\n",
      "Step 15200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1425.9491336345673 seconds\n",
      "Step 15300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1435.2502126693726 seconds\n",
      "Step 15400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1444.7462770938873 seconds\n",
      "Step 15500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1454.0778224468231 seconds\n",
      "Step 15600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1463.4120960235596 seconds\n",
      "Step 15700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1472.8894629478455 seconds\n",
      "Step 15800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1482.2155911922455 seconds\n",
      "Step 15900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1491.5436630249023 seconds\n",
      "Step 16000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1501.0211250782013 seconds\n",
      "Step 16100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1510.3325550556183 seconds\n",
      "Step 16200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1519.7900745868683 seconds\n",
      "Step 16300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1529.147472858429 seconds\n",
      "Step 16400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1538.5037610530853 seconds\n",
      "Step 16500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1548.0082333087921 seconds\n",
      "Step 16600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1557.3322730064392 seconds\n",
      "Step 16700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1566.669814825058 seconds\n",
      "Step 16800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1576.1481926441193 seconds\n",
      "Step 16900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1585.4568135738373 seconds\n",
      "Step 17000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1594.746779680252 seconds\n",
      "Step 17100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1604.033015012741 seconds\n",
      "Step 17200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1613.1348798274994 seconds\n",
      "Step 17300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1622.2542107105255 seconds\n",
      "Step 17400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1631.5554509162903 seconds\n",
      "Step 17500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1640.6992764472961 seconds\n",
      "Step 17600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1649.8103902339935 seconds\n",
      "Step 17700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1659.0792231559753 seconds\n",
      "Step 17800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1668.1469399929047 seconds\n",
      "Step 17900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1677.2417628765106 seconds\n",
      "Step 18000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1686.5254592895508 seconds\n",
      "Step 18100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1695.6408400535583 seconds\n",
      "Step 18200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1704.7643909454346 seconds\n",
      "Step 18300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1714.1175887584686 seconds\n",
      "Step 18400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1723.2692935466766 seconds\n",
      "Step 18500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1732.3905518054962 seconds\n",
      "Step 18600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1741.6516032218933 seconds\n",
      "Step 18700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1750.7482500076294 seconds\n",
      "Step 18800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1760.092395067215 seconds\n",
      "Step 18900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1769.1973650455475 seconds\n",
      "Step 19000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1778.334354877472 seconds\n",
      "Step 19100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1787.6452419757843 seconds\n",
      "Step 19200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1796.7698085308075 seconds\n",
      "Step 19300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 1805.893075466156 seconds\n",
      "Step 19400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1815.2240505218506 seconds\n",
      "Step 19500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1824.3391082286835 seconds\n",
      "Step 19600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1833.4728782176971 seconds\n",
      "Step 19700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1842.8067853450775 seconds\n",
      "Step 19800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1851.9571356773376 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1861.0726029872894 seconds\n",
      "Step 20000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1870.384906053543 seconds\n",
      "Step 20100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1879.5274772644043 seconds\n",
      "Step 20200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1888.6630051136017 seconds\n",
      "Step 20300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1897.9737808704376 seconds\n",
      "Step 20400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1907.1265513896942 seconds\n",
      "Step 20500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1916.245941877365 seconds\n",
      "Step 20600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1925.5466084480286 seconds\n",
      "Step 20700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1934.701005935669 seconds\n",
      "Step 20800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1943.8477802276611 seconds\n",
      "Step 20900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1953.1673283576965 seconds\n",
      "Step 21000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1962.318051815033 seconds\n",
      "Step 21100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1971.4456646442413 seconds\n",
      "Step 21200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1980.752910375595 seconds\n",
      "Step 21300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1989.8667812347412 seconds\n",
      "Step 21400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1999.2124135494232 seconds\n",
      "Step 21500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2008.3272533416748 seconds\n",
      "Step 21600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2017.4838464260101 seconds\n",
      "Step 21700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2026.9966077804565 seconds\n",
      "Step 21800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2036.1252105236053 seconds\n",
      "Step 21900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2045.2440178394318 seconds\n",
      "Step 22000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2054.5686419010162 seconds\n",
      "Step 22100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2063.682467699051 seconds\n",
      "Step 22200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2072.826171398163 seconds\n",
      "Step 22300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2082.4864881038666 seconds\n",
      "Step 22400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2092.005275964737 seconds\n",
      "Step 22500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2101.1413292884827 seconds\n",
      "Step 22600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2110.504039287567 seconds\n",
      "Step 22700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2119.6563596725464 seconds\n",
      "Step 22800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2128.7694211006165 seconds\n",
      "Step 22900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2138.059193134308 seconds\n",
      "Step 23000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2147.175208568573 seconds\n",
      "Step 23100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 2156.3772218227386 seconds\n",
      "Step 23200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2165.8035492897034 seconds\n",
      "Step 23300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2175.0391199588776 seconds\n",
      "Step 23400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2184.1623480319977 seconds\n",
      "Step 23500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2193.48903131485 seconds\n",
      "Step 23600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2202.6584990024567 seconds\n",
      "Step 23700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2211.8561551570892 seconds\n",
      "Step 23800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2222.044930934906 seconds\n",
      "Step 23900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2231.201057910919 seconds\n",
      "Step 24000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2240.3373188972473 seconds\n",
      "Step 24100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2249.651756286621 seconds\n",
      "Step 24200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2258.7821505069733 seconds\n",
      "Step 24300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2268.1295461654663 seconds\n",
      "Step 24400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2277.3499443531036 seconds\n",
      "Step 24500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2286.4986987113953 seconds\n",
      "Step 24600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2296.0738468170166 seconds\n",
      "Step 24700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2305.202165365219 seconds\n",
      "Step 24800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2314.3568913936615 seconds\n",
      "Step 24900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2323.698915719986 seconds\n",
      "Step 25000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2333.633430957794 seconds\n",
      "Step 25100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2342.782995700836 seconds\n",
      "Step 25200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2352.089914083481 seconds\n",
      "Step 25300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2361.2297196388245 seconds\n",
      "Step 25400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2370.3614580631256 seconds\n",
      "Step 25500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2379.7403190135956 seconds\n",
      "Step 25600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2389.153806447983 seconds\n",
      "Step 25700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2398.3261539936066 seconds\n",
      "Step 25800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2407.681933403015 seconds\n",
      "Step 25900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2416.8650183677673 seconds\n",
      "Step 26000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2426.0588998794556 seconds\n",
      "Step 26100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2436.463534116745 seconds\n",
      "Step 26200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2445.665941476822 seconds\n",
      "Step 26300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2454.8281755447388 seconds\n",
      "Step 26400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2464.262817144394 seconds\n",
      "Step 26500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2473.3711729049683 seconds\n",
      "Step 26600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 2482.6155984401703 seconds\n",
      "Step 26700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2491.92333483696 seconds\n",
      "Step 26800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2501.4132895469666 seconds\n",
      "Step 26900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2510.5997908115387 seconds\n",
      "Step 27000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2519.995507478714 seconds\n",
      "Step 27100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2529.4710462093353 seconds\n",
      "Step 27200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2538.5981171131134 seconds\n",
      "Step 27300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2547.9495978355408 seconds\n",
      "Step 27400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2557.0947785377502 seconds\n",
      "Step 27500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2566.419111967087 seconds\n",
      "Step 27600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2576.066967010498 seconds\n",
      "Step 27700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2585.3317062854767 seconds\n",
      "Step 27800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2594.672262430191 seconds\n",
      "Step 27900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2603.797262907028 seconds\n",
      "Step 28000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2612.965951681137 seconds\n",
      "Step 28100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2623.096178293228 seconds\n",
      "Step 28200, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2632.240865945816 seconds\n",
      "Step 28300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2641.3856065273285 seconds\n",
      "Step 28400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2650.702790260315 seconds\n",
      "Step 28500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2659.847178220749 seconds\n",
      "Step 28600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2668.983501434326 seconds\n",
      "Step 28700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2678.3372507095337 seconds\n",
      "Step 28800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2687.466316461563 seconds\n",
      "Step 28900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2696.627892971039 seconds\n",
      "Step 29000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2705.959546804428 seconds\n",
      "Step 29100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2715.0858025550842 seconds\n",
      "Step 29200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2724.220562696457 seconds\n",
      "Step 29300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2733.5388176441193 seconds\n",
      "Step 29400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2742.6551880836487 seconds\n",
      "Step 29500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2751.808568954468 seconds\n",
      "Step 29600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2761.1896030902863 seconds\n",
      "Step 29700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2770.331359386444 seconds\n",
      "Step 29800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2779.4942903518677 seconds\n",
      "Step 29900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2788.882089614868 seconds\n",
      "Step 30000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2798.0286741256714 seconds\n",
      "Step 30100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2807.7998769283295 seconds\n",
      "Step 30200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2817.2341723442078 seconds\n",
      "Step 30300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2826.3582077026367 seconds\n",
      "Step 30400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2835.496657848358 seconds\n",
      "Step 30500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2844.8543996810913 seconds\n",
      "Step 30600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2854.871216058731 seconds\n",
      "Step 30700, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2864.026493549347 seconds\n",
      "Step 30800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2873.3310856819153 seconds\n",
      "Step 30900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2882.721666574478 seconds\n",
      "Step 31000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2892.025535583496 seconds\n",
      "Step 31100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2901.137683391571 seconds\n",
      "Step 31200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2910.2718012332916 seconds\n",
      "Step 31300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2920.239086866379 seconds\n",
      "Step 31400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2929.3414216041565 seconds\n",
      "Step 31500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2938.550342321396 seconds\n",
      "Step 31600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2947.9781205654144 seconds\n",
      "Step 31700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2957.082777261734 seconds\n",
      "Step 31800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2966.7137718200684 seconds\n",
      "Step 31900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 2976.073274374008 seconds\n",
      "Step 32000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2985.1791145801544 seconds\n",
      "Step 32100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2994.288947582245 seconds\n",
      "Step 32200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3003.6580238342285 seconds\n",
      "Step 32300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3013.6916155815125 seconds\n",
      "Step 32400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3022.8162591457367 seconds\n",
      "Step 32500, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3032.280268907547 seconds\n",
      "Step 32600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3041.552495241165 seconds\n",
      "Step 32700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3050.6712787151337 seconds\n",
      "Step 32800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3060.010338306427 seconds\n",
      "Step 32900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3069.1636736392975 seconds\n",
      "Step 33000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3079.0049641132355 seconds\n",
      "Step 33100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3088.3372690677643 seconds\n",
      "Step 33200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3097.4678978919983 seconds\n",
      "Step 33300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3106.6918861865997 seconds\n",
      "Step 33400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3116.044130086899 seconds\n",
      "Step 33500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3125.4074535369873 seconds\n",
      "Step 33600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3134.5326523780823 seconds\n",
      "Step 33700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3143.8975048065186 seconds\n",
      "Step 33800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3153.8200516700745 seconds\n",
      "Step 33900, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3162.918846845627 seconds\n",
      "Step 34000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3172.2354147434235 seconds\n",
      "Step 34100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3181.3528871536255 seconds\n",
      "Step 34200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3190.723248720169 seconds\n",
      "Step 34300, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3200.0362305641174 seconds\n",
      "Step 34400, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3209.162615776062 seconds\n",
      "Step 34500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3218.3424129486084 seconds\n",
      "Step 34600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3227.6918637752533 seconds\n",
      "Step 34700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3237.5538034439087 seconds\n",
      "Step 34800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3246.7520830631256 seconds\n",
      "Step 34900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3256.343820810318 seconds\n",
      "Step 35000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3265.537773132324 seconds\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0140, grad_fn=<SubBackward0>), time elapsed: 0.1519320011138916 seconds\n",
      "Step 100, loss: tensor(0.0089, grad_fn=<SubBackward0>), time elapsed: 10.152662515640259 seconds\n",
      "Step 200, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 19.244704961776733 seconds\n",
      "Step 300, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 28.356046676635742 seconds\n",
      "Step 400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 37.587791204452515 seconds\n",
      "Step 500, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 46.661311864852905 seconds\n",
      "Step 600, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 55.728649854660034 seconds\n",
      "Step 700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 64.98943495750427 seconds\n",
      "Step 800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 74.08785009384155 seconds\n",
      "Step 900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 83.19013738632202 seconds\n",
      "Step 1000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 92.45364928245544 seconds\n",
      "Step 1100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 101.58561778068542 seconds\n",
      "Step 1200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 110.72400522232056 seconds\n",
      "Step 1300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 120.01566052436829 seconds\n",
      "Step 1400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 129.12312126159668 seconds\n",
      "Step 1500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 138.23449039459229 seconds\n",
      "Step 1600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 147.51088309288025 seconds\n",
      "Step 1700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 156.6168761253357 seconds\n",
      "Step 1800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 165.87215375900269 seconds\n",
      "Step 1900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 174.96956944465637 seconds\n",
      "Step 2000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 184.08909511566162 seconds\n",
      "Step 2100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 193.36197352409363 seconds\n",
      "Step 2200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 202.45419120788574 seconds\n",
      "Step 2300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 211.58197045326233 seconds\n",
      "Step 2400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 220.83260464668274 seconds\n",
      "Step 2500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 229.91008710861206 seconds\n",
      "Step 2600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 238.9954023361206 seconds\n",
      "Step 2700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 248.26568293571472 seconds\n",
      "Step 2800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 257.3559339046478 seconds\n",
      "Step 2900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 266.43443870544434 seconds\n",
      "Step 3000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 275.6837043762207 seconds\n",
      "Step 3100, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 284.79665207862854 seconds\n",
      "Step 3200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 293.910756111145 seconds\n",
      "Step 3300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 303.2098877429962 seconds\n",
      "Step 3400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 312.32348346710205 seconds\n",
      "Step 3500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 321.608952999115 seconds\n",
      "Step 3600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 330.6975255012512 seconds\n",
      "Step 3700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 339.85536456108093 seconds\n",
      "Step 3800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 349.1262764930725 seconds\n",
      "Step 3900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 358.23800230026245 seconds\n",
      "Step 4000, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 367.3794746398926 seconds\n",
      "Step 4100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 376.6509282588959 seconds\n",
      "Step 4200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 385.77388072013855 seconds\n",
      "Step 4300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 394.90174674987793 seconds\n",
      "Step 4400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 404.2124238014221 seconds\n",
      "Step 4500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 413.35523080825806 seconds\n",
      "Step 4600, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 422.4774181842804 seconds\n",
      "Step 4700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 431.8015511035919 seconds\n",
      "Step 4800, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 441.83061003685 seconds\n",
      "Step 4900, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 450.94139671325684 seconds\n",
      "Step 5000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 460.22071957588196 seconds\n",
      "Step 5100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 469.2970337867737 seconds\n",
      "Step 5200, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 478.5622568130493 seconds\n",
      "Step 5300, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 487.6986496448517 seconds\n",
      "Step 5400, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 496.8430938720703 seconds\n",
      "Step 5500, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 506.4864344596863 seconds\n",
      "Step 5600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 515.6819803714752 seconds\n",
      "Step 5700, loss: tensor(0.0006, grad_fn=<SubBackward0>), time elapsed: 526.3208842277527 seconds\n",
      "Step 5800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 535.5872232913971 seconds\n",
      "Step 5900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 544.6747896671295 seconds\n",
      "Step 6000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 553.7918152809143 seconds\n",
      "Step 6100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 563.0647766590118 seconds\n",
      "Step 6200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 572.1913273334503 seconds\n",
      "Step 6300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 581.3064410686493 seconds\n",
      "Step 6400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 590.5671920776367 seconds\n",
      "Step 6500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 599.6795973777771 seconds\n",
      "Step 6600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 609.6544322967529 seconds\n",
      "Step 6700, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 618.9178175926208 seconds\n",
      "Step 6800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 628.0259947776794 seconds\n",
      "Step 6900, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 637.3273193836212 seconds\n",
      "Step 7000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 646.4600238800049 seconds\n",
      "Step 7100, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 655.6042704582214 seconds\n",
      "Step 7200, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 664.9202222824097 seconds\n",
      "Step 7300, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 674.0980005264282 seconds\n",
      "Step 7400, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 683.2313401699066 seconds\n",
      "Step 7500, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 692.5642082691193 seconds\n",
      "Step 7600, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 701.6952748298645 seconds\n",
      "Step 7700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 712.2946670055389 seconds\n",
      "Step 7800, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 721.5968379974365 seconds\n",
      "Step 7900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 730.7324588298798 seconds\n",
      "Step 8000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 739.8559346199036 seconds\n",
      "Step 8100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 749.128493309021 seconds\n",
      "Step 8200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 758.2510402202606 seconds\n",
      "Step 8300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 767.3839755058289 seconds\n",
      "Step 8400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 776.6389133930206 seconds\n",
      "Step 8500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 785.7586417198181 seconds\n",
      "Step 8600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 795.6565096378326 seconds\n",
      "Step 8700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 804.9295830726624 seconds\n",
      "Step 8800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 814.0197830200195 seconds\n",
      "Step 8900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 823.3226459026337 seconds\n",
      "Step 9000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 832.407306432724 seconds\n",
      "Step 9100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 841.5141201019287 seconds\n",
      "Step 9200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 850.8514065742493 seconds\n",
      "Step 9300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 860.0294690132141 seconds\n",
      "Step 9400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 869.1780867576599 seconds\n",
      "Step 9500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 878.5112209320068 seconds\n",
      "Step 9600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 887.6648721694946 seconds\n",
      "Step 9700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 897.558984041214 seconds\n",
      "Step 9800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 906.8474912643433 seconds\n",
      "Step 9900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 915.9935574531555 seconds\n",
      "Step 10000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 925.3248212337494 seconds\n",
      "Step 10100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 934.680659532547 seconds\n",
      "Step 10200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 943.81605219841 seconds\n",
      "Step 10300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 952.9773154258728 seconds\n",
      "Step 10400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 962.2998085021973 seconds\n",
      "Step 10500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 971.7239224910736 seconds\n",
      "Step 10600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 981.0152614116669 seconds\n",
      "Step 10700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 990.5740511417389 seconds\n",
      "Step 10800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 999.700740814209 seconds\n",
      "Step 10900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1009.0361552238464 seconds\n",
      "Step 11000, loss: tensor(0.0005, grad_fn=<SubBackward0>), time elapsed: 1018.7738513946533 seconds\n",
      "Step 11100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1027.9755392074585 seconds\n",
      "Step 11200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1037.367022037506 seconds\n",
      "Step 11300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1046.4713823795319 seconds\n",
      "Step 11400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1055.5767104625702 seconds\n",
      "Step 11500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1064.8783025741577 seconds\n",
      "Step 11600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1074.0471904277802 seconds\n",
      "Step 11700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1083.9958353042603 seconds\n",
      "Step 11800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1093.316859960556 seconds\n",
      "Step 11900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1102.4928345680237 seconds\n",
      "Step 12000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1111.6328887939453 seconds\n",
      "Step 12100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1120.952868938446 seconds\n",
      "Step 12200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1130.1366999149323 seconds\n",
      "Step 12300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1139.2922720909119 seconds\n",
      "Step 12400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1148.6152985095978 seconds\n",
      "Step 12500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1157.7698559761047 seconds\n",
      "Step 12600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1166.944230556488 seconds\n",
      "Step 12700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1176.2595312595367 seconds\n",
      "Step 12800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1185.4227302074432 seconds\n",
      "Step 12900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1194.7795288562775 seconds\n",
      "Step 13000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1203.9016797542572 seconds\n",
      "Step 13100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1213.6241500377655 seconds\n",
      "Step 13200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1222.938266992569 seconds\n",
      "Step 13300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1232.0544016361237 seconds\n",
      "Step 13400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1241.1849224567413 seconds\n",
      "Step 13500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1250.5660626888275 seconds\n",
      "Step 13600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1259.6855072975159 seconds\n",
      "Step 13700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1268.82368516922 seconds\n",
      "Step 13800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1278.1765449047089 seconds\n",
      "Step 13900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1288.8667042255402 seconds\n",
      "Step 14000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1297.9920077323914 seconds\n",
      "Step 14100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1307.2879490852356 seconds\n",
      "Step 14200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1316.4217021465302 seconds\n",
      "Step 14300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1325.5423612594604 seconds\n",
      "Step 14400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1334.8446261882782 seconds\n",
      "Step 14500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1343.9977295398712 seconds\n",
      "Step 14600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1353.1278529167175 seconds\n",
      "Step 14700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1362.507024526596 seconds\n",
      "Step 14800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1371.6564099788666 seconds\n",
      "Step 14900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1381.322527885437 seconds\n",
      "Step 15000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1390.6891844272614 seconds\n",
      "Step 15100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1399.847578048706 seconds\n",
      "Step 15200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1409.1943078041077 seconds\n",
      "Step 15300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1418.3115708827972 seconds\n",
      "Step 15400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1427.474205493927 seconds\n",
      "Step 15500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1437.545982837677 seconds\n",
      "Step 15600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1446.7002294063568 seconds\n",
      "Step 15700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1455.8435790538788 seconds\n",
      "Step 15800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1465.2427113056183 seconds\n",
      "Step 15900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1474.4456827640533 seconds\n",
      "Step 16000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1483.5593135356903 seconds\n",
      "Step 16100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1492.8439950942993 seconds\n",
      "Step 16200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1501.9743330478668 seconds\n",
      "Step 16300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1511.2333359718323 seconds\n",
      "Step 16400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1520.6531507968903 seconds\n",
      "Step 16500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1530.564383506775 seconds\n",
      "Step 16600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1539.6700932979584 seconds\n",
      "Step 16700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1548.9849827289581 seconds\n",
      "Step 16800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1558.1462423801422 seconds\n",
      "Step 16900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1567.2889428138733 seconds\n",
      "Step 17000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1576.5993127822876 seconds\n",
      "Step 17100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1586.589819908142 seconds\n",
      "Step 17200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1595.7029156684875 seconds\n",
      "Step 17300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1605.0950107574463 seconds\n",
      "Step 17400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1614.2647609710693 seconds\n",
      "Step 17500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1623.4233758449554 seconds\n",
      "Step 17600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1632.6982262134552 seconds\n",
      "Step 17700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1641.8165345191956 seconds\n",
      "Step 17800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1651.861126422882 seconds\n",
      "Step 17900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1661.0005667209625 seconds\n",
      "Step 18000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1670.4076611995697 seconds\n",
      "Step 18100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1679.7646684646606 seconds\n",
      "Step 18200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1688.9406735897064 seconds\n",
      "Step 18300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1698.1568751335144 seconds\n",
      "Step 18400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1708.2355501651764 seconds\n",
      "Step 18500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1717.4060015678406 seconds\n",
      "Step 18600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1726.5805160999298 seconds\n",
      "Step 18700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1735.8843512535095 seconds\n",
      "Step 18800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1744.9983704090118 seconds\n",
      "Step 18900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1754.1396071910858 seconds\n",
      "Step 19000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1763.4433097839355 seconds\n",
      "Step 19100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1772.7411217689514 seconds\n",
      "Step 19200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1781.8877954483032 seconds\n",
      "Step 19300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1791.2468707561493 seconds\n",
      "Step 19400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1800.7919685840607 seconds\n",
      "Step 19500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1809.9345784187317 seconds\n",
      "Step 19600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1819.2675123214722 seconds\n",
      "Step 19700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1828.3982574939728 seconds\n",
      "Step 19800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1837.5297148227692 seconds\n",
      "Step 19900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1846.8201410770416 seconds\n",
      "Step 20000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1856.3397989273071 seconds\n",
      "Step 20100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1865.5144934654236 seconds\n",
      "Step 20200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1874.8806974887848 seconds\n",
      "Step 20300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1884.0696597099304 seconds\n",
      "Step 20400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1893.4286398887634 seconds\n",
      "Step 20500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1902.5771052837372 seconds\n",
      "Step 20600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1911.8671114444733 seconds\n",
      "Step 20700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1921.2893664836884 seconds\n",
      "Step 20800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1930.5280618667603 seconds\n",
      "Step 20900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1939.7905766963959 seconds\n",
      "Step 21000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1949.2170429229736 seconds\n",
      "Step 21100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1958.4881236553192 seconds\n",
      "Step 21200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1967.7437107563019 seconds\n",
      "Step 21300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1977.2110497951508 seconds\n",
      "Step 21400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1987.2023146152496 seconds\n",
      "Step 21500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 1996.4466214179993 seconds\n",
      "Step 21600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2005.9029970169067 seconds\n",
      "Step 21700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2015.1112177371979 seconds\n",
      "Step 21800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2024.3911962509155 seconds\n",
      "Step 21900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2033.7277331352234 seconds\n",
      "Step 22000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2042.9739274978638 seconds\n",
      "Step 22100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2052.1567556858063 seconds\n",
      "Step 22200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2061.511594772339 seconds\n",
      "Step 22300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2071.480722427368 seconds\n",
      "Step 22400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2080.63050532341 seconds\n",
      "Step 22500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2089.9744222164154 seconds\n",
      "Step 22600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2099.1449851989746 seconds\n",
      "Step 22700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2108.417285680771 seconds\n",
      "Step 22800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2117.757792234421 seconds\n",
      "Step 22900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2126.928181409836 seconds\n",
      "Step 23000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2136.086796760559 seconds\n",
      "Step 23100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2146.2516453266144 seconds\n",
      "Step 23200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2155.423919439316 seconds\n",
      "Step 23300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2164.80419254303 seconds\n",
      "Step 23400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2173.980548143387 seconds\n",
      "Step 23500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2183.222780227661 seconds\n",
      "Step 23600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2192.5719101428986 seconds\n",
      "Step 23700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2201.736902475357 seconds\n",
      "Step 23800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2210.8825323581696 seconds\n",
      "Step 23900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2221.063066959381 seconds\n",
      "Step 24000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2230.2490775585175 seconds\n",
      "Step 24100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2239.5149142742157 seconds\n",
      "Step 24200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2248.9952578544617 seconds\n",
      "Step 24300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2258.2042741775513 seconds\n",
      "Step 24400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2267.373386621475 seconds\n",
      "Step 24500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2276.699876308441 seconds\n",
      "Step 24600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2286.334457397461 seconds\n",
      "Step 24700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2296.1899349689484 seconds\n",
      "Step 24800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2305.5390026569366 seconds\n",
      "Step 24900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2314.7056148052216 seconds\n",
      "Step 25000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2323.8844571113586 seconds\n",
      "Step 25100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2333.238007545471 seconds\n",
      "Step 25200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2342.4292426109314 seconds\n",
      "Step 25300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2351.6174926757812 seconds\n",
      "Step 25400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2361.076495885849 seconds\n",
      "Step 25500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2370.231589794159 seconds\n",
      "Step 25600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2379.364287853241 seconds\n",
      "Step 25700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2388.739791870117 seconds\n",
      "Step 25800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2397.886993408203 seconds\n",
      "Step 25900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2407.8050146102905 seconds\n",
      "Step 26000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2417.143479347229 seconds\n",
      "Step 26100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2426.2748231887817 seconds\n",
      "Step 26200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2435.4437057971954 seconds\n",
      "Step 26300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2444.8255319595337 seconds\n",
      "Step 26400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2453.985429286957 seconds\n",
      "Step 26500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2463.354986667633 seconds\n",
      "Step 26600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2472.9882085323334 seconds\n",
      "Step 26700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2482.197940826416 seconds\n",
      "Step 26800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2491.5448582172394 seconds\n",
      "Step 26900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2500.677029132843 seconds\n",
      "Step 27000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2509.8421897888184 seconds\n",
      "Step 27100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2519.193035840988 seconds\n",
      "Step 27200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2528.4794178009033 seconds\n",
      "Step 27300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2537.666328907013 seconds\n",
      "Step 27400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2547.016215801239 seconds\n",
      "Step 27500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2556.1669414043427 seconds\n",
      "Step 27600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2565.3549525737762 seconds\n",
      "Step 27700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2575.044391155243 seconds\n",
      "Step 27800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2584.190374135971 seconds\n",
      "Step 27900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2593.35938000679 seconds\n",
      "Step 28000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2603.5776059627533 seconds\n",
      "Step 28100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2612.8508133888245 seconds\n",
      "Step 28200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2622.0151813030243 seconds\n",
      "Step 28300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2631.4170315265656 seconds\n",
      "Step 28400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2640.5668709278107 seconds\n",
      "Step 28500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2649.7274885177612 seconds\n",
      "Step 28600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2659.0859129428864 seconds\n",
      "Step 28700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2668.2346982955933 seconds\n",
      "Step 28800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2677.3810613155365 seconds\n",
      "Step 28900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2686.7711384296417 seconds\n",
      "Step 29000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2695.9299323558807 seconds\n",
      "Step 29100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2705.0623273849487 seconds\n",
      "Step 29200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2714.393777370453 seconds\n",
      "Step 29300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2723.550661802292 seconds\n",
      "Step 29400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2732.709090948105 seconds\n",
      "Step 29500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2742.05206990242 seconds\n",
      "Step 29600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2751.2012412548065 seconds\n",
      "Step 29700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2760.339629173279 seconds\n",
      "Step 29800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2769.682451248169 seconds\n",
      "Step 29900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2778.841290473938 seconds\n",
      "Step 30000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2788.201620578766 seconds\n",
      "Step 30100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2797.3079550266266 seconds\n",
      "Step 30200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2806.4528777599335 seconds\n",
      "Step 30300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2815.799151659012 seconds\n",
      "Step 30400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2824.919404029846 seconds\n",
      "Step 30500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2834.064204454422 seconds\n",
      "Step 30600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2843.8093502521515 seconds\n",
      "Step 30700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2852.915963411331 seconds\n",
      "Step 30800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2862.0356974601746 seconds\n",
      "Step 30900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2871.363503217697 seconds\n",
      "Step 31000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2880.478794336319 seconds\n",
      "Step 31100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2889.5907957553864 seconds\n",
      "Step 31200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2898.916098833084 seconds\n",
      "Step 31300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2908.0328657627106 seconds\n",
      "Step 31400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2917.166429042816 seconds\n",
      "Step 31500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2926.5040922164917 seconds\n",
      "Step 31600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2935.7459359169006 seconds\n",
      "Step 31700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2944.914425611496 seconds\n",
      "Step 31800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2954.2636139392853 seconds\n",
      "Step 31900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2963.6153950691223 seconds\n",
      "Step 32000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2972.8370406627655 seconds\n",
      "Step 32100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2982.1618373394012 seconds\n",
      "Step 32200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 2991.2930183410645 seconds\n",
      "Step 32300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3000.476114988327 seconds\n",
      "Step 32400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3009.8157222270966 seconds\n",
      "Step 32500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3019.5789482593536 seconds\n",
      "Step 32600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3028.6972744464874 seconds\n",
      "Step 32700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3037.9988985061646 seconds\n",
      "Step 32800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3047.1167969703674 seconds\n",
      "Step 32900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3056.237726211548 seconds\n",
      "Step 33000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3065.5383994579315 seconds\n",
      "Step 33100, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3074.7382383346558 seconds\n",
      "Step 33200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3083.8867568969727 seconds\n",
      "Step 33300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3093.2321031093597 seconds\n",
      "Step 33400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3102.396165370941 seconds\n",
      "Step 33500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3111.525227546692 seconds\n",
      "Step 33600, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3121.572725057602 seconds\n",
      "Step 33700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3130.668146133423 seconds\n",
      "Step 33800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3139.7982761859894 seconds\n",
      "Step 33900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3149.122421503067 seconds\n",
      "Step 34000, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3158.2373447418213 seconds\n",
      "Step 34100, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3167.3405125141144 seconds\n",
      "Step 34200, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3176.87496137619 seconds\n",
      "Step 34300, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3186.0064890384674 seconds\n",
      "Step 34400, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3195.3905329704285 seconds\n",
      "Step 34500, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3204.5171723365784 seconds\n",
      "Step 34600, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3213.6774055957794 seconds\n",
      "Step 34700, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3223.5232124328613 seconds\n",
      "Step 34800, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3232.662832260132 seconds\n",
      "Step 34900, loss: tensor(0.0004, grad_fn=<SubBackward0>), time elapsed: 3241.836517572403 seconds\n",
      "Step 35000, loss: tensor(0.0003, grad_fn=<SubBackward0>), time elapsed: 3251.170833349228 seconds\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 0.13635969161987305 seconds\n",
      "Step 100, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 9.387060165405273 seconds\n",
      "Step 200, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 18.50055980682373 seconds\n",
      "Step 300, loss: tensor(0.0030, grad_fn=<SubBackward0>), time elapsed: 28.29373002052307 seconds\n",
      "Step 400, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 37.404139280319214 seconds\n",
      "Step 500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 46.5446343421936 seconds\n",
      "Step 600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 55.78949809074402 seconds\n",
      "Step 700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 64.88089942932129 seconds\n",
      "Step 800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 73.98065137863159 seconds\n",
      "Step 900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 83.24153876304626 seconds\n",
      "Step 1000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 92.30152750015259 seconds\n",
      "Step 1100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 101.36868977546692 seconds\n",
      "Step 1200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 110.63027596473694 seconds\n",
      "Step 1300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 119.74473452568054 seconds\n",
      "Step 1400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 129.00487804412842 seconds\n",
      "Step 1500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 138.1265652179718 seconds\n",
      "Step 1600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 147.23766922950745 seconds\n",
      "Step 1700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 156.49224591255188 seconds\n",
      "Step 1800, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 165.5746111869812 seconds\n",
      "Step 1900, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 174.71461033821106 seconds\n",
      "Step 2000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 183.95300555229187 seconds\n",
      "Step 2100, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 193.03390336036682 seconds\n",
      "Step 2200, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 202.13641452789307 seconds\n",
      "Step 2300, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 211.4107689857483 seconds\n",
      "Step 2400, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 220.51118445396423 seconds\n",
      "Step 2500, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 229.63806676864624 seconds\n",
      "Step 2600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 238.90285778045654 seconds\n",
      "Step 2700, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 248.0276415348053 seconds\n",
      "Step 2800, loss: tensor(0.0012, grad_fn=<SubBackward0>), time elapsed: 257.1287410259247 seconds\n",
      "Step 2900, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 266.42421984672546 seconds\n",
      "Step 3000, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 275.5567846298218 seconds\n",
      "Step 3100, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 284.8235921859741 seconds\n",
      "Step 3200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 293.94664764404297 seconds\n",
      "Step 3300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 303.0765013694763 seconds\n",
      "Step 3400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 312.370658159256 seconds\n",
      "Step 3500, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 321.50406861305237 seconds\n",
      "Step 3600, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 330.6403474807739 seconds\n",
      "Step 3700, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 339.9602496623993 seconds\n",
      "Step 3800, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 349.74318647384644 seconds\n",
      "Step 3900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 358.8808252811432 seconds\n",
      "Step 4000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 368.1581995487213 seconds\n",
      "Step 4100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 377.25441002845764 seconds\n",
      "Step 4200, loss: tensor(0.0011, grad_fn=<SubBackward0>), time elapsed: 386.35055208206177 seconds\n",
      "Step 4300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 395.6172397136688 seconds\n",
      "Step 4400, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 404.6993932723999 seconds\n",
      "Step 4500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 413.79645013809204 seconds\n",
      "Step 4600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 423.0274569988251 seconds\n",
      "Step 4700, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 432.1244342327118 seconds\n",
      "Step 4800, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 441.3819787502289 seconds\n",
      "Step 4900, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 450.5258345603943 seconds\n",
      "Step 5000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 459.62033557891846 seconds\n",
      "Step 5100, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 468.9401400089264 seconds\n",
      "Step 5200, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 478.091100692749 seconds\n",
      "Step 5300, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 487.21181416511536 seconds\n",
      "Step 5400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 496.8213212490082 seconds\n",
      "Step 5500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 505.94383120536804 seconds\n",
      "Step 5600, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 515.0176377296448 seconds\n",
      "Step 5700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 524.2578155994415 seconds\n",
      "Step 5800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 533.378918170929 seconds\n",
      "Step 5900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 542.514984369278 seconds\n",
      "Step 6000, loss: tensor(0.0010, grad_fn=<SubBackward0>), time elapsed: 551.8149282932281 seconds\n",
      "Step 6100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 560.9492483139038 seconds\n",
      "Step 6200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 570.3573744297028 seconds\n",
      "Step 6300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 579.8451495170593 seconds\n",
      "Step 6400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 589.1618528366089 seconds\n",
      "Step 6500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 598.6811871528625 seconds\n",
      "Step 6600, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 607.9674036502838 seconds\n",
      "Step 6700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 617.2591898441315 seconds\n",
      "Step 6800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 626.7427642345428 seconds\n",
      "Step 6900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 636.0208470821381 seconds\n",
      "Step 7000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 647.3260328769684 seconds\n",
      "Step 7100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 656.8429567813873 seconds\n",
      "Step 7200, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 666.0814535617828 seconds\n",
      "Step 7300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 675.3378169536591 seconds\n",
      "Step 7400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 684.7808792591095 seconds\n",
      "Step 7500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 694.1067850589752 seconds\n",
      "Step 7600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 703.3913750648499 seconds\n",
      "Step 7700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 712.8413202762604 seconds\n",
      "Step 7800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 722.1226682662964 seconds\n",
      "Step 7900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 731.4070789813995 seconds\n",
      "Step 8000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 740.8666732311249 seconds\n",
      "Step 8100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 750.1927797794342 seconds\n",
      "Step 8200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 759.4676723480225 seconds\n",
      "Step 8300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 768.9117515087128 seconds\n",
      "Step 8400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 778.1919901371002 seconds\n",
      "Step 8500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 787.6640846729279 seconds\n",
      "Step 8600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 796.9234781265259 seconds\n",
      "Step 8700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 806.1955361366272 seconds\n",
      "Step 8800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 815.6633009910583 seconds\n",
      "Step 8900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 824.9414653778076 seconds\n",
      "Step 9000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 834.2508754730225 seconds\n",
      "Step 9100, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 843.7544372081757 seconds\n",
      "Step 9200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 853.0345621109009 seconds\n",
      "Step 9300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 862.304322719574 seconds\n",
      "Step 9400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 871.7805576324463 seconds\n",
      "Step 9500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 881.0504059791565 seconds\n",
      "Step 9600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 890.3264563083649 seconds\n",
      "Step 9700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 899.7776951789856 seconds\n",
      "Step 9800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 909.124669790268 seconds\n",
      "Step 9900, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 918.6092977523804 seconds\n",
      "Step 10000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 928.3650104999542 seconds\n",
      "Step 10100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 937.6661140918732 seconds\n",
      "Step 10200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 946.9432747364044 seconds\n",
      "Step 10300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 956.3642203807831 seconds\n",
      "Step 10400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 965.6653034687042 seconds\n",
      "Step 10500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 975.1374802589417 seconds\n",
      "Step 10600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 984.41978764534 seconds\n",
      "Step 10700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 993.7188513278961 seconds\n",
      "Step 10800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1003.1418979167938 seconds\n",
      "Step 10900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1012.4241156578064 seconds\n",
      "Step 11000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 1021.680624961853 seconds\n",
      "Step 11100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1031.1388976573944 seconds\n",
      "Step 11200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1040.4161417484283 seconds\n",
      "Step 11300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1049.6890363693237 seconds\n",
      "Step 11400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 1059.1517169475555 seconds\n",
      "Step 11500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1068.4424097537994 seconds\n",
      "Step 11600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1077.708023071289 seconds\n",
      "Step 11700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1087.1384992599487 seconds\n",
      "Step 11800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1096.3863513469696 seconds\n",
      "Step 11900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1105.6400084495544 seconds\n",
      "Step 12000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1115.1063544750214 seconds\n",
      "Step 12100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1124.3500289916992 seconds\n",
      "Step 12200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1133.7817268371582 seconds\n",
      "Step 12300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1143.0242354869843 seconds\n",
      "Step 12400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1152.3180544376373 seconds\n",
      "Step 12500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1161.7515535354614 seconds\n",
      "Step 12600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1171.031944990158 seconds\n",
      "Step 12700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1180.3391070365906 seconds\n",
      "Step 12800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1189.7926206588745 seconds\n",
      "Step 12900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1199.0799341201782 seconds\n",
      "Step 13000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1208.382307767868 seconds\n",
      "Step 13100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1217.8050818443298 seconds\n",
      "Step 13200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1227.1072137355804 seconds\n",
      "Step 13300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1236.3635747432709 seconds\n",
      "Step 13400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1245.7951893806458 seconds\n",
      "Step 13500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1255.0231757164001 seconds\n",
      "Step 13600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1264.2948870658875 seconds\n",
      "Step 13700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1273.7514367103577 seconds\n",
      "Step 13800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1283.0046653747559 seconds\n",
      "Step 13900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1292.2838201522827 seconds\n",
      "Step 14000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1301.7268381118774 seconds\n",
      "Step 14100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1311.0225727558136 seconds\n",
      "Step 14200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1320.3790619373322 seconds\n",
      "Step 14300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1329.8929679393768 seconds\n",
      "Step 14400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1339.202283859253 seconds\n",
      "Step 14500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1348.4826383590698 seconds\n",
      "Step 14600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1357.9618027210236 seconds\n",
      "Step 14700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1367.2940304279327 seconds\n",
      "Step 14800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1376.7235560417175 seconds\n",
      "Step 14900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1385.982937335968 seconds\n",
      "Step 15000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1395.2674281597137 seconds\n",
      "Step 15100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1404.7165076732635 seconds\n",
      "Step 15200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1413.9654960632324 seconds\n",
      "Step 15300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1423.266593694687 seconds\n",
      "Step 15400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1432.7261781692505 seconds\n",
      "Step 15500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1441.9981157779694 seconds\n",
      "Step 15600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1451.3188927173615 seconds\n",
      "Step 15700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1460.8201835155487 seconds\n",
      "Step 15800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1470.128259897232 seconds\n",
      "Step 15900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1479.3141720294952 seconds\n",
      "Step 16000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1488.6590797901154 seconds\n",
      "Step 16100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1497.7606530189514 seconds\n",
      "Step 16200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1506.8556954860687 seconds\n",
      "Step 16300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1516.1957399845123 seconds\n",
      "Step 16400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1525.3151671886444 seconds\n",
      "Step 16500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1534.426016330719 seconds\n",
      "Step 16600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1543.7461144924164 seconds\n",
      "Step 16700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1552.85990858078 seconds\n",
      "Step 16800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1561.9708533287048 seconds\n",
      "Step 16900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1571.2235417366028 seconds\n",
      "Step 17000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1580.3465812206268 seconds\n",
      "Step 17100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1589.6692168712616 seconds\n",
      "Step 17200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1598.778650045395 seconds\n",
      "Step 17300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1607.9110901355743 seconds\n",
      "Step 17400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1617.2140107154846 seconds\n",
      "Step 17500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1626.3634707927704 seconds\n",
      "Step 17600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1635.5132999420166 seconds\n",
      "Step 17700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1644.8298060894012 seconds\n",
      "Step 17800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1653.958732366562 seconds\n",
      "Step 17900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1663.0841834545135 seconds\n",
      "Step 18000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1672.4495406150818 seconds\n",
      "Step 18100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1681.6351981163025 seconds\n",
      "Step 18200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1690.7717597484589 seconds\n",
      "Step 18300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1700.155909538269 seconds\n",
      "Step 18400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1709.2961621284485 seconds\n",
      "Step 18500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1718.470544576645 seconds\n",
      "Step 18600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1727.839290380478 seconds\n",
      "Step 18700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1737.1444308757782 seconds\n",
      "Step 18800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1746.2908244132996 seconds\n",
      "Step 18900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1756.6458055973053 seconds\n",
      "Step 19000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1765.8113057613373 seconds\n",
      "Step 19100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1774.955875635147 seconds\n",
      "Step 19200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1784.256409406662 seconds\n",
      "Step 19300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1793.3774836063385 seconds\n",
      "Step 19400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1802.4750950336456 seconds\n",
      "Step 19500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1811.7728633880615 seconds\n",
      "Step 19600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1820.8941411972046 seconds\n",
      "Step 19700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1830.1982448101044 seconds\n",
      "Step 19800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1839.2816236019135 seconds\n",
      "Step 19900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1848.4137651920319 seconds\n",
      "Step 20000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1858.0715553760529 seconds\n",
      "Step 20100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1867.2578411102295 seconds\n",
      "Step 20200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1876.463854789734 seconds\n",
      "Step 20300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1885.8664765357971 seconds\n",
      "Step 20400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1895.4175791740417 seconds\n",
      "Step 20500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1904.5529561042786 seconds\n",
      "Step 20600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1915.0705263614655 seconds\n",
      "Step 20700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1924.2095656394958 seconds\n",
      "Step 20800, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 1933.3576102256775 seconds\n",
      "Step 20900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1942.7021613121033 seconds\n",
      "Step 21000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1951.864610671997 seconds\n",
      "Step 21100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1961.0437095165253 seconds\n",
      "Step 21200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1971.0615298748016 seconds\n",
      "Step 21300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1980.1888391971588 seconds\n",
      "Step 21400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 1989.3999209403992 seconds\n",
      "Step 21500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 1998.7170565128326 seconds\n",
      "Step 21600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2007.9035248756409 seconds\n",
      "Step 21700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2017.05437707901 seconds\n",
      "Step 21800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2026.3974578380585 seconds\n",
      "Step 21900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2036.0925915241241 seconds\n",
      "Step 22000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2045.1911928653717 seconds\n",
      "Step 22100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2054.502675294876 seconds\n",
      "Step 22200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2063.746967315674 seconds\n",
      "Step 22300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2073.138758420944 seconds\n",
      "Step 22400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2082.261951684952 seconds\n",
      "Step 22500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2091.4358265399933 seconds\n",
      "Step 22600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2100.792797088623 seconds\n",
      "Step 22700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2111.1087939739227 seconds\n",
      "Step 22800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2120.271171092987 seconds\n",
      "Step 22900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2129.673614501953 seconds\n",
      "Step 23000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 2138.8129308223724 seconds\n",
      "Step 23100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2147.9248044490814 seconds\n",
      "Step 23200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2157.2410893440247 seconds\n",
      "Step 23300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2166.368983268738 seconds\n",
      "Step 23400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2175.511776447296 seconds\n",
      "Step 23500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2184.8783667087555 seconds\n",
      "Step 23600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2194.0538511276245 seconds\n",
      "Step 23700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2203.198605298996 seconds\n",
      "Step 23800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2212.524887084961 seconds\n",
      "Step 23900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2221.6909766197205 seconds\n",
      "Step 24000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2230.8189969062805 seconds\n",
      "Step 24100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2240.153322696686 seconds\n",
      "Step 24200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2249.36781001091 seconds\n",
      "Step 24300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2258.5271170139313 seconds\n",
      "Step 24400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2267.9163722991943 seconds\n",
      "Step 24500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2277.0801799297333 seconds\n",
      "Step 24600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2286.2331347465515 seconds\n",
      "Step 24700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2295.577300786972 seconds\n",
      "Step 24800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2304.7461111545563 seconds\n",
      "Step 24900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2314.43815946579 seconds\n",
      "Step 25000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2323.88702249527 seconds\n",
      "Step 25100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2333.044879436493 seconds\n",
      "Step 25200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2342.397279024124 seconds\n",
      "Step 25300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2351.5543286800385 seconds\n",
      "Step 25400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2361.4132623672485 seconds\n",
      "Step 25500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2370.8153257369995 seconds\n",
      "Step 25600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2380.063960313797 seconds\n",
      "Step 25700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2389.3178613185883 seconds\n",
      "Step 25800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2398.6612553596497 seconds\n",
      "Step 25900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2407.828164577484 seconds\n",
      "Step 26000, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 2416.957322359085 seconds\n",
      "Step 26100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2426.8799273967743 seconds\n",
      "Step 26200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2436.064549922943 seconds\n",
      "Step 26300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2445.2773542404175 seconds\n",
      "Step 26400, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 2454.6472792625427 seconds\n",
      "Step 26500, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 2463.908511161804 seconds\n",
      "Step 26600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2473.779209136963 seconds\n",
      "Step 26700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2483.119984149933 seconds\n",
      "Step 26800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2492.359492778778 seconds\n",
      "Step 26900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2501.4926784038544 seconds\n",
      "Step 27000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2510.913552045822 seconds\n",
      "Step 27100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2520.0535600185394 seconds\n",
      "Step 27200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2530.0240485668182 seconds\n",
      "Step 27300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2539.423258304596 seconds\n",
      "Step 27400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2548.597693681717 seconds\n",
      "Step 27500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2557.791286468506 seconds\n",
      "Step 27600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2567.240688562393 seconds\n",
      "Step 27700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2576.4596729278564 seconds\n",
      "Step 27800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2585.6759617328644 seconds\n",
      "Step 27900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2595.066319704056 seconds\n",
      "Step 28000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2604.2671859264374 seconds\n",
      "Step 28100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2614.087924718857 seconds\n",
      "Step 28200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2623.441696166992 seconds\n",
      "Step 28300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2632.6880416870117 seconds\n",
      "Step 28400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2641.837311029434 seconds\n",
      "Step 28500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2651.2576217651367 seconds\n",
      "Step 28600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2660.4161105155945 seconds\n",
      "Step 28700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2669.7978155612946 seconds\n",
      "Step 28800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2679.1276693344116 seconds\n",
      "Step 28900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2688.871742248535 seconds\n",
      "Step 29000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2698.267570734024 seconds\n",
      "Step 29100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2707.4812531471252 seconds\n",
      "Step 29200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2716.5921726226807 seconds\n",
      "Step 29300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2726.1854679584503 seconds\n",
      "Step 29400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2735.3554272651672 seconds\n",
      "Step 29500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2744.498767375946 seconds\n",
      "Step 29600, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2753.8450536727905 seconds\n",
      "Step 29700, loss: tensor(0.0009, grad_fn=<SubBackward0>), time elapsed: 2763.4964911937714 seconds\n",
      "Step 29800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2773.432855606079 seconds\n",
      "Step 29900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2782.811012983322 seconds\n",
      "Step 30000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2791.9235248565674 seconds\n",
      "Step 30100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2801.0719106197357 seconds\n",
      "Step 30200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2810.376258611679 seconds\n",
      "Step 30300, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2819.4960877895355 seconds\n",
      "Step 30400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2828.627158641815 seconds\n",
      "Step 30500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2837.984902858734 seconds\n",
      "Step 30600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2847.9517908096313 seconds\n",
      "Step 30700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2857.073052883148 seconds\n",
      "Step 30800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2866.3957438468933 seconds\n",
      "Step 30900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2875.7716448307037 seconds\n",
      "Step 31000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2884.900184392929 seconds\n",
      "Step 31100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2894.2679097652435 seconds\n",
      "Step 31200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2903.4106974601746 seconds\n",
      "Step 31300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2912.5700652599335 seconds\n",
      "Step 31400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2922.2162475585938 seconds\n",
      "Step 31500, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2932.145532846451 seconds\n",
      "Step 31600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2941.2813761234283 seconds\n",
      "Step 31700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2950.6120936870575 seconds\n",
      "Step 31800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2959.720479488373 seconds\n",
      "Step 31900, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2968.8344786167145 seconds\n",
      "Step 32000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2978.182933330536 seconds\n",
      "Step 32100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 2987.3477580547333 seconds\n",
      "Step 32200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 2996.5150094032288 seconds\n",
      "Step 32300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3005.8587448596954 seconds\n",
      "Step 32400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3015.0335335731506 seconds\n",
      "Step 32500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3024.1864397525787 seconds\n",
      "Step 32600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3033.5327467918396 seconds\n",
      "Step 32700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3042.6801104545593 seconds\n",
      "Step 32800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 3052.0656094551086 seconds\n",
      "Step 32900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 3061.2471313476562 seconds\n",
      "Step 33000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3070.4149730205536 seconds\n",
      "Step 33100, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3079.7760486602783 seconds\n",
      "Step 33200, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 3088.9027121067047 seconds\n",
      "Step 33300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3098.0371384620667 seconds\n",
      "Step 33400, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 3107.568949699402 seconds\n",
      "Step 33500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3117.027079343796 seconds\n",
      "Step 33600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3126.2875003814697 seconds\n",
      "Step 33700, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3135.8004009723663 seconds\n",
      "Step 33800, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3144.927965402603 seconds\n",
      "Step 33900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 3154.1628885269165 seconds\n",
      "Step 34000, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 3163.558254003525 seconds\n",
      "Step 34100, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 3174.037927865982 seconds\n",
      "Step 34200, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3183.175503730774 seconds\n",
      "Step 34300, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3192.5512356758118 seconds\n",
      "Step 34400, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3201.6994042396545 seconds\n",
      "Step 34500, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3210.8612201213837 seconds\n",
      "Step 34600, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3220.2102422714233 seconds\n",
      "Step 34700, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 3229.4607088565826 seconds\n",
      "Step 34800, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 3238.597929954529 seconds\n",
      "Step 34900, loss: tensor(0.0007, grad_fn=<SubBackward0>), time elapsed: 3247.945008993149 seconds\n",
      "Step 35000, loss: tensor(0.0008, grad_fn=<SubBackward0>), time elapsed: 3257.1285333633423 seconds\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0195, grad_fn=<SubBackward0>), time elapsed: 0.14730000495910645 seconds\n",
      "Step 100, loss: tensor(0.0133, grad_fn=<SubBackward0>), time elapsed: 9.442373991012573 seconds\n",
      "Step 200, loss: tensor(0.0094, grad_fn=<SubBackward0>), time elapsed: 19.392103672027588 seconds\n",
      "Step 300, loss: tensor(0.0075, grad_fn=<SubBackward0>), time elapsed: 28.51398205757141 seconds\n",
      "Step 400, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 37.61577653884888 seconds\n",
      "Step 500, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 46.83487868309021 seconds\n",
      "Step 600, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 55.955233573913574 seconds\n",
      "Step 700, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 65.05068182945251 seconds\n",
      "Step 800, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 74.30174517631531 seconds\n",
      "Step 900, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 83.42614650726318 seconds\n",
      "Step 1000, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 92.68058013916016 seconds\n",
      "Step 1100, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 101.74936580657959 seconds\n",
      "Step 1200, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 110.8522458076477 seconds\n",
      "Step 1300, loss: tensor(0.0039, grad_fn=<SubBackward0>), time elapsed: 120.15508937835693 seconds\n",
      "Step 1400, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 129.25653624534607 seconds\n",
      "Step 1500, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 138.3676562309265 seconds\n",
      "Step 1600, loss: tensor(0.0038, grad_fn=<SubBackward0>), time elapsed: 147.62970733642578 seconds\n",
      "Step 1700, loss: tensor(0.0036, grad_fn=<SubBackward0>), time elapsed: 156.73937821388245 seconds\n",
      "Step 1800, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 165.82806420326233 seconds\n",
      "Step 1900, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 175.12271165847778 seconds\n",
      "Step 2000, loss: tensor(0.0034, grad_fn=<SubBackward0>), time elapsed: 184.21933484077454 seconds\n",
      "Step 2100, loss: tensor(0.0035, grad_fn=<SubBackward0>), time elapsed: 193.33190369606018 seconds\n",
      "Step 2200, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 202.580650806427 seconds\n",
      "Step 2300, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 211.71362257003784 seconds\n",
      "Step 2400, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 220.9827241897583 seconds\n",
      "Step 2500, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 230.07882046699524 seconds\n",
      "Step 2600, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 239.19733309745789 seconds\n",
      "Step 2700, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 248.48194193840027 seconds\n",
      "Step 2800, loss: tensor(0.0033, grad_fn=<SubBackward0>), time elapsed: 257.59098505973816 seconds\n",
      "Step 2900, loss: tensor(0.0032, grad_fn=<SubBackward0>), time elapsed: 266.7571804523468 seconds\n",
      "Step 3000, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 276.0219612121582 seconds\n",
      "Step 3100, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 285.1256654262543 seconds\n",
      "Step 3200, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 294.2295434474945 seconds\n",
      "Step 3300, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 303.5170953273773 seconds\n",
      "Step 3400, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 312.6474754810333 seconds\n",
      "Step 3500, loss: tensor(0.0031, grad_fn=<SubBackward0>), time elapsed: 321.7592294216156 seconds\n",
      "Step 3600, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 331.7113709449768 seconds\n",
      "Step 3700, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 340.8341782093048 seconds\n",
      "Step 3800, loss: tensor(0.0029, grad_fn=<SubBackward0>), time elapsed: 349.93705320358276 seconds\n",
      "Step 3900, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 359.2195327281952 seconds\n",
      "Step 4000, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 368.35858726501465 seconds\n",
      "Step 4100, loss: tensor(0.0027, grad_fn=<SubBackward0>), time elapsed: 377.6961522102356 seconds\n",
      "Step 4200, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 387.5740633010864 seconds\n",
      "Step 4300, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 396.7155559062958 seconds\n",
      "Step 4400, loss: tensor(0.0024, grad_fn=<SubBackward0>), time elapsed: 406.0240924358368 seconds\n",
      "Step 4500, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 415.2256908416748 seconds\n",
      "Step 4600, loss: tensor(0.0025, grad_fn=<SubBackward0>), time elapsed: 424.44727540016174 seconds\n",
      "Step 4700, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 433.7351713180542 seconds\n",
      "Step 4800, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 442.84478878974915 seconds\n",
      "Step 4900, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 451.95578956604004 seconds\n",
      "Step 5000, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 462.0394732952118 seconds\n",
      "Step 5100, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 471.21206641197205 seconds\n",
      "Step 5200, loss: tensor(0.0023, grad_fn=<SubBackward0>), time elapsed: 480.3474323749542 seconds\n",
      "Step 5300, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 489.6439199447632 seconds\n",
      "Step 5400, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 498.7517466545105 seconds\n",
      "Step 5500, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 507.9894120693207 seconds\n",
      "Step 5600, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 517.3464126586914 seconds\n",
      "Step 5700, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 526.4583387374878 seconds\n",
      "Step 5800, loss: tensor(0.0022, grad_fn=<SubBackward0>), time elapsed: 535.5861902236938 seconds\n",
      "Step 5900, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 544.8933727741241 seconds\n",
      "Step 6000, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 554.7563714981079 seconds\n",
      "Step 6100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 564.1161370277405 seconds\n",
      "Step 6200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 573.261839389801 seconds\n",
      "Step 6300, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 582.3916347026825 seconds\n",
      "Step 6400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 591.8743352890015 seconds\n",
      "Step 6500, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 600.9964895248413 seconds\n",
      "Step 6600, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 610.34490275383 seconds\n",
      "Step 6700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 619.6574413776398 seconds\n",
      "Step 6800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 628.7758107185364 seconds\n",
      "Step 6900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 637.9443826675415 seconds\n",
      "Step 7000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 647.8255369663239 seconds\n",
      "Step 7100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 657.0594873428345 seconds\n",
      "Step 7200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 666.1857326030731 seconds\n",
      "Step 7300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 675.4872019290924 seconds\n",
      "Step 7400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 684.6426193714142 seconds\n",
      "Step 7500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 694.6709697246552 seconds\n",
      "Step 7600, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 703.9734311103821 seconds\n",
      "Step 7700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 713.1053719520569 seconds\n",
      "Step 7800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 722.3810927867889 seconds\n",
      "Step 7900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 731.6420776844025 seconds\n",
      "Step 8000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 740.7533657550812 seconds\n",
      "Step 8100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 750.0256087779999 seconds\n",
      "Step 8200, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 759.1739795207977 seconds\n",
      "Step 8300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 768.2945494651794 seconds\n",
      "Step 8400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 777.7301471233368 seconds\n",
      "Step 8500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 786.9894328117371 seconds\n",
      "Step 8600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 796.5935513973236 seconds\n",
      "Step 8700, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 805.9996275901794 seconds\n",
      "Step 8800, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 815.1463131904602 seconds\n",
      "Step 8900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 824.2935245037079 seconds\n",
      "Step 9000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 833.7236697673798 seconds\n",
      "Step 9100, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 844.440810918808 seconds\n",
      "Step 9200, loss: tensor(0.0021, grad_fn=<SubBackward0>), time elapsed: 853.5715570449829 seconds\n",
      "Step 9300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 862.870768070221 seconds\n",
      "Step 9400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 872.0277869701385 seconds\n",
      "Step 9500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 881.174733877182 seconds\n",
      "Step 9600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 890.4360685348511 seconds\n",
      "Step 9700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 899.5525794029236 seconds\n",
      "Step 9800, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 908.8590679168701 seconds\n",
      "Step 9900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 917.9698276519775 seconds\n",
      "Step 10000, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 927.1044051647186 seconds\n",
      "Step 10100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 936.4594006538391 seconds\n",
      "Step 10200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 945.5939297676086 seconds\n",
      "Step 10300, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 954.7224273681641 seconds\n",
      "Step 10400, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 964.0283555984497 seconds\n",
      "Step 10500, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 973.1503593921661 seconds\n",
      "Step 10600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 982.2875807285309 seconds\n",
      "Step 10700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 991.5876996517181 seconds\n",
      "Step 10800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1000.7503521442413 seconds\n",
      "Step 10900, loss: tensor(0.0020, grad_fn=<SubBackward0>), time elapsed: 1009.8851704597473 seconds\n",
      "Step 11000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1019.2195112705231 seconds\n",
      "Step 11100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1028.355339050293 seconds\n",
      "Step 11200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1037.5274991989136 seconds\n",
      "Step 11300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1046.8449456691742 seconds\n",
      "Step 11400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1055.9689161777496 seconds\n",
      "Step 11500, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1065.132423400879 seconds\n",
      "Step 11600, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1075.0511076450348 seconds\n",
      "Step 11700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1084.359393119812 seconds\n",
      "Step 11800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1093.6952407360077 seconds\n",
      "Step 11900, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1102.8149304389954 seconds\n",
      "Step 12000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1111.971343278885 seconds\n",
      "Step 12100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1121.3183994293213 seconds\n",
      "Step 12200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1130.9338092803955 seconds\n",
      "Step 12300, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1140.1928486824036 seconds\n",
      "Step 12400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1149.5070142745972 seconds\n",
      "Step 12500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1158.6807944774628 seconds\n",
      "Step 12600, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1167.8178074359894 seconds\n",
      "Step 12700, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1177.3663213253021 seconds\n",
      "Step 12800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1186.5985357761383 seconds\n",
      "Step 12900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1195.7410356998444 seconds\n",
      "Step 13000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1205.0480678081512 seconds\n",
      "Step 13100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1215.6937642097473 seconds\n",
      "Step 13200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1224.80890417099 seconds\n",
      "Step 13300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1234.1273562908173 seconds\n",
      "Step 13400, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1243.2878532409668 seconds\n",
      "Step 13500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1252.4162237644196 seconds\n",
      "Step 13600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1261.7131900787354 seconds\n",
      "Step 13700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1270.81440782547 seconds\n",
      "Step 13800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1279.9484105110168 seconds\n",
      "Step 13900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1289.2249312400818 seconds\n",
      "Step 14000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1298.4546165466309 seconds\n",
      "Step 14100, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1307.6160991191864 seconds\n",
      "Step 14200, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1316.9486651420593 seconds\n",
      "Step 14300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1326.074372768402 seconds\n",
      "Step 14400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1335.4364249706268 seconds\n",
      "Step 14500, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1344.6435680389404 seconds\n",
      "Step 14600, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1354.0690326690674 seconds\n",
      "Step 14700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1363.4948177337646 seconds\n",
      "Step 14800, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1372.6031861305237 seconds\n",
      "Step 14900, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1381.7363657951355 seconds\n",
      "Step 15000, loss: tensor(0.0019, grad_fn=<SubBackward0>), time elapsed: 1391.4309029579163 seconds\n",
      "Step 15100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1400.54319024086 seconds\n",
      "Step 15200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1409.6785190105438 seconds\n",
      "Step 15300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1418.9873847961426 seconds\n",
      "Step 15400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1428.6839129924774 seconds\n",
      "Step 15500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1437.8139383792877 seconds\n",
      "Step 15600, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1447.2468416690826 seconds\n",
      "Step 15700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1456.4892992973328 seconds\n",
      "Step 15800, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1465.6584992408752 seconds\n",
      "Step 15900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1475.0034744739532 seconds\n",
      "Step 16000, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1484.1774039268494 seconds\n",
      "Step 16100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1493.420484304428 seconds\n",
      "Step 16200, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1503.1266803741455 seconds\n",
      "Step 16300, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1512.2903289794922 seconds\n",
      "Step 16400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1521.6716847419739 seconds\n",
      "Step 16500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1531.0351958274841 seconds\n",
      "Step 16600, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1540.2183237075806 seconds\n",
      "Step 16700, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1549.5626480579376 seconds\n",
      "Step 16800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1558.6738078594208 seconds\n",
      "Step 16900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1568.2590816020966 seconds\n",
      "Step 17000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1577.691617488861 seconds\n",
      "Step 17100, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1586.8100094795227 seconds\n",
      "Step 17200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1595.91508436203 seconds\n",
      "Step 17300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1605.255680322647 seconds\n",
      "Step 17400, loss: tensor(0.0018, grad_fn=<SubBackward0>), time elapsed: 1614.5934793949127 seconds\n",
      "Step 17500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1624.306391954422 seconds\n",
      "Step 17600, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1633.6817169189453 seconds\n",
      "Step 17700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1642.9337561130524 seconds\n",
      "Step 17800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1652.047498703003 seconds\n",
      "Step 17900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1661.3209600448608 seconds\n",
      "Step 18000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1670.433346748352 seconds\n",
      "Step 18100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1680.1350266933441 seconds\n",
      "Step 18200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1689.5812418460846 seconds\n",
      "Step 18300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1698.7053911685944 seconds\n",
      "Step 18400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1707.853678226471 seconds\n",
      "Step 18500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1717.1671109199524 seconds\n",
      "Step 18600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1726.855705022812 seconds\n",
      "Step 18700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1736.009786605835 seconds\n",
      "Step 18800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1745.5496578216553 seconds\n",
      "Step 18900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1754.6931746006012 seconds\n",
      "Step 19000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1763.9100711345673 seconds\n",
      "Step 19100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1773.272206068039 seconds\n",
      "Step 19200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1782.4234013557434 seconds\n",
      "Step 19300, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1791.7808423042297 seconds\n",
      "Step 19400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1800.9441845417023 seconds\n",
      "Step 19500, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1810.0726265907288 seconds\n",
      "Step 19600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1819.3933806419373 seconds\n",
      "Step 19700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1828.5404160022736 seconds\n",
      "Step 19800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1837.6710348129272 seconds\n",
      "Step 19900, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1846.985605955124 seconds\n",
      "Step 20000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1857.2984273433685 seconds\n",
      "Step 20100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1867.2555141448975 seconds\n",
      "Step 20200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1876.5584182739258 seconds\n",
      "Step 20300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1885.6846013069153 seconds\n",
      "Step 20400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1894.8047482967377 seconds\n",
      "Step 20500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1904.0721561908722 seconds\n",
      "Step 20600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1913.2016124725342 seconds\n",
      "Step 20700, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1922.3643870353699 seconds\n",
      "Step 20800, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1931.695336818695 seconds\n",
      "Step 20900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1940.8540570735931 seconds\n",
      "Step 21000, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1950.1525938510895 seconds\n",
      "Step 21100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1959.5781495571136 seconds\n",
      "Step 21200, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 1968.9424426555634 seconds\n",
      "Step 21300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1978.3853299617767 seconds\n",
      "Step 21400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 1987.7000937461853 seconds\n",
      "Step 21500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 1996.813393831253 seconds\n",
      "Step 21600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2005.9304735660553 seconds\n",
      "Step 21700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2015.7181341648102 seconds\n",
      "Step 21800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2024.8953576087952 seconds\n",
      "Step 21900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2034.2534766197205 seconds\n",
      "Step 22000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2043.4274134635925 seconds\n",
      "Step 22100, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 2052.612773180008 seconds\n",
      "Step 22200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2061.9912145137787 seconds\n",
      "Step 22300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2071.2054307460785 seconds\n",
      "Step 22400, loss: tensor(0.0017, grad_fn=<SubBackward0>), time elapsed: 2080.397535800934 seconds\n",
      "Step 22500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2089.8181834220886 seconds\n",
      "Step 22600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2099.0363867282867 seconds\n",
      "Step 22700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2108.6468760967255 seconds\n",
      "Step 22800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2117.97984623909 seconds\n",
      "Step 22900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2127.122497320175 seconds\n",
      "Step 23000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2136.2985756397247 seconds\n",
      "Step 23100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2145.6274106502533 seconds\n",
      "Step 23200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2154.828953027725 seconds\n",
      "Step 23300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2164.058897256851 seconds\n",
      "Step 23400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2173.418295145035 seconds\n",
      "Step 23500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2182.5980095863342 seconds\n",
      "Step 23600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2191.8442170619965 seconds\n",
      "Step 23700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2201.3422768115997 seconds\n",
      "Step 23800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2210.553095817566 seconds\n",
      "Step 23900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2219.7924296855927 seconds\n",
      "Step 24000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2229.5623528957367 seconds\n",
      "Step 24100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2238.8136818408966 seconds\n",
      "Step 24200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2248.0435733795166 seconds\n",
      "Step 24300, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2257.478828191757 seconds\n",
      "Step 24400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2266.6509902477264 seconds\n",
      "Step 24500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2275.8025500774384 seconds\n",
      "Step 24600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2285.5999166965485 seconds\n",
      "Step 24700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2294.7193262577057 seconds\n",
      "Step 24800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2303.863442659378 seconds\n",
      "Step 24900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2313.2286689281464 seconds\n",
      "Step 25000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2322.3567402362823 seconds\n",
      "Step 25100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2331.6851313114166 seconds\n",
      "Step 25200, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2340.8187124729156 seconds\n",
      "Step 25300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2349.992134332657 seconds\n",
      "Step 25400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2359.783225774765 seconds\n",
      "Step 25500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2368.9123179912567 seconds\n",
      "Step 25600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2378.0914890766144 seconds\n",
      "Step 25700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2387.4871633052826 seconds\n",
      "Step 25800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2396.6243357658386 seconds\n",
      "Step 25900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2405.802237510681 seconds\n",
      "Step 26000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2415.5724251270294 seconds\n",
      "Step 26100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2424.698324203491 seconds\n",
      "Step 26200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2433.8190784454346 seconds\n",
      "Step 26300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2443.1661536693573 seconds\n",
      "Step 26400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2452.3109409809113 seconds\n",
      "Step 26500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2461.4253582954407 seconds\n",
      "Step 26600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2471.441648006439 seconds\n",
      "Step 26700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2480.6143820285797 seconds\n",
      "Step 26800, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2489.765654563904 seconds\n",
      "Step 26900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2499.131534099579 seconds\n",
      "Step 27000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2508.270780324936 seconds\n",
      "Step 27100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2517.4530477523804 seconds\n",
      "Step 27200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2526.8427131175995 seconds\n",
      "Step 27300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2536.0229218006134 seconds\n",
      "Step 27400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2545.1833720207214 seconds\n",
      "Step 27500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2554.5636682510376 seconds\n",
      "Step 27600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2563.7338185310364 seconds\n",
      "Step 27700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2572.8806278705597 seconds\n",
      "Step 27800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2582.2257175445557 seconds\n",
      "Step 27900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2591.4292693138123 seconds\n",
      "Step 28000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2600.54035115242 seconds\n",
      "Step 28100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2610.3781011104584 seconds\n",
      "Step 28200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2619.5464990139008 seconds\n",
      "Step 28300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2628.7183907032013 seconds\n",
      "Step 28400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2638.0794684886932 seconds\n",
      "Step 28500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2647.2389857769012 seconds\n",
      "Step 28600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2656.65838599205 seconds\n",
      "Step 28700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2665.7871351242065 seconds\n",
      "Step 28800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2674.9536023139954 seconds\n",
      "Step 28900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2684.4266378879547 seconds\n",
      "Step 29000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2693.731699705124 seconds\n",
      "Step 29100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2703.4021711349487 seconds\n",
      "Step 29200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2712.8650164604187 seconds\n",
      "Step 29300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2722.0482852458954 seconds\n",
      "Step 29400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2731.204103946686 seconds\n",
      "Step 29500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2740.5638885498047 seconds\n",
      "Step 29600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2750.505003452301 seconds\n",
      "Step 29700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2759.650048971176 seconds\n",
      "Step 29800, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2769.004367828369 seconds\n",
      "Step 29900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2778.1878650188446 seconds\n",
      "Step 30000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2787.4582962989807 seconds\n",
      "Step 30100, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2796.8153836727142 seconds\n",
      "Step 30200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2805.979129076004 seconds\n",
      "Step 30300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2815.131884098053 seconds\n",
      "Step 30400, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2824.516662120819 seconds\n",
      "Step 30500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2834.282270669937 seconds\n",
      "Step 30600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2843.4666368961334 seconds\n",
      "Step 30700, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2852.9605615139008 seconds\n",
      "Step 30800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2862.099817276001 seconds\n",
      "Step 30900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2871.2412478923798 seconds\n",
      "Step 31000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2880.568223953247 seconds\n",
      "Step 31100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2890.015261411667 seconds\n",
      "Step 31200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 2899.2741837501526 seconds\n",
      "Step 31300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2908.612822532654 seconds\n",
      "Step 31400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2917.772325515747 seconds\n",
      "Step 31500, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2927.0429270267487 seconds\n",
      "Step 31600, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2937.0833406448364 seconds\n",
      "Step 31700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2946.2163755893707 seconds\n",
      "Step 31800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2955.380537033081 seconds\n",
      "Step 31900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 2964.7619807720184 seconds\n",
      "Step 32000, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2973.909042596817 seconds\n",
      "Step 32100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2983.038400411606 seconds\n",
      "Step 32200, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 2993.768419981003 seconds\n",
      "Step 32300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3002.8951094150543 seconds\n",
      "Step 32400, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 3012.245745897293 seconds\n",
      "Step 32500, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 3021.358793735504 seconds\n",
      "Step 32600, loss: tensor(0.0013, grad_fn=<SubBackward0>), time elapsed: 3030.4771633148193 seconds\n",
      "Step 32700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3039.790955066681 seconds\n",
      "Step 32800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3048.93510389328 seconds\n",
      "Step 32900, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 3058.0366263389587 seconds\n",
      "Step 33000, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 3067.3670876026154 seconds\n",
      "Step 33100, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 3076.4665179252625 seconds\n",
      "Step 33200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 3085.6285519599915 seconds\n",
      "Step 33300, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3095.583758831024 seconds\n",
      "Step 33400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3104.7072072029114 seconds\n",
      "Step 33500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3113.863894224167 seconds\n",
      "Step 33600, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 3123.200753927231 seconds\n",
      "Step 33700, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3132.334357023239 seconds\n",
      "Step 33800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3141.480639696121 seconds\n",
      "Step 33900, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 3150.8033730983734 seconds\n",
      "Step 34000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 3159.9337487220764 seconds\n",
      "Step 34100, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3169.072913169861 seconds\n",
      "Step 34200, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 3178.37526345253 seconds\n",
      "Step 34300, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 3187.506692647934 seconds\n",
      "Step 34400, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3196.6405189037323 seconds\n",
      "Step 34500, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3206.0197319984436 seconds\n",
      "Step 34600, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 3215.1886687278748 seconds\n",
      "Step 34700, loss: tensor(0.0016, grad_fn=<SubBackward0>), time elapsed: 3224.358763694763 seconds\n",
      "Step 34800, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3233.750274658203 seconds\n",
      "Step 34900, loss: tensor(0.0015, grad_fn=<SubBackward0>), time elapsed: 3242.9092915058136 seconds\n",
      "Step 35000, loss: tensor(0.0014, grad_fn=<SubBackward0>), time elapsed: 3252.1115095615387 seconds\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0373, grad_fn=<SubBackward0>), time elapsed: 0.1421797275543213 seconds\n",
      "Step 100, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 10.334471702575684 seconds\n",
      "Step 200, loss: tensor(0.0249, grad_fn=<SubBackward0>), time elapsed: 19.43570852279663 seconds\n",
      "Step 300, loss: tensor(0.0209, grad_fn=<SubBackward0>), time elapsed: 28.519947290420532 seconds\n",
      "Step 400, loss: tensor(0.0178, grad_fn=<SubBackward0>), time elapsed: 37.763771057128906 seconds\n",
      "Step 500, loss: tensor(0.0154, grad_fn=<SubBackward0>), time elapsed: 46.839110136032104 seconds\n",
      "Step 600, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 56.10249304771423 seconds\n",
      "Step 700, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 65.21308946609497 seconds\n",
      "Step 800, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 74.32118082046509 seconds\n",
      "Step 900, loss: tensor(0.0102, grad_fn=<SubBackward0>), time elapsed: 83.57382655143738 seconds\n",
      "Step 1000, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 92.66296601295471 seconds\n",
      "Step 1100, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 101.81649374961853 seconds\n",
      "Step 1200, loss: tensor(0.0087, grad_fn=<SubBackward0>), time elapsed: 111.0846335887909 seconds\n",
      "Step 1300, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 120.20363855361938 seconds\n",
      "Step 1400, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 129.33219933509827 seconds\n",
      "Step 1500, loss: tensor(0.0077, grad_fn=<SubBackward0>), time elapsed: 138.5862638950348 seconds\n",
      "Step 1600, loss: tensor(0.0075, grad_fn=<SubBackward0>), time elapsed: 147.68265056610107 seconds\n",
      "Step 1700, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 156.79213309288025 seconds\n",
      "Step 1800, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 166.06623339653015 seconds\n",
      "Step 1900, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 175.19191884994507 seconds\n",
      "Step 2000, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 184.29901313781738 seconds\n",
      "Step 2100, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 193.59211707115173 seconds\n",
      "Step 2200, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 202.69892287254333 seconds\n",
      "Step 2300, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 211.9559347629547 seconds\n",
      "Step 2400, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 221.06986331939697 seconds\n",
      "Step 2500, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 230.18681859970093 seconds\n",
      "Step 2600, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 239.4308943748474 seconds\n",
      "Step 2700, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 248.704519033432 seconds\n",
      "Step 2800, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 258.4581768512726 seconds\n",
      "Step 2900, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 267.99985575675964 seconds\n",
      "Step 3000, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 277.3646876811981 seconds\n",
      "Step 3100, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 286.68238282203674 seconds\n",
      "Step 3200, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 296.1648771762848 seconds\n",
      "Step 3300, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 305.46399784088135 seconds\n",
      "Step 3400, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 314.7775089740753 seconds\n",
      "Step 3500, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 324.26420044898987 seconds\n",
      "Step 3600, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 333.57991075515747 seconds\n",
      "Step 3700, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 342.91038513183594 seconds\n",
      "Step 3800, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 352.71205711364746 seconds\n",
      "Step 3900, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 362.4998092651367 seconds\n",
      "Step 4000, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 372.0063488483429 seconds\n",
      "Step 4100, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 381.30220913887024 seconds\n",
      "Step 4200, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 390.5794360637665 seconds\n",
      "Step 4300, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 400.00449895858765 seconds\n",
      "Step 4400, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 409.28787541389465 seconds\n",
      "Step 4500, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 418.6035828590393 seconds\n",
      "Step 4600, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 428.0798318386078 seconds\n",
      "Step 4700, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 437.4061281681061 seconds\n",
      "Step 4800, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 446.6820251941681 seconds\n",
      "Step 4900, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 456.1687481403351 seconds\n",
      "Step 5000, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 465.53465700149536 seconds\n",
      "Step 5100, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 474.90646386146545 seconds\n",
      "Step 5200, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 484.3702187538147 seconds\n",
      "Step 5300, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 493.67313170433044 seconds\n",
      "Step 5400, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 503.0037190914154 seconds\n",
      "Step 5500, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 512.5079755783081 seconds\n",
      "Step 5600, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 521.8501527309418 seconds\n",
      "Step 5700, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 531.3731124401093 seconds\n",
      "Step 5800, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 540.693710565567 seconds\n",
      "Step 5900, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 550.0119519233704 seconds\n",
      "Step 6000, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 559.5193254947662 seconds\n",
      "Step 6100, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 568.7807669639587 seconds\n",
      "Step 6200, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 578.0964450836182 seconds\n",
      "Step 6300, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 587.5917088985443 seconds\n",
      "Step 6400, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 596.8799772262573 seconds\n",
      "Step 6500, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 606.1864202022552 seconds\n",
      "Step 6600, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 615.6708776950836 seconds\n",
      "Step 6700, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 625.0170273780823 seconds\n",
      "Step 6800, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 634.7225904464722 seconds\n",
      "Step 6900, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 644.2115545272827 seconds\n",
      "Step 7000, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 653.5522058010101 seconds\n",
      "Step 7100, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 662.8821852207184 seconds\n",
      "Step 7200, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 672.4003448486328 seconds\n",
      "Step 7300, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 681.7329230308533 seconds\n",
      "Step 7400, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 691.2048237323761 seconds\n",
      "Step 7500, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 700.480899810791 seconds\n",
      "Step 7600, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 709.8078329563141 seconds\n",
      "Step 7700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 719.7984070777893 seconds\n",
      "Step 7800, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 729.0807235240936 seconds\n",
      "Step 7900, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 738.3652236461639 seconds\n",
      "Step 8000, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 747.8705911636353 seconds\n",
      "Step 8100, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 757.1716828346252 seconds\n",
      "Step 8200, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 766.4770822525024 seconds\n",
      "Step 8300, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 775.9576139450073 seconds\n",
      "Step 8400, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 785.2503743171692 seconds\n",
      "Step 8500, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 794.5606682300568 seconds\n",
      "Step 8600, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 804.1046891212463 seconds\n",
      "Step 8700, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 813.401585817337 seconds\n",
      "Step 8800, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 822.7425756454468 seconds\n",
      "Step 8900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 832.6742706298828 seconds\n",
      "Step 9000, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 842.1025309562683 seconds\n",
      "Step 9100, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 851.4213540554047 seconds\n",
      "Step 9200, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 860.9059717655182 seconds\n",
      "Step 9300, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 870.2696316242218 seconds\n",
      "Step 9400, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 879.756317615509 seconds\n",
      "Step 9500, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 889.0387396812439 seconds\n",
      "Step 9600, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 898.3656888008118 seconds\n",
      "Step 9700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 907.8399591445923 seconds\n",
      "Step 9800, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 917.4800367355347 seconds\n",
      "Step 9900, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 927.0625867843628 seconds\n",
      "Step 10000, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 936.6035203933716 seconds\n",
      "Step 10100, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 946.1599781513214 seconds\n",
      "Step 10200, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 955.5436315536499 seconds\n",
      "Step 10300, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 964.9958825111389 seconds\n",
      "Step 10400, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 974.2585611343384 seconds\n",
      "Step 10500, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 983.5469298362732 seconds\n",
      "Step 10600, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 993.0023429393768 seconds\n",
      "Step 10700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1002.3003752231598 seconds\n",
      "Step 10800, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1011.5595541000366 seconds\n",
      "Step 10900, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1021.0349860191345 seconds\n",
      "Step 11000, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1030.3182485103607 seconds\n",
      "Step 11100, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1039.6204149723053 seconds\n",
      "Step 11200, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1049.129014968872 seconds\n",
      "Step 11300, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1058.4686908721924 seconds\n",
      "Step 11400, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1067.9543735980988 seconds\n",
      "Step 11500, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1077.2478275299072 seconds\n",
      "Step 11600, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1086.5181472301483 seconds\n",
      "Step 11700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1096.0194323062897 seconds\n",
      "Step 11800, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1105.3026089668274 seconds\n",
      "Step 11900, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1114.6297628879547 seconds\n",
      "Step 12000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1124.1267955303192 seconds\n",
      "Step 12100, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1133.4405026435852 seconds\n",
      "Step 12200, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1142.7405393123627 seconds\n",
      "Step 12300, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1152.2303738594055 seconds\n",
      "Step 12400, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1161.5470550060272 seconds\n",
      "Step 12500, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1170.8577711582184 seconds\n",
      "Step 12600, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1180.3005220890045 seconds\n",
      "Step 12700, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1189.6370284557343 seconds\n",
      "Step 12800, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1198.957087278366 seconds\n",
      "Step 12900, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1208.4123804569244 seconds\n",
      "Step 13000, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1217.7154793739319 seconds\n",
      "Step 13100, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1227.0395185947418 seconds\n",
      "Step 13200, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1236.5576884746552 seconds\n",
      "Step 13300, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1245.9002966880798 seconds\n",
      "Step 13400, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1255.2033185958862 seconds\n",
      "Step 13500, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1264.6776518821716 seconds\n",
      "Step 13600, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1273.9415168762207 seconds\n",
      "Step 13700, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1283.4174118041992 seconds\n",
      "Step 13800, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1292.701878786087 seconds\n",
      "Step 13900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1301.9821367263794 seconds\n",
      "Step 14000, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1311.4417264461517 seconds\n",
      "Step 14100, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1320.780339717865 seconds\n",
      "Step 14200, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1330.0784018039703 seconds\n",
      "Step 14300, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1339.5337541103363 seconds\n",
      "Step 14400, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1348.81600522995 seconds\n",
      "Step 14500, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1358.1432118415833 seconds\n",
      "Step 14600, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1367.6559364795685 seconds\n",
      "Step 14700, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1376.9748673439026 seconds\n",
      "Step 14800, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1386.2936556339264 seconds\n",
      "Step 14900, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1395.776094198227 seconds\n",
      "Step 15000, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1405.118908405304 seconds\n",
      "Step 15100, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1414.4671721458435 seconds\n",
      "Step 15200, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1423.9766945838928 seconds\n",
      "Step 15300, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1433.4184205532074 seconds\n",
      "Step 15400, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1442.737270116806 seconds\n",
      "Step 15500, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1452.2281634807587 seconds\n",
      "Step 15600, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1461.556561946869 seconds\n",
      "Step 15700, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1470.9118270874023 seconds\n",
      "Step 15800, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1480.4557559490204 seconds\n",
      "Step 15900, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1489.7777247428894 seconds\n",
      "Step 16000, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1499.240356683731 seconds\n",
      "Step 16100, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1508.5706598758698 seconds\n",
      "Step 16200, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1517.8594467639923 seconds\n",
      "Step 16300, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1527.340765953064 seconds\n",
      "Step 16400, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1536.6553575992584 seconds\n",
      "Step 16500, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1546.0169563293457 seconds\n",
      "Step 16600, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1555.6850850582123 seconds\n",
      "Step 16700, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1565.0880274772644 seconds\n",
      "Step 16800, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1574.383181810379 seconds\n",
      "Step 16900, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1583.8385906219482 seconds\n",
      "Step 17000, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1593.119022846222 seconds\n",
      "Step 17100, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1602.4271585941315 seconds\n",
      "Step 17200, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1611.8588469028473 seconds\n",
      "Step 17300, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1621.1390070915222 seconds\n",
      "Step 17400, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1630.4411087036133 seconds\n",
      "Step 17500, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1639.89848279953 seconds\n",
      "Step 17600, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 1649.1819183826447 seconds\n",
      "Step 17700, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1658.518652677536 seconds\n",
      "Step 17800, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1667.9801423549652 seconds\n",
      "Step 17900, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1677.320900440216 seconds\n",
      "Step 18000, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1686.6731057167053 seconds\n",
      "Step 18100, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1696.1983003616333 seconds\n",
      "Step 18200, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1705.4982941150665 seconds\n",
      "Step 18300, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1714.983992099762 seconds\n",
      "Step 18400, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1724.3077490329742 seconds\n",
      "Step 18500, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1733.7383389472961 seconds\n",
      "Step 18600, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1743.3001885414124 seconds\n",
      "Step 18700, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1752.631977558136 seconds\n",
      "Step 18800, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1762.0441491603851 seconds\n",
      "Step 18900, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1771.6257932186127 seconds\n",
      "Step 19000, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1780.937285900116 seconds\n",
      "Step 19100, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 1790.3188478946686 seconds\n",
      "Step 19200, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1799.8261985778809 seconds\n",
      "Step 19300, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1809.1638429164886 seconds\n",
      "Step 19400, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1818.4806141853333 seconds\n",
      "Step 19500, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 1828.0246224403381 seconds\n",
      "Step 19600, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1837.3799788951874 seconds\n",
      "Step 19700, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1846.7426071166992 seconds\n",
      "Step 19800, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 1856.302199602127 seconds\n",
      "Step 19900, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1865.6292147636414 seconds\n",
      "Step 20000, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 1874.9671547412872 seconds\n",
      "Step 20100, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1884.480808019638 seconds\n",
      "Step 20200, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 1893.824640750885 seconds\n",
      "Step 20300, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1903.2185530662537 seconds\n",
      "Step 20400, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1913.2104835510254 seconds\n",
      "Step 20500, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 1922.7028079032898 seconds\n",
      "Step 20600, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1932.1104047298431 seconds\n",
      "Step 20700, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 1942.036896944046 seconds\n",
      "Step 20800, loss: tensor(0.0046, grad_fn=<SubBackward0>), time elapsed: 1951.6404719352722 seconds\n",
      "Step 20900, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1961.4215784072876 seconds\n",
      "Step 21000, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1971.278336763382 seconds\n",
      "Step 21100, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 1980.795180797577 seconds\n",
      "Step 21200, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 1990.925262928009 seconds\n",
      "Step 21300, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2000.341421365738 seconds\n",
      "Step 21400, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2009.6964933872223 seconds\n",
      "Step 21500, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2019.2327845096588 seconds\n",
      "Step 21600, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2028.608516216278 seconds\n",
      "Step 21700, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2037.9182314872742 seconds\n",
      "Step 21800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2047.9456300735474 seconds\n",
      "Step 21900, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 2057.3497157096863 seconds\n",
      "Step 22000, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2066.9191732406616 seconds\n",
      "Step 22100, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 2076.60533118248 seconds\n",
      "Step 22200, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2086.1151790618896 seconds\n",
      "Step 22300, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2095.7713117599487 seconds\n",
      "Step 22400, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2105.611936569214 seconds\n",
      "Step 22500, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2115.2235124111176 seconds\n",
      "Step 22600, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2124.654515981674 seconds\n",
      "Step 22700, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2134.1501944065094 seconds\n",
      "Step 22800, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2143.466945409775 seconds\n",
      "Step 22900, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2152.797919988632 seconds\n",
      "Step 23000, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2162.2691559791565 seconds\n",
      "Step 23100, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2171.576262950897 seconds\n",
      "Step 23200, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2180.9350802898407 seconds\n",
      "Step 23300, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2190.5559256076813 seconds\n",
      "Step 23400, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2199.941359758377 seconds\n",
      "Step 23500, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2209.3405632972717 seconds\n",
      "Step 23600, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2218.8669514656067 seconds\n",
      "Step 23700, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2228.278206348419 seconds\n",
      "Step 23800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2237.8549892902374 seconds\n",
      "Step 23900, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2247.3469381332397 seconds\n",
      "Step 24000, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2256.8117604255676 seconds\n",
      "Step 24100, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2266.4373185634613 seconds\n",
      "Step 24200, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2275.8566493988037 seconds\n",
      "Step 24300, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2285.2354352474213 seconds\n",
      "Step 24400, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2294.8059203624725 seconds\n",
      "Step 24500, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2304.259033679962 seconds\n",
      "Step 24600, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2313.685602903366 seconds\n",
      "Step 24700, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2323.6369154453278 seconds\n",
      "Step 24800, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2333.3888041973114 seconds\n",
      "Step 24900, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2343.0201745033264 seconds\n",
      "Step 25000, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2352.6838240623474 seconds\n",
      "Step 25100, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2362.125100135803 seconds\n",
      "Step 25200, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 2371.5501816272736 seconds\n",
      "Step 25300, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 2381.162757873535 seconds\n",
      "Step 25400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2390.597207546234 seconds\n",
      "Step 25500, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2400.0477097034454 seconds\n",
      "Step 25600, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2409.6537387371063 seconds\n",
      "Step 25700, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2419.0766892433167 seconds\n",
      "Step 25800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2428.531190633774 seconds\n",
      "Step 25900, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2438.14533495903 seconds\n",
      "Step 26000, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2447.5735301971436 seconds\n",
      "Step 26100, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2457.001983642578 seconds\n",
      "Step 26200, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2466.5842134952545 seconds\n",
      "Step 26300, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2476.0113711357117 seconds\n",
      "Step 26400, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2485.4541046619415 seconds\n",
      "Step 26500, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2495.02969121933 seconds\n",
      "Step 26600, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2504.4281029701233 seconds\n",
      "Step 26700, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2513.8202068805695 seconds\n",
      "Step 26800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2523.434215784073 seconds\n",
      "Step 26900, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2532.8195774555206 seconds\n",
      "Step 27000, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2542.2410368919373 seconds\n",
      "Step 27100, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2551.8324048519135 seconds\n",
      "Step 27200, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2561.228744029999 seconds\n",
      "Step 27300, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2570.9908957481384 seconds\n",
      "Step 27400, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2580.557147502899 seconds\n",
      "Step 27500, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2589.9342925548553 seconds\n",
      "Step 27600, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2599.5042436122894 seconds\n",
      "Step 27700, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2608.931748867035 seconds\n",
      "Step 27800, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2618.5824451446533 seconds\n",
      "Step 27900, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2628.388658761978 seconds\n",
      "Step 28000, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2638.024580001831 seconds\n",
      "Step 28100, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2647.505985736847 seconds\n",
      "Step 28200, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2657.253252029419 seconds\n",
      "Step 28300, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2666.6401426792145 seconds\n",
      "Step 28400, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2676.0005078315735 seconds\n",
      "Step 28500, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2685.544732570648 seconds\n",
      "Step 28600, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2694.882360935211 seconds\n",
      "Step 28700, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2704.203197479248 seconds\n",
      "Step 28800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2713.729727268219 seconds\n",
      "Step 28900, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2723.0640466213226 seconds\n",
      "Step 29000, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2732.423620223999 seconds\n",
      "Step 29100, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2741.9414319992065 seconds\n",
      "Step 29200, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2751.2821860313416 seconds\n",
      "Step 29300, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2760.6604158878326 seconds\n",
      "Step 29400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2770.19513630867 seconds\n",
      "Step 29500, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2779.5211973190308 seconds\n",
      "Step 29600, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2788.859931707382 seconds\n",
      "Step 29700, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2798.3861453533173 seconds\n",
      "Step 29800, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2807.736205816269 seconds\n",
      "Step 29900, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2817.062345981598 seconds\n",
      "Step 30000, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2826.637678861618 seconds\n",
      "Step 30100, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2835.9805076122284 seconds\n",
      "Step 30200, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2845.336796760559 seconds\n",
      "Step 30300, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2854.8850808143616 seconds\n",
      "Step 30400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2864.223895549774 seconds\n",
      "Step 30500, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2873.6019937992096 seconds\n",
      "Step 30600, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2883.1626641750336 seconds\n",
      "Step 30700, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2892.5108981132507 seconds\n",
      "Step 30800, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2902.0747368335724 seconds\n",
      "Step 30900, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2911.4007024765015 seconds\n",
      "Step 31000, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 2920.7404210567474 seconds\n",
      "Step 31100, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2930.2584087848663 seconds\n",
      "Step 31200, loss: tensor(0.0045, grad_fn=<SubBackward0>), time elapsed: 2939.682470560074 seconds\n",
      "Step 31300, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2949.0639460086823 seconds\n",
      "Step 31400, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 2958.6027941703796 seconds\n",
      "Step 31500, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2967.933057308197 seconds\n",
      "Step 31600, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2977.26021361351 seconds\n",
      "Step 31700, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2986.75936460495 seconds\n",
      "Step 31800, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 2996.0333836078644 seconds\n",
      "Step 31900, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3005.35631775856 seconds\n",
      "Step 32000, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3014.9013957977295 seconds\n",
      "Step 32100, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 3024.2401473522186 seconds\n",
      "Step 32200, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3033.602642059326 seconds\n",
      "Step 32300, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3043.14560341835 seconds\n",
      "Step 32400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 3052.453040122986 seconds\n",
      "Step 32500, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 3061.756138563156 seconds\n",
      "Step 32600, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3071.2999579906464 seconds\n",
      "Step 32700, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 3080.601339817047 seconds\n",
      "Step 32800, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3089.920270681381 seconds\n",
      "Step 32900, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3099.505651473999 seconds\n",
      "Step 33000, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3108.828856945038 seconds\n",
      "Step 33100, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3118.118720293045 seconds\n",
      "Step 33200, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 3127.62003159523 seconds\n",
      "Step 33300, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3136.9000532627106 seconds\n",
      "Step 33400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 3146.2167539596558 seconds\n",
      "Step 33500, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3155.723462343216 seconds\n",
      "Step 33600, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 3165.0746269226074 seconds\n",
      "Step 33700, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3174.394297361374 seconds\n",
      "Step 33800, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3183.918736934662 seconds\n",
      "Step 33900, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3193.274025440216 seconds\n",
      "Step 34000, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3202.5832827091217 seconds\n",
      "Step 34100, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3212.101267337799 seconds\n",
      "Step 34200, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 3221.4430027008057 seconds\n",
      "Step 34300, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3230.774545431137 seconds\n",
      "Step 34400, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 3240.3632593154907 seconds\n",
      "Step 34500, loss: tensor(0.0042, grad_fn=<SubBackward0>), time elapsed: 3249.7602767944336 seconds\n",
      "Step 34600, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3259.0863904953003 seconds\n",
      "Step 34700, loss: tensor(0.0043, grad_fn=<SubBackward0>), time elapsed: 3268.5919137001038 seconds\n",
      "Step 34800, loss: tensor(0.0041, grad_fn=<SubBackward0>), time elapsed: 3277.9013047218323 seconds\n",
      "Step 34900, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 3287.262587785721 seconds\n",
      "Step 35000, loss: tensor(0.0044, grad_fn=<SubBackward0>), time elapsed: 3296.8289926052094 seconds\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0366, grad_fn=<SubBackward0>), time elapsed: 0.11054015159606934 seconds\n",
      "Step 100, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 9.709927797317505 seconds\n",
      "Step 200, loss: tensor(0.0255, grad_fn=<SubBackward0>), time elapsed: 19.419699668884277 seconds\n",
      "Step 300, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 28.935420989990234 seconds\n",
      "Step 400, loss: tensor(0.0179, grad_fn=<SubBackward0>), time elapsed: 38.3765172958374 seconds\n",
      "Step 500, loss: tensor(0.0163, grad_fn=<SubBackward0>), time elapsed: 47.85159349441528 seconds\n",
      "Step 600, loss: tensor(0.0151, grad_fn=<SubBackward0>), time elapsed: 57.15067911148071 seconds\n",
      "Step 700, loss: tensor(0.0144, grad_fn=<SubBackward0>), time elapsed: 66.50075244903564 seconds\n",
      "Step 800, loss: tensor(0.0134, grad_fn=<SubBackward0>), time elapsed: 76.17184329032898 seconds\n",
      "Step 900, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 85.77862024307251 seconds\n",
      "Step 1000, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 95.13829255104065 seconds\n",
      "Step 1100, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 104.63486242294312 seconds\n",
      "Step 1200, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 113.91337656974792 seconds\n",
      "Step 1300, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 123.19362664222717 seconds\n",
      "Step 1400, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 132.61776471138 seconds\n",
      "Step 1500, loss: tensor(0.0109, grad_fn=<SubBackward0>), time elapsed: 141.93646025657654 seconds\n",
      "Step 1600, loss: tensor(0.0106, grad_fn=<SubBackward0>), time elapsed: 151.39931750297546 seconds\n",
      "Step 1700, loss: tensor(0.0103, grad_fn=<SubBackward0>), time elapsed: 160.68805742263794 seconds\n",
      "Step 1800, loss: tensor(0.0100, grad_fn=<SubBackward0>), time elapsed: 170.00151109695435 seconds\n",
      "Step 1900, loss: tensor(0.0096, grad_fn=<SubBackward0>), time elapsed: 179.5154218673706 seconds\n",
      "Step 2000, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 188.85811400413513 seconds\n",
      "Step 2100, loss: tensor(0.0097, grad_fn=<SubBackward0>), time elapsed: 198.259295463562 seconds\n",
      "Step 2200, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 207.78562545776367 seconds\n",
      "Step 2300, loss: tensor(0.0092, grad_fn=<SubBackward0>), time elapsed: 217.10317206382751 seconds\n",
      "Step 2400, loss: tensor(0.0091, grad_fn=<SubBackward0>), time elapsed: 226.4524586200714 seconds\n",
      "Step 2500, loss: tensor(0.0088, grad_fn=<SubBackward0>), time elapsed: 235.9183065891266 seconds\n",
      "Step 2600, loss: tensor(0.0086, grad_fn=<SubBackward0>), time elapsed: 245.24044013023376 seconds\n",
      "Step 2700, loss: tensor(0.0082, grad_fn=<SubBackward0>), time elapsed: 254.53190445899963 seconds\n",
      "Step 2800, loss: tensor(0.0084, grad_fn=<SubBackward0>), time elapsed: 264.0093686580658 seconds\n",
      "Step 2900, loss: tensor(0.0085, grad_fn=<SubBackward0>), time elapsed: 273.321661233902 seconds\n",
      "Step 3000, loss: tensor(0.0083, grad_fn=<SubBackward0>), time elapsed: 282.6312184333801 seconds\n",
      "Step 3100, loss: tensor(0.0081, grad_fn=<SubBackward0>), time elapsed: 292.1002504825592 seconds\n",
      "Step 3200, loss: tensor(0.0079, grad_fn=<SubBackward0>), time elapsed: 301.40568137168884 seconds\n",
      "Step 3300, loss: tensor(0.0080, grad_fn=<SubBackward0>), time elapsed: 310.86093497276306 seconds\n",
      "Step 3400, loss: tensor(0.0080, grad_fn=<SubBackward0>), time elapsed: 320.1787316799164 seconds\n",
      "Step 3500, loss: tensor(0.0077, grad_fn=<SubBackward0>), time elapsed: 329.48831725120544 seconds\n",
      "Step 3600, loss: tensor(0.0076, grad_fn=<SubBackward0>), time elapsed: 338.98965311050415 seconds\n",
      "Step 3700, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 348.33536171913147 seconds\n",
      "Step 3800, loss: tensor(0.0076, grad_fn=<SubBackward0>), time elapsed: 357.68773007392883 seconds\n",
      "Step 3900, loss: tensor(0.0075, grad_fn=<SubBackward0>), time elapsed: 367.19513463974 seconds\n",
      "Step 4000, loss: tensor(0.0077, grad_fn=<SubBackward0>), time elapsed: 376.5514392852783 seconds\n",
      "Step 4100, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 385.93497252464294 seconds\n",
      "Step 4200, loss: tensor(0.0076, grad_fn=<SubBackward0>), time elapsed: 395.43433833122253 seconds\n",
      "Step 4300, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 404.73117184638977 seconds\n",
      "Step 4400, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 414.0029730796814 seconds\n",
      "Step 4500, loss: tensor(0.0075, grad_fn=<SubBackward0>), time elapsed: 423.4375309944153 seconds\n",
      "Step 4600, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 432.7333631515503 seconds\n",
      "Step 4700, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 442.05015563964844 seconds\n",
      "Step 4800, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 451.5576984882355 seconds\n",
      "Step 4900, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 460.9766933917999 seconds\n",
      "Step 5000, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 470.33398389816284 seconds\n",
      "Step 5100, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 479.77796697616577 seconds\n",
      "Step 5200, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 489.0791103839874 seconds\n",
      "Step 5300, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 498.5679349899292 seconds\n",
      "Step 5400, loss: tensor(0.0073, grad_fn=<SubBackward0>), time elapsed: 507.9565198421478 seconds\n",
      "Step 5500, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 517.2993297576904 seconds\n",
      "Step 5600, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 526.8006327152252 seconds\n",
      "Step 5700, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 536.1339902877808 seconds\n",
      "Step 5800, loss: tensor(0.0074, grad_fn=<SubBackward0>), time elapsed: 545.4361870288849 seconds\n",
      "Step 5900, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 554.899932384491 seconds\n",
      "Step 6000, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 564.2124524116516 seconds\n",
      "Step 6100, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 573.5039067268372 seconds\n",
      "Step 6200, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 582.9450223445892 seconds\n",
      "Step 6300, loss: tensor(0.0071, grad_fn=<SubBackward0>), time elapsed: 592.2357409000397 seconds\n",
      "Step 6400, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 601.5013432502747 seconds\n",
      "Step 6500, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 610.9567472934723 seconds\n",
      "Step 6600, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 620.3109745979309 seconds\n",
      "Step 6700, loss: tensor(0.0072, grad_fn=<SubBackward0>), time elapsed: 629.8803198337555 seconds\n",
      "Step 6800, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 639.8830587863922 seconds\n",
      "Step 6900, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 649.5570573806763 seconds\n",
      "Step 7000, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 659.2215735912323 seconds\n",
      "Step 7100, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 668.5954146385193 seconds\n",
      "Step 7200, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 678.063708782196 seconds\n",
      "Step 7300, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 687.7386152744293 seconds\n",
      "Step 7400, loss: tensor(0.0068, grad_fn=<SubBackward0>), time elapsed: 697.1187510490417 seconds\n",
      "Step 7500, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 706.4737493991852 seconds\n",
      "Step 7600, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 716.0267713069916 seconds\n",
      "Step 7700, loss: tensor(0.0070, grad_fn=<SubBackward0>), time elapsed: 725.4441061019897 seconds\n",
      "Step 7800, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 736.7271192073822 seconds\n",
      "Step 7900, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 746.2842221260071 seconds\n",
      "Step 8000, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 755.6070184707642 seconds\n",
      "Step 8100, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 764.9987664222717 seconds\n",
      "Step 8200, loss: tensor(0.0069, grad_fn=<SubBackward0>), time elapsed: 774.6539418697357 seconds\n",
      "Step 8300, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 784.1470518112183 seconds\n",
      "Step 8400, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 793.6161022186279 seconds\n",
      "Step 8500, loss: tensor(0.0067, grad_fn=<SubBackward0>), time elapsed: 803.2110879421234 seconds\n",
      "Step 8600, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 812.65864777565 seconds\n",
      "Step 8700, loss: tensor(0.0066, grad_fn=<SubBackward0>), time elapsed: 822.3968822956085 seconds\n",
      "Step 8800, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 831.8949811458588 seconds\n",
      "Step 8900, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 841.3808863162994 seconds\n",
      "Step 9000, loss: tensor(0.0065, grad_fn=<SubBackward0>), time elapsed: 851.040723323822 seconds\n",
      "Step 9100, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 860.5448796749115 seconds\n",
      "Step 9200, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 870.0427711009979 seconds\n",
      "Step 9300, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 879.7457818984985 seconds\n",
      "Step 9400, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 889.2654366493225 seconds\n",
      "Step 9500, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 898.8215153217316 seconds\n",
      "Step 9600, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 908.4762823581696 seconds\n",
      "Step 9700, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 917.9696190357208 seconds\n",
      "Step 9800, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 927.3793201446533 seconds\n",
      "Step 9900, loss: tensor(0.0064, grad_fn=<SubBackward0>), time elapsed: 936.9221110343933 seconds\n",
      "Step 10000, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 947.0634274482727 seconds\n",
      "Step 10100, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 956.4210965633392 seconds\n",
      "Step 10200, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 966.0105526447296 seconds\n",
      "Step 10300, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 975.3678667545319 seconds\n",
      "Step 10400, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 984.762421131134 seconds\n",
      "Step 10500, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 994.3699674606323 seconds\n",
      "Step 10600, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 1005.2027308940887 seconds\n",
      "Step 10700, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 1015.0574247837067 seconds\n",
      "Step 10800, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 1024.4375066757202 seconds\n",
      "Step 10900, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 1033.943029165268 seconds\n",
      "Step 11000, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 1043.475694656372 seconds\n",
      "Step 11100, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 1052.9636192321777 seconds\n",
      "Step 11200, loss: tensor(0.0057, grad_fn=<SubBackward0>), time elapsed: 1062.785579919815 seconds\n",
      "Step 11300, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 1072.3816816806793 seconds\n",
      "Step 11400, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 1081.7566940784454 seconds\n",
      "Step 11500, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 1091.121288061142 seconds\n",
      "Step 11600, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 1100.6736178398132 seconds\n",
      "Step 11700, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 1110.0688276290894 seconds\n",
      "Step 11800, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 1119.4669859409332 seconds\n",
      "Step 11900, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 1129.086770772934 seconds\n",
      "Step 12000, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 1138.5849635601044 seconds\n",
      "Step 12100, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 1148.1158776283264 seconds\n",
      "Step 12200, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 1157.7691991329193 seconds\n",
      "Step 12300, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 1167.2268114089966 seconds\n",
      "Step 12400, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 1176.7112128734589 seconds\n",
      "Step 12500, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 1186.3571557998657 seconds\n",
      "Step 12600, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 1195.8240485191345 seconds\n",
      "Step 12700, loss: tensor(0.0060, grad_fn=<SubBackward0>), time elapsed: 1205.5039093494415 seconds\n",
      "Step 12800, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 1215.0310652256012 seconds\n",
      "Step 12900, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 1224.5275266170502 seconds\n",
      "Step 13000, loss: tensor(0.0062, grad_fn=<SubBackward0>), time elapsed: 1234.184475660324 seconds\n",
      "Step 13100, loss: tensor(0.0063, grad_fn=<SubBackward0>), time elapsed: 1243.627091884613 seconds\n",
      "Step 13200, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 1253.0165009498596 seconds\n",
      "Step 13300, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 1262.7207760810852 seconds\n",
      "Step 13400, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 1272.2961058616638 seconds\n",
      "Step 13500, loss: tensor(0.0061, grad_fn=<SubBackward0>), time elapsed: 1281.8662960529327 seconds\n",
      "Step 13600, loss: tensor(0.0059, grad_fn=<SubBackward0>), time elapsed: 1291.5744054317474 seconds\n",
      "Step 13700, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 1301.1342997550964 seconds\n",
      "Step 13800, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 1310.6624069213867 seconds\n",
      "Step 13900, loss: tensor(0.0058, grad_fn=<SubBackward0>), time elapsed: 1320.32084774971 seconds\n",
      "Step 14000, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 1329.8209292888641 seconds\n",
      "Step 14100, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 1339.39577126503 seconds\n",
      "Step 14200, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 1349.1275308132172 seconds\n",
      "Step 14300, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 1358.7441174983978 seconds\n",
      "Step 14400, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 1368.287899017334 seconds\n",
      "Step 14500, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 1377.8369748592377 seconds\n",
      "Step 14600, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1387.200649023056 seconds\n",
      "Step 14700, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 1396.7726564407349 seconds\n",
      "Step 14800, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 1406.571537733078 seconds\n",
      "Step 14900, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 1416.1672863960266 seconds\n",
      "Step 15000, loss: tensor(0.0056, grad_fn=<SubBackward0>), time elapsed: 1425.7486217021942 seconds\n",
      "Step 15100, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 1435.4604749679565 seconds\n",
      "Step 15200, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1444.8964173793793 seconds\n",
      "Step 15300, loss: tensor(0.0055, grad_fn=<SubBackward0>), time elapsed: 1454.5255119800568 seconds\n",
      "Step 15400, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1463.9262919425964 seconds\n",
      "Step 15500, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1473.3465254306793 seconds\n",
      "Step 15600, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 1483.049220085144 seconds\n",
      "Step 15700, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1492.4779584407806 seconds\n",
      "Step 15800, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1502.002773284912 seconds\n",
      "Step 15900, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1511.6269669532776 seconds\n",
      "Step 16000, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1521.2063040733337 seconds\n",
      "Step 16100, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 1530.6803307533264 seconds\n",
      "Step 16200, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1540.29944729805 seconds\n",
      "Step 16300, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 1549.7943019866943 seconds\n",
      "Step 16400, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1559.2786786556244 seconds\n",
      "Step 16500, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1568.9492526054382 seconds\n",
      "Step 16600, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1578.5104730129242 seconds\n",
      "Step 16700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1588.013503074646 seconds\n",
      "Step 16800, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1597.6495339870453 seconds\n",
      "Step 16900, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1607.2043795585632 seconds\n",
      "Step 17000, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1616.7172746658325 seconds\n",
      "Step 17100, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1626.401935338974 seconds\n",
      "Step 17200, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 1635.9587888717651 seconds\n",
      "Step 17300, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 1645.5370395183563 seconds\n",
      "Step 17400, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 1655.2085974216461 seconds\n",
      "Step 17500, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1664.8269000053406 seconds\n",
      "Step 17600, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1674.5651392936707 seconds\n",
      "Step 17700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1684.1329865455627 seconds\n",
      "Step 17800, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1693.6649954319 seconds\n",
      "Step 17900, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1703.3799300193787 seconds\n",
      "Step 18000, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1712.872153520584 seconds\n",
      "Step 18100, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1722.4711656570435 seconds\n",
      "Step 18200, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1732.3104553222656 seconds\n",
      "Step 18300, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 1742.0371658802032 seconds\n",
      "Step 18400, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1751.778307914734 seconds\n",
      "Step 18500, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1761.57284450531 seconds\n",
      "Step 18600, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1771.1460058689117 seconds\n",
      "Step 18700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1780.8440282344818 seconds\n",
      "Step 18800, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1790.5814771652222 seconds\n",
      "Step 18900, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1800.101077079773 seconds\n",
      "Step 19000, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1809.6409034729004 seconds\n",
      "Step 19100, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1819.3561482429504 seconds\n",
      "Step 19200, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 1828.9198112487793 seconds\n",
      "Step 19300, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1838.4623582363129 seconds\n",
      "Step 19400, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1848.2138712406158 seconds\n",
      "Step 19500, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1857.8265225887299 seconds\n",
      "Step 19600, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1867.3872072696686 seconds\n",
      "Step 19700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1877.1449992656708 seconds\n",
      "Step 19800, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1886.8861801624298 seconds\n",
      "Step 19900, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1896.4473323822021 seconds\n",
      "Step 20000, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1906.2127544879913 seconds\n",
      "Step 20100, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1915.8832263946533 seconds\n",
      "Step 20200, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 1925.573225736618 seconds\n",
      "Step 20300, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 1935.0967738628387 seconds\n",
      "Step 20400, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 1944.6636414527893 seconds\n",
      "Step 20500, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1954.401636838913 seconds\n",
      "Step 20600, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1964.0208990573883 seconds\n",
      "Step 20700, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 1973.726173877716 seconds\n",
      "Step 20800, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1983.538449048996 seconds\n",
      "Step 20900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 1993.156286239624 seconds\n",
      "Step 21000, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2002.7407674789429 seconds\n",
      "Step 21100, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2012.539969921112 seconds\n",
      "Step 21200, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2022.0862946510315 seconds\n",
      "Step 21300, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2031.7173249721527 seconds\n",
      "Step 21400, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2041.459035396576 seconds\n",
      "Step 21500, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 2051.033478975296 seconds\n",
      "Step 21600, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 2060.567498922348 seconds\n",
      "Step 21700, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 2070.304549217224 seconds\n",
      "Step 21800, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2079.844140768051 seconds\n",
      "Step 21900, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2089.375275850296 seconds\n",
      "Step 22000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2099.0647492408752 seconds\n",
      "Step 22100, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2108.982892036438 seconds\n",
      "Step 22200, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 2118.7046024799347 seconds\n",
      "Step 22300, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2128.6237812042236 seconds\n",
      "Step 22400, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 2138.3869314193726 seconds\n",
      "Step 22500, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2148.0386860370636 seconds\n",
      "Step 22600, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2157.747431755066 seconds\n",
      "Step 22700, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2167.312649488449 seconds\n",
      "Step 22800, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2177.5470945835114 seconds\n",
      "Step 22900, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2187.1539397239685 seconds\n",
      "Step 23000, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2196.80672955513 seconds\n",
      "Step 23100, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2206.658183336258 seconds\n",
      "Step 23200, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 2216.3236491680145 seconds\n",
      "Step 23300, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2225.9359681606293 seconds\n",
      "Step 23400, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2235.698837518692 seconds\n",
      "Step 23500, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 2245.287223815918 seconds\n",
      "Step 23600, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 2254.881694316864 seconds\n",
      "Step 23700, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2264.69842505455 seconds\n",
      "Step 23800, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 2274.320832014084 seconds\n",
      "Step 23900, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2283.9296176433563 seconds\n",
      "Step 24000, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 2293.7370150089264 seconds\n",
      "Step 24100, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2303.3631205558777 seconds\n",
      "Step 24200, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 2312.942432165146 seconds\n",
      "Step 24300, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2322.7978739738464 seconds\n",
      "Step 24400, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2332.3991146087646 seconds\n",
      "Step 24500, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 2341.9588334560394 seconds\n",
      "Step 24600, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2351.668396472931 seconds\n",
      "Step 24700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2361.1537580490112 seconds\n",
      "Step 24800, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2370.624766588211 seconds\n",
      "Step 24900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2380.3163266181946 seconds\n",
      "Step 25000, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2389.824992418289 seconds\n",
      "Step 25100, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2399.3225495815277 seconds\n",
      "Step 25200, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2409.0352251529694 seconds\n",
      "Step 25300, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2418.52020072937 seconds\n",
      "Step 25400, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 2428.0320217609406 seconds\n",
      "Step 25500, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2437.706118583679 seconds\n",
      "Step 25600, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2447.2165875434875 seconds\n",
      "Step 25700, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2456.7282993793488 seconds\n",
      "Step 25800, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 2466.416110754013 seconds\n",
      "Step 25900, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 2475.9331090450287 seconds\n",
      "Step 26000, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2485.642343044281 seconds\n",
      "Step 26100, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2495.131637096405 seconds\n",
      "Step 26200, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2504.6461889743805 seconds\n",
      "Step 26300, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2514.327317237854 seconds\n",
      "Step 26400, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 2523.8206944465637 seconds\n",
      "Step 26500, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2533.341710805893 seconds\n",
      "Step 26600, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2543.030575990677 seconds\n",
      "Step 26700, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 2552.5044133663177 seconds\n",
      "Step 26800, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2562.0432879924774 seconds\n",
      "Step 26900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2571.7967042922974 seconds\n",
      "Step 27000, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2581.3003766536713 seconds\n",
      "Step 27100, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 2590.7871763706207 seconds\n",
      "Step 27200, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2600.502158641815 seconds\n",
      "Step 27300, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2610.001950263977 seconds\n",
      "Step 27400, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2619.5140533447266 seconds\n",
      "Step 27500, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2629.2503504753113 seconds\n",
      "Step 27600, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2638.7675251960754 seconds\n",
      "Step 27700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2648.2451782226562 seconds\n",
      "Step 27800, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2657.957774877548 seconds\n",
      "Step 27900, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2667.472240924835 seconds\n",
      "Step 28000, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2676.9869384765625 seconds\n",
      "Step 28100, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2686.758500814438 seconds\n",
      "Step 28200, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2696.3164463043213 seconds\n",
      "Step 28300, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 2705.8506393432617 seconds\n",
      "Step 28400, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 2715.6037378311157 seconds\n",
      "Step 28500, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2725.149300098419 seconds\n",
      "Step 28600, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2734.6740062236786 seconds\n",
      "Step 28700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2744.4031777381897 seconds\n",
      "Step 28800, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2753.9500777721405 seconds\n",
      "Step 28900, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2763.466881752014 seconds\n",
      "Step 29000, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2773.161150455475 seconds\n",
      "Step 29100, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2782.6898396015167 seconds\n",
      "Step 29200, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2792.183047056198 seconds\n",
      "Step 29300, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2801.8708527088165 seconds\n",
      "Step 29400, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2811.3822836875916 seconds\n",
      "Step 29500, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 2821.084286928177 seconds\n",
      "Step 29600, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2830.590637922287 seconds\n",
      "Step 29700, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2840.135584115982 seconds\n",
      "Step 29800, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2849.8617141246796 seconds\n",
      "Step 29900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2859.366374731064 seconds\n",
      "Step 30000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2869.5649058818817 seconds\n",
      "Step 30100, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2879.420634508133 seconds\n",
      "Step 30200, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2888.948712825775 seconds\n",
      "Step 30300, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2898.4377574920654 seconds\n",
      "Step 30400, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2908.158506631851 seconds\n",
      "Step 30500, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2917.649838209152 seconds\n",
      "Step 30600, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 2927.1448681354523 seconds\n",
      "Step 30700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2936.8421022892 seconds\n",
      "Step 30800, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 2946.327951669693 seconds\n",
      "Step 30900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 2955.831983566284 seconds\n",
      "Step 31000, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2965.5559735298157 seconds\n",
      "Step 31100, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 2975.4441072940826 seconds\n",
      "Step 31200, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2984.9609570503235 seconds\n",
      "Step 31300, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 2994.6969406604767 seconds\n",
      "Step 31400, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 3004.200710296631 seconds\n",
      "Step 31500, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 3013.692054748535 seconds\n",
      "Step 31600, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 3023.4024527072906 seconds\n",
      "Step 31700, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 3032.936700820923 seconds\n",
      "Step 31800, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 3042.4872300624847 seconds\n",
      "Step 31900, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 3052.1429464817047 seconds\n",
      "Step 32000, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 3061.7616906166077 seconds\n",
      "Step 32100, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 3071.3279032707214 seconds\n",
      "Step 32200, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 3081.0355339050293 seconds\n",
      "Step 32300, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 3092.5763607025146 seconds\n",
      "Step 32400, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 3102.0379860401154 seconds\n",
      "Step 32500, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 3111.7234494686127 seconds\n",
      "Step 32600, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 3121.2355971336365 seconds\n",
      "Step 32700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 3130.7126834392548 seconds\n",
      "Step 32800, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 3140.3996675014496 seconds\n",
      "Step 32900, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 3149.937489271164 seconds\n",
      "Step 33000, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 3159.4498665332794 seconds\n",
      "Step 33100, loss: tensor(0.0054, grad_fn=<SubBackward0>), time elapsed: 3169.1634349823 seconds\n",
      "Step 33200, loss: tensor(0.0048, grad_fn=<SubBackward0>), time elapsed: 3178.686998605728 seconds\n",
      "Step 33300, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 3188.460912466049 seconds\n",
      "Step 33400, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 3198.155184984207 seconds\n",
      "Step 33500, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 3207.6482298374176 seconds\n",
      "Step 33600, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 3217.379883289337 seconds\n",
      "Step 33700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 3226.8567304611206 seconds\n",
      "Step 33800, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 3236.340407371521 seconds\n",
      "Step 33900, loss: tensor(0.0051, grad_fn=<SubBackward0>), time elapsed: 3246.093637228012 seconds\n",
      "Step 34000, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 3255.5813796520233 seconds\n",
      "Step 34100, loss: tensor(0.0053, grad_fn=<SubBackward0>), time elapsed: 3265.069134950638 seconds\n",
      "Step 34200, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 3274.7899072170258 seconds\n",
      "Step 34300, loss: tensor(0.0047, grad_fn=<SubBackward0>), time elapsed: 3284.5578870773315 seconds\n",
      "Step 34400, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 3294.0263121128082 seconds\n",
      "Step 34500, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 3303.7446455955505 seconds\n",
      "Step 34600, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 3313.2143275737762 seconds\n",
      "Step 34700, loss: tensor(0.0049, grad_fn=<SubBackward0>), time elapsed: 3322.731029510498 seconds\n",
      "Step 34800, loss: tensor(0.0052, grad_fn=<SubBackward0>), time elapsed: 3332.4601287841797 seconds\n",
      "Step 34900, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 3341.9602892398834 seconds\n",
      "Step 35000, loss: tensor(0.0050, grad_fn=<SubBackward0>), time elapsed: 3351.481878757477 seconds\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.0560, grad_fn=<SubBackward0>), time elapsed: 0.1288299560546875 seconds\n",
      "Step 100, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 10.288109540939331 seconds\n",
      "Step 200, loss: tensor(0.0448, grad_fn=<SubBackward0>), time elapsed: 19.86575198173523 seconds\n",
      "Step 300, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 29.37165880203247 seconds\n",
      "Step 400, loss: tensor(0.0406, grad_fn=<SubBackward0>), time elapsed: 39.01616930961609 seconds\n",
      "Step 500, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 48.46453547477722 seconds\n",
      "Step 600, loss: tensor(0.0335, grad_fn=<SubBackward0>), time elapsed: 57.92009878158569 seconds\n",
      "Step 700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 67.53005313873291 seconds\n",
      "Step 800, loss: tensor(0.0289, grad_fn=<SubBackward0>), time elapsed: 77.00697922706604 seconds\n",
      "Step 900, loss: tensor(0.0275, grad_fn=<SubBackward0>), time elapsed: 86.63607597351074 seconds\n",
      "Step 1000, loss: tensor(0.0253, grad_fn=<SubBackward0>), time elapsed: 96.09367442131042 seconds\n",
      "Step 1100, loss: tensor(0.0242, grad_fn=<SubBackward0>), time elapsed: 105.61536574363708 seconds\n",
      "Step 1200, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 115.23409581184387 seconds\n",
      "Step 1300, loss: tensor(0.0220, grad_fn=<SubBackward0>), time elapsed: 124.67332029342651 seconds\n",
      "Step 1400, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 134.15085697174072 seconds\n",
      "Step 1500, loss: tensor(0.0213, grad_fn=<SubBackward0>), time elapsed: 143.75809359550476 seconds\n",
      "Step 1600, loss: tensor(0.0202, grad_fn=<SubBackward0>), time elapsed: 153.20554494857788 seconds\n",
      "Step 1700, loss: tensor(0.0203, grad_fn=<SubBackward0>), time elapsed: 162.6821949481964 seconds\n",
      "Step 1800, loss: tensor(0.0195, grad_fn=<SubBackward0>), time elapsed: 172.29530024528503 seconds\n",
      "Step 1900, loss: tensor(0.0198, grad_fn=<SubBackward0>), time elapsed: 181.74881505966187 seconds\n",
      "Step 2000, loss: tensor(0.0194, grad_fn=<SubBackward0>), time elapsed: 191.2044665813446 seconds\n",
      "Step 2100, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 200.83762383460999 seconds\n",
      "Step 2200, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 210.30365133285522 seconds\n",
      "Step 2300, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 219.76838755607605 seconds\n",
      "Step 2400, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 229.41652870178223 seconds\n",
      "Step 2500, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 238.87926816940308 seconds\n",
      "Step 2600, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 248.51197791099548 seconds\n",
      "Step 2700, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 257.9698271751404 seconds\n",
      "Step 2800, loss: tensor(0.0176, grad_fn=<SubBackward0>), time elapsed: 267.39662742614746 seconds\n",
      "Step 2900, loss: tensor(0.0171, grad_fn=<SubBackward0>), time elapsed: 277.0139870643616 seconds\n",
      "Step 3000, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 286.47781801223755 seconds\n",
      "Step 3100, loss: tensor(0.0162, grad_fn=<SubBackward0>), time elapsed: 295.9480164051056 seconds\n",
      "Step 3200, loss: tensor(0.0155, grad_fn=<SubBackward0>), time elapsed: 305.5816900730133 seconds\n",
      "Step 3300, loss: tensor(0.0149, grad_fn=<SubBackward0>), time elapsed: 315.0515203475952 seconds\n",
      "Step 3400, loss: tensor(0.0144, grad_fn=<SubBackward0>), time elapsed: 324.61038517951965 seconds\n",
      "Step 3500, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 334.2852940559387 seconds\n",
      "Step 3600, loss: tensor(0.0143, grad_fn=<SubBackward0>), time elapsed: 343.72001099586487 seconds\n",
      "Step 3700, loss: tensor(0.0141, grad_fn=<SubBackward0>), time elapsed: 353.1983458995819 seconds\n",
      "Step 3800, loss: tensor(0.0132, grad_fn=<SubBackward0>), time elapsed: 362.81460309028625 seconds\n",
      "Step 3900, loss: tensor(0.0127, grad_fn=<SubBackward0>), time elapsed: 372.2596652507782 seconds\n",
      "Step 4000, loss: tensor(0.0131, grad_fn=<SubBackward0>), time elapsed: 381.7451500892639 seconds\n",
      "Step 4100, loss: tensor(0.0128, grad_fn=<SubBackward0>), time elapsed: 391.3537790775299 seconds\n",
      "Step 4200, loss: tensor(0.0124, grad_fn=<SubBackward0>), time elapsed: 400.8254163265228 seconds\n",
      "Step 4300, loss: tensor(0.0129, grad_fn=<SubBackward0>), time elapsed: 410.4914231300354 seconds\n",
      "Step 4400, loss: tensor(0.0126, grad_fn=<SubBackward0>), time elapsed: 419.94930028915405 seconds\n",
      "Step 4500, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 429.42152523994446 seconds\n",
      "Step 4600, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 439.0618534088135 seconds\n",
      "Step 4700, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 448.51417899131775 seconds\n",
      "Step 4800, loss: tensor(0.0126, grad_fn=<SubBackward0>), time elapsed: 457.9767162799835 seconds\n",
      "Step 4900, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 467.5878412723541 seconds\n",
      "Step 5000, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 477.1686997413635 seconds\n",
      "Step 5100, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 486.61754727363586 seconds\n",
      "Step 5200, loss: tensor(0.0126, grad_fn=<SubBackward0>), time elapsed: 496.14038705825806 seconds\n",
      "Step 5300, loss: tensor(0.0124, grad_fn=<SubBackward0>), time elapsed: 505.6685492992401 seconds\n",
      "Step 5400, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 515.1593859195709 seconds\n",
      "Step 5500, loss: tensor(0.0125, grad_fn=<SubBackward0>), time elapsed: 524.7678639888763 seconds\n",
      "Step 5600, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 534.2741930484772 seconds\n",
      "Step 5700, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 543.7647941112518 seconds\n",
      "Step 5800, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 553.4215729236603 seconds\n",
      "Step 5900, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 562.9555106163025 seconds\n",
      "Step 6000, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 572.633576631546 seconds\n",
      "Step 6100, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 582.1154093742371 seconds\n",
      "Step 6200, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 591.6279299259186 seconds\n",
      "Step 6300, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 601.2900466918945 seconds\n",
      "Step 6400, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 610.7752687931061 seconds\n",
      "Step 6500, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 620.2456469535828 seconds\n",
      "Step 6600, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 629.9869465827942 seconds\n",
      "Step 6700, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 639.4429051876068 seconds\n",
      "Step 6800, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 648.8793318271637 seconds\n",
      "Step 6900, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 658.516877412796 seconds\n",
      "Step 7000, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 667.9566385746002 seconds\n",
      "Step 7100, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 677.3876218795776 seconds\n",
      "Step 7200, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 687.0018262863159 seconds\n",
      "Step 7300, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 696.4349400997162 seconds\n",
      "Step 7400, loss: tensor(0.0124, grad_fn=<SubBackward0>), time elapsed: 705.8637568950653 seconds\n",
      "Step 7500, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 715.5067076683044 seconds\n",
      "Step 7600, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 724.9544117450714 seconds\n",
      "Step 7700, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 734.4190547466278 seconds\n",
      "Step 7800, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 744.0349431037903 seconds\n",
      "Step 7900, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 753.5352411270142 seconds\n",
      "Step 8000, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 763.1733198165894 seconds\n",
      "Step 8100, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 772.6255102157593 seconds\n",
      "Step 8200, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 782.1180510520935 seconds\n",
      "Step 8300, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 791.7685754299164 seconds\n",
      "Step 8400, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 801.208841085434 seconds\n",
      "Step 8500, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 810.6805438995361 seconds\n",
      "Step 8600, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 820.2810392379761 seconds\n",
      "Step 8700, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 829.7236642837524 seconds\n",
      "Step 8800, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 839.2262828350067 seconds\n",
      "Step 8900, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 848.8331773281097 seconds\n",
      "Step 9000, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 858.2729785442352 seconds\n",
      "Step 9100, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 867.7218565940857 seconds\n",
      "Step 9200, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 877.3742489814758 seconds\n",
      "Step 9300, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 886.8325231075287 seconds\n",
      "Step 9400, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 896.2871210575104 seconds\n",
      "Step 9500, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 905.9297089576721 seconds\n",
      "Step 9600, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 915.3741478919983 seconds\n",
      "Step 9700, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 924.8715989589691 seconds\n",
      "Step 9800, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 934.5986452102661 seconds\n",
      "Step 9900, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 944.0594127178192 seconds\n",
      "Step 10000, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 953.6920874118805 seconds\n",
      "Step 10100, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 963.1490468978882 seconds\n",
      "Step 10200, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 972.5659799575806 seconds\n",
      "Step 10300, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 982.192104101181 seconds\n",
      "Step 10400, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 991.6385259628296 seconds\n",
      "Step 10500, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 1001.0867381095886 seconds\n",
      "Step 10600, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 1010.6806738376617 seconds\n",
      "Step 10700, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 1020.125394821167 seconds\n",
      "Step 10800, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1029.582853794098 seconds\n",
      "Step 10900, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1039.2046411037445 seconds\n",
      "Step 11000, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 1048.6520028114319 seconds\n",
      "Step 11100, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1058.1383135318756 seconds\n",
      "Step 11200, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 1067.7667706012726 seconds\n",
      "Step 11300, loss: tensor(0.0124, grad_fn=<SubBackward0>), time elapsed: 1077.2453429698944 seconds\n",
      "Step 11400, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 1086.723922252655 seconds\n",
      "Step 11500, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 1096.3694314956665 seconds\n",
      "Step 11600, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 1105.8100144863129 seconds\n",
      "Step 11700, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 1115.4496483802795 seconds\n",
      "Step 11800, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 1124.8752291202545 seconds\n",
      "Step 11900, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 1134.3339941501617 seconds\n",
      "Step 12000, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 1143.9475662708282 seconds\n",
      "Step 12100, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 1153.4179239273071 seconds\n",
      "Step 12200, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 1162.8636875152588 seconds\n",
      "Step 12300, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1172.497900724411 seconds\n",
      "Step 12400, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 1181.9892497062683 seconds\n",
      "Step 12500, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 1191.47274684906 seconds\n",
      "Step 12600, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 1201.1063599586487 seconds\n",
      "Step 12700, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 1210.5978932380676 seconds\n",
      "Step 12800, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 1220.0431461334229 seconds\n",
      "Step 12900, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 1229.6574046611786 seconds\n",
      "Step 13000, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 1239.1241629123688 seconds\n",
      "Step 13100, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1248.5747745037079 seconds\n",
      "Step 13200, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 1258.2254738807678 seconds\n",
      "Step 13300, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 1267.687227010727 seconds\n",
      "Step 13400, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1277.166719675064 seconds\n",
      "Step 13500, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 1286.815616607666 seconds\n",
      "Step 13600, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1296.2750988006592 seconds\n",
      "Step 13700, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 1305.7634093761444 seconds\n",
      "Step 13800, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1315.3808965682983 seconds\n",
      "Step 13900, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1324.8080186843872 seconds\n",
      "Step 14000, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 1334.2770442962646 seconds\n",
      "Step 14100, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1343.8952248096466 seconds\n",
      "Step 14200, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1353.3425784111023 seconds\n",
      "Step 14300, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 1363.001443862915 seconds\n",
      "Step 14400, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 1372.4507944583893 seconds\n",
      "Step 14500, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 1381.903932094574 seconds\n",
      "Step 14600, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1391.5048913955688 seconds\n",
      "Step 14700, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 1400.9577198028564 seconds\n",
      "Step 14800, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 1410.3999605178833 seconds\n",
      "Step 14900, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 1420.0278861522675 seconds\n",
      "Step 15000, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 1429.5000734329224 seconds\n",
      "Step 15100, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 1438.9698421955109 seconds\n",
      "Step 15200, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1448.6024780273438 seconds\n",
      "Step 15300, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1458.0820968151093 seconds\n",
      "Step 15400, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 1467.5585098266602 seconds\n",
      "Step 15500, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1477.1848771572113 seconds\n",
      "Step 15600, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 1486.6780455112457 seconds\n",
      "Step 15700, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 1496.136975288391 seconds\n",
      "Step 15800, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 1505.7714066505432 seconds\n",
      "Step 15900, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 1515.238332748413 seconds\n",
      "Step 16000, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 1524.7103555202484 seconds\n",
      "Step 16100, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 1534.3428933620453 seconds\n",
      "Step 16200, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 1543.8113718032837 seconds\n",
      "Step 16300, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 1553.289781332016 seconds\n",
      "Step 16400, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 1562.9592411518097 seconds\n",
      "Step 16500, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 1572.4421377182007 seconds\n",
      "Step 16600, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 1582.1465611457825 seconds\n",
      "Step 16700, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 1591.6047668457031 seconds\n",
      "Step 16800, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1601.0811910629272 seconds\n",
      "Step 16900, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1610.8332016468048 seconds\n",
      "Step 17000, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 1620.3124442100525 seconds\n",
      "Step 17100, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 1629.8090171813965 seconds\n",
      "Step 17200, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1639.5335161685944 seconds\n",
      "Step 17300, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 1649.007234096527 seconds\n",
      "Step 17400, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 1658.4677047729492 seconds\n",
      "Step 17500, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1668.1312890052795 seconds\n",
      "Step 17600, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 1677.7201149463654 seconds\n",
      "Step 17700, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1687.236174583435 seconds\n",
      "Step 17800, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 1696.881872177124 seconds\n",
      "Step 17900, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 1707.0599155426025 seconds\n",
      "Step 18000, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1716.547189950943 seconds\n",
      "Step 18100, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1726.2853844165802 seconds\n",
      "Step 18200, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1735.8768887519836 seconds\n",
      "Step 18300, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 1745.3940196037292 seconds\n",
      "Step 18400, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 1755.1807944774628 seconds\n",
      "Step 18500, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 1764.6898036003113 seconds\n",
      "Step 18600, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 1774.083744764328 seconds\n",
      "Step 18700, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1783.5973598957062 seconds\n",
      "Step 18800, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 1792.931872367859 seconds\n",
      "Step 18900, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1802.2466251850128 seconds\n",
      "Step 19000, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 1811.74183344841 seconds\n",
      "Step 19100, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1821.0636250972748 seconds\n",
      "Step 19200, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1830.5984783172607 seconds\n",
      "Step 19300, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 1839.950440645218 seconds\n",
      "Step 19400, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 1849.2671840190887 seconds\n",
      "Step 19500, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 1858.8102045059204 seconds\n",
      "Step 19600, loss: tensor(0.0111, grad_fn=<SubBackward0>), time elapsed: 1868.240804195404 seconds\n",
      "Step 19700, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 1877.641963005066 seconds\n",
      "Step 19800, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 1887.1432580947876 seconds\n",
      "Step 19900, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 1896.5285141468048 seconds\n",
      "Step 20000, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 1905.8914194107056 seconds\n",
      "Step 20100, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1915.4343256950378 seconds\n",
      "Step 20200, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 1924.86976146698 seconds\n",
      "Step 20300, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1934.2054071426392 seconds\n",
      "Step 20400, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 1943.6878826618195 seconds\n",
      "Step 20500, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 1953.0098459720612 seconds\n",
      "Step 20600, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 1962.3402984142303 seconds\n",
      "Step 20700, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 1971.8695633411407 seconds\n",
      "Step 20800, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 1981.2218503952026 seconds\n",
      "Step 20900, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 1990.5687289237976 seconds\n",
      "Step 21000, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2000.1134593486786 seconds\n",
      "Step 21100, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 2009.583949804306 seconds\n",
      "Step 21200, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 2018.968344449997 seconds\n",
      "Step 21300, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 2028.6657299995422 seconds\n",
      "Step 21400, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2038.8624198436737 seconds\n",
      "Step 21500, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2048.2198996543884 seconds\n",
      "Step 21600, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2057.76189661026 seconds\n",
      "Step 21700, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2067.089004278183 seconds\n",
      "Step 21800, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 2076.6268882751465 seconds\n",
      "Step 21900, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2085.940398454666 seconds\n",
      "Step 22000, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2096.9434485435486 seconds\n",
      "Step 22100, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2106.8249895572662 seconds\n",
      "Step 22200, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2116.4281198978424 seconds\n",
      "Step 22300, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2125.943640232086 seconds\n",
      "Step 22400, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2136.2695710659027 seconds\n",
      "Step 22500, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 2145.7676799297333 seconds\n",
      "Step 22600, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2155.1668343544006 seconds\n",
      "Step 22700, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 2167.1854677200317 seconds\n",
      "Step 22800, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 2176.5866425037384 seconds\n",
      "Step 22900, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 2185.982347011566 seconds\n",
      "Step 23000, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2195.5682175159454 seconds\n",
      "Step 23100, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 2204.9862399101257 seconds\n",
      "Step 23200, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 2214.391630411148 seconds\n",
      "Step 23300, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 2224.0430614948273 seconds\n",
      "Step 23400, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 2233.4524307250977 seconds\n",
      "Step 23500, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2242.8785927295685 seconds\n",
      "Step 23600, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2252.5217895507812 seconds\n",
      "Step 23700, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 2261.9416892528534 seconds\n",
      "Step 23800, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2271.3374853134155 seconds\n",
      "Step 23900, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2280.92862200737 seconds\n",
      "Step 24000, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2290.3261981010437 seconds\n",
      "Step 24100, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 2299.7247347831726 seconds\n",
      "Step 24200, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 2309.3294060230255 seconds\n",
      "Step 24300, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2318.7222011089325 seconds\n",
      "Step 24400, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2328.0901420116425 seconds\n",
      "Step 24500, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 2337.6268830299377 seconds\n",
      "Step 24600, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 2346.996630191803 seconds\n",
      "Step 24700, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2356.3438341617584 seconds\n",
      "Step 24800, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 2365.8835196495056 seconds\n",
      "Step 24900, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2375.2690596580505 seconds\n",
      "Step 25000, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2384.818463087082 seconds\n",
      "Step 25100, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 2394.1641936302185 seconds\n",
      "Step 25200, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 2403.508059024811 seconds\n",
      "Step 25300, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 2413.0635616779327 seconds\n",
      "Step 25400, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 2422.4093277454376 seconds\n",
      "Step 25500, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2431.895372390747 seconds\n",
      "Step 25600, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2441.5299229621887 seconds\n",
      "Step 25700, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 2450.861304998398 seconds\n",
      "Step 25800, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 2460.198862552643 seconds\n",
      "Step 25900, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 2469.7376761436462 seconds\n",
      "Step 26000, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2479.038964986801 seconds\n",
      "Step 26100, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 2488.3513219356537 seconds\n",
      "Step 26200, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2497.8955941200256 seconds\n",
      "Step 26300, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2507.2089779376984 seconds\n",
      "Step 26400, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2516.5528950691223 seconds\n",
      "Step 26500, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 2526.094970226288 seconds\n",
      "Step 26600, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 2535.4919991493225 seconds\n",
      "Step 26700, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2544.830450296402 seconds\n",
      "Step 26800, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 2554.3402490615845 seconds\n",
      "Step 26900, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2563.69139957428 seconds\n",
      "Step 27000, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 2573.0548932552338 seconds\n",
      "Step 27100, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2582.5562040805817 seconds\n",
      "Step 27200, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2591.9158475399017 seconds\n",
      "Step 27300, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2601.2709748744965 seconds\n",
      "Step 27400, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 2610.797518491745 seconds\n",
      "Step 27500, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 2620.1744005680084 seconds\n",
      "Step 27600, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2629.551835298538 seconds\n",
      "Step 27700, loss: tensor(0.0109, grad_fn=<SubBackward0>), time elapsed: 2639.0928230285645 seconds\n",
      "Step 27800, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 2648.427107810974 seconds\n",
      "Step 27900, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2657.8076009750366 seconds\n",
      "Step 28000, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2667.3867180347443 seconds\n",
      "Step 28100, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2676.782913684845 seconds\n",
      "Step 28200, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 2686.1905376911163 seconds\n",
      "Step 28300, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 2695.754235267639 seconds\n",
      "Step 28400, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2705.1616866588593 seconds\n",
      "Step 28500, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 2714.769335746765 seconds\n",
      "Step 28600, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 2724.2301621437073 seconds\n",
      "Step 28700, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 2733.8733110427856 seconds\n",
      "Step 28800, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2743.4692504405975 seconds\n",
      "Step 28900, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2752.8078832626343 seconds\n",
      "Step 29000, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2762.149595975876 seconds\n",
      "Step 29100, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 2771.710373878479 seconds\n",
      "Step 29200, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 2781.053172826767 seconds\n",
      "Step 29300, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 2790.4114146232605 seconds\n",
      "Step 29400, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 2799.9295551776886 seconds\n",
      "Step 29500, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 2809.2818439006805 seconds\n",
      "Step 29600, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2818.6078972816467 seconds\n",
      "Step 29700, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2828.1634950637817 seconds\n",
      "Step 29800, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 2837.5154554843903 seconds\n",
      "Step 29900, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2846.9646854400635 seconds\n",
      "Step 30000, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 2856.7523069381714 seconds\n",
      "Step 30100, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2866.2938961982727 seconds\n",
      "Step 30200, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 2875.69172334671 seconds\n",
      "Step 30300, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2885.271299600601 seconds\n",
      "Step 30400, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 2894.648517847061 seconds\n",
      "Step 30500, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 2904.0111231803894 seconds\n",
      "Step 30600, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 2913.554281949997 seconds\n",
      "Step 30700, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 2922.8929047584534 seconds\n",
      "Step 30800, loss: tensor(0.0112, grad_fn=<SubBackward0>), time elapsed: 2932.2689151763916 seconds\n",
      "Step 30900, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 2941.805658340454 seconds\n",
      "Step 31000, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 2951.1547226905823 seconds\n",
      "Step 31100, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2960.5287923812866 seconds\n",
      "Step 31200, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 2970.1208357810974 seconds\n",
      "Step 31300, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 2979.8087527751923 seconds\n",
      "Step 31400, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 2989.765772819519 seconds\n",
      "Step 31500, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 2999.367163658142 seconds\n",
      "Step 31600, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 3008.7297694683075 seconds\n",
      "Step 31700, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 3018.1684651374817 seconds\n",
      "Step 31800, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 3027.7532024383545 seconds\n",
      "Step 31900, loss: tensor(0.0123, grad_fn=<SubBackward0>), time elapsed: 3037.3670909404755 seconds\n",
      "Step 32000, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 3046.7682886123657 seconds\n",
      "Step 32100, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 3056.369668006897 seconds\n",
      "Step 32200, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 3065.708337545395 seconds\n",
      "Step 32300, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 3075.2431313991547 seconds\n",
      "Step 32400, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 3084.6494143009186 seconds\n",
      "Step 32500, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 3093.9911329746246 seconds\n",
      "Step 32600, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 3103.5299637317657 seconds\n",
      "Step 32700, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 3112.8603146076202 seconds\n",
      "Step 32800, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 3122.1874577999115 seconds\n",
      "Step 32900, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 3131.715564966202 seconds\n",
      "Step 33000, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 3141.0389845371246 seconds\n",
      "Step 33100, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 3150.3524193763733 seconds\n",
      "Step 33200, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 3159.884984970093 seconds\n",
      "Step 33300, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 3169.196677684784 seconds\n",
      "Step 33400, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 3178.527902364731 seconds\n",
      "Step 33500, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 3188.0407774448395 seconds\n",
      "Step 33600, loss: tensor(0.0121, grad_fn=<SubBackward0>), time elapsed: 3197.3770048618317 seconds\n",
      "Step 33700, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 3206.709615945816 seconds\n",
      "Step 33800, loss: tensor(0.0122, grad_fn=<SubBackward0>), time elapsed: 3216.227498292923 seconds\n",
      "Step 33900, loss: tensor(0.0118, grad_fn=<SubBackward0>), time elapsed: 3225.541279554367 seconds\n",
      "Step 34000, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 3234.908935546875 seconds\n",
      "Step 34100, loss: tensor(0.0119, grad_fn=<SubBackward0>), time elapsed: 3244.4143948554993 seconds\n",
      "Step 34200, loss: tensor(0.0116, grad_fn=<SubBackward0>), time elapsed: 3253.7279970645905 seconds\n",
      "Step 34300, loss: tensor(0.0113, grad_fn=<SubBackward0>), time elapsed: 3263.084403991699 seconds\n",
      "Step 34400, loss: tensor(0.0125, grad_fn=<SubBackward0>), time elapsed: 3272.649254798889 seconds\n",
      "Step 34500, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 3281.986799955368 seconds\n",
      "Step 34600, loss: tensor(0.0124, grad_fn=<SubBackward0>), time elapsed: 3291.353619813919 seconds\n",
      "Step 34700, loss: tensor(0.0114, grad_fn=<SubBackward0>), time elapsed: 3300.967532157898 seconds\n",
      "Step 34800, loss: tensor(0.0117, grad_fn=<SubBackward0>), time elapsed: 3310.368729352951 seconds\n",
      "Step 34900, loss: tensor(0.0115, grad_fn=<SubBackward0>), time elapsed: 3319.7658166885376 seconds\n",
      "Step 35000, loss: tensor(0.0120, grad_fn=<SubBackward0>), time elapsed: 3329.3148334026337 seconds\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.1124, grad_fn=<SubBackward0>), time elapsed: 0.12489748001098633 seconds\n",
      "Step 100, loss: tensor(0.1038, grad_fn=<SubBackward0>), time elapsed: 9.62620496749878 seconds\n",
      "Step 200, loss: tensor(0.0919, grad_fn=<SubBackward0>), time elapsed: 18.9524884223938 seconds\n",
      "Step 300, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 28.641387224197388 seconds\n",
      "Step 400, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 37.959076166152954 seconds\n",
      "Step 500, loss: tensor(0.0697, grad_fn=<SubBackward0>), time elapsed: 47.268595695495605 seconds\n",
      "Step 600, loss: tensor(0.0656, grad_fn=<SubBackward0>), time elapsed: 56.737529039382935 seconds\n",
      "Step 700, loss: tensor(0.0629, grad_fn=<SubBackward0>), time elapsed: 66.04062080383301 seconds\n",
      "Step 800, loss: tensor(0.0602, grad_fn=<SubBackward0>), time elapsed: 75.52632021903992 seconds\n",
      "Step 900, loss: tensor(0.0578, grad_fn=<SubBackward0>), time elapsed: 84.88373255729675 seconds\n",
      "Step 1000, loss: tensor(0.0564, grad_fn=<SubBackward0>), time elapsed: 94.20567226409912 seconds\n",
      "Step 1100, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 103.64540910720825 seconds\n",
      "Step 1200, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 112.94230937957764 seconds\n",
      "Step 1300, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 122.23623394966125 seconds\n",
      "Step 1400, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 131.6959490776062 seconds\n",
      "Step 1500, loss: tensor(0.0472, grad_fn=<SubBackward0>), time elapsed: 141.0093638896942 seconds\n",
      "Step 1600, loss: tensor(0.0454, grad_fn=<SubBackward0>), time elapsed: 150.3449125289917 seconds\n",
      "Step 1700, loss: tensor(0.0448, grad_fn=<SubBackward0>), time elapsed: 159.8440203666687 seconds\n",
      "Step 1800, loss: tensor(0.0426, grad_fn=<SubBackward0>), time elapsed: 169.25990056991577 seconds\n",
      "Step 1900, loss: tensor(0.0417, grad_fn=<SubBackward0>), time elapsed: 178.62138414382935 seconds\n",
      "Step 2000, loss: tensor(0.0406, grad_fn=<SubBackward0>), time elapsed: 188.0831606388092 seconds\n",
      "Step 2100, loss: tensor(0.0389, grad_fn=<SubBackward0>), time elapsed: 197.36021256446838 seconds\n",
      "Step 2200, loss: tensor(0.0390, grad_fn=<SubBackward0>), time elapsed: 206.65095496177673 seconds\n",
      "Step 2300, loss: tensor(0.0385, grad_fn=<SubBackward0>), time elapsed: 216.08337020874023 seconds\n",
      "Step 2400, loss: tensor(0.0373, grad_fn=<SubBackward0>), time elapsed: 225.34483695030212 seconds\n",
      "Step 2500, loss: tensor(0.0364, grad_fn=<SubBackward0>), time elapsed: 234.81281042099 seconds\n",
      "Step 2600, loss: tensor(0.0362, grad_fn=<SubBackward0>), time elapsed: 244.10967683792114 seconds\n",
      "Step 2700, loss: tensor(0.0355, grad_fn=<SubBackward0>), time elapsed: 253.464989900589 seconds\n",
      "Step 2800, loss: tensor(0.0353, grad_fn=<SubBackward0>), time elapsed: 263.0090842247009 seconds\n",
      "Step 2900, loss: tensor(0.0354, grad_fn=<SubBackward0>), time elapsed: 272.4061162471771 seconds\n",
      "Step 3000, loss: tensor(0.0342, grad_fn=<SubBackward0>), time elapsed: 281.7521607875824 seconds\n",
      "Step 3100, loss: tensor(0.0347, grad_fn=<SubBackward0>), time elapsed: 291.2509820461273 seconds\n",
      "Step 3200, loss: tensor(0.0339, grad_fn=<SubBackward0>), time elapsed: 300.6065137386322 seconds\n",
      "Step 3300, loss: tensor(0.0333, grad_fn=<SubBackward0>), time elapsed: 309.92637753486633 seconds\n",
      "Step 3400, loss: tensor(0.0328, grad_fn=<SubBackward0>), time elapsed: 319.4109959602356 seconds\n",
      "Step 3500, loss: tensor(0.0330, grad_fn=<SubBackward0>), time elapsed: 328.7585849761963 seconds\n",
      "Step 3600, loss: tensor(0.0325, grad_fn=<SubBackward0>), time elapsed: 337.8790955543518 seconds\n",
      "Step 3700, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 347.13002252578735 seconds\n",
      "Step 3800, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 356.26110339164734 seconds\n",
      "Step 3900, loss: tensor(0.0319, grad_fn=<SubBackward0>), time elapsed: 365.3971884250641 seconds\n",
      "Step 4000, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 374.6712682247162 seconds\n",
      "Step 4100, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 383.7744493484497 seconds\n",
      "Step 4200, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 393.0773310661316 seconds\n",
      "Step 4300, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 402.2006072998047 seconds\n",
      "Step 4400, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 411.29437589645386 seconds\n",
      "Step 4500, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 420.58613300323486 seconds\n",
      "Step 4600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 429.69034814834595 seconds\n",
      "Step 4700, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 438.7854070663452 seconds\n",
      "Step 4800, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 448.0408446788788 seconds\n",
      "Step 4900, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 457.16402792930603 seconds\n",
      "Step 5000, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 466.2756721973419 seconds\n",
      "Step 5100, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 475.5186171531677 seconds\n",
      "Step 5200, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 484.63004970550537 seconds\n",
      "Step 5300, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 493.72858786582947 seconds\n",
      "Step 5400, loss: tensor(0.0287, grad_fn=<SubBackward0>), time elapsed: 502.9708559513092 seconds\n",
      "Step 5500, loss: tensor(0.0285, grad_fn=<SubBackward0>), time elapsed: 512.0647120475769 seconds\n",
      "Step 5600, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 521.455320596695 seconds\n",
      "Step 5700, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 531.26482629776 seconds\n",
      "Step 5800, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 540.3795783519745 seconds\n",
      "Step 5900, loss: tensor(0.0285, grad_fn=<SubBackward0>), time elapsed: 549.7562019824982 seconds\n",
      "Step 6000, loss: tensor(0.0286, grad_fn=<SubBackward0>), time elapsed: 558.8626358509064 seconds\n",
      "Step 6100, loss: tensor(0.0285, grad_fn=<SubBackward0>), time elapsed: 567.9884374141693 seconds\n",
      "Step 6200, loss: tensor(0.0283, grad_fn=<SubBackward0>), time elapsed: 577.3453392982483 seconds\n",
      "Step 6300, loss: tensor(0.0292, grad_fn=<SubBackward0>), time elapsed: 586.4627130031586 seconds\n",
      "Step 6400, loss: tensor(0.0288, grad_fn=<SubBackward0>), time elapsed: 596.2973864078522 seconds\n",
      "Step 6500, loss: tensor(0.0292, grad_fn=<SubBackward0>), time elapsed: 605.6148030757904 seconds\n",
      "Step 6600, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 614.8229942321777 seconds\n",
      "Step 6700, loss: tensor(0.0287, grad_fn=<SubBackward0>), time elapsed: 623.9278981685638 seconds\n",
      "Step 6800, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 633.2076489925385 seconds\n",
      "Step 6900, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 642.3188405036926 seconds\n",
      "Step 7000, loss: tensor(0.0281, grad_fn=<SubBackward0>), time elapsed: 651.4364290237427 seconds\n",
      "Step 7100, loss: tensor(0.0281, grad_fn=<SubBackward0>), time elapsed: 661.5263726711273 seconds\n",
      "Step 7200, loss: tensor(0.0277, grad_fn=<SubBackward0>), time elapsed: 670.6611263751984 seconds\n",
      "Step 7300, loss: tensor(0.0282, grad_fn=<SubBackward0>), time elapsed: 679.743629693985 seconds\n",
      "Step 7400, loss: tensor(0.0275, grad_fn=<SubBackward0>), time elapsed: 689.1062622070312 seconds\n",
      "Step 7500, loss: tensor(0.0271, grad_fn=<SubBackward0>), time elapsed: 698.3148679733276 seconds\n",
      "Step 7600, loss: tensor(0.0274, grad_fn=<SubBackward0>), time elapsed: 707.6003761291504 seconds\n",
      "Step 7700, loss: tensor(0.0270, grad_fn=<SubBackward0>), time elapsed: 716.7144572734833 seconds\n",
      "Step 7800, loss: tensor(0.0268, grad_fn=<SubBackward0>), time elapsed: 725.8863341808319 seconds\n",
      "Step 7900, loss: tensor(0.0265, grad_fn=<SubBackward0>), time elapsed: 735.7642149925232 seconds\n",
      "Step 8000, loss: tensor(0.0262, grad_fn=<SubBackward0>), time elapsed: 745.0315375328064 seconds\n",
      "Step 8100, loss: tensor(0.0256, grad_fn=<SubBackward0>), time elapsed: 754.2316782474518 seconds\n",
      "Step 8200, loss: tensor(0.0257, grad_fn=<SubBackward0>), time elapsed: 763.9154031276703 seconds\n",
      "Step 8300, loss: tensor(0.0260, grad_fn=<SubBackward0>), time elapsed: 773.0032076835632 seconds\n",
      "Step 8400, loss: tensor(0.0262, grad_fn=<SubBackward0>), time elapsed: 782.0958771705627 seconds\n",
      "Step 8500, loss: tensor(0.0249, grad_fn=<SubBackward0>), time elapsed: 791.4004054069519 seconds\n",
      "Step 8600, loss: tensor(0.0250, grad_fn=<SubBackward0>), time elapsed: 800.529447555542 seconds\n",
      "Step 8700, loss: tensor(0.0245, grad_fn=<SubBackward0>), time elapsed: 810.0837626457214 seconds\n",
      "Step 8800, loss: tensor(0.0253, grad_fn=<SubBackward0>), time elapsed: 819.4726972579956 seconds\n",
      "Step 8900, loss: tensor(0.0242, grad_fn=<SubBackward0>), time elapsed: 828.8386836051941 seconds\n",
      "Step 9000, loss: tensor(0.0239, grad_fn=<SubBackward0>), time elapsed: 838.0589301586151 seconds\n",
      "Step 9100, loss: tensor(0.0250, grad_fn=<SubBackward0>), time elapsed: 847.3621973991394 seconds\n",
      "Step 9200, loss: tensor(0.0242, grad_fn=<SubBackward0>), time elapsed: 857.1779429912567 seconds\n",
      "Step 9300, loss: tensor(0.0244, grad_fn=<SubBackward0>), time elapsed: 866.2828042507172 seconds\n",
      "Step 9400, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 875.5442156791687 seconds\n",
      "Step 9500, loss: tensor(0.0235, grad_fn=<SubBackward0>), time elapsed: 884.683438539505 seconds\n",
      "Step 9600, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 893.9630463123322 seconds\n",
      "Step 9700, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 903.0844025611877 seconds\n",
      "Step 9800, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 912.9353318214417 seconds\n",
      "Step 9900, loss: tensor(0.0231, grad_fn=<SubBackward0>), time elapsed: 922.2283082008362 seconds\n",
      "Step 10000, loss: tensor(0.0231, grad_fn=<SubBackward0>), time elapsed: 931.3428883552551 seconds\n",
      "Step 10100, loss: tensor(0.0230, grad_fn=<SubBackward0>), time elapsed: 940.4367959499359 seconds\n",
      "Step 10200, loss: tensor(0.0228, grad_fn=<SubBackward0>), time elapsed: 949.7083323001862 seconds\n",
      "Step 10300, loss: tensor(0.0226, grad_fn=<SubBackward0>), time elapsed: 958.7859287261963 seconds\n",
      "Step 10400, loss: tensor(0.0227, grad_fn=<SubBackward0>), time elapsed: 968.1348972320557 seconds\n",
      "Step 10500, loss: tensor(0.0223, grad_fn=<SubBackward0>), time elapsed: 977.4054343700409 seconds\n",
      "Step 10600, loss: tensor(0.0213, grad_fn=<SubBackward0>), time elapsed: 986.4970071315765 seconds\n",
      "Step 10700, loss: tensor(0.0222, grad_fn=<SubBackward0>), time elapsed: 996.9482982158661 seconds\n",
      "Step 10800, loss: tensor(0.0222, grad_fn=<SubBackward0>), time elapsed: 1006.288605928421 seconds\n",
      "Step 10900, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 1015.3945064544678 seconds\n",
      "Step 11000, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 1024.496185541153 seconds\n",
      "Step 11100, loss: tensor(0.0218, grad_fn=<SubBackward0>), time elapsed: 1033.7703273296356 seconds\n",
      "Step 11200, loss: tensor(0.0217, grad_fn=<SubBackward0>), time elapsed: 1042.8786416053772 seconds\n",
      "Step 11300, loss: tensor(0.0221, grad_fn=<SubBackward0>), time elapsed: 1052.0168435573578 seconds\n",
      "Step 11400, loss: tensor(0.0221, grad_fn=<SubBackward0>), time elapsed: 1061.4371447563171 seconds\n",
      "Step 11500, loss: tensor(0.0213, grad_fn=<SubBackward0>), time elapsed: 1070.5178935527802 seconds\n",
      "Step 11600, loss: tensor(0.0216, grad_fn=<SubBackward0>), time elapsed: 1079.8143882751465 seconds\n",
      "Step 11700, loss: tensor(0.0213, grad_fn=<SubBackward0>), time elapsed: 1089.3674845695496 seconds\n",
      "Step 11800, loss: tensor(0.0214, grad_fn=<SubBackward0>), time elapsed: 1098.4936475753784 seconds\n",
      "Step 11900, loss: tensor(0.0212, grad_fn=<SubBackward0>), time elapsed: 1107.7671375274658 seconds\n",
      "Step 12000, loss: tensor(0.0200, grad_fn=<SubBackward0>), time elapsed: 1116.9106771945953 seconds\n",
      "Step 12100, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 1126.0782425403595 seconds\n",
      "Step 12200, loss: tensor(0.0203, grad_fn=<SubBackward0>), time elapsed: 1135.5157644748688 seconds\n",
      "Step 12300, loss: tensor(0.0196, grad_fn=<SubBackward0>), time elapsed: 1144.6991515159607 seconds\n",
      "Step 12400, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 1154.3314986228943 seconds\n",
      "Step 12500, loss: tensor(0.0194, grad_fn=<SubBackward0>), time elapsed: 1163.638309955597 seconds\n",
      "Step 12600, loss: tensor(0.0201, grad_fn=<SubBackward0>), time elapsed: 1172.7409143447876 seconds\n",
      "Step 12700, loss: tensor(0.0196, grad_fn=<SubBackward0>), time elapsed: 1181.8383424282074 seconds\n",
      "Step 12800, loss: tensor(0.0192, grad_fn=<SubBackward0>), time elapsed: 1191.8962399959564 seconds\n",
      "Step 12900, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 1201.111627817154 seconds\n",
      "Step 13000, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 1210.2334070205688 seconds\n",
      "Step 13100, loss: tensor(0.0194, grad_fn=<SubBackward0>), time elapsed: 1219.5719647407532 seconds\n",
      "Step 13200, loss: tensor(0.0197, grad_fn=<SubBackward0>), time elapsed: 1228.6823942661285 seconds\n",
      "Step 13300, loss: tensor(0.0196, grad_fn=<SubBackward0>), time elapsed: 1237.820323228836 seconds\n",
      "Step 13400, loss: tensor(0.0195, grad_fn=<SubBackward0>), time elapsed: 1247.6661531925201 seconds\n",
      "Step 13500, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 1256.774405002594 seconds\n",
      "Step 13600, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 1265.8570666313171 seconds\n",
      "Step 13700, loss: tensor(0.0194, grad_fn=<SubBackward0>), time elapsed: 1275.1362030506134 seconds\n",
      "Step 13800, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 1284.2712588310242 seconds\n",
      "Step 13900, loss: tensor(0.0198, grad_fn=<SubBackward0>), time elapsed: 1293.5555937290192 seconds\n",
      "Step 14000, loss: tensor(0.0196, grad_fn=<SubBackward0>), time elapsed: 1302.6634528636932 seconds\n",
      "Step 14100, loss: tensor(0.0196, grad_fn=<SubBackward0>), time elapsed: 1311.8238408565521 seconds\n",
      "Step 14200, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 1321.1526882648468 seconds\n",
      "Step 14300, loss: tensor(0.0196, grad_fn=<SubBackward0>), time elapsed: 1330.279974937439 seconds\n",
      "Step 14400, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 1339.4467890262604 seconds\n",
      "Step 14500, loss: tensor(0.0194, grad_fn=<SubBackward0>), time elapsed: 1348.7736630439758 seconds\n",
      "Step 14600, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 1357.9173777103424 seconds\n",
      "Step 14700, loss: tensor(0.0202, grad_fn=<SubBackward0>), time elapsed: 1367.049362897873 seconds\n",
      "Step 14800, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 1376.5394189357758 seconds\n",
      "Step 14900, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 1385.6466884613037 seconds\n",
      "Step 15000, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 1395.2137160301208 seconds\n",
      "Step 15100, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 1404.5099408626556 seconds\n",
      "Step 15200, loss: tensor(0.0194, grad_fn=<SubBackward0>), time elapsed: 1413.6258518695831 seconds\n",
      "Step 15300, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 1422.749627828598 seconds\n",
      "Step 15400, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 1432.0862905979156 seconds\n",
      "Step 15500, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 1441.2095472812653 seconds\n",
      "Step 15600, loss: tensor(0.0194, grad_fn=<SubBackward0>), time elapsed: 1450.4936542510986 seconds\n",
      "Step 15700, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 1461.0903587341309 seconds\n",
      "Step 15800, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 1470.1712658405304 seconds\n",
      "Step 15900, loss: tensor(0.0191, grad_fn=<SubBackward0>), time elapsed: 1479.2588448524475 seconds\n",
      "Step 16000, loss: tensor(0.0196, grad_fn=<SubBackward0>), time elapsed: 1488.5129215717316 seconds\n",
      "Step 16100, loss: tensor(0.0194, grad_fn=<SubBackward0>), time elapsed: 1497.6051454544067 seconds\n",
      "Step 16200, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 1506.8573627471924 seconds\n",
      "Step 16300, loss: tensor(0.0192, grad_fn=<SubBackward0>), time elapsed: 1515.9237005710602 seconds\n",
      "Step 16400, loss: tensor(0.0192, grad_fn=<SubBackward0>), time elapsed: 1525.0511012077332 seconds\n",
      "Step 16500, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 1534.371589422226 seconds\n",
      "Step 16600, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 1543.5125751495361 seconds\n",
      "Step 16700, loss: tensor(0.0198, grad_fn=<SubBackward0>), time elapsed: 1552.6496217250824 seconds\n",
      "Step 16800, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 1562.1671495437622 seconds\n",
      "Step 16900, loss: tensor(0.0191, grad_fn=<SubBackward0>), time elapsed: 1571.5916593074799 seconds\n",
      "Step 17000, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 1580.8000836372375 seconds\n",
      "Step 17100, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 1590.131327867508 seconds\n",
      "Step 17200, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 1599.2589678764343 seconds\n",
      "Step 17300, loss: tensor(0.0192, grad_fn=<SubBackward0>), time elapsed: 1608.9372909069061 seconds\n",
      "Step 17400, loss: tensor(0.0192, grad_fn=<SubBackward0>), time elapsed: 1618.3701593875885 seconds\n",
      "Step 17500, loss: tensor(0.0194, grad_fn=<SubBackward0>), time elapsed: 1627.5837602615356 seconds\n",
      "Step 17600, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 1636.7076728343964 seconds\n",
      "Step 17700, loss: tensor(0.0195, grad_fn=<SubBackward0>), time elapsed: 1646.0370922088623 seconds\n",
      "Step 17800, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 1655.1768493652344 seconds\n",
      "Step 17900, loss: tensor(0.0196, grad_fn=<SubBackward0>), time elapsed: 1664.3900620937347 seconds\n",
      "Step 18000, loss: tensor(0.0194, grad_fn=<SubBackward0>), time elapsed: 1674.2680027484894 seconds\n",
      "Step 18100, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 1683.4284944534302 seconds\n",
      "Step 18200, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 1692.5367999076843 seconds\n",
      "Step 18300, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 1702.2246096134186 seconds\n",
      "Step 18400, loss: tensor(0.0194, grad_fn=<SubBackward0>), time elapsed: 1711.396378993988 seconds\n",
      "Step 18500, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 1720.4864904880524 seconds\n",
      "Step 18600, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 1729.7620749473572 seconds\n",
      "Step 18700, loss: tensor(0.0191, grad_fn=<SubBackward0>), time elapsed: 1739.4379770755768 seconds\n",
      "Step 18800, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 1748.7428584098816 seconds\n",
      "Step 18900, loss: tensor(0.0191, grad_fn=<SubBackward0>), time elapsed: 1757.8017497062683 seconds\n",
      "Step 19000, loss: tensor(0.0193, grad_fn=<SubBackward0>), time elapsed: 1766.9211311340332 seconds\n",
      "Step 19100, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 1776.7777223587036 seconds\n",
      "Step 19200, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 1786.1981563568115 seconds\n",
      "Step 19300, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 1795.2979061603546 seconds\n",
      "Step 19400, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 1804.6262619495392 seconds\n",
      "Step 19500, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 1813.7164151668549 seconds\n",
      "Step 19600, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 1822.8286774158478 seconds\n",
      "Step 19700, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 1833.1690244674683 seconds\n",
      "Step 19800, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 1842.3658893108368 seconds\n",
      "Step 19900, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 1851.4795382022858 seconds\n",
      "Step 20000, loss: tensor(0.0191, grad_fn=<SubBackward0>), time elapsed: 1860.7915079593658 seconds\n",
      "Step 20100, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 1869.9419391155243 seconds\n",
      "Step 20200, loss: tensor(0.0180, grad_fn=<SubBackward0>), time elapsed: 1879.6441519260406 seconds\n",
      "Step 20300, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 1888.97309923172 seconds\n",
      "Step 20400, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 1898.845918893814 seconds\n",
      "Step 20500, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 1908.0822172164917 seconds\n",
      "Step 20600, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 1917.3699712753296 seconds\n",
      "Step 20700, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 1926.4785933494568 seconds\n",
      "Step 20800, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 1935.5960261821747 seconds\n",
      "Step 20900, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 1944.8644654750824 seconds\n",
      "Step 21000, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 1953.9816946983337 seconds\n",
      "Step 21100, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 1963.097333908081 seconds\n",
      "Step 21200, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 1972.3881673812866 seconds\n",
      "Step 21300, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 1981.545654296875 seconds\n",
      "Step 21400, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 1990.8715662956238 seconds\n",
      "Step 21500, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 1999.9990170001984 seconds\n",
      "Step 21600, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 2009.106172323227 seconds\n",
      "Step 21700, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2018.4700832366943 seconds\n",
      "Step 21800, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2027.600046634674 seconds\n",
      "Step 21900, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2036.7432787418365 seconds\n",
      "Step 22000, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2046.0833933353424 seconds\n",
      "Step 22100, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 2055.2121510505676 seconds\n",
      "Step 22200, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2064.354544878006 seconds\n",
      "Step 22300, loss: tensor(0.0177, grad_fn=<SubBackward0>), time elapsed: 2073.7549772262573 seconds\n",
      "Step 22400, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 2082.9263639450073 seconds\n",
      "Step 22500, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2092.0966663360596 seconds\n",
      "Step 22600, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2101.4858572483063 seconds\n",
      "Step 22700, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 2112.144737482071 seconds\n",
      "Step 22800, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2121.6906864643097 seconds\n",
      "Step 22900, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2131.0889763832092 seconds\n",
      "Step 23000, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2140.32591176033 seconds\n",
      "Step 23100, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 2149.512462615967 seconds\n",
      "Step 23200, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2158.8467276096344 seconds\n",
      "Step 23300, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 2168.029322385788 seconds\n",
      "Step 23400, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 2177.368422269821 seconds\n",
      "Step 23500, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 2186.8678250312805 seconds\n",
      "Step 23600, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 2196.8243844509125 seconds\n",
      "Step 23700, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2206.12126493454 seconds\n",
      "Step 23800, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2215.5895133018494 seconds\n",
      "Step 23900, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 2224.966861963272 seconds\n",
      "Step 24000, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 2234.262314558029 seconds\n",
      "Step 24100, loss: tensor(0.0191, grad_fn=<SubBackward0>), time elapsed: 2243.753313064575 seconds\n",
      "Step 24200, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2253.8707008361816 seconds\n",
      "Step 24300, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2263.3873794078827 seconds\n",
      "Step 24400, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2272.6779551506042 seconds\n",
      "Step 24500, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2282.0185840129852 seconds\n",
      "Step 24600, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 2291.59565615654 seconds\n",
      "Step 24700, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 2300.9729256629944 seconds\n",
      "Step 24800, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2310.3597967624664 seconds\n",
      "Step 24900, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 2320.149356842041 seconds\n",
      "Step 25000, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 2329.3545265197754 seconds\n",
      "Step 25100, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2338.5723507404327 seconds\n",
      "Step 25200, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2348.387127161026 seconds\n",
      "Step 25300, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 2357.6113545894623 seconds\n",
      "Step 25400, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2366.788019180298 seconds\n",
      "Step 25500, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2376.7933444976807 seconds\n",
      "Step 25600, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 2386.0670952796936 seconds\n",
      "Step 25700, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2395.2753989696503 seconds\n",
      "Step 25800, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2404.5939667224884 seconds\n",
      "Step 25900, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 2413.7338597774506 seconds\n",
      "Step 26000, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2422.867209672928 seconds\n",
      "Step 26100, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2432.1843571662903 seconds\n",
      "Step 26200, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 2442.0713708400726 seconds\n",
      "Step 26300, loss: tensor(0.0178, grad_fn=<SubBackward0>), time elapsed: 2451.191072702408 seconds\n",
      "Step 26400, loss: tensor(0.0180, grad_fn=<SubBackward0>), time elapsed: 2460.6475234031677 seconds\n",
      "Step 26500, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2469.787146806717 seconds\n",
      "Step 26600, loss: tensor(0.0180, grad_fn=<SubBackward0>), time elapsed: 2478.927379131317 seconds\n",
      "Step 26700, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2488.2791078090668 seconds\n",
      "Step 26800, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 2498.087901830673 seconds\n",
      "Step 26900, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2507.293295621872 seconds\n",
      "Step 27000, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 2516.7012600898743 seconds\n",
      "Step 27100, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2525.8492588996887 seconds\n",
      "Step 27200, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2535.730188846588 seconds\n",
      "Step 27300, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2545.4226965904236 seconds\n",
      "Step 27400, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 2554.5635499954224 seconds\n",
      "Step 27500, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 2563.7104642391205 seconds\n",
      "Step 27600, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2573.0571053028107 seconds\n",
      "Step 27700, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2582.203110933304 seconds\n",
      "Step 27800, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 2591.5523896217346 seconds\n",
      "Step 27900, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2600.7219614982605 seconds\n",
      "Step 28000, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2609.9126415252686 seconds\n",
      "Step 28100, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2619.282841682434 seconds\n",
      "Step 28200, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 2628.443000793457 seconds\n",
      "Step 28300, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2637.5983114242554 seconds\n",
      "Step 28400, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2647.4538826942444 seconds\n",
      "Step 28500, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 2656.6232767105103 seconds\n",
      "Step 28600, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2665.7463335990906 seconds\n",
      "Step 28700, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2675.0685086250305 seconds\n",
      "Step 28800, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2684.2665684223175 seconds\n",
      "Step 28900, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2693.446424484253 seconds\n",
      "Step 29000, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2702.8473720550537 seconds\n",
      "Step 29100, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 2712.0419137477875 seconds\n",
      "Step 29200, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 2721.2512471675873 seconds\n",
      "Step 29300, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2730.6670837402344 seconds\n",
      "Step 29400, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2739.863178730011 seconds\n",
      "Step 29500, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 2749.0901896953583 seconds\n",
      "Step 29600, loss: tensor(0.0191, grad_fn=<SubBackward0>), time elapsed: 2758.452492952347 seconds\n",
      "Step 29700, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 2767.6063034534454 seconds\n",
      "Step 29800, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 2776.79608130455 seconds\n",
      "Step 29900, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2786.138840198517 seconds\n",
      "Step 30000, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 2795.28413271904 seconds\n",
      "Step 30100, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 2804.4404249191284 seconds\n",
      "Step 30200, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 2813.830489397049 seconds\n",
      "Step 30300, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 2822.9916248321533 seconds\n",
      "Step 30400, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2832.143075466156 seconds\n",
      "Step 30500, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2841.5498118400574 seconds\n",
      "Step 30600, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2850.7367119789124 seconds\n",
      "Step 30700, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2859.9066956043243 seconds\n",
      "Step 30800, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 2870.023844718933 seconds\n",
      "Step 30900, loss: tensor(0.0179, grad_fn=<SubBackward0>), time elapsed: 2879.215478181839 seconds\n",
      "Step 31000, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2888.327068090439 seconds\n",
      "Step 31100, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 2897.696033477783 seconds\n",
      "Step 31200, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2906.873459339142 seconds\n",
      "Step 31300, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2916.0302274227142 seconds\n",
      "Step 31400, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 2925.3777549266815 seconds\n",
      "Step 31500, loss: tensor(0.0192, grad_fn=<SubBackward0>), time elapsed: 2934.5595903396606 seconds\n",
      "Step 31600, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 2943.9007420539856 seconds\n",
      "Step 31700, loss: tensor(0.0180, grad_fn=<SubBackward0>), time elapsed: 2953.0266926288605 seconds\n",
      "Step 31800, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 2962.2152512073517 seconds\n",
      "Step 31900, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 2971.563464641571 seconds\n",
      "Step 32000, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 2980.722916126251 seconds\n",
      "Step 32100, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2989.865439414978 seconds\n",
      "Step 32200, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 2999.2484979629517 seconds\n",
      "Step 32300, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 3008.5844790935516 seconds\n",
      "Step 32400, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 3017.775228738785 seconds\n",
      "Step 32500, loss: tensor(0.0195, grad_fn=<SubBackward0>), time elapsed: 3027.150286912918 seconds\n",
      "Step 32600, loss: tensor(0.0185, grad_fn=<SubBackward0>), time elapsed: 3036.3066511154175 seconds\n",
      "Step 32700, loss: tensor(0.0182, grad_fn=<SubBackward0>), time elapsed: 3045.519637823105 seconds\n",
      "Step 32800, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 3054.922561645508 seconds\n",
      "Step 32900, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 3064.709280729294 seconds\n",
      "Step 33000, loss: tensor(0.0195, grad_fn=<SubBackward0>), time elapsed: 3073.863968849182 seconds\n",
      "Step 33100, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 3083.247876405716 seconds\n",
      "Step 33200, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 3092.3223254680634 seconds\n",
      "Step 33300, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 3101.4334301948547 seconds\n",
      "Step 33400, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 3110.7401728630066 seconds\n",
      "Step 33500, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 3119.8564648628235 seconds\n",
      "Step 33600, loss: tensor(0.0190, grad_fn=<SubBackward0>), time elapsed: 3129.0128054618835 seconds\n",
      "Step 33700, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 3138.332457780838 seconds\n",
      "Step 33800, loss: tensor(0.0184, grad_fn=<SubBackward0>), time elapsed: 3147.6140806674957 seconds\n",
      "Step 33900, loss: tensor(0.0181, grad_fn=<SubBackward0>), time elapsed: 3156.7360138893127 seconds\n",
      "Step 34000, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 3166.112396478653 seconds\n",
      "Step 34100, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 3175.3049833774567 seconds\n",
      "Step 34200, loss: tensor(0.0191, grad_fn=<SubBackward0>), time elapsed: 3184.4395582675934 seconds\n",
      "Step 34300, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 3193.9927127361298 seconds\n",
      "Step 34400, loss: tensor(0.0183, grad_fn=<SubBackward0>), time elapsed: 3204.1879851818085 seconds\n",
      "Step 34500, loss: tensor(0.0187, grad_fn=<SubBackward0>), time elapsed: 3213.319365978241 seconds\n",
      "Step 34600, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 3222.6513862609863 seconds\n",
      "Step 34700, loss: tensor(0.0188, grad_fn=<SubBackward0>), time elapsed: 3231.766196012497 seconds\n",
      "Step 34800, loss: tensor(0.0189, grad_fn=<SubBackward0>), time elapsed: 3240.919759988785 seconds\n",
      "Step 34900, loss: tensor(0.0186, grad_fn=<SubBackward0>), time elapsed: 3250.2643718719482 seconds\n",
      "Step 35000, loss: tensor(0.0191, grad_fn=<SubBackward0>), time elapsed: 3259.4351196289062 seconds\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.1732, grad_fn=<SubBackward0>), time elapsed: 0.1471266746520996 seconds\n",
      "Step 100, loss: tensor(0.1576, grad_fn=<SubBackward0>), time elapsed: 10.59526515007019 seconds\n",
      "Step 200, loss: tensor(0.1396, grad_fn=<SubBackward0>), time elapsed: 19.809653520584106 seconds\n",
      "Step 300, loss: tensor(0.1251, grad_fn=<SubBackward0>), time elapsed: 28.907712697982788 seconds\n",
      "Step 400, loss: tensor(0.1138, grad_fn=<SubBackward0>), time elapsed: 38.18952822685242 seconds\n",
      "Step 500, loss: tensor(0.0994, grad_fn=<SubBackward0>), time elapsed: 47.27031874656677 seconds\n",
      "Step 600, loss: tensor(0.0895, grad_fn=<SubBackward0>), time elapsed: 56.3870005607605 seconds\n",
      "Step 700, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 65.64974451065063 seconds\n",
      "Step 800, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 74.74407029151917 seconds\n",
      "Step 900, loss: tensor(0.0693, grad_fn=<SubBackward0>), time elapsed: 83.81980156898499 seconds\n",
      "Step 1000, loss: tensor(0.0673, grad_fn=<SubBackward0>), time elapsed: 93.04626202583313 seconds\n",
      "Step 1100, loss: tensor(0.0639, grad_fn=<SubBackward0>), time elapsed: 102.13712668418884 seconds\n",
      "Step 1200, loss: tensor(0.0622, grad_fn=<SubBackward0>), time elapsed: 111.2386543750763 seconds\n",
      "Step 1300, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 120.48810148239136 seconds\n",
      "Step 1400, loss: tensor(0.0566, grad_fn=<SubBackward0>), time elapsed: 129.60167288780212 seconds\n",
      "Step 1500, loss: tensor(0.0552, grad_fn=<SubBackward0>), time elapsed: 138.70420169830322 seconds\n",
      "Step 1600, loss: tensor(0.0535, grad_fn=<SubBackward0>), time elapsed: 147.9628782272339 seconds\n",
      "Step 1700, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 157.02748227119446 seconds\n",
      "Step 1800, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 166.29707050323486 seconds\n",
      "Step 1900, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 175.37767148017883 seconds\n",
      "Step 2000, loss: tensor(0.0467, grad_fn=<SubBackward0>), time elapsed: 184.47472167015076 seconds\n",
      "Step 2100, loss: tensor(0.0475, grad_fn=<SubBackward0>), time elapsed: 193.7703197002411 seconds\n",
      "Step 2200, loss: tensor(0.0450, grad_fn=<SubBackward0>), time elapsed: 202.89057993888855 seconds\n",
      "Step 2300, loss: tensor(0.0432, grad_fn=<SubBackward0>), time elapsed: 212.01151251792908 seconds\n",
      "Step 2400, loss: tensor(0.0447, grad_fn=<SubBackward0>), time elapsed: 221.29957699775696 seconds\n",
      "Step 2500, loss: tensor(0.0431, grad_fn=<SubBackward0>), time elapsed: 230.3913974761963 seconds\n",
      "Step 2600, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 239.50240302085876 seconds\n",
      "Step 2700, loss: tensor(0.0411, grad_fn=<SubBackward0>), time elapsed: 248.75562572479248 seconds\n",
      "Step 2800, loss: tensor(0.0396, grad_fn=<SubBackward0>), time elapsed: 257.8613781929016 seconds\n",
      "Step 2900, loss: tensor(0.0406, grad_fn=<SubBackward0>), time elapsed: 266.94981813430786 seconds\n",
      "Step 3000, loss: tensor(0.0395, grad_fn=<SubBackward0>), time elapsed: 276.1990849971771 seconds\n",
      "Step 3100, loss: tensor(0.0382, grad_fn=<SubBackward0>), time elapsed: 285.3401575088501 seconds\n",
      "Step 3200, loss: tensor(0.0375, grad_fn=<SubBackward0>), time elapsed: 294.4580011367798 seconds\n",
      "Step 3300, loss: tensor(0.0380, grad_fn=<SubBackward0>), time elapsed: 303.7256889343262 seconds\n",
      "Step 3400, loss: tensor(0.0377, grad_fn=<SubBackward0>), time elapsed: 312.8436629772186 seconds\n",
      "Step 3500, loss: tensor(0.0386, grad_fn=<SubBackward0>), time elapsed: 322.1098082065582 seconds\n",
      "Step 3600, loss: tensor(0.0381, grad_fn=<SubBackward0>), time elapsed: 331.25023317337036 seconds\n",
      "Step 3700, loss: tensor(0.0392, grad_fn=<SubBackward0>), time elapsed: 340.3659722805023 seconds\n",
      "Step 3800, loss: tensor(0.0388, grad_fn=<SubBackward0>), time elapsed: 349.6781132221222 seconds\n",
      "Step 3900, loss: tensor(0.0388, grad_fn=<SubBackward0>), time elapsed: 358.7936153411865 seconds\n",
      "Step 4000, loss: tensor(0.0378, grad_fn=<SubBackward0>), time elapsed: 367.9116520881653 seconds\n",
      "Step 4100, loss: tensor(0.0383, grad_fn=<SubBackward0>), time elapsed: 377.20275115966797 seconds\n",
      "Step 4200, loss: tensor(0.0371, grad_fn=<SubBackward0>), time elapsed: 386.30979585647583 seconds\n",
      "Step 4300, loss: tensor(0.0387, grad_fn=<SubBackward0>), time elapsed: 395.40326404571533 seconds\n",
      "Step 4400, loss: tensor(0.0378, grad_fn=<SubBackward0>), time elapsed: 404.68049001693726 seconds\n",
      "Step 4500, loss: tensor(0.0379, grad_fn=<SubBackward0>), time elapsed: 413.8018448352814 seconds\n",
      "Step 4600, loss: tensor(0.0378, grad_fn=<SubBackward0>), time elapsed: 422.9318082332611 seconds\n",
      "Step 4700, loss: tensor(0.0373, grad_fn=<SubBackward0>), time elapsed: 432.2430565357208 seconds\n",
      "Step 4800, loss: tensor(0.0374, grad_fn=<SubBackward0>), time elapsed: 441.78861927986145 seconds\n",
      "Step 4900, loss: tensor(0.0369, grad_fn=<SubBackward0>), time elapsed: 450.88934326171875 seconds\n",
      "Step 5000, loss: tensor(0.0342, grad_fn=<SubBackward0>), time elapsed: 460.1471450328827 seconds\n",
      "Step 5100, loss: tensor(0.0355, grad_fn=<SubBackward0>), time elapsed: 469.27326798439026 seconds\n",
      "Step 5200, loss: tensor(0.0341, grad_fn=<SubBackward0>), time elapsed: 478.55457973480225 seconds\n",
      "Step 5300, loss: tensor(0.0335, grad_fn=<SubBackward0>), time elapsed: 487.6408123970032 seconds\n",
      "Step 5400, loss: tensor(0.0330, grad_fn=<SubBackward0>), time elapsed: 496.74667501449585 seconds\n",
      "Step 5500, loss: tensor(0.0325, grad_fn=<SubBackward0>), time elapsed: 506.05322098731995 seconds\n",
      "Step 5600, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 515.2632184028625 seconds\n",
      "Step 5700, loss: tensor(0.0329, grad_fn=<SubBackward0>), time elapsed: 524.3968694210052 seconds\n",
      "Step 5800, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 533.6757142543793 seconds\n",
      "Step 5900, loss: tensor(0.0323, grad_fn=<SubBackward0>), time elapsed: 542.7595887184143 seconds\n",
      "Step 6000, loss: tensor(0.0323, grad_fn=<SubBackward0>), time elapsed: 551.8642287254333 seconds\n",
      "Step 6100, loss: tensor(0.0318, grad_fn=<SubBackward0>), time elapsed: 561.152263879776 seconds\n",
      "Step 6200, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 570.6501202583313 seconds\n",
      "Step 6300, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 579.7609179019928 seconds\n",
      "Step 6400, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 589.0385890007019 seconds\n",
      "Step 6500, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 598.1623246669769 seconds\n",
      "Step 6600, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 607.2871160507202 seconds\n",
      "Step 6700, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 616.5714409351349 seconds\n",
      "Step 6800, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 625.7497019767761 seconds\n",
      "Step 6900, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 634.8842394351959 seconds\n",
      "Step 7000, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 644.1431634426117 seconds\n",
      "Step 7100, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 653.3030536174774 seconds\n",
      "Step 7200, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 662.7727506160736 seconds\n",
      "Step 7300, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 671.9228734970093 seconds\n",
      "Step 7400, loss: tensor(0.0292, grad_fn=<SubBackward0>), time elapsed: 681.0586965084076 seconds\n",
      "Step 7500, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 690.3992373943329 seconds\n",
      "Step 7600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 700.3668718338013 seconds\n",
      "Step 7700, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 709.5600905418396 seconds\n",
      "Step 7800, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 718.8662526607513 seconds\n",
      "Step 7900, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 727.9801781177521 seconds\n",
      "Step 8000, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 737.089287519455 seconds\n",
      "Step 8100, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 746.3775105476379 seconds\n",
      "Step 8200, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 755.4378936290741 seconds\n",
      "Step 8300, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 764.5201892852783 seconds\n",
      "Step 8400, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 773.7617933750153 seconds\n",
      "Step 8500, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 782.8477115631104 seconds\n",
      "Step 8600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 791.9251139163971 seconds\n",
      "Step 8700, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 801.2271478176117 seconds\n",
      "Step 8800, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 810.3359537124634 seconds\n",
      "Step 8900, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 819.6459369659424 seconds\n",
      "Step 9000, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 829.4172604084015 seconds\n",
      "Step 9100, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 838.5768988132477 seconds\n",
      "Step 9200, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 847.8568229675293 seconds\n",
      "Step 9300, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 856.9648823738098 seconds\n",
      "Step 9400, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 866.0922548770905 seconds\n",
      "Step 9500, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 875.368949174881 seconds\n",
      "Step 9600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 884.4872951507568 seconds\n",
      "Step 9700, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 893.636127948761 seconds\n",
      "Step 9800, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 902.8905742168427 seconds\n",
      "Step 9900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 911.9930186271667 seconds\n",
      "Step 10000, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 921.0696663856506 seconds\n",
      "Step 10100, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 930.3644306659698 seconds\n",
      "Step 10200, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 939.4677500724792 seconds\n",
      "Step 10300, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 948.5820469856262 seconds\n",
      "Step 10400, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 957.8966586589813 seconds\n",
      "Step 10500, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 966.9978682994843 seconds\n",
      "Step 10600, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 976.1158435344696 seconds\n",
      "Step 10700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 985.4126784801483 seconds\n",
      "Step 10800, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 994.5137157440186 seconds\n",
      "Step 10900, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 1003.8063402175903 seconds\n",
      "Step 11000, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 1012.919394493103 seconds\n",
      "Step 11100, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 1022.0534105300903 seconds\n",
      "Step 11200, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 1031.3267798423767 seconds\n",
      "Step 11300, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 1040.4573771953583 seconds\n",
      "Step 11400, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 1049.572904586792 seconds\n",
      "Step 11500, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 1058.8638529777527 seconds\n",
      "Step 11600, loss: tensor(0.0317, grad_fn=<SubBackward0>), time elapsed: 1067.9994399547577 seconds\n",
      "Step 11700, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 1077.1199123859406 seconds\n",
      "Step 11800, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 1087.0168025493622 seconds\n",
      "Step 11900, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 1096.1162967681885 seconds\n",
      "Step 12000, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1105.2359399795532 seconds\n",
      "Step 12100, loss: tensor(0.0288, grad_fn=<SubBackward0>), time elapsed: 1114.5283036231995 seconds\n",
      "Step 12200, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 1123.6274876594543 seconds\n",
      "Step 12300, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 1132.7169125080109 seconds\n",
      "Step 12400, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 1142.1058268547058 seconds\n",
      "Step 12500, loss: tensor(0.0314, grad_fn=<SubBackward0>), time elapsed: 1151.206801891327 seconds\n",
      "Step 12600, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 1160.3239262104034 seconds\n",
      "Step 12700, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 1169.6336514949799 seconds\n",
      "Step 12800, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 1180.3050284385681 seconds\n",
      "Step 12900, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 1189.6295738220215 seconds\n",
      "Step 13000, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 1198.7460732460022 seconds\n",
      "Step 13100, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 1207.859127521515 seconds\n",
      "Step 13200, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 1217.1229786872864 seconds\n",
      "Step 13300, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 1226.2050125598907 seconds\n",
      "Step 13400, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 1235.3463606834412 seconds\n",
      "Step 13500, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 1244.6594557762146 seconds\n",
      "Step 13600, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 1253.7550284862518 seconds\n",
      "Step 13700, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 1262.865434885025 seconds\n",
      "Step 13800, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1272.1402442455292 seconds\n",
      "Step 13900, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 1281.2701108455658 seconds\n",
      "Step 14000, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 1290.4429750442505 seconds\n",
      "Step 14100, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 1299.7850496768951 seconds\n",
      "Step 14200, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 1308.9109325408936 seconds\n",
      "Step 14300, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1318.0335938930511 seconds\n",
      "Step 14400, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 1327.9235544204712 seconds\n",
      "Step 14500, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 1337.0296630859375 seconds\n",
      "Step 14600, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 1346.1514949798584 seconds\n",
      "Step 14700, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 1355.4343347549438 seconds\n",
      "Step 14800, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 1364.5399718284607 seconds\n",
      "Step 14900, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 1373.6454682350159 seconds\n",
      "Step 15000, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 1382.933631181717 seconds\n",
      "Step 15100, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1392.0597169399261 seconds\n",
      "Step 15200, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 1401.1630263328552 seconds\n",
      "Step 15300, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 1410.4714126586914 seconds\n",
      "Step 15400, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1419.6132435798645 seconds\n",
      "Step 15500, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 1428.9287087917328 seconds\n",
      "Step 15600, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1438.0626175403595 seconds\n",
      "Step 15700, loss: tensor(0.0294, grad_fn=<SubBackward0>), time elapsed: 1447.1910610198975 seconds\n",
      "Step 15800, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1457.0154070854187 seconds\n",
      "Step 15900, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 1466.1343235969543 seconds\n",
      "Step 16000, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 1475.2731277942657 seconds\n",
      "Step 16100, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1484.563334941864 seconds\n",
      "Step 16200, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 1493.6755208969116 seconds\n",
      "Step 16300, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 1502.7563862800598 seconds\n",
      "Step 16400, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 1512.7020862102509 seconds\n",
      "Step 16500, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 1521.7713971138 seconds\n",
      "Step 16600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1530.8508713245392 seconds\n",
      "Step 16700, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 1540.2152750492096 seconds\n",
      "Step 16800, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 1549.3384499549866 seconds\n",
      "Step 16900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 1558.437240600586 seconds\n",
      "Step 17000, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 1567.7539870738983 seconds\n",
      "Step 17100, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 1576.9121396541595 seconds\n",
      "Step 17200, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 1586.0558021068573 seconds\n",
      "Step 17300, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1595.3875784873962 seconds\n",
      "Step 17400, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 1604.5135123729706 seconds\n",
      "Step 17500, loss: tensor(0.0294, grad_fn=<SubBackward0>), time elapsed: 1613.6387393474579 seconds\n",
      "Step 17600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1622.9817037582397 seconds\n",
      "Step 17700, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 1632.1188213825226 seconds\n",
      "Step 17800, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 1641.4360966682434 seconds\n",
      "Step 17900, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 1650.5369098186493 seconds\n",
      "Step 18000, loss: tensor(0.0312, grad_fn=<SubBackward0>), time elapsed: 1659.6851575374603 seconds\n",
      "Step 18100, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 1669.0021741390228 seconds\n",
      "Step 18200, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 1678.1249940395355 seconds\n",
      "Step 18300, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 1687.2590703964233 seconds\n",
      "Step 18400, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 1696.5808651447296 seconds\n",
      "Step 18500, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1705.7148978710175 seconds\n",
      "Step 18600, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 1714.8349196910858 seconds\n",
      "Step 18700, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 1724.1865079402924 seconds\n",
      "Step 18800, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 1733.315040588379 seconds\n",
      "Step 18900, loss: tensor(0.0288, grad_fn=<SubBackward0>), time elapsed: 1742.4557764530182 seconds\n",
      "Step 19000, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 1752.327895641327 seconds\n",
      "Step 19100, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 1761.4756882190704 seconds\n",
      "Step 19200, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 1770.582293510437 seconds\n",
      "Step 19300, loss: tensor(0.0294, grad_fn=<SubBackward0>), time elapsed: 1779.9239475727081 seconds\n",
      "Step 19400, loss: tensor(0.0294, grad_fn=<SubBackward0>), time elapsed: 1789.0579402446747 seconds\n",
      "Step 19500, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1798.1476662158966 seconds\n",
      "Step 19600, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 1807.4046096801758 seconds\n",
      "Step 19700, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 1816.5363569259644 seconds\n",
      "Step 19800, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 1826.395895242691 seconds\n",
      "Step 19900, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 1835.9760930538177 seconds\n",
      "Step 20000, loss: tensor(0.0315, grad_fn=<SubBackward0>), time elapsed: 1845.0754396915436 seconds\n",
      "Step 20100, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1854.3727796077728 seconds\n",
      "Step 20200, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 1864.0915479660034 seconds\n",
      "Step 20300, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 1873.2384459972382 seconds\n",
      "Step 20400, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 1882.6028187274933 seconds\n",
      "Step 20500, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 1891.8321425914764 seconds\n",
      "Step 20600, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 1900.9371919631958 seconds\n",
      "Step 20700, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 1910.275966644287 seconds\n",
      "Step 20800, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 1919.4353377819061 seconds\n",
      "Step 20900, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 1928.6396958827972 seconds\n",
      "Step 21000, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 1937.9613635540009 seconds\n",
      "Step 21100, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 1947.0704226493835 seconds\n",
      "Step 21200, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 1957.4630076885223 seconds\n",
      "Step 21300, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 1966.8269798755646 seconds\n",
      "Step 21400, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 1975.9581038951874 seconds\n",
      "Step 21500, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 1985.0806622505188 seconds\n",
      "Step 21600, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 1994.370198726654 seconds\n",
      "Step 21700, loss: tensor(0.0294, grad_fn=<SubBackward0>), time elapsed: 2003.5095038414001 seconds\n",
      "Step 21800, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 2012.6243450641632 seconds\n",
      "Step 21900, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 2021.8995206356049 seconds\n",
      "Step 22000, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 2031.0206415653229 seconds\n",
      "Step 22100, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2040.1365020275116 seconds\n",
      "Step 22200, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2049.4163885116577 seconds\n",
      "Step 22300, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 2058.5448729991913 seconds\n",
      "Step 22400, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 2067.681237220764 seconds\n",
      "Step 22500, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 2076.9964034557343 seconds\n",
      "Step 22600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 2087.7488474845886 seconds\n",
      "Step 22700, loss: tensor(0.0292, grad_fn=<SubBackward0>), time elapsed: 2097.4037177562714 seconds\n",
      "Step 22800, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 2107.0093398094177 seconds\n",
      "Step 22900, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 2116.330038547516 seconds\n",
      "Step 23000, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 2125.8248403072357 seconds\n",
      "Step 23100, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 2135.0387535095215 seconds\n",
      "Step 23200, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 2144.2002811431885 seconds\n",
      "Step 23300, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 2158.0862069129944 seconds\n",
      "Step 23400, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 2167.328528881073 seconds\n",
      "Step 23500, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 2176.49654006958 seconds\n",
      "Step 23600, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 2185.8939793109894 seconds\n",
      "Step 23700, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 2195.08082985878 seconds\n",
      "Step 23800, loss: tensor(0.0308, grad_fn=<SubBackward0>), time elapsed: 2204.782112121582 seconds\n",
      "Step 23900, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 2214.3133883476257 seconds\n",
      "Step 24000, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 2223.5021617412567 seconds\n",
      "Step 24100, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 2232.7367815971375 seconds\n",
      "Step 24200, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2242.1581172943115 seconds\n",
      "Step 24300, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 2251.4752752780914 seconds\n",
      "Step 24400, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 2260.8113503456116 seconds\n",
      "Step 24500, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 2270.854027748108 seconds\n",
      "Step 24600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2280.166233778 seconds\n",
      "Step 24700, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 2289.4621562957764 seconds\n",
      "Step 24800, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 2298.8808917999268 seconds\n",
      "Step 24900, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 2308.19895029068 seconds\n",
      "Step 25000, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 2317.778398036957 seconds\n",
      "Step 25100, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 2327.2177259922028 seconds\n",
      "Step 25200, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 2336.4107620716095 seconds\n",
      "Step 25300, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 2345.5635483264923 seconds\n",
      "Step 25400, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 2354.8992159366608 seconds\n",
      "Step 25500, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 2364.157838821411 seconds\n",
      "Step 25600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 2373.701470851898 seconds\n",
      "Step 25700, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 2383.0745232105255 seconds\n",
      "Step 25800, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 2392.3398022651672 seconds\n",
      "Step 25900, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 2402.5118272304535 seconds\n",
      "Step 26000, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 2411.6389615535736 seconds\n",
      "Step 26100, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 2420.800663948059 seconds\n",
      "Step 26200, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 2430.3427062034607 seconds\n",
      "Step 26300, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 2439.479070663452 seconds\n",
      "Step 26400, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 2448.660450220108 seconds\n",
      "Step 26500, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2458.0505363941193 seconds\n",
      "Step 26600, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 2467.179886817932 seconds\n",
      "Step 26700, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2476.3377645015717 seconds\n",
      "Step 26800, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 2486.0266573429108 seconds\n",
      "Step 26900, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2495.2725868225098 seconds\n",
      "Step 27000, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2504.412771463394 seconds\n",
      "Step 27100, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 2514.5033342838287 seconds\n",
      "Step 27200, loss: tensor(0.0294, grad_fn=<SubBackward0>), time elapsed: 2523.662841320038 seconds\n",
      "Step 27300, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 2532.811361551285 seconds\n",
      "Step 27400, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 2542.263519525528 seconds\n",
      "Step 27500, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 2551.458099126816 seconds\n",
      "Step 27600, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 2562.8592326641083 seconds\n",
      "Step 27700, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 2572.2385427951813 seconds\n",
      "Step 27800, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 2581.448058128357 seconds\n",
      "Step 27900, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 2590.6095979213715 seconds\n",
      "Step 28000, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 2599.944015979767 seconds\n",
      "Step 28100, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 2609.1266481876373 seconds\n",
      "Step 28200, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2618.2876670360565 seconds\n",
      "Step 28300, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 2627.6407754421234 seconds\n",
      "Step 28400, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 2636.8024773597717 seconds\n",
      "Step 28500, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 2645.980472803116 seconds\n",
      "Step 28600, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2656.051365375519 seconds\n",
      "Step 28700, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 2665.2715945243835 seconds\n",
      "Step 28800, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 2674.435483455658 seconds\n",
      "Step 28900, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 2683.7833087444305 seconds\n",
      "Step 29000, loss: tensor(0.0294, grad_fn=<SubBackward0>), time elapsed: 2692.9084470272064 seconds\n",
      "Step 29100, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 2702.105328798294 seconds\n",
      "Step 29200, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 2711.462565422058 seconds\n",
      "Step 29300, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2720.6505103111267 seconds\n",
      "Step 29400, loss: tensor(0.0313, grad_fn=<SubBackward0>), time elapsed: 2729.8396644592285 seconds\n",
      "Step 29500, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 2739.217537879944 seconds\n",
      "Step 29600, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 2748.3878173828125 seconds\n",
      "Step 29700, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 2757.7738995552063 seconds\n",
      "Step 29800, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 2766.9766561985016 seconds\n",
      "Step 29900, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 2776.5427508354187 seconds\n",
      "Step 30000, loss: tensor(0.0297, grad_fn=<SubBackward0>), time elapsed: 2785.963830471039 seconds\n",
      "Step 30100, loss: tensor(0.0316, grad_fn=<SubBackward0>), time elapsed: 2795.112740755081 seconds\n",
      "Step 30200, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 2804.2865660190582 seconds\n",
      "Step 30300, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 2814.12033700943 seconds\n",
      "Step 30400, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 2823.4146077632904 seconds\n",
      "Step 30500, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 2832.5837111473083 seconds\n",
      "Step 30600, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 2841.9098329544067 seconds\n",
      "Step 30700, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 2851.0660457611084 seconds\n",
      "Step 30800, loss: tensor(0.0290, grad_fn=<SubBackward0>), time elapsed: 2860.4936861991882 seconds\n",
      "Step 30900, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 2869.965315103531 seconds\n",
      "Step 31000, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 2879.2148542404175 seconds\n",
      "Step 31100, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 2888.391646385193 seconds\n",
      "Step 31200, loss: tensor(0.0291, grad_fn=<SubBackward0>), time elapsed: 2898.5714712142944 seconds\n",
      "Step 31300, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 2907.692079782486 seconds\n",
      "Step 31400, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 2916.8321990966797 seconds\n",
      "Step 31500, loss: tensor(0.0293, grad_fn=<SubBackward0>), time elapsed: 2926.164320707321 seconds\n",
      "Step 31600, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 2935.3122923374176 seconds\n",
      "Step 31700, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 2944.486797809601 seconds\n",
      "Step 31800, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 2953.8399546146393 seconds\n",
      "Step 31900, loss: tensor(0.0310, grad_fn=<SubBackward0>), time elapsed: 2963.004253387451 seconds\n",
      "Step 32000, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 2972.1866805553436 seconds\n",
      "Step 32100, loss: tensor(0.0295, grad_fn=<SubBackward0>), time elapsed: 2981.5783405303955 seconds\n",
      "Step 32200, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 2990.775633096695 seconds\n",
      "Step 32300, loss: tensor(0.0318, grad_fn=<SubBackward0>), time elapsed: 2999.927277326584 seconds\n",
      "Step 32400, loss: tensor(0.0294, grad_fn=<SubBackward0>), time elapsed: 3009.3336782455444 seconds\n",
      "Step 32500, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 3018.480367898941 seconds\n",
      "Step 32600, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 3027.6778621673584 seconds\n",
      "Step 32700, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 3037.121308326721 seconds\n",
      "Step 32800, loss: tensor(0.0305, grad_fn=<SubBackward0>), time elapsed: 3048.0105080604553 seconds\n",
      "Step 32900, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 3057.216726541519 seconds\n",
      "Step 33000, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 3066.640575170517 seconds\n",
      "Step 33100, loss: tensor(0.0309, grad_fn=<SubBackward0>), time elapsed: 3075.7654881477356 seconds\n",
      "Step 33200, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 3084.8865983486176 seconds\n",
      "Step 33300, loss: tensor(0.0296, grad_fn=<SubBackward0>), time elapsed: 3094.2142190933228 seconds\n",
      "Step 33400, loss: tensor(0.0294, grad_fn=<SubBackward0>), time elapsed: 3103.3553104400635 seconds\n",
      "Step 33500, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 3112.473631620407 seconds\n",
      "Step 33600, loss: tensor(0.0301, grad_fn=<SubBackward0>), time elapsed: 3121.7712709903717 seconds\n",
      "Step 33700, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 3130.9056367874146 seconds\n",
      "Step 33800, loss: tensor(0.0303, grad_fn=<SubBackward0>), time elapsed: 3140.0415437221527 seconds\n",
      "Step 33900, loss: tensor(0.0307, grad_fn=<SubBackward0>), time elapsed: 3149.383899450302 seconds\n",
      "Step 34000, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 3158.5576906204224 seconds\n",
      "Step 34100, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 3167.93089056015 seconds\n",
      "Step 34200, loss: tensor(0.0306, grad_fn=<SubBackward0>), time elapsed: 3177.085958957672 seconds\n",
      "Step 34300, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 3186.4279754161835 seconds\n",
      "Step 34400, loss: tensor(0.0299, grad_fn=<SubBackward0>), time elapsed: 3195.8125541210175 seconds\n",
      "Step 34500, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 3205.434721469879 seconds\n",
      "Step 34600, loss: tensor(0.0304, grad_fn=<SubBackward0>), time elapsed: 3214.5429208278656 seconds\n",
      "Step 34700, loss: tensor(0.0311, grad_fn=<SubBackward0>), time elapsed: 3224.050595521927 seconds\n",
      "Step 34800, loss: tensor(0.0298, grad_fn=<SubBackward0>), time elapsed: 3233.164631843567 seconds\n",
      "Step 34900, loss: tensor(0.0302, grad_fn=<SubBackward0>), time elapsed: 3242.321527481079 seconds\n",
      "Step 35000, loss: tensor(0.0300, grad_fn=<SubBackward0>), time elapsed: 3251.7897896766663 seconds\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.2854, grad_fn=<SubBackward0>), time elapsed: 0.1435563564300537 seconds\n",
      "Step 100, loss: tensor(0.2648, grad_fn=<SubBackward0>), time elapsed: 9.399710178375244 seconds\n",
      "Step 200, loss: tensor(0.2472, grad_fn=<SubBackward0>), time elapsed: 18.58270001411438 seconds\n",
      "Step 300, loss: tensor(0.2245, grad_fn=<SubBackward0>), time elapsed: 28.685434103012085 seconds\n",
      "Step 400, loss: tensor(0.1962, grad_fn=<SubBackward0>), time elapsed: 37.81096625328064 seconds\n",
      "Step 500, loss: tensor(0.1646, grad_fn=<SubBackward0>), time elapsed: 46.926326513290405 seconds\n",
      "Step 600, loss: tensor(0.1363, grad_fn=<SubBackward0>), time elapsed: 56.186614990234375 seconds\n",
      "Step 700, loss: tensor(0.1183, grad_fn=<SubBackward0>), time elapsed: 65.29225635528564 seconds\n",
      "Step 800, loss: tensor(0.1078, grad_fn=<SubBackward0>), time elapsed: 74.40104031562805 seconds\n",
      "Step 900, loss: tensor(0.1002, grad_fn=<SubBackward0>), time elapsed: 83.68490982055664 seconds\n",
      "Step 1000, loss: tensor(0.0940, grad_fn=<SubBackward0>), time elapsed: 92.80559635162354 seconds\n",
      "Step 1100, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 101.90561866760254 seconds\n",
      "Step 1200, loss: tensor(0.0869, grad_fn=<SubBackward0>), time elapsed: 111.15362739562988 seconds\n",
      "Step 1300, loss: tensor(0.0850, grad_fn=<SubBackward0>), time elapsed: 120.26141405105591 seconds\n",
      "Step 1400, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 129.50559377670288 seconds\n",
      "Step 1500, loss: tensor(0.0826, grad_fn=<SubBackward0>), time elapsed: 138.56678676605225 seconds\n",
      "Step 1600, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 147.65965223312378 seconds\n",
      "Step 1700, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 156.93519639968872 seconds\n",
      "Step 1800, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 166.01331186294556 seconds\n",
      "Step 1900, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 175.15954732894897 seconds\n",
      "Step 2000, loss: tensor(0.0787, grad_fn=<SubBackward0>), time elapsed: 184.41684341430664 seconds\n",
      "Step 2100, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 193.5287435054779 seconds\n",
      "Step 2200, loss: tensor(0.0779, grad_fn=<SubBackward0>), time elapsed: 202.65097904205322 seconds\n",
      "Step 2300, loss: tensor(0.0779, grad_fn=<SubBackward0>), time elapsed: 211.92918372154236 seconds\n",
      "Step 2400, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 221.04075860977173 seconds\n",
      "Step 2500, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 230.1291332244873 seconds\n",
      "Step 2600, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 239.39801383018494 seconds\n",
      "Step 2700, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 248.5748372077942 seconds\n",
      "Step 2800, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 257.70333981513977 seconds\n",
      "Step 2900, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 266.9617545604706 seconds\n",
      "Step 3000, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 276.0695011615753 seconds\n",
      "Step 3100, loss: tensor(0.0731, grad_fn=<SubBackward0>), time elapsed: 285.3662576675415 seconds\n",
      "Step 3200, loss: tensor(0.0728, grad_fn=<SubBackward0>), time elapsed: 294.4507439136505 seconds\n",
      "Step 3300, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 303.5745038986206 seconds\n",
      "Step 3400, loss: tensor(0.0714, grad_fn=<SubBackward0>), time elapsed: 312.85883045196533 seconds\n",
      "Step 3500, loss: tensor(0.0725, grad_fn=<SubBackward0>), time elapsed: 322.0005865097046 seconds\n",
      "Step 3600, loss: tensor(0.0703, grad_fn=<SubBackward0>), time elapsed: 332.7586326599121 seconds\n",
      "Step 3700, loss: tensor(0.0692, grad_fn=<SubBackward0>), time elapsed: 342.0402739048004 seconds\n",
      "Step 3800, loss: tensor(0.0691, grad_fn=<SubBackward0>), time elapsed: 351.13690876960754 seconds\n",
      "Step 3900, loss: tensor(0.0675, grad_fn=<SubBackward0>), time elapsed: 360.25953817367554 seconds\n",
      "Step 4000, loss: tensor(0.0649, grad_fn=<SubBackward0>), time elapsed: 369.5939345359802 seconds\n",
      "Step 4100, loss: tensor(0.0635, grad_fn=<SubBackward0>), time elapsed: 378.7052788734436 seconds\n",
      "Step 4200, loss: tensor(0.0628, grad_fn=<SubBackward0>), time elapsed: 387.79485297203064 seconds\n",
      "Step 4300, loss: tensor(0.0612, grad_fn=<SubBackward0>), time elapsed: 397.06519293785095 seconds\n",
      "Step 4400, loss: tensor(0.0607, grad_fn=<SubBackward0>), time elapsed: 406.16371035575867 seconds\n",
      "Step 4500, loss: tensor(0.0599, grad_fn=<SubBackward0>), time elapsed: 415.2439196109772 seconds\n",
      "Step 4600, loss: tensor(0.0609, grad_fn=<SubBackward0>), time elapsed: 424.52738094329834 seconds\n",
      "Step 4700, loss: tensor(0.0598, grad_fn=<SubBackward0>), time elapsed: 433.7045261859894 seconds\n",
      "Step 4800, loss: tensor(0.0595, grad_fn=<SubBackward0>), time elapsed: 442.9920983314514 seconds\n",
      "Step 4900, loss: tensor(0.0591, grad_fn=<SubBackward0>), time elapsed: 452.09340238571167 seconds\n",
      "Step 5000, loss: tensor(0.0586, grad_fn=<SubBackward0>), time elapsed: 461.22949051856995 seconds\n",
      "Step 5100, loss: tensor(0.0576, grad_fn=<SubBackward0>), time elapsed: 470.69762325286865 seconds\n",
      "Step 5200, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 479.82253432273865 seconds\n",
      "Step 5300, loss: tensor(0.0573, grad_fn=<SubBackward0>), time elapsed: 488.9748342037201 seconds\n",
      "Step 5400, loss: tensor(0.0575, grad_fn=<SubBackward0>), time elapsed: 498.2711501121521 seconds\n",
      "Step 5500, loss: tensor(0.0581, grad_fn=<SubBackward0>), time elapsed: 507.94718194007874 seconds\n",
      "Step 5600, loss: tensor(0.0574, grad_fn=<SubBackward0>), time elapsed: 517.0660018920898 seconds\n",
      "Step 5700, loss: tensor(0.0565, grad_fn=<SubBackward0>), time elapsed: 526.3997824192047 seconds\n",
      "Step 5800, loss: tensor(0.0559, grad_fn=<SubBackward0>), time elapsed: 535.8301723003387 seconds\n",
      "Step 5900, loss: tensor(0.0568, grad_fn=<SubBackward0>), time elapsed: 544.9294998645782 seconds\n",
      "Step 6000, loss: tensor(0.0558, grad_fn=<SubBackward0>), time elapsed: 554.2323315143585 seconds\n",
      "Step 6100, loss: tensor(0.0562, grad_fn=<SubBackward0>), time elapsed: 563.3606986999512 seconds\n",
      "Step 6200, loss: tensor(0.0563, grad_fn=<SubBackward0>), time elapsed: 572.4775643348694 seconds\n",
      "Step 6300, loss: tensor(0.0542, grad_fn=<SubBackward0>), time elapsed: 581.7804090976715 seconds\n",
      "Step 6400, loss: tensor(0.0536, grad_fn=<SubBackward0>), time elapsed: 590.8756260871887 seconds\n",
      "Step 6500, loss: tensor(0.0541, grad_fn=<SubBackward0>), time elapsed: 600.1181397438049 seconds\n",
      "Step 6600, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 610.1713855266571 seconds\n",
      "Step 6700, loss: tensor(0.0543, grad_fn=<SubBackward0>), time elapsed: 619.2744190692902 seconds\n",
      "Step 6800, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 628.5392506122589 seconds\n",
      "Step 6900, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 637.6163158416748 seconds\n",
      "Step 7000, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 646.7489633560181 seconds\n",
      "Step 7100, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 656.0747668743134 seconds\n",
      "Step 7200, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 665.194581747055 seconds\n",
      "Step 7300, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 674.3652186393738 seconds\n",
      "Step 7400, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 684.2926728725433 seconds\n",
      "Step 7500, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 693.395247220993 seconds\n",
      "Step 7600, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 702.6533124446869 seconds\n",
      "Step 7700, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 711.8866684436798 seconds\n",
      "Step 7800, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 721.000049829483 seconds\n",
      "Step 7900, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 730.2256858348846 seconds\n",
      "Step 8000, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 740.1180860996246 seconds\n",
      "Step 8100, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 749.1900658607483 seconds\n",
      "Step 8200, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 758.2663128376007 seconds\n",
      "Step 8300, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 767.5445668697357 seconds\n",
      "Step 8400, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 776.6692562103271 seconds\n",
      "Step 8500, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 785.9840791225433 seconds\n",
      "Step 8600, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 795.1352736949921 seconds\n",
      "Step 8700, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 804.2680943012238 seconds\n",
      "Step 8800, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 813.5858170986176 seconds\n",
      "Step 8900, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 822.68226146698 seconds\n",
      "Step 9000, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 831.8011689186096 seconds\n",
      "Step 9100, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 841.0877401828766 seconds\n",
      "Step 9200, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 850.2073073387146 seconds\n",
      "Step 9300, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 859.3242864608765 seconds\n",
      "Step 9400, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 868.6254556179047 seconds\n",
      "Step 9500, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 877.7382433414459 seconds\n",
      "Step 9600, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 886.8634061813354 seconds\n",
      "Step 9700, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 896.1516258716583 seconds\n",
      "Step 9800, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 905.2552211284637 seconds\n",
      "Step 9900, loss: tensor(0.0487, grad_fn=<SubBackward0>), time elapsed: 914.6349074840546 seconds\n",
      "Step 10000, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 924.7300703525543 seconds\n",
      "Step 10100, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 933.8211097717285 seconds\n",
      "Step 10200, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 942.9245421886444 seconds\n",
      "Step 10300, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 952.2319550514221 seconds\n",
      "Step 10400, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 961.341616153717 seconds\n",
      "Step 10500, loss: tensor(0.0496, grad_fn=<SubBackward0>), time elapsed: 970.663106918335 seconds\n",
      "Step 10600, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 979.7872452735901 seconds\n",
      "Step 10700, loss: tensor(0.0497, grad_fn=<SubBackward0>), time elapsed: 988.8971059322357 seconds\n",
      "Step 10800, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 999.2445919513702 seconds\n",
      "Step 10900, loss: tensor(0.0474, grad_fn=<SubBackward0>), time elapsed: 1008.4352784156799 seconds\n",
      "Step 11000, loss: tensor(0.0470, grad_fn=<SubBackward0>), time elapsed: 1017.525306224823 seconds\n",
      "Step 11100, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1026.8006777763367 seconds\n",
      "Step 11200, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 1035.8994500637054 seconds\n",
      "Step 11300, loss: tensor(0.0474, grad_fn=<SubBackward0>), time elapsed: 1045.0225315093994 seconds\n",
      "Step 11400, loss: tensor(0.0474, grad_fn=<SubBackward0>), time elapsed: 1054.30024600029 seconds\n",
      "Step 11500, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1063.425987958908 seconds\n",
      "Step 11600, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 1072.541160106659 seconds\n",
      "Step 11700, loss: tensor(0.0470, grad_fn=<SubBackward0>), time elapsed: 1081.8216006755829 seconds\n",
      "Step 11800, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 1091.2176570892334 seconds\n",
      "Step 11900, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 1100.379700422287 seconds\n",
      "Step 12000, loss: tensor(0.0484, grad_fn=<SubBackward0>), time elapsed: 1109.6467776298523 seconds\n",
      "Step 12100, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 1118.7679510116577 seconds\n",
      "Step 12200, loss: tensor(0.0489, grad_fn=<SubBackward0>), time elapsed: 1128.097814798355 seconds\n",
      "Step 12300, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 1137.3950364589691 seconds\n",
      "Step 12400, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 1146.490777015686 seconds\n",
      "Step 12500, loss: tensor(0.0488, grad_fn=<SubBackward0>), time elapsed: 1156.5445261001587 seconds\n",
      "Step 12600, loss: tensor(0.0490, grad_fn=<SubBackward0>), time elapsed: 1165.6795167922974 seconds\n",
      "Step 12700, loss: tensor(0.0475, grad_fn=<SubBackward0>), time elapsed: 1174.7742748260498 seconds\n",
      "Step 12800, loss: tensor(0.0472, grad_fn=<SubBackward0>), time elapsed: 1184.049211025238 seconds\n",
      "Step 12900, loss: tensor(0.0467, grad_fn=<SubBackward0>), time elapsed: 1193.264680147171 seconds\n",
      "Step 13000, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 1202.365761756897 seconds\n",
      "Step 13100, loss: tensor(0.0472, grad_fn=<SubBackward0>), time elapsed: 1211.671869277954 seconds\n",
      "Step 13200, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 1220.834005355835 seconds\n",
      "Step 13300, loss: tensor(0.0474, grad_fn=<SubBackward0>), time elapsed: 1230.7394399642944 seconds\n",
      "Step 13400, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 1240.0611233711243 seconds\n",
      "Step 13500, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 1249.1829507350922 seconds\n",
      "Step 13600, loss: tensor(0.0475, grad_fn=<SubBackward0>), time elapsed: 1258.3096947669983 seconds\n",
      "Step 13700, loss: tensor(0.0472, grad_fn=<SubBackward0>), time elapsed: 1267.683085680008 seconds\n",
      "Step 13800, loss: tensor(0.0493, grad_fn=<SubBackward0>), time elapsed: 1276.846378326416 seconds\n",
      "Step 13900, loss: tensor(0.0472, grad_fn=<SubBackward0>), time elapsed: 1285.9720203876495 seconds\n",
      "Step 14000, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1295.2491555213928 seconds\n",
      "Step 14100, loss: tensor(0.0475, grad_fn=<SubBackward0>), time elapsed: 1304.7681148052216 seconds\n",
      "Step 14200, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 1313.9363868236542 seconds\n",
      "Step 14300, loss: tensor(0.0490, grad_fn=<SubBackward0>), time elapsed: 1323.2977676391602 seconds\n",
      "Step 14400, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 1332.4586029052734 seconds\n",
      "Step 14500, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 1342.1457266807556 seconds\n",
      "Step 14600, loss: tensor(0.0474, grad_fn=<SubBackward0>), time elapsed: 1351.4519939422607 seconds\n",
      "Step 14700, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 1360.5492160320282 seconds\n",
      "Step 14800, loss: tensor(0.0468, grad_fn=<SubBackward0>), time elapsed: 1369.8616034984589 seconds\n",
      "Step 14900, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 1379.508929491043 seconds\n",
      "Step 15000, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 1388.7353773117065 seconds\n",
      "Step 15100, loss: tensor(0.0458, grad_fn=<SubBackward0>), time elapsed: 1398.086431980133 seconds\n",
      "Step 15200, loss: tensor(0.0475, grad_fn=<SubBackward0>), time elapsed: 1407.216966867447 seconds\n",
      "Step 15300, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 1416.3199954032898 seconds\n",
      "Step 15400, loss: tensor(0.0492, grad_fn=<SubBackward0>), time elapsed: 1425.6079947948456 seconds\n",
      "Step 15500, loss: tensor(0.0471, grad_fn=<SubBackward0>), time elapsed: 1435.5720701217651 seconds\n",
      "Step 15600, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 1444.6678566932678 seconds\n",
      "Step 15700, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 1453.9654483795166 seconds\n",
      "Step 15800, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 1463.3901207447052 seconds\n",
      "Step 15900, loss: tensor(0.0459, grad_fn=<SubBackward0>), time elapsed: 1472.511884689331 seconds\n",
      "Step 16000, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 1481.789273262024 seconds\n",
      "Step 16100, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 1490.8887894153595 seconds\n",
      "Step 16200, loss: tensor(0.0462, grad_fn=<SubBackward0>), time elapsed: 1500.0468561649323 seconds\n",
      "Step 16300, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 1510.9891216754913 seconds\n",
      "Step 16400, loss: tensor(0.0476, grad_fn=<SubBackward0>), time elapsed: 1520.1016631126404 seconds\n",
      "Step 16500, loss: tensor(0.0460, grad_fn=<SubBackward0>), time elapsed: 1529.2186295986176 seconds\n",
      "Step 16600, loss: tensor(0.0480, grad_fn=<SubBackward0>), time elapsed: 1538.4941351413727 seconds\n",
      "Step 16700, loss: tensor(0.0475, grad_fn=<SubBackward0>), time elapsed: 1547.5803391933441 seconds\n",
      "Step 16800, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 1556.6822304725647 seconds\n",
      "Step 16900, loss: tensor(0.0471, grad_fn=<SubBackward0>), time elapsed: 1565.9786472320557 seconds\n",
      "Step 17000, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 1575.1232106685638 seconds\n",
      "Step 17100, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 1584.2813503742218 seconds\n",
      "Step 17200, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 1594.1170995235443 seconds\n",
      "Step 17300, loss: tensor(0.0468, grad_fn=<SubBackward0>), time elapsed: 1603.5422940254211 seconds\n",
      "Step 17400, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 1613.227153301239 seconds\n",
      "Step 17500, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 1622.8260025978088 seconds\n",
      "Step 17600, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 1632.2171733379364 seconds\n",
      "Step 17700, loss: tensor(0.0481, grad_fn=<SubBackward0>), time elapsed: 1641.6892857551575 seconds\n",
      "Step 17800, loss: tensor(0.0486, grad_fn=<SubBackward0>), time elapsed: 1650.998776435852 seconds\n",
      "Step 17900, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 1660.2891733646393 seconds\n",
      "Step 18000, loss: tensor(0.0477, grad_fn=<SubBackward0>), time elapsed: 1669.7979469299316 seconds\n",
      "Step 18100, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 1679.1022777557373 seconds\n",
      "Step 18200, loss: tensor(0.0476, grad_fn=<SubBackward0>), time elapsed: 1688.399121761322 seconds\n",
      "Step 18300, loss: tensor(0.0471, grad_fn=<SubBackward0>), time elapsed: 1697.8712480068207 seconds\n",
      "Step 18400, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 1707.168130159378 seconds\n",
      "Step 18500, loss: tensor(0.0473, grad_fn=<SubBackward0>), time elapsed: 1716.4817185401917 seconds\n",
      "Step 18600, loss: tensor(0.0464, grad_fn=<SubBackward0>), time elapsed: 1726.5648682117462 seconds\n",
      "Step 18700, loss: tensor(0.0483, grad_fn=<SubBackward0>), time elapsed: 1735.9222643375397 seconds\n",
      "Step 18800, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 1745.2776288986206 seconds\n",
      "Step 18900, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1754.7725839614868 seconds\n",
      "Step 19000, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 1764.0527725219727 seconds\n",
      "Step 19100, loss: tensor(0.0474, grad_fn=<SubBackward0>), time elapsed: 1773.322657585144 seconds\n",
      "Step 19200, loss: tensor(0.0462, grad_fn=<SubBackward0>), time elapsed: 1784.1178443431854 seconds\n",
      "Step 19300, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1793.39910364151 seconds\n",
      "Step 19400, loss: tensor(0.0465, grad_fn=<SubBackward0>), time elapsed: 1802.7065980434418 seconds\n",
      "Step 19500, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 1812.1786994934082 seconds\n",
      "Step 19600, loss: tensor(0.0456, grad_fn=<SubBackward0>), time elapsed: 1821.4651954174042 seconds\n",
      "Step 19700, loss: tensor(0.0468, grad_fn=<SubBackward0>), time elapsed: 1830.7767696380615 seconds\n",
      "Step 19800, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 1840.2613201141357 seconds\n",
      "Step 19900, loss: tensor(0.0467, grad_fn=<SubBackward0>), time elapsed: 1849.5601642131805 seconds\n",
      "Step 20000, loss: tensor(0.0453, grad_fn=<SubBackward0>), time elapsed: 1859.0553441047668 seconds\n",
      "Step 20100, loss: tensor(0.0458, grad_fn=<SubBackward0>), time elapsed: 1868.3439757823944 seconds\n",
      "Step 20200, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 1877.7240006923676 seconds\n",
      "Step 20300, loss: tensor(0.0470, grad_fn=<SubBackward0>), time elapsed: 1887.196442604065 seconds\n",
      "Step 20400, loss: tensor(0.0462, grad_fn=<SubBackward0>), time elapsed: 1896.5061519145966 seconds\n",
      "Step 20500, loss: tensor(0.0482, grad_fn=<SubBackward0>), time elapsed: 1906.1778509616852 seconds\n",
      "Step 20600, loss: tensor(0.0475, grad_fn=<SubBackward0>), time elapsed: 1916.5219240188599 seconds\n",
      "Step 20700, loss: tensor(0.0457, grad_fn=<SubBackward0>), time elapsed: 1926.323536157608 seconds\n",
      "Step 20800, loss: tensor(0.0472, grad_fn=<SubBackward0>), time elapsed: 1935.8082983493805 seconds\n",
      "Step 20900, loss: tensor(0.0468, grad_fn=<SubBackward0>), time elapsed: 1945.3970274925232 seconds\n",
      "Step 21000, loss: tensor(0.0460, grad_fn=<SubBackward0>), time elapsed: 1954.7784531116486 seconds\n",
      "Step 21100, loss: tensor(0.0462, grad_fn=<SubBackward0>), time elapsed: 1964.1451852321625 seconds\n",
      "Step 21200, loss: tensor(0.0485, grad_fn=<SubBackward0>), time elapsed: 1973.767624616623 seconds\n",
      "Step 21300, loss: tensor(0.0467, grad_fn=<SubBackward0>), time elapsed: 1983.06871175766 seconds\n",
      "Step 21400, loss: tensor(0.0470, grad_fn=<SubBackward0>), time elapsed: 1992.3747107982635 seconds\n",
      "Step 21500, loss: tensor(0.0470, grad_fn=<SubBackward0>), time elapsed: 2001.8354260921478 seconds\n",
      "Step 21600, loss: tensor(0.0478, grad_fn=<SubBackward0>), time elapsed: 2011.120665550232 seconds\n",
      "Step 21700, loss: tensor(0.0471, grad_fn=<SubBackward0>), time elapsed: 2020.469146490097 seconds\n",
      "Step 21800, loss: tensor(0.0457, grad_fn=<SubBackward0>), time elapsed: 2029.9243819713593 seconds\n",
      "Step 21900, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 2039.2337770462036 seconds\n",
      "Step 22000, loss: tensor(0.0469, grad_fn=<SubBackward0>), time elapsed: 2048.818652868271 seconds\n",
      "Step 22100, loss: tensor(0.0461, grad_fn=<SubBackward0>), time elapsed: 2058.3498322963715 seconds\n",
      "Step 22200, loss: tensor(0.0463, grad_fn=<SubBackward0>), time elapsed: 2067.673978805542 seconds\n",
      "Step 22300, loss: tensor(0.0462, grad_fn=<SubBackward0>), time elapsed: 2077.0306153297424 seconds\n",
      "Step 22400, loss: tensor(0.0452, grad_fn=<SubBackward0>), time elapsed: 2086.6020488739014 seconds\n",
      "Step 22500, loss: tensor(0.0472, grad_fn=<SubBackward0>), time elapsed: 2096.030655860901 seconds\n",
      "Step 22600, loss: tensor(0.0479, grad_fn=<SubBackward0>), time elapsed: 2105.651280641556 seconds\n",
      "Step 22700, loss: tensor(0.0467, grad_fn=<SubBackward0>), time elapsed: 2115.0845108032227 seconds\n",
      "Step 22800, loss: tensor(0.0476, grad_fn=<SubBackward0>), time elapsed: 2124.4828441143036 seconds\n",
      "Step 22900, loss: tensor(0.0470, grad_fn=<SubBackward0>), time elapsed: 2134.1790170669556 seconds\n",
      "Step 23000, loss: tensor(0.0449, grad_fn=<SubBackward0>), time elapsed: 2143.8958806991577 seconds\n",
      "Step 23100, loss: tensor(0.0455, grad_fn=<SubBackward0>), time elapsed: 2155.181833744049 seconds\n",
      "Step 23200, loss: tensor(0.0467, grad_fn=<SubBackward0>), time elapsed: 2165.147838115692 seconds\n",
      "Step 23300, loss: tensor(0.0465, grad_fn=<SubBackward0>), time elapsed: 2175.6202075481415 seconds\n",
      "Step 23400, loss: tensor(0.0454, grad_fn=<SubBackward0>), time elapsed: 2185.393539428711 seconds\n",
      "Step 23500, loss: tensor(0.0459, grad_fn=<SubBackward0>), time elapsed: 2195.2404356002808 seconds\n",
      "Step 23600, loss: tensor(0.0460, grad_fn=<SubBackward0>), time elapsed: 2204.8820309638977 seconds\n",
      "Step 23700, loss: tensor(0.0447, grad_fn=<SubBackward0>), time elapsed: 2214.4909603595734 seconds\n",
      "Step 23800, loss: tensor(0.0440, grad_fn=<SubBackward0>), time elapsed: 2227.1595962047577 seconds\n",
      "Step 23900, loss: tensor(0.0456, grad_fn=<SubBackward0>), time elapsed: 2237.010861158371 seconds\n",
      "Step 24000, loss: tensor(0.0446, grad_fn=<SubBackward0>), time elapsed: 2246.8951444625854 seconds\n",
      "Step 24100, loss: tensor(0.0442, grad_fn=<SubBackward0>), time elapsed: 2257.019761800766 seconds\n",
      "Step 24200, loss: tensor(0.0446, grad_fn=<SubBackward0>), time elapsed: 2266.8199265003204 seconds\n",
      "Step 24300, loss: tensor(0.0426, grad_fn=<SubBackward0>), time elapsed: 2276.6650590896606 seconds\n",
      "Step 24400, loss: tensor(0.0445, grad_fn=<SubBackward0>), time elapsed: 2286.734211921692 seconds\n",
      "Step 24500, loss: tensor(0.0420, grad_fn=<SubBackward0>), time elapsed: 2296.6184153556824 seconds\n",
      "Step 24600, loss: tensor(0.0431, grad_fn=<SubBackward0>), time elapsed: 2306.7585804462433 seconds\n",
      "Step 24700, loss: tensor(0.0439, grad_fn=<SubBackward0>), time elapsed: 2316.530056476593 seconds\n",
      "Step 24800, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 2326.5143110752106 seconds\n",
      "Step 24900, loss: tensor(0.0431, grad_fn=<SubBackward0>), time elapsed: 2336.0685358047485 seconds\n",
      "Step 25000, loss: tensor(0.0433, grad_fn=<SubBackward0>), time elapsed: 2345.8223226070404 seconds\n",
      "Step 25100, loss: tensor(0.0434, grad_fn=<SubBackward0>), time elapsed: 2355.4087941646576 seconds\n",
      "Step 25200, loss: tensor(0.0436, grad_fn=<SubBackward0>), time elapsed: 2364.953293323517 seconds\n",
      "Step 25300, loss: tensor(0.0448, grad_fn=<SubBackward0>), time elapsed: 2374.7096202373505 seconds\n",
      "Step 25400, loss: tensor(0.0419, grad_fn=<SubBackward0>), time elapsed: 2384.2880239486694 seconds\n",
      "Step 25500, loss: tensor(0.0429, grad_fn=<SubBackward0>), time elapsed: 2394.0372371673584 seconds\n",
      "Step 25600, loss: tensor(0.0429, grad_fn=<SubBackward0>), time elapsed: 2403.5969622135162 seconds\n",
      "Step 25700, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 2413.148363351822 seconds\n",
      "Step 25800, loss: tensor(0.0438, grad_fn=<SubBackward0>), time elapsed: 2423.4050538539886 seconds\n",
      "Step 25900, loss: tensor(0.0445, grad_fn=<SubBackward0>), time elapsed: 2432.9756524562836 seconds\n",
      "Step 26000, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 2442.5538544654846 seconds\n",
      "Step 26100, loss: tensor(0.0436, grad_fn=<SubBackward0>), time elapsed: 2452.323525428772 seconds\n",
      "Step 26200, loss: tensor(0.0449, grad_fn=<SubBackward0>), time elapsed: 2461.962938785553 seconds\n",
      "Step 26300, loss: tensor(0.0426, grad_fn=<SubBackward0>), time elapsed: 2471.52597117424 seconds\n",
      "Step 26400, loss: tensor(0.0417, grad_fn=<SubBackward0>), time elapsed: 2481.294132709503 seconds\n",
      "Step 26500, loss: tensor(0.0434, grad_fn=<SubBackward0>), time elapsed: 2490.8399295806885 seconds\n",
      "Step 26600, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 2500.844106912613 seconds\n",
      "Step 26700, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 2511.003301858902 seconds\n",
      "Step 26800, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 2520.5426042079926 seconds\n",
      "Step 26900, loss: tensor(0.0417, grad_fn=<SubBackward0>), time elapsed: 2530.2915930747986 seconds\n",
      "Step 27000, loss: tensor(0.0426, grad_fn=<SubBackward0>), time elapsed: 2540.687876224518 seconds\n",
      "Step 27100, loss: tensor(0.0436, grad_fn=<SubBackward0>), time elapsed: 2550.7803785800934 seconds\n",
      "Step 27200, loss: tensor(0.0435, grad_fn=<SubBackward0>), time elapsed: 2560.4795660972595 seconds\n",
      "Step 27300, loss: tensor(0.0420, grad_fn=<SubBackward0>), time elapsed: 2570.5528798103333 seconds\n",
      "Step 27400, loss: tensor(0.0415, grad_fn=<SubBackward0>), time elapsed: 2580.5310299396515 seconds\n",
      "Step 27500, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 2590.0917410850525 seconds\n",
      "Step 27600, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 2599.855821609497 seconds\n",
      "Step 27700, loss: tensor(0.0429, grad_fn=<SubBackward0>), time elapsed: 2609.4385571479797 seconds\n",
      "Step 27800, loss: tensor(0.0422, grad_fn=<SubBackward0>), time elapsed: 2618.988911151886 seconds\n",
      "Step 27900, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 2628.722331762314 seconds\n",
      "Step 28000, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 2638.2797300815582 seconds\n",
      "Step 28100, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 2647.8557710647583 seconds\n",
      "Step 28200, loss: tensor(0.0432, grad_fn=<SubBackward0>), time elapsed: 2657.5145156383514 seconds\n",
      "Step 28300, loss: tensor(0.0418, grad_fn=<SubBackward0>), time elapsed: 2666.750648021698 seconds\n",
      "Step 28400, loss: tensor(0.0422, grad_fn=<SubBackward0>), time elapsed: 2675.9653244018555 seconds\n",
      "Step 28500, loss: tensor(0.0425, grad_fn=<SubBackward0>), time elapsed: 2685.3690247535706 seconds\n",
      "Step 28600, loss: tensor(0.0418, grad_fn=<SubBackward0>), time elapsed: 2694.595525741577 seconds\n",
      "Step 28700, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 2703.8122382164 seconds\n",
      "Step 28800, loss: tensor(0.0421, grad_fn=<SubBackward0>), time elapsed: 2713.1919469833374 seconds\n",
      "Step 28900, loss: tensor(0.0436, grad_fn=<SubBackward0>), time elapsed: 2722.955621242523 seconds\n",
      "Step 29000, loss: tensor(0.0427, grad_fn=<SubBackward0>), time elapsed: 2732.9937376976013 seconds\n",
      "Step 29100, loss: tensor(0.0432, grad_fn=<SubBackward0>), time elapsed: 2742.648365497589 seconds\n",
      "Step 29200, loss: tensor(0.0409, grad_fn=<SubBackward0>), time elapsed: 2752.177038669586 seconds\n",
      "Step 29300, loss: tensor(0.0439, grad_fn=<SubBackward0>), time elapsed: 2761.9279894828796 seconds\n",
      "Step 29400, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 2771.452224969864 seconds\n",
      "Step 29500, loss: tensor(0.0416, grad_fn=<SubBackward0>), time elapsed: 2781.026559114456 seconds\n",
      "Step 29600, loss: tensor(0.0433, grad_fn=<SubBackward0>), time elapsed: 2790.7886912822723 seconds\n",
      "Step 29700, loss: tensor(0.0438, grad_fn=<SubBackward0>), time elapsed: 2800.3272576332092 seconds\n",
      "Step 29800, loss: tensor(0.0433, grad_fn=<SubBackward0>), time elapsed: 2809.5516436100006 seconds\n",
      "Step 29900, loss: tensor(0.0438, grad_fn=<SubBackward0>), time elapsed: 2819.2113511562347 seconds\n",
      "Step 30000, loss: tensor(0.0417, grad_fn=<SubBackward0>), time elapsed: 2829.238477706909 seconds\n",
      "Step 30100, loss: tensor(0.0425, grad_fn=<SubBackward0>), time elapsed: 2839.0043711662292 seconds\n",
      "Step 30200, loss: tensor(0.0425, grad_fn=<SubBackward0>), time elapsed: 2848.7143049240112 seconds\n",
      "Step 30300, loss: tensor(0.0419, grad_fn=<SubBackward0>), time elapsed: 2858.208466529846 seconds\n",
      "Step 30400, loss: tensor(0.0425, grad_fn=<SubBackward0>), time elapsed: 2867.743729352951 seconds\n",
      "Step 30500, loss: tensor(0.0432, grad_fn=<SubBackward0>), time elapsed: 2877.350859642029 seconds\n",
      "Step 30600, loss: tensor(0.0437, grad_fn=<SubBackward0>), time elapsed: 2886.573742389679 seconds\n",
      "Step 30700, loss: tensor(0.0449, grad_fn=<SubBackward0>), time elapsed: 2895.8033430576324 seconds\n",
      "Step 30800, loss: tensor(0.0419, grad_fn=<SubBackward0>), time elapsed: 2905.7918272018433 seconds\n",
      "Step 30900, loss: tensor(0.0448, grad_fn=<SubBackward0>), time elapsed: 2915.491478204727 seconds\n",
      "Step 31000, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 2925.1975989341736 seconds\n",
      "Step 31100, loss: tensor(0.0416, grad_fn=<SubBackward0>), time elapsed: 2934.919702768326 seconds\n",
      "Step 31200, loss: tensor(0.0427, grad_fn=<SubBackward0>), time elapsed: 2944.434051990509 seconds\n",
      "Step 31300, loss: tensor(0.0434, grad_fn=<SubBackward0>), time elapsed: 2953.957590818405 seconds\n",
      "Step 31400, loss: tensor(0.0428, grad_fn=<SubBackward0>), time elapsed: 2963.669881105423 seconds\n",
      "Step 31500, loss: tensor(0.0419, grad_fn=<SubBackward0>), time elapsed: 2973.275237560272 seconds\n",
      "Step 31600, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 2982.879508972168 seconds\n",
      "Step 31700, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 2992.5199053287506 seconds\n",
      "Step 31800, loss: tensor(0.0413, grad_fn=<SubBackward0>), time elapsed: 3002.002907037735 seconds\n",
      "Step 31900, loss: tensor(0.0444, grad_fn=<SubBackward0>), time elapsed: 3011.284927368164 seconds\n",
      "Step 32000, loss: tensor(0.0425, grad_fn=<SubBackward0>), time elapsed: 3020.6840806007385 seconds\n",
      "Step 32100, loss: tensor(0.0425, grad_fn=<SubBackward0>), time elapsed: 3029.9167964458466 seconds\n",
      "Step 32200, loss: tensor(0.0425, grad_fn=<SubBackward0>), time elapsed: 3039.1443345546722 seconds\n",
      "Step 32300, loss: tensor(0.0429, grad_fn=<SubBackward0>), time elapsed: 3048.4994983673096 seconds\n",
      "Step 32400, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 3057.9712381362915 seconds\n",
      "Step 32500, loss: tensor(0.0433, grad_fn=<SubBackward0>), time elapsed: 3067.7234551906586 seconds\n",
      "Step 32600, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 3077.4570231437683 seconds\n",
      "Step 32700, loss: tensor(0.0420, grad_fn=<SubBackward0>), time elapsed: 3086.9630575180054 seconds\n",
      "Step 32800, loss: tensor(0.0419, grad_fn=<SubBackward0>), time elapsed: 3096.41277551651 seconds\n",
      "Step 32900, loss: tensor(0.0424, grad_fn=<SubBackward0>), time elapsed: 3106.22239112854 seconds\n",
      "Step 33000, loss: tensor(0.0436, grad_fn=<SubBackward0>), time elapsed: 3116.1996920108795 seconds\n",
      "Step 33100, loss: tensor(0.0418, grad_fn=<SubBackward0>), time elapsed: 3125.9374074935913 seconds\n",
      "Step 33200, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 3135.8651914596558 seconds\n",
      "Step 33300, loss: tensor(0.0422, grad_fn=<SubBackward0>), time elapsed: 3145.6005249023438 seconds\n",
      "Step 33400, loss: tensor(0.0429, grad_fn=<SubBackward0>), time elapsed: 3155.2630472183228 seconds\n",
      "Step 33500, loss: tensor(0.0425, grad_fn=<SubBackward0>), time elapsed: 3164.681403160095 seconds\n",
      "Step 33600, loss: tensor(0.0426, grad_fn=<SubBackward0>), time elapsed: 3174.1382451057434 seconds\n",
      "Step 33700, loss: tensor(0.0437, grad_fn=<SubBackward0>), time elapsed: 3183.8182327747345 seconds\n",
      "Step 33800, loss: tensor(0.0443, grad_fn=<SubBackward0>), time elapsed: 3193.1883034706116 seconds\n",
      "Step 33900, loss: tensor(0.0431, grad_fn=<SubBackward0>), time elapsed: 3202.479452610016 seconds\n",
      "Step 34000, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 3212.0806357860565 seconds\n",
      "Step 34100, loss: tensor(0.0426, grad_fn=<SubBackward0>), time elapsed: 3221.4317762851715 seconds\n",
      "Step 34200, loss: tensor(0.0427, grad_fn=<SubBackward0>), time elapsed: 3230.791264295578 seconds\n",
      "Step 34300, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 3240.3987758159637 seconds\n",
      "Step 34400, loss: tensor(0.0438, grad_fn=<SubBackward0>), time elapsed: 3249.777354001999 seconds\n",
      "Step 34500, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 3259.149247646332 seconds\n",
      "Step 34600, loss: tensor(0.0430, grad_fn=<SubBackward0>), time elapsed: 3268.722442626953 seconds\n",
      "Step 34700, loss: tensor(0.0423, grad_fn=<SubBackward0>), time elapsed: 3278.0685386657715 seconds\n",
      "Step 34800, loss: tensor(0.0424, grad_fn=<SubBackward0>), time elapsed: 3287.411150455475 seconds\n",
      "Step 34900, loss: tensor(0.0417, grad_fn=<SubBackward0>), time elapsed: 3296.9784412384033 seconds\n",
      "Step 35000, loss: tensor(0.0426, grad_fn=<SubBackward0>), time elapsed: 3306.351282596588 seconds\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.4145, grad_fn=<SubBackward0>), time elapsed: 0.14363694190979004 seconds\n",
      "Step 100, loss: tensor(0.3925, grad_fn=<SubBackward0>), time elapsed: 9.743325233459473 seconds\n",
      "Step 200, loss: tensor(0.3515, grad_fn=<SubBackward0>), time elapsed: 20.242438554763794 seconds\n",
      "Step 300, loss: tensor(0.3076, grad_fn=<SubBackward0>), time elapsed: 29.64128875732422 seconds\n",
      "Step 400, loss: tensor(0.2702, grad_fn=<SubBackward0>), time elapsed: 38.96560716629028 seconds\n",
      "Step 500, loss: tensor(0.2432, grad_fn=<SubBackward0>), time elapsed: 48.43670201301575 seconds\n",
      "Step 600, loss: tensor(0.2166, grad_fn=<SubBackward0>), time elapsed: 57.74189472198486 seconds\n",
      "Step 700, loss: tensor(0.1933, grad_fn=<SubBackward0>), time elapsed: 67.05150413513184 seconds\n",
      "Step 800, loss: tensor(0.1717, grad_fn=<SubBackward0>), time elapsed: 76.54009485244751 seconds\n",
      "Step 900, loss: tensor(0.1515, grad_fn=<SubBackward0>), time elapsed: 85.89739775657654 seconds\n",
      "Step 1000, loss: tensor(0.1356, grad_fn=<SubBackward0>), time elapsed: 95.40092301368713 seconds\n",
      "Step 1100, loss: tensor(0.1224, grad_fn=<SubBackward0>), time elapsed: 104.6811318397522 seconds\n",
      "Step 1200, loss: tensor(0.1125, grad_fn=<SubBackward0>), time elapsed: 114.06781101226807 seconds\n",
      "Step 1300, loss: tensor(0.1050, grad_fn=<SubBackward0>), time elapsed: 123.52703905105591 seconds\n",
      "Step 1400, loss: tensor(0.1010, grad_fn=<SubBackward0>), time elapsed: 132.80982875823975 seconds\n",
      "Step 1500, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 142.1232442855835 seconds\n",
      "Step 1600, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 151.59842586517334 seconds\n",
      "Step 1700, loss: tensor(0.0856, grad_fn=<SubBackward0>), time elapsed: 160.89335298538208 seconds\n",
      "Step 1800, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 170.22257113456726 seconds\n",
      "Step 1900, loss: tensor(0.0806, grad_fn=<SubBackward0>), time elapsed: 179.76345109939575 seconds\n",
      "Step 2000, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 189.30022883415222 seconds\n",
      "Step 2100, loss: tensor(0.0791, grad_fn=<SubBackward0>), time elapsed: 199.06536507606506 seconds\n",
      "Step 2200, loss: tensor(0.0784, grad_fn=<SubBackward0>), time elapsed: 208.55744338035583 seconds\n",
      "Step 2300, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 217.87919926643372 seconds\n",
      "Step 2400, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 227.2096643447876 seconds\n",
      "Step 2500, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 236.76708126068115 seconds\n",
      "Step 2600, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 246.1411702632904 seconds\n",
      "Step 2700, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 255.62161421775818 seconds\n",
      "Step 2800, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 264.9604444503784 seconds\n",
      "Step 2900, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 274.28109884262085 seconds\n",
      "Step 3000, loss: tensor(0.0724, grad_fn=<SubBackward0>), time elapsed: 283.76698303222656 seconds\n",
      "Step 3100, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 293.10127234458923 seconds\n",
      "Step 3200, loss: tensor(0.0705, grad_fn=<SubBackward0>), time elapsed: 302.4107096195221 seconds\n",
      "Step 3300, loss: tensor(0.0723, grad_fn=<SubBackward0>), time elapsed: 311.8650224208832 seconds\n",
      "Step 3400, loss: tensor(0.0709, grad_fn=<SubBackward0>), time elapsed: 321.2203493118286 seconds\n",
      "Step 3500, loss: tensor(0.0709, grad_fn=<SubBackward0>), time elapsed: 330.70612263679504 seconds\n",
      "Step 3600, loss: tensor(0.0701, grad_fn=<SubBackward0>), time elapsed: 340.24072527885437 seconds\n",
      "Step 3700, loss: tensor(0.0703, grad_fn=<SubBackward0>), time elapsed: 349.6999409198761 seconds\n",
      "Step 3800, loss: tensor(0.0695, grad_fn=<SubBackward0>), time elapsed: 359.1684286594391 seconds\n",
      "Step 3900, loss: tensor(0.0671, grad_fn=<SubBackward0>), time elapsed: 368.65719962120056 seconds\n",
      "Step 4000, loss: tensor(0.0668, grad_fn=<SubBackward0>), time elapsed: 377.9665243625641 seconds\n",
      "Step 4100, loss: tensor(0.0655, grad_fn=<SubBackward0>), time elapsed: 387.3635218143463 seconds\n",
      "Step 4200, loss: tensor(0.0638, grad_fn=<SubBackward0>), time elapsed: 396.90310168266296 seconds\n",
      "Step 4300, loss: tensor(0.0642, grad_fn=<SubBackward0>), time elapsed: 406.0936555862427 seconds\n",
      "Step 4400, loss: tensor(0.0601, grad_fn=<SubBackward0>), time elapsed: 415.474689245224 seconds\n",
      "Step 4500, loss: tensor(0.0596, grad_fn=<SubBackward0>), time elapsed: 424.68437600135803 seconds\n",
      "Step 4600, loss: tensor(0.0610, grad_fn=<SubBackward0>), time elapsed: 434.1497769355774 seconds\n",
      "Step 4700, loss: tensor(0.0587, grad_fn=<SubBackward0>), time elapsed: 443.79784655570984 seconds\n",
      "Step 4800, loss: tensor(0.0571, grad_fn=<SubBackward0>), time elapsed: 453.2392210960388 seconds\n",
      "Step 4900, loss: tensor(0.0585, grad_fn=<SubBackward0>), time elapsed: 462.44481682777405 seconds\n",
      "Step 5000, loss: tensor(0.0573, grad_fn=<SubBackward0>), time elapsed: 471.83561754226685 seconds\n",
      "Step 5100, loss: tensor(0.0597, grad_fn=<SubBackward0>), time elapsed: 481.0410099029541 seconds\n",
      "Step 5200, loss: tensor(0.0565, grad_fn=<SubBackward0>), time elapsed: 490.2457094192505 seconds\n",
      "Step 5300, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 499.6105206012726 seconds\n",
      "Step 5400, loss: tensor(0.0555, grad_fn=<SubBackward0>), time elapsed: 508.85467529296875 seconds\n",
      "Step 5500, loss: tensor(0.0561, grad_fn=<SubBackward0>), time elapsed: 518.067390203476 seconds\n",
      "Step 5600, loss: tensor(0.0569, grad_fn=<SubBackward0>), time elapsed: 527.5921804904938 seconds\n",
      "Step 5700, loss: tensor(0.0559, grad_fn=<SubBackward0>), time elapsed: 536.7898151874542 seconds\n",
      "Step 5800, loss: tensor(0.0553, grad_fn=<SubBackward0>), time elapsed: 546.0168089866638 seconds\n",
      "Step 5900, loss: tensor(0.0545, grad_fn=<SubBackward0>), time elapsed: 555.3740742206573 seconds\n",
      "Step 6000, loss: tensor(0.0542, grad_fn=<SubBackward0>), time elapsed: 564.598769903183 seconds\n",
      "Step 6100, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 573.9794673919678 seconds\n",
      "Step 6200, loss: tensor(0.0540, grad_fn=<SubBackward0>), time elapsed: 583.1760520935059 seconds\n",
      "Step 6300, loss: tensor(0.0536, grad_fn=<SubBackward0>), time elapsed: 592.37824177742 seconds\n",
      "Step 6400, loss: tensor(0.0543, grad_fn=<SubBackward0>), time elapsed: 601.7735805511475 seconds\n",
      "Step 6500, loss: tensor(0.0540, grad_fn=<SubBackward0>), time elapsed: 610.9755945205688 seconds\n",
      "Step 6600, loss: tensor(0.0538, grad_fn=<SubBackward0>), time elapsed: 620.2832274436951 seconds\n",
      "Step 6700, loss: tensor(0.0546, grad_fn=<SubBackward0>), time elapsed: 629.7232711315155 seconds\n",
      "Step 6800, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 639.0878503322601 seconds\n",
      "Step 6900, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 648.2894248962402 seconds\n",
      "Step 7000, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 657.7426550388336 seconds\n",
      "Step 7100, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 667.0086331367493 seconds\n",
      "Step 7200, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 676.2383823394775 seconds\n",
      "Step 7300, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 685.6256940364838 seconds\n",
      "Step 7400, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 694.8363070487976 seconds\n",
      "Step 7500, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 704.0908887386322 seconds\n",
      "Step 7600, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 713.4727683067322 seconds\n",
      "Step 7700, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 722.6466310024261 seconds\n",
      "Step 7800, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 732.0703828334808 seconds\n",
      "Step 7900, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 741.1489775180817 seconds\n",
      "Step 8000, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 750.2676458358765 seconds\n",
      "Step 8100, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 759.7412667274475 seconds\n",
      "Step 8200, loss: tensor(0.0495, grad_fn=<SubBackward0>), time elapsed: 768.9006333351135 seconds\n",
      "Step 8300, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 778.0782718658447 seconds\n",
      "Step 8400, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 787.3971545696259 seconds\n",
      "Step 8500, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 796.6813750267029 seconds\n",
      "Step 8600, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 805.8266017436981 seconds\n",
      "Step 8700, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 815.2078726291656 seconds\n",
      "Step 8800, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 824.4636657238007 seconds\n",
      "Step 8900, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 833.6928794384003 seconds\n",
      "Step 9000, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 843.0072929859161 seconds\n",
      "Step 9100, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 852.2466893196106 seconds\n",
      "Step 9200, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 861.4002361297607 seconds\n",
      "Step 9300, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 870.9042716026306 seconds\n",
      "Step 9400, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 880.2292895317078 seconds\n",
      "Step 9500, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 889.4816808700562 seconds\n",
      "Step 9600, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 898.7563230991364 seconds\n",
      "Step 9700, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 907.9370565414429 seconds\n",
      "Step 9800, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 917.5707750320435 seconds\n",
      "Step 9900, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 927.1898016929626 seconds\n",
      "Step 10000, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 936.8953974246979 seconds\n",
      "Step 10100, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 946.6327519416809 seconds\n",
      "Step 10200, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 956.0231068134308 seconds\n",
      "Step 10300, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 965.4899206161499 seconds\n",
      "Step 10400, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 975.1843574047089 seconds\n",
      "Step 10500, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 984.5939285755157 seconds\n",
      "Step 10600, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 993.8812229633331 seconds\n",
      "Step 10700, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 1003.3377287387848 seconds\n",
      "Step 10800, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 1012.6554989814758 seconds\n",
      "Step 10900, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 1021.9304721355438 seconds\n",
      "Step 11000, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 1031.416167974472 seconds\n",
      "Step 11100, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 1040.704751253128 seconds\n",
      "Step 11200, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 1050.1779201030731 seconds\n",
      "Step 11300, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 1059.7000856399536 seconds\n",
      "Step 11400, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 1069.035564661026 seconds\n",
      "Step 11500, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 1078.3689827919006 seconds\n",
      "Step 11600, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 1087.856704235077 seconds\n",
      "Step 11700, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 1097.2161858081818 seconds\n",
      "Step 11800, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 1106.7071886062622 seconds\n",
      "Step 11900, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 1115.9729025363922 seconds\n",
      "Step 12000, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 1125.2571120262146 seconds\n",
      "Step 12100, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 1134.689567565918 seconds\n",
      "Step 12200, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 1143.9844255447388 seconds\n",
      "Step 12300, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 1153.3147161006927 seconds\n",
      "Step 12400, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 1162.7619452476501 seconds\n",
      "Step 12500, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 1172.0524518489838 seconds\n",
      "Step 12600, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 1181.6197936534882 seconds\n",
      "Step 12700, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 1191.2901403903961 seconds\n",
      "Step 12800, loss: tensor(0.0535, grad_fn=<SubBackward0>), time elapsed: 1200.7574450969696 seconds\n",
      "Step 12900, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 1210.0610687732697 seconds\n",
      "Step 13000, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 1219.5838203430176 seconds\n",
      "Step 13100, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 1228.9976153373718 seconds\n",
      "Step 13200, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 1238.662471294403 seconds\n",
      "Step 13300, loss: tensor(0.0538, grad_fn=<SubBackward0>), time elapsed: 1248.2464365959167 seconds\n",
      "Step 13400, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 1257.5456564426422 seconds\n",
      "Step 13500, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 1267.0262401103973 seconds\n",
      "Step 13600, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 1276.6448965072632 seconds\n",
      "Step 13700, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 1286.004911661148 seconds\n",
      "Step 13800, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 1295.4397132396698 seconds\n",
      "Step 13900, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 1305.0248847007751 seconds\n",
      "Step 14000, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 1314.3362483978271 seconds\n",
      "Step 14100, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 1323.8168716430664 seconds\n",
      "Step 14200, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 1333.0997664928436 seconds\n",
      "Step 14300, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 1342.4469513893127 seconds\n",
      "Step 14400, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 1352.0369727611542 seconds\n",
      "Step 14500, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 1361.369214773178 seconds\n",
      "Step 14600, loss: tensor(0.0533, grad_fn=<SubBackward0>), time elapsed: 1370.7405557632446 seconds\n",
      "Step 14700, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 1380.3938765525818 seconds\n",
      "Step 14800, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 1389.832444190979 seconds\n",
      "Step 14900, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 1399.2269940376282 seconds\n",
      "Step 15000, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 1408.7478318214417 seconds\n",
      "Step 15100, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 1418.0706150531769 seconds\n",
      "Step 15200, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 1427.4251103401184 seconds\n",
      "Step 15300, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 1437.010586977005 seconds\n",
      "Step 15400, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 1446.3712542057037 seconds\n",
      "Step 15500, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 1455.8297851085663 seconds\n",
      "Step 15600, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 1465.4476940631866 seconds\n",
      "Step 15700, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 1474.9106285572052 seconds\n",
      "Step 15800, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 1484.3627440929413 seconds\n",
      "Step 15900, loss: tensor(0.0500, grad_fn=<SubBackward0>), time elapsed: 1494.0103645324707 seconds\n",
      "Step 16000, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 1503.376436471939 seconds\n",
      "Step 16100, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 1512.7019348144531 seconds\n",
      "Step 16200, loss: tensor(0.0533, grad_fn=<SubBackward0>), time elapsed: 1522.2177670001984 seconds\n",
      "Step 16300, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 1531.5741481781006 seconds\n",
      "Step 16400, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 1541.3465406894684 seconds\n",
      "Step 16500, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 1550.7708368301392 seconds\n",
      "Step 16600, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 1560.0144543647766 seconds\n",
      "Step 16700, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 1569.4072873592377 seconds\n",
      "Step 16800, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 1578.6821866035461 seconds\n",
      "Step 16900, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 1588.0853271484375 seconds\n",
      "Step 17000, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 1597.8674867153168 seconds\n",
      "Step 17100, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 1607.2600378990173 seconds\n",
      "Step 17200, loss: tensor(0.0536, grad_fn=<SubBackward0>), time elapsed: 1616.6055421829224 seconds\n",
      "Step 17300, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 1626.2610006332397 seconds\n",
      "Step 17400, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 1635.7494885921478 seconds\n",
      "Step 17500, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 1645.1329417228699 seconds\n",
      "Step 17600, loss: tensor(0.0531, grad_fn=<SubBackward0>), time elapsed: 1654.5778195858002 seconds\n",
      "Step 17700, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 1663.9321024417877 seconds\n",
      "Step 17800, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 1673.327378988266 seconds\n",
      "Step 17900, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 1682.8450479507446 seconds\n",
      "Step 18000, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 1692.1600377559662 seconds\n",
      "Step 18100, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 1701.5298833847046 seconds\n",
      "Step 18200, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 1711.223027229309 seconds\n",
      "Step 18300, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 1720.6865787506104 seconds\n",
      "Step 18400, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 1730.0575306415558 seconds\n",
      "Step 18500, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 1739.72158908844 seconds\n",
      "Step 18600, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 1749.1800858974457 seconds\n",
      "Step 18700, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 1758.5438742637634 seconds\n",
      "Step 18800, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 1768.1725749969482 seconds\n",
      "Step 18900, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 1777.6830251216888 seconds\n",
      "Step 19000, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 1787.3333685398102 seconds\n",
      "Step 19100, loss: tensor(0.0501, grad_fn=<SubBackward0>), time elapsed: 1796.7598943710327 seconds\n",
      "Step 19200, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 1806.1065385341644 seconds\n",
      "Step 19300, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 1815.6359388828278 seconds\n",
      "Step 19400, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 1825.0172777175903 seconds\n",
      "Step 19500, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 1834.538423538208 seconds\n",
      "Step 19600, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 1844.3054614067078 seconds\n",
      "Step 19700, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 1853.7818341255188 seconds\n",
      "Step 19800, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 1863.2455294132233 seconds\n",
      "Step 19900, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 1872.812439441681 seconds\n",
      "Step 20000, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 1882.2851326465607 seconds\n",
      "Step 20100, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 1891.6556508541107 seconds\n",
      "Step 20200, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 1901.304033756256 seconds\n",
      "Step 20300, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 1910.6634240150452 seconds\n",
      "Step 20400, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 1920.0761425495148 seconds\n",
      "Step 20500, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 1929.9246017932892 seconds\n",
      "Step 20600, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 1939.4151554107666 seconds\n",
      "Step 20700, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 1948.9051592350006 seconds\n",
      "Step 20800, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 1958.5464384555817 seconds\n",
      "Step 20900, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 1968.0909323692322 seconds\n",
      "Step 21000, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 1977.7097251415253 seconds\n",
      "Step 21100, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 1987.3668875694275 seconds\n",
      "Step 21200, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 1996.807090997696 seconds\n",
      "Step 21300, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 2006.2534582614899 seconds\n",
      "Step 21400, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 2015.8732368946075 seconds\n",
      "Step 21500, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 2025.262862920761 seconds\n",
      "Step 21600, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 2034.9029953479767 seconds\n",
      "Step 21700, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 2044.401195049286 seconds\n",
      "Step 21800, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 2054.0358641147614 seconds\n",
      "Step 21900, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 2063.7757635116577 seconds\n",
      "Step 22000, loss: tensor(0.0530, grad_fn=<SubBackward0>), time elapsed: 2073.259489774704 seconds\n",
      "Step 22100, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 2082.725448846817 seconds\n",
      "Step 22200, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 2092.3474361896515 seconds\n",
      "Step 22300, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 2101.7835857868195 seconds\n",
      "Step 22400, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 2111.218546628952 seconds\n",
      "Step 22500, loss: tensor(0.0533, grad_fn=<SubBackward0>), time elapsed: 2120.839419603348 seconds\n",
      "Step 22600, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 2130.2964792251587 seconds\n",
      "Step 22700, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 2140.0319595336914 seconds\n",
      "Step 22800, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 2149.8135957717896 seconds\n",
      "Step 22900, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 2159.248877763748 seconds\n",
      "Step 23000, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 2170.6679363250732 seconds\n",
      "Step 23100, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 2180.7965195178986 seconds\n",
      "Step 23200, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 2190.444808244705 seconds\n",
      "Step 23300, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 2200.009923219681 seconds\n",
      "Step 23400, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 2209.6936135292053 seconds\n",
      "Step 23500, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 2219.1866025924683 seconds\n",
      "Step 23600, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 2228.795365333557 seconds\n",
      "Step 23700, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 2238.4983310699463 seconds\n",
      "Step 23800, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 2250.4397060871124 seconds\n",
      "Step 23900, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 2260.2450561523438 seconds\n",
      "Step 24000, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 2269.9408025741577 seconds\n",
      "Step 24100, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 2279.5886471271515 seconds\n",
      "Step 24200, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 2289.2002408504486 seconds\n",
      "Step 24300, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 2298.9871442317963 seconds\n",
      "Step 24400, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 2308.6584408283234 seconds\n",
      "Step 24500, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 2318.558161020279 seconds\n",
      "Step 24600, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 2328.094430208206 seconds\n",
      "Step 24700, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 2337.623136281967 seconds\n",
      "Step 24800, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 2347.3081815242767 seconds\n",
      "Step 24900, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 2356.806096315384 seconds\n",
      "Step 25000, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 2366.4077236652374 seconds\n",
      "Step 25100, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 2376.208217382431 seconds\n",
      "Step 25200, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 2385.9076795578003 seconds\n",
      "Step 25300, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 2395.4726645946503 seconds\n",
      "Step 25400, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 2405.240991830826 seconds\n",
      "Step 25500, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 2414.826595067978 seconds\n",
      "Step 25600, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 2424.3194386959076 seconds\n",
      "Step 25700, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 2433.9741911888123 seconds\n",
      "Step 25800, loss: tensor(0.0498, grad_fn=<SubBackward0>), time elapsed: 2443.725690603256 seconds\n",
      "Step 25900, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 2453.207045316696 seconds\n",
      "Step 26000, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 2462.841540813446 seconds\n",
      "Step 26100, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 2472.275887489319 seconds\n",
      "Step 26200, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 2481.672700881958 seconds\n",
      "Step 26300, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 2491.2581131458282 seconds\n",
      "Step 26400, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 2500.7342233657837 seconds\n",
      "Step 26500, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 2510.227745294571 seconds\n",
      "Step 26600, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 2519.9163846969604 seconds\n",
      "Step 26700, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 2529.3887197971344 seconds\n",
      "Step 26800, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 2538.9564673900604 seconds\n",
      "Step 26900, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 2548.680282354355 seconds\n",
      "Step 27000, loss: tensor(0.0504, grad_fn=<SubBackward0>), time elapsed: 2558.109184741974 seconds\n",
      "Step 27100, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 2567.5434925556183 seconds\n",
      "Step 27200, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 2577.1526448726654 seconds\n",
      "Step 27300, loss: tensor(0.0524, grad_fn=<SubBackward0>), time elapsed: 2586.5662598609924 seconds\n",
      "Step 27400, loss: tensor(0.0529, grad_fn=<SubBackward0>), time elapsed: 2595.954959154129 seconds\n",
      "Step 27500, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 2605.490622997284 seconds\n",
      "Step 27600, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 2614.9599916934967 seconds\n",
      "Step 27700, loss: tensor(0.0502, grad_fn=<SubBackward0>), time elapsed: 2624.6696932315826 seconds\n",
      "Step 27800, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 2634.2085814476013 seconds\n",
      "Step 27900, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 2643.6864907741547 seconds\n",
      "Step 28000, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 2653.373200893402 seconds\n",
      "Step 28100, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 2662.902934074402 seconds\n",
      "Step 28200, loss: tensor(0.0508, grad_fn=<SubBackward0>), time elapsed: 2672.72486948967 seconds\n",
      "Step 28300, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 2682.560377597809 seconds\n",
      "Step 28400, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 2692.154995203018 seconds\n",
      "Step 28500, loss: tensor(0.0509, grad_fn=<SubBackward0>), time elapsed: 2701.9151725769043 seconds\n",
      "Step 28600, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 2711.6516139507294 seconds\n",
      "Step 28700, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 2721.126699924469 seconds\n",
      "Step 28800, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 2730.7696454524994 seconds\n",
      "Step 28900, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 2740.650720834732 seconds\n",
      "Step 29000, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 2750.3088912963867 seconds\n",
      "Step 29100, loss: tensor(0.0523, grad_fn=<SubBackward0>), time elapsed: 2759.7825531959534 seconds\n",
      "Step 29200, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 2769.4682037830353 seconds\n",
      "Step 29300, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 2778.955197572708 seconds\n",
      "Step 29400, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 2788.496000766754 seconds\n",
      "Step 29500, loss: tensor(0.0519, grad_fn=<SubBackward0>), time elapsed: 2798.3282952308655 seconds\n",
      "Step 29600, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 2808.2860827445984 seconds\n",
      "Step 29700, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 2817.960170507431 seconds\n",
      "Step 29800, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 2827.5898702144623 seconds\n",
      "Step 29900, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 2837.1388313770294 seconds\n",
      "Step 30000, loss: tensor(0.0507, grad_fn=<SubBackward0>), time elapsed: 2846.590593099594 seconds\n",
      "Step 30100, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 2856.213721036911 seconds\n",
      "Step 30200, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 2865.712924003601 seconds\n",
      "Step 30300, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 2875.2071380615234 seconds\n",
      "Step 30400, loss: tensor(0.0506, grad_fn=<SubBackward0>), time elapsed: 2884.8138217926025 seconds\n",
      "Step 30500, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 2894.3366315364838 seconds\n",
      "Step 30600, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 2904.058134317398 seconds\n",
      "Step 30700, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 2913.6856055259705 seconds\n",
      "Step 30800, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 2923.3510415554047 seconds\n",
      "Step 30900, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 2933.218822956085 seconds\n",
      "Step 31000, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 2942.919328212738 seconds\n",
      "Step 31100, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 2952.371385574341 seconds\n",
      "Step 31200, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 2961.9752464294434 seconds\n",
      "Step 31300, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 2971.7995784282684 seconds\n",
      "Step 31400, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 2981.4286274909973 seconds\n",
      "Step 31500, loss: tensor(0.0520, grad_fn=<SubBackward0>), time elapsed: 2990.9256117343903 seconds\n",
      "Step 31600, loss: tensor(0.0528, grad_fn=<SubBackward0>), time elapsed: 3000.5872917175293 seconds\n",
      "Step 31700, loss: tensor(0.0526, grad_fn=<SubBackward0>), time elapsed: 3010.1153404712677 seconds\n",
      "Step 31800, loss: tensor(0.0499, grad_fn=<SubBackward0>), time elapsed: 3019.896381855011 seconds\n",
      "Step 31900, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 3029.7436757087708 seconds\n",
      "Step 32000, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 3039.9576597213745 seconds\n",
      "Step 32100, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 3049.8813421726227 seconds\n",
      "Step 32200, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 3059.4095346927643 seconds\n",
      "Step 32300, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 3068.961359977722 seconds\n",
      "Step 32400, loss: tensor(0.0532, grad_fn=<SubBackward0>), time elapsed: 3078.6454944610596 seconds\n",
      "Step 32500, loss: tensor(0.0513, grad_fn=<SubBackward0>), time elapsed: 3088.2208609580994 seconds\n",
      "Step 32600, loss: tensor(0.0503, grad_fn=<SubBackward0>), time elapsed: 3097.9438984394073 seconds\n",
      "Step 32700, loss: tensor(0.0537, grad_fn=<SubBackward0>), time elapsed: 3107.643021583557 seconds\n",
      "Step 32800, loss: tensor(0.0539, grad_fn=<SubBackward0>), time elapsed: 3117.0854184627533 seconds\n",
      "Step 32900, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 3126.422800064087 seconds\n",
      "Step 33000, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 3136.023949623108 seconds\n",
      "Step 33100, loss: tensor(0.0525, grad_fn=<SubBackward0>), time elapsed: 3145.8546299934387 seconds\n",
      "Step 33200, loss: tensor(0.0521, grad_fn=<SubBackward0>), time elapsed: 3155.637835741043 seconds\n",
      "Step 33300, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 3165.453502178192 seconds\n",
      "Step 33400, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 3175.0955469608307 seconds\n",
      "Step 33500, loss: tensor(0.0534, grad_fn=<SubBackward0>), time elapsed: 3184.5999524593353 seconds\n",
      "Step 33600, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 3194.3456525802612 seconds\n",
      "Step 33700, loss: tensor(0.0510, grad_fn=<SubBackward0>), time elapsed: 3204.0594465732574 seconds\n",
      "Step 33800, loss: tensor(0.0518, grad_fn=<SubBackward0>), time elapsed: 3213.7705137729645 seconds\n",
      "Step 33900, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 3223.6983551979065 seconds\n",
      "Step 34000, loss: tensor(0.0516, grad_fn=<SubBackward0>), time elapsed: 3233.344200849533 seconds\n",
      "Step 34100, loss: tensor(0.0522, grad_fn=<SubBackward0>), time elapsed: 3243.039874315262 seconds\n",
      "Step 34200, loss: tensor(0.0538, grad_fn=<SubBackward0>), time elapsed: 3252.800797224045 seconds\n",
      "Step 34300, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 3262.244947910309 seconds\n",
      "Step 34400, loss: tensor(0.0514, grad_fn=<SubBackward0>), time elapsed: 3271.8242785930634 seconds\n",
      "Step 34500, loss: tensor(0.0505, grad_fn=<SubBackward0>), time elapsed: 3281.7409257888794 seconds\n",
      "Step 34600, loss: tensor(0.0515, grad_fn=<SubBackward0>), time elapsed: 3291.565682411194 seconds\n",
      "Step 34700, loss: tensor(0.0512, grad_fn=<SubBackward0>), time elapsed: 3301.4354383945465 seconds\n",
      "Step 34800, loss: tensor(0.0527, grad_fn=<SubBackward0>), time elapsed: 3311.3433282375336 seconds\n",
      "Step 34900, loss: tensor(0.0517, grad_fn=<SubBackward0>), time elapsed: 3320.8855612277985 seconds\n",
      "Step 35000, loss: tensor(0.0511, grad_fn=<SubBackward0>), time elapsed: 3331.135697364807 seconds\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.5906, grad_fn=<SubBackward0>), time elapsed: 0.14978432655334473 seconds\n",
      "Step 100, loss: tensor(0.5358, grad_fn=<SubBackward0>), time elapsed: 10.365855932235718 seconds\n",
      "Step 200, loss: tensor(0.4898, grad_fn=<SubBackward0>), time elapsed: 19.968090057373047 seconds\n",
      "Step 300, loss: tensor(0.4428, grad_fn=<SubBackward0>), time elapsed: 29.590835332870483 seconds\n",
      "Step 400, loss: tensor(0.3849, grad_fn=<SubBackward0>), time elapsed: 39.4950225353241 seconds\n",
      "Step 500, loss: tensor(0.3268, grad_fn=<SubBackward0>), time elapsed: 49.44017958641052 seconds\n",
      "Step 600, loss: tensor(0.2748, grad_fn=<SubBackward0>), time elapsed: 58.98328256607056 seconds\n",
      "Step 700, loss: tensor(0.2352, grad_fn=<SubBackward0>), time elapsed: 68.67762613296509 seconds\n",
      "Step 800, loss: tensor(0.2167, grad_fn=<SubBackward0>), time elapsed: 78.55041337013245 seconds\n",
      "Step 900, loss: tensor(0.1979, grad_fn=<SubBackward0>), time elapsed: 88.24009656906128 seconds\n",
      "Step 1000, loss: tensor(0.1850, grad_fn=<SubBackward0>), time elapsed: 97.75766444206238 seconds\n",
      "Step 1100, loss: tensor(0.1778, grad_fn=<SubBackward0>), time elapsed: 107.49159097671509 seconds\n",
      "Step 1200, loss: tensor(0.1653, grad_fn=<SubBackward0>), time elapsed: 117.23229503631592 seconds\n",
      "Step 1300, loss: tensor(0.1562, grad_fn=<SubBackward0>), time elapsed: 126.83465480804443 seconds\n",
      "Step 1400, loss: tensor(0.1464, grad_fn=<SubBackward0>), time elapsed: 136.80180263519287 seconds\n",
      "Step 1500, loss: tensor(0.1417, grad_fn=<SubBackward0>), time elapsed: 146.68945288658142 seconds\n",
      "Step 1600, loss: tensor(0.1366, grad_fn=<SubBackward0>), time elapsed: 156.12504768371582 seconds\n",
      "Step 1700, loss: tensor(0.1303, grad_fn=<SubBackward0>), time elapsed: 166.05783557891846 seconds\n",
      "Step 1800, loss: tensor(0.1215, grad_fn=<SubBackward0>), time elapsed: 175.83239459991455 seconds\n",
      "Step 1900, loss: tensor(0.1218, grad_fn=<SubBackward0>), time elapsed: 185.3145215511322 seconds\n",
      "Step 2000, loss: tensor(0.1183, grad_fn=<SubBackward0>), time elapsed: 195.08667039871216 seconds\n",
      "Step 2100, loss: tensor(0.1130, grad_fn=<SubBackward0>), time elapsed: 204.81099677085876 seconds\n",
      "Step 2200, loss: tensor(0.1124, grad_fn=<SubBackward0>), time elapsed: 214.44094705581665 seconds\n",
      "Step 2300, loss: tensor(0.1086, grad_fn=<SubBackward0>), time elapsed: 224.32557487487793 seconds\n",
      "Step 2400, loss: tensor(0.1060, grad_fn=<SubBackward0>), time elapsed: 234.186101436615 seconds\n",
      "Step 2500, loss: tensor(0.1032, grad_fn=<SubBackward0>), time elapsed: 243.80113410949707 seconds\n",
      "Step 2600, loss: tensor(0.1012, grad_fn=<SubBackward0>), time elapsed: 253.52576875686646 seconds\n",
      "Step 2700, loss: tensor(0.0992, grad_fn=<SubBackward0>), time elapsed: 263.04348516464233 seconds\n",
      "Step 2800, loss: tensor(0.0974, grad_fn=<SubBackward0>), time elapsed: 272.49279618263245 seconds\n",
      "Step 2900, loss: tensor(0.0933, grad_fn=<SubBackward0>), time elapsed: 282.1178777217865 seconds\n",
      "Step 3000, loss: tensor(0.0925, grad_fn=<SubBackward0>), time elapsed: 291.78978180885315 seconds\n",
      "Step 3100, loss: tensor(0.0907, grad_fn=<SubBackward0>), time elapsed: 301.48368525505066 seconds\n",
      "Step 3200, loss: tensor(0.0886, grad_fn=<SubBackward0>), time elapsed: 311.2285258769989 seconds\n",
      "Step 3300, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 321.1315875053406 seconds\n",
      "Step 3400, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 331.2765564918518 seconds\n",
      "Step 3500, loss: tensor(0.0855, grad_fn=<SubBackward0>), time elapsed: 340.78544998168945 seconds\n",
      "Step 3600, loss: tensor(0.0843, grad_fn=<SubBackward0>), time elapsed: 350.3324010372162 seconds\n",
      "Step 3700, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 360.06042981147766 seconds\n",
      "Step 3800, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 369.72996044158936 seconds\n",
      "Step 3900, loss: tensor(0.0828, grad_fn=<SubBackward0>), time elapsed: 379.28631353378296 seconds\n",
      "Step 4000, loss: tensor(0.0815, grad_fn=<SubBackward0>), time elapsed: 388.89989280700684 seconds\n",
      "Step 4100, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 398.3259708881378 seconds\n",
      "Step 4200, loss: tensor(0.0791, grad_fn=<SubBackward0>), time elapsed: 407.8373064994812 seconds\n",
      "Step 4300, loss: tensor(0.0810, grad_fn=<SubBackward0>), time elapsed: 417.449401140213 seconds\n",
      "Step 4400, loss: tensor(0.0805, grad_fn=<SubBackward0>), time elapsed: 426.8348650932312 seconds\n",
      "Step 4500, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 436.3224892616272 seconds\n",
      "Step 4600, loss: tensor(0.0785, grad_fn=<SubBackward0>), time elapsed: 445.97502183914185 seconds\n",
      "Step 4700, loss: tensor(0.0807, grad_fn=<SubBackward0>), time elapsed: 455.41381645202637 seconds\n",
      "Step 4800, loss: tensor(0.0822, grad_fn=<SubBackward0>), time elapsed: 465.0903356075287 seconds\n",
      "Step 4900, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 474.62101316452026 seconds\n",
      "Step 5000, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 483.9173059463501 seconds\n",
      "Step 5100, loss: tensor(0.0809, grad_fn=<SubBackward0>), time elapsed: 493.4482765197754 seconds\n",
      "Step 5200, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 502.81517720222473 seconds\n",
      "Step 5300, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 512.1964094638824 seconds\n",
      "Step 5400, loss: tensor(0.0796, grad_fn=<SubBackward0>), time elapsed: 521.7978279590607 seconds\n",
      "Step 5500, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 531.3804612159729 seconds\n",
      "Step 5600, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 540.8629314899445 seconds\n",
      "Step 5700, loss: tensor(0.0807, grad_fn=<SubBackward0>), time elapsed: 550.4352645874023 seconds\n",
      "Step 5800, loss: tensor(0.0791, grad_fn=<SubBackward0>), time elapsed: 559.8488008975983 seconds\n",
      "Step 5900, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 569.234546661377 seconds\n",
      "Step 6000, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 578.9154803752899 seconds\n",
      "Step 6100, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 588.4069173336029 seconds\n",
      "Step 6200, loss: tensor(0.0795, grad_fn=<SubBackward0>), time elapsed: 597.8672745227814 seconds\n",
      "Step 6300, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 607.5358066558838 seconds\n",
      "Step 6400, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 617.0642924308777 seconds\n",
      "Step 6500, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 626.520457983017 seconds\n",
      "Step 6600, loss: tensor(0.0777, grad_fn=<SubBackward0>), time elapsed: 636.0511519908905 seconds\n",
      "Step 6700, loss: tensor(0.0788, grad_fn=<SubBackward0>), time elapsed: 645.4522161483765 seconds\n",
      "Step 6800, loss: tensor(0.0785, grad_fn=<SubBackward0>), time elapsed: 655.0551843643188 seconds\n",
      "Step 6900, loss: tensor(0.0805, grad_fn=<SubBackward0>), time elapsed: 664.478232383728 seconds\n",
      "Step 7000, loss: tensor(0.0784, grad_fn=<SubBackward0>), time elapsed: 673.8644709587097 seconds\n",
      "Step 7100, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 683.5156362056732 seconds\n",
      "Step 7200, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 692.9426538944244 seconds\n",
      "Step 7300, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 702.3689861297607 seconds\n",
      "Step 7400, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 712.0251162052155 seconds\n",
      "Step 7500, loss: tensor(0.0786, grad_fn=<SubBackward0>), time elapsed: 721.4662992954254 seconds\n",
      "Step 7600, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 730.8950486183167 seconds\n",
      "Step 7700, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 740.4985930919647 seconds\n",
      "Step 7800, loss: tensor(0.0781, grad_fn=<SubBackward0>), time elapsed: 749.8663699626923 seconds\n",
      "Step 7900, loss: tensor(0.0806, grad_fn=<SubBackward0>), time elapsed: 759.3141703605652 seconds\n",
      "Step 8000, loss: tensor(0.0784, grad_fn=<SubBackward0>), time elapsed: 768.8494508266449 seconds\n",
      "Step 8100, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 778.3077170848846 seconds\n",
      "Step 8200, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 787.763112783432 seconds\n",
      "Step 8300, loss: tensor(0.0777, grad_fn=<SubBackward0>), time elapsed: 797.3147943019867 seconds\n",
      "Step 8400, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 806.7240989208221 seconds\n",
      "Step 8500, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 816.096006155014 seconds\n",
      "Step 8600, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 825.66819024086 seconds\n",
      "Step 8700, loss: tensor(0.0787, grad_fn=<SubBackward0>), time elapsed: 835.2487814426422 seconds\n",
      "Step 8800, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 844.8162496089935 seconds\n",
      "Step 8900, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 854.1971054077148 seconds\n",
      "Step 9000, loss: tensor(0.0781, grad_fn=<SubBackward0>), time elapsed: 863.5983145236969 seconds\n",
      "Step 9100, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 873.1746726036072 seconds\n",
      "Step 9200, loss: tensor(0.0784, grad_fn=<SubBackward0>), time elapsed: 882.5767805576324 seconds\n",
      "Step 9300, loss: tensor(0.0785, grad_fn=<SubBackward0>), time elapsed: 892.02295088768 seconds\n",
      "Step 9400, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 901.6285238265991 seconds\n",
      "Step 9500, loss: tensor(0.0777, grad_fn=<SubBackward0>), time elapsed: 911.0843019485474 seconds\n",
      "Step 9600, loss: tensor(0.0779, grad_fn=<SubBackward0>), time elapsed: 920.4955470561981 seconds\n",
      "Step 9700, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 930.115706205368 seconds\n",
      "Step 9800, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 939.5961496829987 seconds\n",
      "Step 9900, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 949.0735218524933 seconds\n",
      "Step 10000, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 958.7916853427887 seconds\n",
      "Step 10100, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 968.2262971401215 seconds\n",
      "Step 10200, loss: tensor(0.0777, grad_fn=<SubBackward0>), time elapsed: 977.6684787273407 seconds\n",
      "Step 10300, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 987.3696608543396 seconds\n",
      "Step 10400, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 996.7789924144745 seconds\n",
      "Step 10500, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 1006.2191648483276 seconds\n",
      "Step 10600, loss: tensor(0.0787, grad_fn=<SubBackward0>), time elapsed: 1015.8963575363159 seconds\n",
      "Step 10700, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 1025.3497364521027 seconds\n",
      "Step 10800, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 1034.9897320270538 seconds\n",
      "Step 10900, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 1044.4690990447998 seconds\n",
      "Step 11000, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 1053.837096452713 seconds\n",
      "Step 11100, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 1063.4362580776215 seconds\n",
      "Step 11200, loss: tensor(0.0779, grad_fn=<SubBackward0>), time elapsed: 1072.9932131767273 seconds\n",
      "Step 11300, loss: tensor(0.0779, grad_fn=<SubBackward0>), time elapsed: 1082.4867904186249 seconds\n",
      "Step 11400, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 1092.1465787887573 seconds\n",
      "Step 11500, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 1101.6639249324799 seconds\n",
      "Step 11600, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 1111.187037229538 seconds\n",
      "Step 11700, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 1120.8843421936035 seconds\n",
      "Step 11800, loss: tensor(0.0796, grad_fn=<SubBackward0>), time elapsed: 1130.416595697403 seconds\n",
      "Step 11900, loss: tensor(0.0793, grad_fn=<SubBackward0>), time elapsed: 1139.9992485046387 seconds\n",
      "Step 12000, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 1149.6230840682983 seconds\n",
      "Step 12100, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 1159.0690188407898 seconds\n",
      "Step 12200, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 1168.511521100998 seconds\n",
      "Step 12300, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 1178.1132922172546 seconds\n",
      "Step 12400, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 1187.5361723899841 seconds\n",
      "Step 12500, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 1196.9800453186035 seconds\n",
      "Step 12600, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 1206.5785217285156 seconds\n",
      "Step 12700, loss: tensor(0.0788, grad_fn=<SubBackward0>), time elapsed: 1216.0805134773254 seconds\n",
      "Step 12800, loss: tensor(0.0787, grad_fn=<SubBackward0>), time elapsed: 1225.7465574741364 seconds\n",
      "Step 12900, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 1235.1999278068542 seconds\n",
      "Step 13000, loss: tensor(0.0781, grad_fn=<SubBackward0>), time elapsed: 1244.646964788437 seconds\n",
      "Step 13100, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 1254.2649419307709 seconds\n",
      "Step 13200, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 1263.7041132450104 seconds\n",
      "Step 13300, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 1273.1664781570435 seconds\n",
      "Step 13400, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 1282.8982725143433 seconds\n",
      "Step 13500, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 1292.320283651352 seconds\n",
      "Step 13600, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 1301.7121725082397 seconds\n",
      "Step 13700, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 1311.399309873581 seconds\n",
      "Step 13800, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 1320.8316192626953 seconds\n",
      "Step 13900, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 1330.350831270218 seconds\n",
      "Step 14000, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 1339.923778295517 seconds\n",
      "Step 14100, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 1349.2947566509247 seconds\n",
      "Step 14200, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 1358.8054089546204 seconds\n",
      "Step 14300, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 1368.4515817165375 seconds\n",
      "Step 14400, loss: tensor(0.0784, grad_fn=<SubBackward0>), time elapsed: 1378.0822124481201 seconds\n",
      "Step 14500, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 1387.700658082962 seconds\n",
      "Step 14600, loss: tensor(0.0787, grad_fn=<SubBackward0>), time elapsed: 1397.3687822818756 seconds\n",
      "Step 14700, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 1407.105905532837 seconds\n",
      "Step 14800, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 1416.5837342739105 seconds\n",
      "Step 14900, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 1426.3978044986725 seconds\n",
      "Step 15000, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 1435.9329628944397 seconds\n",
      "Step 15100, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 1445.5866944789886 seconds\n",
      "Step 15200, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 1455.1284861564636 seconds\n",
      "Step 15300, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 1464.5124785900116 seconds\n",
      "Step 15400, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 1474.1164543628693 seconds\n",
      "Step 15500, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 1483.5767693519592 seconds\n",
      "Step 15600, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 1492.9782078266144 seconds\n",
      "Step 15700, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 1502.5484073162079 seconds\n",
      "Step 15800, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 1511.9094903469086 seconds\n",
      "Step 15900, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 1521.2773184776306 seconds\n",
      "Step 16000, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 1530.7858242988586 seconds\n",
      "Step 16100, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 1540.3163661956787 seconds\n",
      "Step 16200, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 1549.7508625984192 seconds\n",
      "Step 16300, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 1559.4273526668549 seconds\n",
      "Step 16400, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 1568.7962884902954 seconds\n",
      "Step 16500, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 1578.1452686786652 seconds\n",
      "Step 16600, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 1587.6112649440765 seconds\n",
      "Step 16700, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 1596.9303262233734 seconds\n",
      "Step 16800, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 1606.2754077911377 seconds\n",
      "Step 16900, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 1615.8202011585236 seconds\n",
      "Step 17000, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 1625.25505900383 seconds\n",
      "Step 17100, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 1634.5748252868652 seconds\n",
      "Step 17200, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 1644.0706751346588 seconds\n",
      "Step 17300, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 1653.3862357139587 seconds\n",
      "Step 17400, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 1662.7510993480682 seconds\n",
      "Step 17500, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 1672.4723432064056 seconds\n",
      "Step 17600, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 1681.931802034378 seconds\n",
      "Step 17700, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 1691.581024646759 seconds\n",
      "Step 17800, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 1700.9595515727997 seconds\n",
      "Step 17900, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 1710.419305562973 seconds\n",
      "Step 18000, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 1719.9439985752106 seconds\n",
      "Step 18100, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 1729.3350989818573 seconds\n",
      "Step 18200, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 1738.7853345870972 seconds\n",
      "Step 18300, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 1748.3076815605164 seconds\n",
      "Step 18400, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 1757.6095395088196 seconds\n",
      "Step 18500, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 1766.897125005722 seconds\n",
      "Step 18600, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 1776.4067213535309 seconds\n",
      "Step 18700, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 1785.7371323108673 seconds\n",
      "Step 18800, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 1795.0453221797943 seconds\n",
      "Step 18900, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 1804.568433523178 seconds\n",
      "Step 19000, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 1813.9084749221802 seconds\n",
      "Step 19100, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 1823.4680581092834 seconds\n",
      "Step 19200, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 1833.1183388233185 seconds\n",
      "Step 19300, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 1842.536517381668 seconds\n",
      "Step 19400, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 1851.9405279159546 seconds\n",
      "Step 19500, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 1861.5189621448517 seconds\n",
      "Step 19600, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 1870.8960552215576 seconds\n",
      "Step 19700, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 1880.221197605133 seconds\n",
      "Step 19800, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 1889.8033254146576 seconds\n",
      "Step 19900, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 1899.159383058548 seconds\n",
      "Step 20000, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 1908.5517637729645 seconds\n",
      "Step 20100, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 1918.1038031578064 seconds\n",
      "Step 20200, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 1927.514024734497 seconds\n",
      "Step 20300, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 1937.3422646522522 seconds\n",
      "Step 20400, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 1946.7751915454865 seconds\n",
      "Step 20500, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 1956.1394488811493 seconds\n",
      "Step 20600, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 1965.771761894226 seconds\n",
      "Step 20700, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 1975.220700263977 seconds\n",
      "Step 20800, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 1985.3309426307678 seconds\n",
      "Step 20900, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 1995.2310018539429 seconds\n",
      "Step 21000, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 2004.7196106910706 seconds\n",
      "Step 21100, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 2015.1211585998535 seconds\n",
      "Step 21200, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 2025.0663266181946 seconds\n",
      "Step 21300, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 2034.5357944965363 seconds\n",
      "Step 21400, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 2044.3923382759094 seconds\n",
      "Step 21500, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 2054.2737493515015 seconds\n",
      "Step 21600, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 2063.7880911827087 seconds\n",
      "Step 21700, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 2073.318959712982 seconds\n",
      "Step 21800, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 2082.986794948578 seconds\n",
      "Step 21900, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 2092.5384709835052 seconds\n",
      "Step 22000, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 2102.0069172382355 seconds\n",
      "Step 22100, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 2111.6757917404175 seconds\n",
      "Step 22200, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 2121.191020488739 seconds\n",
      "Step 22300, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 2130.724101305008 seconds\n",
      "Step 22400, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 2140.408187150955 seconds\n",
      "Step 22500, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 2149.8957929611206 seconds\n",
      "Step 22600, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 2159.3602788448334 seconds\n",
      "Step 22700, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 2169.0099341869354 seconds\n",
      "Step 22800, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 2178.6352972984314 seconds\n",
      "Step 22900, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 2188.08576130867 seconds\n",
      "Step 23000, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 2197.7879617214203 seconds\n",
      "Step 23100, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 2207.1891062259674 seconds\n",
      "Step 23200, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 2216.751131296158 seconds\n",
      "Step 23300, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 2226.1053154468536 seconds\n",
      "Step 23400, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 2236.054089784622 seconds\n",
      "Step 23500, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 2245.7693376541138 seconds\n",
      "Step 23600, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 2255.18269944191 seconds\n",
      "Step 23700, loss: tensor(0.0777, grad_fn=<SubBackward0>), time elapsed: 2264.5806274414062 seconds\n",
      "Step 23800, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 2274.1693634986877 seconds\n",
      "Step 23900, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 2283.528230905533 seconds\n",
      "Step 24000, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 2292.8821449279785 seconds\n",
      "Step 24100, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 2303.237737417221 seconds\n",
      "Step 24200, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 2312.657557487488 seconds\n",
      "Step 24300, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 2322.0888724327087 seconds\n",
      "Step 24400, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 2331.7107870578766 seconds\n",
      "Step 24500, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 2341.2041013240814 seconds\n",
      "Step 24600, loss: tensor(0.0779, grad_fn=<SubBackward0>), time elapsed: 2350.75758767128 seconds\n",
      "Step 24700, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 2360.4444291591644 seconds\n",
      "Step 24800, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 2369.9251313209534 seconds\n",
      "Step 24900, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 2379.397053718567 seconds\n",
      "Step 25000, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 2389.0712435245514 seconds\n",
      "Step 25100, loss: tensor(0.0770, grad_fn=<SubBackward0>), time elapsed: 2398.536443710327 seconds\n",
      "Step 25200, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 2408.0739278793335 seconds\n",
      "Step 25300, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 2417.896181821823 seconds\n",
      "Step 25400, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 2427.364242553711 seconds\n",
      "Step 25500, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 2436.8959147930145 seconds\n",
      "Step 25600, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 2446.6233756542206 seconds\n",
      "Step 25700, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 2456.1990785598755 seconds\n",
      "Step 25800, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 2465.6757402420044 seconds\n",
      "Step 25900, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 2475.4110260009766 seconds\n",
      "Step 26000, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 2485.0375893115997 seconds\n",
      "Step 26100, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 2494.681405544281 seconds\n",
      "Step 26200, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 2504.274570465088 seconds\n",
      "Step 26300, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 2513.6788511276245 seconds\n",
      "Step 26400, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 2523.2552394866943 seconds\n",
      "Step 26500, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 2532.6334574222565 seconds\n",
      "Step 26600, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 2541.9784054756165 seconds\n",
      "Step 26700, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 2551.5327656269073 seconds\n",
      "Step 26800, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 2560.8560383319855 seconds\n",
      "Step 26900, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 2570.1777908802032 seconds\n",
      "Step 27000, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 2579.7158076763153 seconds\n",
      "Step 27100, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 2589.033276081085 seconds\n",
      "Step 27200, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 2598.36257147789 seconds\n",
      "Step 27300, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 2607.8785376548767 seconds\n",
      "Step 27400, loss: tensor(0.0726, grad_fn=<SubBackward0>), time elapsed: 2617.1992161273956 seconds\n",
      "Step 27500, loss: tensor(0.0770, grad_fn=<SubBackward0>), time elapsed: 2626.51699757576 seconds\n",
      "Step 27600, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 2636.0195894241333 seconds\n",
      "Step 27700, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 2645.4144439697266 seconds\n",
      "Step 27800, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 2654.7384688854218 seconds\n",
      "Step 27900, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 2664.264988422394 seconds\n",
      "Step 28000, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 2673.6202533245087 seconds\n",
      "Step 28100, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 2682.9538350105286 seconds\n",
      "Step 28200, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 2692.58101272583 seconds\n",
      "Step 28300, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 2702.1266310214996 seconds\n",
      "Step 28400, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 2711.4281253814697 seconds\n",
      "Step 28500, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 2721.0016765594482 seconds\n",
      "Step 28600, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 2730.408079147339 seconds\n",
      "Step 28700, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 2739.7624077796936 seconds\n",
      "Step 28800, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 2749.3009781837463 seconds\n",
      "Step 28900, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 2758.6979949474335 seconds\n",
      "Step 29000, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 2768.0568590164185 seconds\n",
      "Step 29100, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 2777.6657207012177 seconds\n",
      "Step 29200, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 2787.0602633953094 seconds\n",
      "Step 29300, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 2796.4656212329865 seconds\n",
      "Step 29400, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 2806.0099680423737 seconds\n",
      "Step 29500, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 2815.342736005783 seconds\n",
      "Step 29600, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 2824.672092437744 seconds\n",
      "Step 29700, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 2834.3205921649933 seconds\n",
      "Step 29800, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 2843.982893228531 seconds\n",
      "Step 29900, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 2853.7084867954254 seconds\n",
      "Step 30000, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 2863.047634124756 seconds\n",
      "Step 30100, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 2872.3942546844482 seconds\n",
      "Step 30200, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 2881.9425563812256 seconds\n",
      "Step 30300, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 2891.3062620162964 seconds\n",
      "Step 30400, loss: tensor(0.0744, grad_fn=<SubBackward0>), time elapsed: 2900.685709953308 seconds\n",
      "Step 30500, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 2910.247304201126 seconds\n",
      "Step 30600, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 2919.6405391693115 seconds\n",
      "Step 30700, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 2928.9873044490814 seconds\n",
      "Step 30800, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 2938.5199365615845 seconds\n",
      "Step 30900, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 2948.0087316036224 seconds\n",
      "Step 31000, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 2957.470328092575 seconds\n",
      "Step 31100, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 2967.16792011261 seconds\n",
      "Step 31200, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 2976.87061214447 seconds\n",
      "Step 31300, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 2986.5415921211243 seconds\n",
      "Step 31400, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 2996.1135964393616 seconds\n",
      "Step 31500, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 3005.531653881073 seconds\n",
      "Step 31600, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 3015.0666196346283 seconds\n",
      "Step 31700, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 3024.868983745575 seconds\n",
      "Step 31800, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 3034.4348423480988 seconds\n",
      "Step 31900, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 3043.958849668503 seconds\n",
      "Step 32000, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 3053.663784265518 seconds\n",
      "Step 32100, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 3063.0565226078033 seconds\n",
      "Step 32200, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 3072.4712312221527 seconds\n",
      "Step 32300, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 3082.2122626304626 seconds\n",
      "Step 32400, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 3091.69677400589 seconds\n",
      "Step 32500, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 3101.376376390457 seconds\n",
      "Step 32600, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 3111.114180326462 seconds\n",
      "Step 32700, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 3120.7318432331085 seconds\n",
      "Step 32800, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 3130.3978266716003 seconds\n",
      "Step 32900, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 3140.157065153122 seconds\n",
      "Step 33000, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 3149.673392057419 seconds\n",
      "Step 33100, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 3159.066259622574 seconds\n",
      "Step 33200, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 3168.5885779857635 seconds\n",
      "Step 33300, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 3177.931392431259 seconds\n",
      "Step 33400, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 3187.324051141739 seconds\n",
      "Step 33500, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 3196.8545351028442 seconds\n",
      "Step 33600, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 3206.243421316147 seconds\n",
      "Step 33700, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 3215.6404192447662 seconds\n",
      "Step 33800, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 3225.2418069839478 seconds\n",
      "Step 33900, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 3234.6165471076965 seconds\n",
      "Step 34000, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 3244.0197925567627 seconds\n",
      "Step 34100, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 3253.753885746002 seconds\n",
      "Step 34200, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 3263.173858642578 seconds\n",
      "Step 34300, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 3272.722018957138 seconds\n",
      "Step 34400, loss: tensor(0.0764, grad_fn=<SubBackward0>), time elapsed: 3282.1328551769257 seconds\n",
      "Step 34500, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 3291.454577922821 seconds\n",
      "Step 34600, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 3300.9811136722565 seconds\n",
      "Step 34700, loss: tensor(0.0736, grad_fn=<SubBackward0>), time elapsed: 3310.3528969287872 seconds\n",
      "Step 34800, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 3319.671775817871 seconds\n",
      "Step 34900, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 3329.2136750221252 seconds\n",
      "Step 35000, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 3338.594030857086 seconds\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.7882, grad_fn=<SubBackward0>), time elapsed: 0.09989786148071289 seconds\n",
      "Step 100, loss: tensor(0.7365, grad_fn=<SubBackward0>), time elapsed: 9.577157497406006 seconds\n",
      "Step 200, loss: tensor(0.6469, grad_fn=<SubBackward0>), time elapsed: 19.077709436416626 seconds\n",
      "Step 300, loss: tensor(0.5222, grad_fn=<SubBackward0>), time elapsed: 28.645312786102295 seconds\n",
      "Step 400, loss: tensor(0.4096, grad_fn=<SubBackward0>), time elapsed: 37.97831463813782 seconds\n",
      "Step 500, loss: tensor(0.3225, grad_fn=<SubBackward0>), time elapsed: 47.4079966545105 seconds\n",
      "Step 600, loss: tensor(0.2770, grad_fn=<SubBackward0>), time elapsed: 56.75933241844177 seconds\n",
      "Step 700, loss: tensor(0.2491, grad_fn=<SubBackward0>), time elapsed: 66.07422947883606 seconds\n",
      "Step 800, loss: tensor(0.2219, grad_fn=<SubBackward0>), time elapsed: 75.50866723060608 seconds\n",
      "Step 900, loss: tensor(0.2038, grad_fn=<SubBackward0>), time elapsed: 84.784663438797 seconds\n",
      "Step 1000, loss: tensor(0.1952, grad_fn=<SubBackward0>), time elapsed: 94.10264658927917 seconds\n",
      "Step 1100, loss: tensor(0.1792, grad_fn=<SubBackward0>), time elapsed: 103.54426789283752 seconds\n",
      "Step 1200, loss: tensor(0.1694, grad_fn=<SubBackward0>), time elapsed: 112.86314129829407 seconds\n",
      "Step 1300, loss: tensor(0.1621, grad_fn=<SubBackward0>), time elapsed: 122.48537802696228 seconds\n",
      "Step 1400, loss: tensor(0.1546, grad_fn=<SubBackward0>), time elapsed: 131.76561069488525 seconds\n",
      "Step 1500, loss: tensor(0.1510, grad_fn=<SubBackward0>), time elapsed: 141.0292592048645 seconds\n",
      "Step 1600, loss: tensor(0.1456, grad_fn=<SubBackward0>), time elapsed: 150.46797800064087 seconds\n",
      "Step 1700, loss: tensor(0.1433, grad_fn=<SubBackward0>), time elapsed: 159.76374697685242 seconds\n",
      "Step 1800, loss: tensor(0.1426, grad_fn=<SubBackward0>), time elapsed: 169.07216835021973 seconds\n",
      "Step 1900, loss: tensor(0.1386, grad_fn=<SubBackward0>), time elapsed: 178.5286045074463 seconds\n",
      "Step 2000, loss: tensor(0.1380, grad_fn=<SubBackward0>), time elapsed: 187.84329843521118 seconds\n",
      "Step 2100, loss: tensor(0.1366, grad_fn=<SubBackward0>), time elapsed: 197.16524529457092 seconds\n",
      "Step 2200, loss: tensor(0.1354, grad_fn=<SubBackward0>), time elapsed: 206.64460945129395 seconds\n",
      "Step 2300, loss: tensor(0.1322, grad_fn=<SubBackward0>), time elapsed: 215.97209930419922 seconds\n",
      "Step 2400, loss: tensor(0.1333, grad_fn=<SubBackward0>), time elapsed: 225.30098009109497 seconds\n",
      "Step 2500, loss: tensor(0.1321, grad_fn=<SubBackward0>), time elapsed: 234.78043937683105 seconds\n",
      "Step 2600, loss: tensor(0.1303, grad_fn=<SubBackward0>), time elapsed: 244.13537335395813 seconds\n",
      "Step 2700, loss: tensor(0.1298, grad_fn=<SubBackward0>), time elapsed: 253.46290850639343 seconds\n",
      "Step 2800, loss: tensor(0.1291, grad_fn=<SubBackward0>), time elapsed: 262.95276832580566 seconds\n",
      "Step 2900, loss: tensor(0.1286, grad_fn=<SubBackward0>), time elapsed: 272.3236644268036 seconds\n",
      "Step 3000, loss: tensor(0.1284, grad_fn=<SubBackward0>), time elapsed: 281.82614755630493 seconds\n",
      "Step 3100, loss: tensor(0.1271, grad_fn=<SubBackward0>), time elapsed: 291.1188702583313 seconds\n",
      "Step 3200, loss: tensor(0.1262, grad_fn=<SubBackward0>), time elapsed: 300.45219683647156 seconds\n",
      "Step 3300, loss: tensor(0.1268, grad_fn=<SubBackward0>), time elapsed: 309.94621992111206 seconds\n",
      "Step 3400, loss: tensor(0.1256, grad_fn=<SubBackward0>), time elapsed: 319.2430770397186 seconds\n",
      "Step 3500, loss: tensor(0.1245, grad_fn=<SubBackward0>), time elapsed: 328.54424357414246 seconds\n",
      "Step 3600, loss: tensor(0.1244, grad_fn=<SubBackward0>), time elapsed: 338.0288519859314 seconds\n",
      "Step 3700, loss: tensor(0.1236, grad_fn=<SubBackward0>), time elapsed: 347.32586121559143 seconds\n",
      "Step 3800, loss: tensor(0.1252, grad_fn=<SubBackward0>), time elapsed: 356.63628363609314 seconds\n",
      "Step 3900, loss: tensor(0.1233, grad_fn=<SubBackward0>), time elapsed: 366.11583709716797 seconds\n",
      "Step 4000, loss: tensor(0.1228, grad_fn=<SubBackward0>), time elapsed: 375.4125955104828 seconds\n",
      "Step 4100, loss: tensor(0.1243, grad_fn=<SubBackward0>), time elapsed: 384.6792104244232 seconds\n",
      "Step 4200, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 394.1649160385132 seconds\n",
      "Step 4300, loss: tensor(0.1246, grad_fn=<SubBackward0>), time elapsed: 403.4982807636261 seconds\n",
      "Step 4400, loss: tensor(0.1234, grad_fn=<SubBackward0>), time elapsed: 412.7837905883789 seconds\n",
      "Step 4500, loss: tensor(0.1231, grad_fn=<SubBackward0>), time elapsed: 422.3288972377777 seconds\n",
      "Step 4600, loss: tensor(0.1215, grad_fn=<SubBackward0>), time elapsed: 431.6445686817169 seconds\n",
      "Step 4700, loss: tensor(0.1223, grad_fn=<SubBackward0>), time elapsed: 441.1395959854126 seconds\n",
      "Step 4800, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 450.41358494758606 seconds\n",
      "Step 4900, loss: tensor(0.1232, grad_fn=<SubBackward0>), time elapsed: 459.6854918003082 seconds\n",
      "Step 5000, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 469.132559299469 seconds\n",
      "Step 5100, loss: tensor(0.1232, grad_fn=<SubBackward0>), time elapsed: 478.4764246940613 seconds\n",
      "Step 5200, loss: tensor(0.1236, grad_fn=<SubBackward0>), time elapsed: 487.82127022743225 seconds\n",
      "Step 5300, loss: tensor(0.1241, grad_fn=<SubBackward0>), time elapsed: 497.2860836982727 seconds\n",
      "Step 5400, loss: tensor(0.1216, grad_fn=<SubBackward0>), time elapsed: 506.59969902038574 seconds\n",
      "Step 5500, loss: tensor(0.1225, grad_fn=<SubBackward0>), time elapsed: 515.9133086204529 seconds\n",
      "Step 5600, loss: tensor(0.1224, grad_fn=<SubBackward0>), time elapsed: 525.3572959899902 seconds\n",
      "Step 5700, loss: tensor(0.1234, grad_fn=<SubBackward0>), time elapsed: 534.6351799964905 seconds\n",
      "Step 5800, loss: tensor(0.1226, grad_fn=<SubBackward0>), time elapsed: 544.1489396095276 seconds\n",
      "Step 5900, loss: tensor(0.1214, grad_fn=<SubBackward0>), time elapsed: 553.73730301857 seconds\n",
      "Step 6000, loss: tensor(0.1210, grad_fn=<SubBackward0>), time elapsed: 563.0153739452362 seconds\n",
      "Step 6100, loss: tensor(0.1215, grad_fn=<SubBackward0>), time elapsed: 572.2999346256256 seconds\n",
      "Step 6200, loss: tensor(0.1208, grad_fn=<SubBackward0>), time elapsed: 581.7842299938202 seconds\n",
      "Step 6300, loss: tensor(0.1207, grad_fn=<SubBackward0>), time elapsed: 591.0873951911926 seconds\n",
      "Step 6400, loss: tensor(0.1204, grad_fn=<SubBackward0>), time elapsed: 600.610570192337 seconds\n",
      "Step 6500, loss: tensor(0.1206, grad_fn=<SubBackward0>), time elapsed: 609.9294242858887 seconds\n",
      "Step 6600, loss: tensor(0.1200, grad_fn=<SubBackward0>), time elapsed: 619.2264113426208 seconds\n",
      "Step 6700, loss: tensor(0.1197, grad_fn=<SubBackward0>), time elapsed: 628.6736195087433 seconds\n",
      "Step 6800, loss: tensor(0.1204, grad_fn=<SubBackward0>), time elapsed: 638.0454142093658 seconds\n",
      "Step 6900, loss: tensor(0.1207, grad_fn=<SubBackward0>), time elapsed: 647.4602248668671 seconds\n",
      "Step 7000, loss: tensor(0.1204, grad_fn=<SubBackward0>), time elapsed: 656.9771692752838 seconds\n",
      "Step 7100, loss: tensor(0.1186, grad_fn=<SubBackward0>), time elapsed: 666.3021476268768 seconds\n",
      "Step 7200, loss: tensor(0.1189, grad_fn=<SubBackward0>), time elapsed: 675.6460645198822 seconds\n",
      "Step 7300, loss: tensor(0.1193, grad_fn=<SubBackward0>), time elapsed: 685.1108820438385 seconds\n",
      "Step 7400, loss: tensor(0.1181, grad_fn=<SubBackward0>), time elapsed: 694.4152128696442 seconds\n",
      "Step 7500, loss: tensor(0.1176, grad_fn=<SubBackward0>), time elapsed: 703.7579264640808 seconds\n",
      "Step 7600, loss: tensor(0.1186, grad_fn=<SubBackward0>), time elapsed: 713.2258849143982 seconds\n",
      "Step 7700, loss: tensor(0.1185, grad_fn=<SubBackward0>), time elapsed: 722.5268697738647 seconds\n",
      "Step 7800, loss: tensor(0.1168, grad_fn=<SubBackward0>), time elapsed: 731.84881067276 seconds\n",
      "Step 7900, loss: tensor(0.1174, grad_fn=<SubBackward0>), time elapsed: 741.2833812236786 seconds\n",
      "Step 8000, loss: tensor(0.1167, grad_fn=<SubBackward0>), time elapsed: 750.5595462322235 seconds\n",
      "Step 8100, loss: tensor(0.1179, grad_fn=<SubBackward0>), time elapsed: 759.8315179347992 seconds\n",
      "Step 8200, loss: tensor(0.1172, grad_fn=<SubBackward0>), time elapsed: 769.303439617157 seconds\n",
      "Step 8300, loss: tensor(0.1169, grad_fn=<SubBackward0>), time elapsed: 778.5836846828461 seconds\n",
      "Step 8400, loss: tensor(0.1163, grad_fn=<SubBackward0>), time elapsed: 788.0379853248596 seconds\n",
      "Step 8500, loss: tensor(0.1157, grad_fn=<SubBackward0>), time elapsed: 797.4257023334503 seconds\n",
      "Step 8600, loss: tensor(0.1143, grad_fn=<SubBackward0>), time elapsed: 806.766410112381 seconds\n",
      "Step 8700, loss: tensor(0.1157, grad_fn=<SubBackward0>), time elapsed: 816.2260465621948 seconds\n",
      "Step 8800, loss: tensor(0.1155, grad_fn=<SubBackward0>), time elapsed: 825.5854785442352 seconds\n",
      "Step 8900, loss: tensor(0.1164, grad_fn=<SubBackward0>), time elapsed: 834.9282786846161 seconds\n",
      "Step 9000, loss: tensor(0.1144, grad_fn=<SubBackward0>), time elapsed: 844.4045562744141 seconds\n",
      "Step 9100, loss: tensor(0.1140, grad_fn=<SubBackward0>), time elapsed: 853.6993792057037 seconds\n",
      "Step 9200, loss: tensor(0.1160, grad_fn=<SubBackward0>), time elapsed: 863.0610482692719 seconds\n",
      "Step 9300, loss: tensor(0.1144, grad_fn=<SubBackward0>), time elapsed: 872.5820436477661 seconds\n",
      "Step 9400, loss: tensor(0.1123, grad_fn=<SubBackward0>), time elapsed: 881.8842051029205 seconds\n",
      "Step 9500, loss: tensor(0.1130, grad_fn=<SubBackward0>), time elapsed: 891.2062175273895 seconds\n",
      "Step 9600, loss: tensor(0.1126, grad_fn=<SubBackward0>), time elapsed: 900.7002141475677 seconds\n",
      "Step 9700, loss: tensor(0.1108, grad_fn=<SubBackward0>), time elapsed: 910.0398018360138 seconds\n",
      "Step 9800, loss: tensor(0.1095, grad_fn=<SubBackward0>), time elapsed: 919.4066443443298 seconds\n",
      "Step 9900, loss: tensor(0.1111, grad_fn=<SubBackward0>), time elapsed: 928.8734893798828 seconds\n",
      "Step 10000, loss: tensor(0.1094, grad_fn=<SubBackward0>), time elapsed: 938.1755006313324 seconds\n",
      "Step 10100, loss: tensor(0.1074, grad_fn=<SubBackward0>), time elapsed: 947.5236463546753 seconds\n",
      "Step 10200, loss: tensor(0.1074, grad_fn=<SubBackward0>), time elapsed: 956.9820141792297 seconds\n",
      "Step 10300, loss: tensor(0.1053, grad_fn=<SubBackward0>), time elapsed: 966.3228590488434 seconds\n",
      "Step 10400, loss: tensor(0.1053, grad_fn=<SubBackward0>), time elapsed: 975.8128983974457 seconds\n",
      "Step 10500, loss: tensor(0.1046, grad_fn=<SubBackward0>), time elapsed: 985.1566183567047 seconds\n",
      "Step 10600, loss: tensor(0.1027, grad_fn=<SubBackward0>), time elapsed: 994.477570772171 seconds\n",
      "Step 10700, loss: tensor(0.1031, grad_fn=<SubBackward0>), time elapsed: 1004.0123453140259 seconds\n",
      "Step 10800, loss: tensor(0.1004, grad_fn=<SubBackward0>), time elapsed: 1013.3632853031158 seconds\n",
      "Step 10900, loss: tensor(0.1020, grad_fn=<SubBackward0>), time elapsed: 1022.9208877086639 seconds\n",
      "Step 11000, loss: tensor(0.1004, grad_fn=<SubBackward0>), time elapsed: 1032.4284389019012 seconds\n",
      "Step 11100, loss: tensor(0.1015, grad_fn=<SubBackward0>), time elapsed: 1041.7983210086823 seconds\n",
      "Step 11200, loss: tensor(0.0976, grad_fn=<SubBackward0>), time elapsed: 1051.0848665237427 seconds\n",
      "Step 11300, loss: tensor(0.0958, grad_fn=<SubBackward0>), time elapsed: 1060.544438123703 seconds\n",
      "Step 11400, loss: tensor(0.0957, grad_fn=<SubBackward0>), time elapsed: 1069.8830580711365 seconds\n",
      "Step 11500, loss: tensor(0.0944, grad_fn=<SubBackward0>), time elapsed: 1079.2010414600372 seconds\n",
      "Step 11600, loss: tensor(0.0967, grad_fn=<SubBackward0>), time elapsed: 1088.683362007141 seconds\n",
      "Step 11700, loss: tensor(0.0952, grad_fn=<SubBackward0>), time elapsed: 1097.986562013626 seconds\n",
      "Step 11800, loss: tensor(0.0938, grad_fn=<SubBackward0>), time elapsed: 1107.3263268470764 seconds\n",
      "Step 11900, loss: tensor(0.0947, grad_fn=<SubBackward0>), time elapsed: 1116.7847757339478 seconds\n",
      "Step 12000, loss: tensor(0.0926, grad_fn=<SubBackward0>), time elapsed: 1126.0858662128448 seconds\n",
      "Step 12100, loss: tensor(0.0932, grad_fn=<SubBackward0>), time elapsed: 1135.620582818985 seconds\n",
      "Step 12200, loss: tensor(0.0931, grad_fn=<SubBackward0>), time elapsed: 1144.9467024803162 seconds\n",
      "Step 12300, loss: tensor(0.0927, grad_fn=<SubBackward0>), time elapsed: 1154.325867652893 seconds\n",
      "Step 12400, loss: tensor(0.0929, grad_fn=<SubBackward0>), time elapsed: 1163.844950914383 seconds\n",
      "Step 12500, loss: tensor(0.0939, grad_fn=<SubBackward0>), time elapsed: 1173.183587551117 seconds\n",
      "Step 12600, loss: tensor(0.0891, grad_fn=<SubBackward0>), time elapsed: 1182.5155794620514 seconds\n",
      "Step 12700, loss: tensor(0.0911, grad_fn=<SubBackward0>), time elapsed: 1192.0235850811005 seconds\n",
      "Step 12800, loss: tensor(0.0919, grad_fn=<SubBackward0>), time elapsed: 1201.353967666626 seconds\n",
      "Step 12900, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 1210.6840929985046 seconds\n",
      "Step 13000, loss: tensor(0.0912, grad_fn=<SubBackward0>), time elapsed: 1220.2134926319122 seconds\n",
      "Step 13100, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 1229.5449090003967 seconds\n",
      "Step 13200, loss: tensor(0.0921, grad_fn=<SubBackward0>), time elapsed: 1238.9002799987793 seconds\n",
      "Step 13300, loss: tensor(0.0895, grad_fn=<SubBackward0>), time elapsed: 1248.426509141922 seconds\n",
      "Step 13400, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 1257.8277010917664 seconds\n",
      "Step 13500, loss: tensor(0.0933, grad_fn=<SubBackward0>), time elapsed: 1267.1757431030273 seconds\n",
      "Step 13600, loss: tensor(0.0917, grad_fn=<SubBackward0>), time elapsed: 1276.6902315616608 seconds\n",
      "Step 13700, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 1286.0377264022827 seconds\n",
      "Step 13800, loss: tensor(0.0922, grad_fn=<SubBackward0>), time elapsed: 1295.3448917865753 seconds\n",
      "Step 13900, loss: tensor(0.0918, grad_fn=<SubBackward0>), time elapsed: 1304.8702170848846 seconds\n",
      "Step 14000, loss: tensor(0.0923, grad_fn=<SubBackward0>), time elapsed: 1314.305832862854 seconds\n",
      "Step 14100, loss: tensor(0.0898, grad_fn=<SubBackward0>), time elapsed: 1323.8905854225159 seconds\n",
      "Step 14200, loss: tensor(0.0915, grad_fn=<SubBackward0>), time elapsed: 1333.7221615314484 seconds\n",
      "Step 14300, loss: tensor(0.0924, grad_fn=<SubBackward0>), time elapsed: 1343.2517709732056 seconds\n",
      "Step 14400, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 1352.6216819286346 seconds\n",
      "Step 14500, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 1362.123963356018 seconds\n",
      "Step 14600, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 1371.4722254276276 seconds\n",
      "Step 14700, loss: tensor(0.0928, grad_fn=<SubBackward0>), time elapsed: 1381.0275552272797 seconds\n",
      "Step 14800, loss: tensor(0.0891, grad_fn=<SubBackward0>), time elapsed: 1390.387046098709 seconds\n",
      "Step 14900, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 1399.7549395561218 seconds\n",
      "Step 15000, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 1409.3249878883362 seconds\n",
      "Step 15100, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 1418.6844804286957 seconds\n",
      "Step 15200, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 1428.029393196106 seconds\n",
      "Step 15300, loss: tensor(0.0916, grad_fn=<SubBackward0>), time elapsed: 1437.5932199954987 seconds\n",
      "Step 15400, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 1446.9819819927216 seconds\n",
      "Step 15500, loss: tensor(0.0919, grad_fn=<SubBackward0>), time elapsed: 1456.3967280387878 seconds\n",
      "Step 15600, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 1465.9698903560638 seconds\n",
      "Step 15700, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 1475.3711078166962 seconds\n",
      "Step 15800, loss: tensor(0.0923, grad_fn=<SubBackward0>), time elapsed: 1484.7389364242554 seconds\n",
      "Step 15900, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 1494.2483186721802 seconds\n",
      "Step 16000, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 1503.5851047039032 seconds\n",
      "Step 16100, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 1512.911413192749 seconds\n",
      "Step 16200, loss: tensor(0.0905, grad_fn=<SubBackward0>), time elapsed: 1522.4041736125946 seconds\n",
      "Step 16300, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 1531.7595734596252 seconds\n",
      "Step 16400, loss: tensor(0.0915, grad_fn=<SubBackward0>), time elapsed: 1541.1273245811462 seconds\n",
      "Step 16500, loss: tensor(0.0917, grad_fn=<SubBackward0>), time elapsed: 1550.6162304878235 seconds\n",
      "Step 16600, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 1559.9985094070435 seconds\n",
      "Step 16700, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 1569.3611772060394 seconds\n",
      "Step 16800, loss: tensor(0.0907, grad_fn=<SubBackward0>), time elapsed: 1578.8864142894745 seconds\n",
      "Step 16900, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 1588.2458918094635 seconds\n",
      "Step 17000, loss: tensor(0.0903, grad_fn=<SubBackward0>), time elapsed: 1597.8890075683594 seconds\n",
      "Step 17100, loss: tensor(0.0892, grad_fn=<SubBackward0>), time elapsed: 1607.2860581874847 seconds\n",
      "Step 17200, loss: tensor(0.0918, grad_fn=<SubBackward0>), time elapsed: 1616.6455121040344 seconds\n",
      "Step 17300, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 1626.2094581127167 seconds\n",
      "Step 17400, loss: tensor(0.0910, grad_fn=<SubBackward0>), time elapsed: 1635.5438272953033 seconds\n",
      "Step 17500, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 1644.8845534324646 seconds\n",
      "Step 17600, loss: tensor(0.0912, grad_fn=<SubBackward0>), time elapsed: 1654.4296395778656 seconds\n",
      "Step 17700, loss: tensor(0.0901, grad_fn=<SubBackward0>), time elapsed: 1663.8184082508087 seconds\n",
      "Step 17800, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 1673.1600360870361 seconds\n",
      "Step 17900, loss: tensor(0.0874, grad_fn=<SubBackward0>), time elapsed: 1682.7093706130981 seconds\n",
      "Step 18000, loss: tensor(0.0901, grad_fn=<SubBackward0>), time elapsed: 1692.0772008895874 seconds\n",
      "Step 18100, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 1701.4783947467804 seconds\n",
      "Step 18200, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 1711.0390915870667 seconds\n",
      "Step 18300, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 1720.4233422279358 seconds\n",
      "Step 18400, loss: tensor(0.0877, grad_fn=<SubBackward0>), time elapsed: 1729.7798936367035 seconds\n",
      "Step 18500, loss: tensor(0.0869, grad_fn=<SubBackward0>), time elapsed: 1739.3116788864136 seconds\n",
      "Step 18600, loss: tensor(0.0855, grad_fn=<SubBackward0>), time elapsed: 1748.7043442726135 seconds\n",
      "Step 18700, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 1758.0993144512177 seconds\n",
      "Step 18800, loss: tensor(0.0874, grad_fn=<SubBackward0>), time elapsed: 1767.6256062984467 seconds\n",
      "Step 18900, loss: tensor(0.0857, grad_fn=<SubBackward0>), time elapsed: 1777.000768661499 seconds\n",
      "Step 19000, loss: tensor(0.0870, grad_fn=<SubBackward0>), time elapsed: 1786.346667766571 seconds\n",
      "Step 19100, loss: tensor(0.0848, grad_fn=<SubBackward0>), time elapsed: 1795.8834280967712 seconds\n",
      "Step 19200, loss: tensor(0.0862, grad_fn=<SubBackward0>), time elapsed: 1805.2770919799805 seconds\n",
      "Step 19300, loss: tensor(0.0860, grad_fn=<SubBackward0>), time elapsed: 1814.680613040924 seconds\n",
      "Step 19400, loss: tensor(0.0836, grad_fn=<SubBackward0>), time elapsed: 1824.2579934597015 seconds\n",
      "Step 19500, loss: tensor(0.0819, grad_fn=<SubBackward0>), time elapsed: 1833.6613564491272 seconds\n",
      "Step 19600, loss: tensor(0.0826, grad_fn=<SubBackward0>), time elapsed: 1843.233638048172 seconds\n",
      "Step 19700, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 1852.622220993042 seconds\n",
      "Step 19800, loss: tensor(0.0836, grad_fn=<SubBackward0>), time elapsed: 1861.9876375198364 seconds\n",
      "Step 19900, loss: tensor(0.0836, grad_fn=<SubBackward0>), time elapsed: 1871.5682718753815 seconds\n",
      "Step 20000, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 1880.9551396369934 seconds\n",
      "Step 20100, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 1890.3603081703186 seconds\n",
      "Step 20200, loss: tensor(0.0841, grad_fn=<SubBackward0>), time elapsed: 1900.0389442443848 seconds\n",
      "Step 20300, loss: tensor(0.0823, grad_fn=<SubBackward0>), time elapsed: 1909.4119637012482 seconds\n",
      "Step 20400, loss: tensor(0.0801, grad_fn=<SubBackward0>), time elapsed: 1918.7744624614716 seconds\n",
      "Step 20500, loss: tensor(0.0850, grad_fn=<SubBackward0>), time elapsed: 1928.5378844738007 seconds\n",
      "Step 20600, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 1938.063045501709 seconds\n",
      "Step 20700, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 1947.6560690402985 seconds\n",
      "Step 20800, loss: tensor(0.0844, grad_fn=<SubBackward0>), time elapsed: 1957.2325744628906 seconds\n",
      "Step 20900, loss: tensor(0.0801, grad_fn=<SubBackward0>), time elapsed: 1966.6439518928528 seconds\n",
      "Step 21000, loss: tensor(0.0826, grad_fn=<SubBackward0>), time elapsed: 1976.084837436676 seconds\n",
      "Step 21100, loss: tensor(0.0794, grad_fn=<SubBackward0>), time elapsed: 1985.67258477211 seconds\n",
      "Step 21200, loss: tensor(0.0813, grad_fn=<SubBackward0>), time elapsed: 1995.104067325592 seconds\n",
      "Step 21300, loss: tensor(0.0810, grad_fn=<SubBackward0>), time elapsed: 2004.5240156650543 seconds\n",
      "Step 21400, loss: tensor(0.0791, grad_fn=<SubBackward0>), time elapsed: 2014.138914823532 seconds\n",
      "Step 21500, loss: tensor(0.0806, grad_fn=<SubBackward0>), time elapsed: 2023.5609753131866 seconds\n",
      "Step 21600, loss: tensor(0.0779, grad_fn=<SubBackward0>), time elapsed: 2032.9717967510223 seconds\n",
      "Step 21700, loss: tensor(0.0801, grad_fn=<SubBackward0>), time elapsed: 2042.5615763664246 seconds\n",
      "Step 21800, loss: tensor(0.0821, grad_fn=<SubBackward0>), time elapsed: 2051.9981503486633 seconds\n",
      "Step 21900, loss: tensor(0.0825, grad_fn=<SubBackward0>), time elapsed: 2061.433959722519 seconds\n",
      "Step 22000, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 2071.0404381752014 seconds\n",
      "Step 22100, loss: tensor(0.0829, grad_fn=<SubBackward0>), time elapsed: 2080.4447300434113 seconds\n",
      "Step 22200, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 2090.0044100284576 seconds\n",
      "Step 22300, loss: tensor(0.0833, grad_fn=<SubBackward0>), time elapsed: 2099.4118547439575 seconds\n",
      "Step 22400, loss: tensor(0.0825, grad_fn=<SubBackward0>), time elapsed: 2108.7860457897186 seconds\n",
      "Step 22500, loss: tensor(0.0789, grad_fn=<SubBackward0>), time elapsed: 2118.401039123535 seconds\n",
      "Step 22600, loss: tensor(0.0807, grad_fn=<SubBackward0>), time elapsed: 2127.8138060569763 seconds\n",
      "Step 22700, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 2137.1940615177155 seconds\n",
      "Step 22800, loss: tensor(0.0817, grad_fn=<SubBackward0>), time elapsed: 2146.820382833481 seconds\n",
      "Step 22900, loss: tensor(0.0836, grad_fn=<SubBackward0>), time elapsed: 2156.246546268463 seconds\n",
      "Step 23000, loss: tensor(0.0791, grad_fn=<SubBackward0>), time elapsed: 2165.7021403312683 seconds\n",
      "Step 23100, loss: tensor(0.0801, grad_fn=<SubBackward0>), time elapsed: 2175.325295686722 seconds\n",
      "Step 23200, loss: tensor(0.0807, grad_fn=<SubBackward0>), time elapsed: 2184.7491850852966 seconds\n",
      "Step 23300, loss: tensor(0.0787, grad_fn=<SubBackward0>), time elapsed: 2194.2069113254547 seconds\n",
      "Step 23400, loss: tensor(0.0806, grad_fn=<SubBackward0>), time elapsed: 2207.5868685245514 seconds\n",
      "Step 23500, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 2217.9027161598206 seconds\n",
      "Step 23600, loss: tensor(0.0822, grad_fn=<SubBackward0>), time elapsed: 2227.9846630096436 seconds\n",
      "Step 23700, loss: tensor(0.0806, grad_fn=<SubBackward0>), time elapsed: 2237.883446455002 seconds\n",
      "Step 23800, loss: tensor(0.0795, grad_fn=<SubBackward0>), time elapsed: 2247.4139337539673 seconds\n",
      "Step 23900, loss: tensor(0.0822, grad_fn=<SubBackward0>), time elapsed: 2256.902797460556 seconds\n",
      "Step 24000, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 2266.516776561737 seconds\n",
      "Step 24100, loss: tensor(0.0801, grad_fn=<SubBackward0>), time elapsed: 2278.4871170520782 seconds\n",
      "Step 24200, loss: tensor(0.0784, grad_fn=<SubBackward0>), time elapsed: 2287.9988062381744 seconds\n",
      "Step 24300, loss: tensor(0.0795, grad_fn=<SubBackward0>), time elapsed: 2297.7126626968384 seconds\n",
      "Step 24400, loss: tensor(0.0802, grad_fn=<SubBackward0>), time elapsed: 2307.2036991119385 seconds\n",
      "Step 24500, loss: tensor(0.0810, grad_fn=<SubBackward0>), time elapsed: 2316.6842460632324 seconds\n",
      "Step 24600, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 2326.3448519706726 seconds\n",
      "Step 24700, loss: tensor(0.0801, grad_fn=<SubBackward0>), time elapsed: 2335.828551530838 seconds\n",
      "Step 24800, loss: tensor(0.0791, grad_fn=<SubBackward0>), time elapsed: 2345.324581384659 seconds\n",
      "Step 24900, loss: tensor(0.0808, grad_fn=<SubBackward0>), time elapsed: 2355.00940656662 seconds\n",
      "Step 25000, loss: tensor(0.0788, grad_fn=<SubBackward0>), time elapsed: 2364.4743843078613 seconds\n",
      "Step 25100, loss: tensor(0.0789, grad_fn=<SubBackward0>), time elapsed: 2373.8630323410034 seconds\n",
      "Step 25200, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 2383.4755823612213 seconds\n",
      "Step 25300, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 2392.9208147525787 seconds\n",
      "Step 25400, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 2402.51305603981 seconds\n",
      "Step 25500, loss: tensor(0.0785, grad_fn=<SubBackward0>), time elapsed: 2411.9013707637787 seconds\n",
      "Step 25600, loss: tensor(0.0826, grad_fn=<SubBackward0>), time elapsed: 2421.3246071338654 seconds\n",
      "Step 25700, loss: tensor(0.0821, grad_fn=<SubBackward0>), time elapsed: 2430.961516857147 seconds\n",
      "Step 25800, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 2440.410431623459 seconds\n",
      "Step 25900, loss: tensor(0.0809, grad_fn=<SubBackward0>), time elapsed: 2449.8441021442413 seconds\n",
      "Step 26000, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 2460.3464896678925 seconds\n",
      "Step 26100, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 2469.7810463905334 seconds\n",
      "Step 26200, loss: tensor(0.0818, grad_fn=<SubBackward0>), time elapsed: 2479.2324056625366 seconds\n",
      "Step 26300, loss: tensor(0.0816, grad_fn=<SubBackward0>), time elapsed: 2488.8294384479523 seconds\n",
      "Step 26400, loss: tensor(0.0791, grad_fn=<SubBackward0>), time elapsed: 2498.2849354743958 seconds\n",
      "Step 26500, loss: tensor(0.0793, grad_fn=<SubBackward0>), time elapsed: 2507.6849551200867 seconds\n",
      "Step 26600, loss: tensor(0.0808, grad_fn=<SubBackward0>), time elapsed: 2517.3126463890076 seconds\n",
      "Step 26700, loss: tensor(0.0795, grad_fn=<SubBackward0>), time elapsed: 2526.882595539093 seconds\n",
      "Step 26800, loss: tensor(0.0801, grad_fn=<SubBackward0>), time elapsed: 2536.470461368561 seconds\n",
      "Step 26900, loss: tensor(0.0821, grad_fn=<SubBackward0>), time elapsed: 2546.085232257843 seconds\n",
      "Step 27000, loss: tensor(0.0813, grad_fn=<SubBackward0>), time elapsed: 2555.459542989731 seconds\n",
      "Step 27100, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 2564.8272984027863 seconds\n",
      "Step 27200, loss: tensor(0.0813, grad_fn=<SubBackward0>), time elapsed: 2574.498807668686 seconds\n",
      "Step 27300, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 2583.9782149791718 seconds\n",
      "Step 27400, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 2593.340594291687 seconds\n",
      "Step 27500, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 2602.92430973053 seconds\n",
      "Step 27600, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 2612.299312353134 seconds\n",
      "Step 27700, loss: tensor(0.0819, grad_fn=<SubBackward0>), time elapsed: 2621.672399044037 seconds\n",
      "Step 27800, loss: tensor(0.0790, grad_fn=<SubBackward0>), time elapsed: 2631.275734424591 seconds\n",
      "Step 27900, loss: tensor(0.0815, grad_fn=<SubBackward0>), time elapsed: 2640.6863026618958 seconds\n",
      "Step 28000, loss: tensor(0.0807, grad_fn=<SubBackward0>), time elapsed: 2650.084575176239 seconds\n",
      "Step 28100, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 2659.688879966736 seconds\n",
      "Step 28200, loss: tensor(0.0818, grad_fn=<SubBackward0>), time elapsed: 2669.103535413742 seconds\n",
      "Step 28300, loss: tensor(0.0785, grad_fn=<SubBackward0>), time elapsed: 2678.461212873459 seconds\n",
      "Step 28400, loss: tensor(0.0809, grad_fn=<SubBackward0>), time elapsed: 2688.01752948761 seconds\n",
      "Step 28500, loss: tensor(0.0807, grad_fn=<SubBackward0>), time elapsed: 2697.4159955978394 seconds\n",
      "Step 28600, loss: tensor(0.0808, grad_fn=<SubBackward0>), time elapsed: 2706.8157567977905 seconds\n",
      "Step 28700, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 2716.3879778385162 seconds\n",
      "Step 28800, loss: tensor(0.0809, grad_fn=<SubBackward0>), time elapsed: 2725.7893283367157 seconds\n",
      "Step 28900, loss: tensor(0.0810, grad_fn=<SubBackward0>), time elapsed: 2735.3834760189056 seconds\n",
      "Step 29000, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 2744.7805831432343 seconds\n",
      "Step 29100, loss: tensor(0.0830, grad_fn=<SubBackward0>), time elapsed: 2754.185589313507 seconds\n",
      "Step 29200, loss: tensor(0.0808, grad_fn=<SubBackward0>), time elapsed: 2763.76641869545 seconds\n",
      "Step 29300, loss: tensor(0.0813, grad_fn=<SubBackward0>), time elapsed: 2773.1634435653687 seconds\n",
      "Step 29400, loss: tensor(0.0806, grad_fn=<SubBackward0>), time elapsed: 2782.580222129822 seconds\n",
      "Step 29500, loss: tensor(0.0809, grad_fn=<SubBackward0>), time elapsed: 2792.211899995804 seconds\n",
      "Step 29600, loss: tensor(0.0805, grad_fn=<SubBackward0>), time elapsed: 2801.583973169327 seconds\n",
      "Step 29700, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 2810.9601516723633 seconds\n",
      "Step 29800, loss: tensor(0.0787, grad_fn=<SubBackward0>), time elapsed: 2820.5613741874695 seconds\n",
      "Step 29900, loss: tensor(0.0819, grad_fn=<SubBackward0>), time elapsed: 2830.104559659958 seconds\n",
      "Step 30000, loss: tensor(0.0807, grad_fn=<SubBackward0>), time elapsed: 2839.613024711609 seconds\n",
      "Step 30100, loss: tensor(0.0825, grad_fn=<SubBackward0>), time elapsed: 2849.219744682312 seconds\n",
      "Step 30200, loss: tensor(0.0819, grad_fn=<SubBackward0>), time elapsed: 2858.612733602524 seconds\n",
      "Step 30300, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 2867.99303483963 seconds\n",
      "Step 30400, loss: tensor(0.0795, grad_fn=<SubBackward0>), time elapsed: 2877.5995202064514 seconds\n",
      "Step 30500, loss: tensor(0.0787, grad_fn=<SubBackward0>), time elapsed: 2886.9989767074585 seconds\n",
      "Step 30600, loss: tensor(0.0801, grad_fn=<SubBackward0>), time elapsed: 2896.411067724228 seconds\n",
      "Step 30700, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 2905.9897911548615 seconds\n",
      "Step 30800, loss: tensor(0.0807, grad_fn=<SubBackward0>), time elapsed: 2915.3825957775116 seconds\n",
      "Step 30900, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 2924.800513267517 seconds\n",
      "Step 31000, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 2934.378905057907 seconds\n",
      "Step 31100, loss: tensor(0.0792, grad_fn=<SubBackward0>), time elapsed: 2943.803045272827 seconds\n",
      "Step 31200, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 2953.16361618042 seconds\n",
      "Step 31300, loss: tensor(0.0809, grad_fn=<SubBackward0>), time elapsed: 2962.7136290073395 seconds\n",
      "Step 31400, loss: tensor(0.0786, grad_fn=<SubBackward0>), time elapsed: 2972.119311571121 seconds\n",
      "Step 31500, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 2981.51643037796 seconds\n",
      "Step 31600, loss: tensor(0.0806, grad_fn=<SubBackward0>), time elapsed: 2991.0802071094513 seconds\n",
      "Step 31700, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 3000.464640378952 seconds\n",
      "Step 31800, loss: tensor(0.0795, grad_fn=<SubBackward0>), time elapsed: 3009.844997882843 seconds\n",
      "Step 31900, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 3019.4109115600586 seconds\n",
      "Step 32000, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 3028.7859659194946 seconds\n",
      "Step 32100, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 3038.184994697571 seconds\n",
      "Step 32200, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 3047.7917630672455 seconds\n",
      "Step 32300, loss: tensor(0.0786, grad_fn=<SubBackward0>), time elapsed: 3057.2138187885284 seconds\n",
      "Step 32400, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 3066.6610045433044 seconds\n",
      "Step 32500, loss: tensor(0.0805, grad_fn=<SubBackward0>), time elapsed: 3076.27588224411 seconds\n",
      "Step 32600, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 3085.6613779067993 seconds\n",
      "Step 32700, loss: tensor(0.0791, grad_fn=<SubBackward0>), time elapsed: 3095.278454065323 seconds\n",
      "Step 32800, loss: tensor(0.0797, grad_fn=<SubBackward0>), time elapsed: 3104.682742357254 seconds\n",
      "Step 32900, loss: tensor(0.0810, grad_fn=<SubBackward0>), time elapsed: 3114.083967447281 seconds\n",
      "Step 33000, loss: tensor(0.0795, grad_fn=<SubBackward0>), time elapsed: 3123.761523246765 seconds\n",
      "Step 33100, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 3133.3034343719482 seconds\n",
      "Step 33200, loss: tensor(0.0805, grad_fn=<SubBackward0>), time elapsed: 3142.7716693878174 seconds\n",
      "Step 33300, loss: tensor(0.0802, grad_fn=<SubBackward0>), time elapsed: 3152.3603320121765 seconds\n",
      "Step 33400, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 3161.7447624206543 seconds\n",
      "Step 33500, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 3171.150157928467 seconds\n",
      "Step 33600, loss: tensor(0.0794, grad_fn=<SubBackward0>), time elapsed: 3180.7599506378174 seconds\n",
      "Step 33700, loss: tensor(0.0810, grad_fn=<SubBackward0>), time elapsed: 3190.2112832069397 seconds\n",
      "Step 33800, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 3199.661269903183 seconds\n",
      "Step 33900, loss: tensor(0.0819, grad_fn=<SubBackward0>), time elapsed: 3209.3014652729034 seconds\n",
      "Step 34000, loss: tensor(0.0808, grad_fn=<SubBackward0>), time elapsed: 3218.760987997055 seconds\n",
      "Step 34100, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 3228.212302684784 seconds\n",
      "Step 34200, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 3237.855305671692 seconds\n",
      "Step 34300, loss: tensor(0.0813, grad_fn=<SubBackward0>), time elapsed: 3247.244000196457 seconds\n",
      "Step 34400, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 3256.59712433815 seconds\n",
      "Step 34500, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 3266.146395921707 seconds\n",
      "Step 34600, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 3275.538140296936 seconds\n",
      "Step 34700, loss: tensor(0.0794, grad_fn=<SubBackward0>), time elapsed: 3284.965470314026 seconds\n",
      "Step 34800, loss: tensor(0.0795, grad_fn=<SubBackward0>), time elapsed: 3294.6221652030945 seconds\n",
      "Step 34900, loss: tensor(0.0803, grad_fn=<SubBackward0>), time elapsed: 3304.072594642639 seconds\n",
      "Step 35000, loss: tensor(0.0802, grad_fn=<SubBackward0>), time elapsed: 3313.5029351711273 seconds\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(0.9770, grad_fn=<SubBackward0>), time elapsed: 0.1147463321685791 seconds\n",
      "Step 100, loss: tensor(0.9106, grad_fn=<SubBackward0>), time elapsed: 10.018563985824585 seconds\n",
      "Step 200, loss: tensor(0.7701, grad_fn=<SubBackward0>), time elapsed: 19.390275478363037 seconds\n",
      "Step 300, loss: tensor(0.6348, grad_fn=<SubBackward0>), time elapsed: 28.81467080116272 seconds\n",
      "Step 400, loss: tensor(0.5274, grad_fn=<SubBackward0>), time elapsed: 38.29311466217041 seconds\n",
      "Step 500, loss: tensor(0.4399, grad_fn=<SubBackward0>), time elapsed: 47.61915302276611 seconds\n",
      "Step 600, loss: tensor(0.3647, grad_fn=<SubBackward0>), time elapsed: 57.15799403190613 seconds\n",
      "Step 700, loss: tensor(0.3086, grad_fn=<SubBackward0>), time elapsed: 66.48420405387878 seconds\n",
      "Step 800, loss: tensor(0.2644, grad_fn=<SubBackward0>), time elapsed: 75.8248438835144 seconds\n",
      "Step 900, loss: tensor(0.2356, grad_fn=<SubBackward0>), time elapsed: 85.35115170478821 seconds\n",
      "Step 1000, loss: tensor(0.2211, grad_fn=<SubBackward0>), time elapsed: 94.69196462631226 seconds\n",
      "Step 1100, loss: tensor(0.2049, grad_fn=<SubBackward0>), time elapsed: 104.05143213272095 seconds\n",
      "Step 1200, loss: tensor(0.1943, grad_fn=<SubBackward0>), time elapsed: 113.6027159690857 seconds\n",
      "Step 1300, loss: tensor(0.1854, grad_fn=<SubBackward0>), time elapsed: 122.97362875938416 seconds\n",
      "Step 1400, loss: tensor(0.1770, grad_fn=<SubBackward0>), time elapsed: 132.3581418991089 seconds\n",
      "Step 1500, loss: tensor(0.1684, grad_fn=<SubBackward0>), time elapsed: 141.90222764015198 seconds\n",
      "Step 1600, loss: tensor(0.1638, grad_fn=<SubBackward0>), time elapsed: 151.27633333206177 seconds\n",
      "Step 1700, loss: tensor(0.1608, grad_fn=<SubBackward0>), time elapsed: 160.61076545715332 seconds\n",
      "Step 1800, loss: tensor(0.1548, grad_fn=<SubBackward0>), time elapsed: 170.1371066570282 seconds\n",
      "Step 1900, loss: tensor(0.1489, grad_fn=<SubBackward0>), time elapsed: 179.50721192359924 seconds\n",
      "Step 2000, loss: tensor(0.1472, grad_fn=<SubBackward0>), time elapsed: 188.83105421066284 seconds\n",
      "Step 2100, loss: tensor(0.1448, grad_fn=<SubBackward0>), time elapsed: 198.29700469970703 seconds\n",
      "Step 2200, loss: tensor(0.1398, grad_fn=<SubBackward0>), time elapsed: 207.6604733467102 seconds\n",
      "Step 2300, loss: tensor(0.1390, grad_fn=<SubBackward0>), time elapsed: 217.1576840877533 seconds\n",
      "Step 2400, loss: tensor(0.1363, grad_fn=<SubBackward0>), time elapsed: 226.49840188026428 seconds\n",
      "Step 2500, loss: tensor(0.1323, grad_fn=<SubBackward0>), time elapsed: 235.8599739074707 seconds\n",
      "Step 2600, loss: tensor(0.1295, grad_fn=<SubBackward0>), time elapsed: 245.3511381149292 seconds\n",
      "Step 2700, loss: tensor(0.1281, grad_fn=<SubBackward0>), time elapsed: 254.675124168396 seconds\n",
      "Step 2800, loss: tensor(0.1257, grad_fn=<SubBackward0>), time elapsed: 264.01770997047424 seconds\n",
      "Step 2900, loss: tensor(0.1247, grad_fn=<SubBackward0>), time elapsed: 273.5941114425659 seconds\n",
      "Step 3000, loss: tensor(0.1219, grad_fn=<SubBackward0>), time elapsed: 282.9368531703949 seconds\n",
      "Step 3100, loss: tensor(0.1213, grad_fn=<SubBackward0>), time elapsed: 292.2880072593689 seconds\n",
      "Step 3200, loss: tensor(0.1178, grad_fn=<SubBackward0>), time elapsed: 301.81037950515747 seconds\n",
      "Step 3300, loss: tensor(0.1174, grad_fn=<SubBackward0>), time elapsed: 311.1281223297119 seconds\n",
      "Step 3400, loss: tensor(0.1139, grad_fn=<SubBackward0>), time elapsed: 320.4751172065735 seconds\n",
      "Step 3500, loss: tensor(0.1135, grad_fn=<SubBackward0>), time elapsed: 330.0765047073364 seconds\n",
      "Step 3600, loss: tensor(0.1133, grad_fn=<SubBackward0>), time elapsed: 339.4482033252716 seconds\n",
      "Step 3700, loss: tensor(0.1113, grad_fn=<SubBackward0>), time elapsed: 348.76190853118896 seconds\n",
      "Step 3800, loss: tensor(0.1120, grad_fn=<SubBackward0>), time elapsed: 358.3007369041443 seconds\n",
      "Step 3900, loss: tensor(0.1108, grad_fn=<SubBackward0>), time elapsed: 367.66456031799316 seconds\n",
      "Step 4000, loss: tensor(0.1086, grad_fn=<SubBackward0>), time elapsed: 377.203227519989 seconds\n",
      "Step 4100, loss: tensor(0.1073, grad_fn=<SubBackward0>), time elapsed: 386.5846526622772 seconds\n",
      "Step 4200, loss: tensor(0.1075, grad_fn=<SubBackward0>), time elapsed: 395.95347452163696 seconds\n",
      "Step 4300, loss: tensor(0.1091, grad_fn=<SubBackward0>), time elapsed: 405.49038767814636 seconds\n",
      "Step 4400, loss: tensor(0.1047, grad_fn=<SubBackward0>), time elapsed: 414.9195668697357 seconds\n",
      "Step 4500, loss: tensor(0.1029, grad_fn=<SubBackward0>), time elapsed: 424.3364357948303 seconds\n",
      "Step 4600, loss: tensor(0.1041, grad_fn=<SubBackward0>), time elapsed: 433.83773851394653 seconds\n",
      "Step 4700, loss: tensor(0.1035, grad_fn=<SubBackward0>), time elapsed: 443.19821333885193 seconds\n",
      "Step 4800, loss: tensor(0.1037, grad_fn=<SubBackward0>), time elapsed: 452.55670189857483 seconds\n",
      "Step 4900, loss: tensor(0.1071, grad_fn=<SubBackward0>), time elapsed: 462.03921818733215 seconds\n",
      "Step 5000, loss: tensor(0.1035, grad_fn=<SubBackward0>), time elapsed: 471.372594833374 seconds\n",
      "Step 5100, loss: tensor(0.1034, grad_fn=<SubBackward0>), time elapsed: 480.7206480503082 seconds\n",
      "Step 5200, loss: tensor(0.1015, grad_fn=<SubBackward0>), time elapsed: 490.2438747882843 seconds\n",
      "Step 5300, loss: tensor(0.1007, grad_fn=<SubBackward0>), time elapsed: 499.57727336883545 seconds\n",
      "Step 5400, loss: tensor(0.1005, grad_fn=<SubBackward0>), time elapsed: 508.9378457069397 seconds\n",
      "Step 5500, loss: tensor(0.1031, grad_fn=<SubBackward0>), time elapsed: 518.4380505084991 seconds\n",
      "Step 5600, loss: tensor(0.1004, grad_fn=<SubBackward0>), time elapsed: 527.7852222919464 seconds\n",
      "Step 5700, loss: tensor(0.0994, grad_fn=<SubBackward0>), time elapsed: 537.2906529903412 seconds\n",
      "Step 5800, loss: tensor(0.1002, grad_fn=<SubBackward0>), time elapsed: 546.6739106178284 seconds\n",
      "Step 5900, loss: tensor(0.0999, grad_fn=<SubBackward0>), time elapsed: 556.0339767932892 seconds\n",
      "Step 6000, loss: tensor(0.0991, grad_fn=<SubBackward0>), time elapsed: 565.5712325572968 seconds\n",
      "Step 6100, loss: tensor(0.1001, grad_fn=<SubBackward0>), time elapsed: 574.9369769096375 seconds\n",
      "Step 6200, loss: tensor(0.0973, grad_fn=<SubBackward0>), time elapsed: 584.3006415367126 seconds\n",
      "Step 6300, loss: tensor(0.1012, grad_fn=<SubBackward0>), time elapsed: 593.8186531066895 seconds\n",
      "Step 6400, loss: tensor(0.0956, grad_fn=<SubBackward0>), time elapsed: 603.2141244411469 seconds\n",
      "Step 6500, loss: tensor(0.0983, grad_fn=<SubBackward0>), time elapsed: 612.8475117683411 seconds\n",
      "Step 6600, loss: tensor(0.0968, grad_fn=<SubBackward0>), time elapsed: 622.4143500328064 seconds\n",
      "Step 6700, loss: tensor(0.0974, grad_fn=<SubBackward0>), time elapsed: 631.7549684047699 seconds\n",
      "Step 6800, loss: tensor(0.0984, grad_fn=<SubBackward0>), time elapsed: 641.139417886734 seconds\n",
      "Step 6900, loss: tensor(0.0974, grad_fn=<SubBackward0>), time elapsed: 650.6496534347534 seconds\n",
      "Step 7000, loss: tensor(0.0982, grad_fn=<SubBackward0>), time elapsed: 659.9858605861664 seconds\n",
      "Step 7100, loss: tensor(0.0985, grad_fn=<SubBackward0>), time elapsed: 669.3732607364655 seconds\n",
      "Step 7200, loss: tensor(0.0949, grad_fn=<SubBackward0>), time elapsed: 678.8860721588135 seconds\n",
      "Step 7300, loss: tensor(0.0948, grad_fn=<SubBackward0>), time elapsed: 688.2518882751465 seconds\n",
      "Step 7400, loss: tensor(0.0933, grad_fn=<SubBackward0>), time elapsed: 697.851131439209 seconds\n",
      "Step 7500, loss: tensor(0.0952, grad_fn=<SubBackward0>), time elapsed: 707.2106375694275 seconds\n",
      "Step 7600, loss: tensor(0.0962, grad_fn=<SubBackward0>), time elapsed: 716.5849058628082 seconds\n",
      "Step 7700, loss: tensor(0.0957, grad_fn=<SubBackward0>), time elapsed: 726.184014081955 seconds\n",
      "Step 7800, loss: tensor(0.0939, grad_fn=<SubBackward0>), time elapsed: 735.5945336818695 seconds\n",
      "Step 7900, loss: tensor(0.0948, grad_fn=<SubBackward0>), time elapsed: 745.0010058879852 seconds\n",
      "Step 8000, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 754.5566203594208 seconds\n",
      "Step 8100, loss: tensor(0.0948, grad_fn=<SubBackward0>), time elapsed: 763.9494490623474 seconds\n",
      "Step 8200, loss: tensor(0.0948, grad_fn=<SubBackward0>), time elapsed: 773.3005096912384 seconds\n",
      "Step 8300, loss: tensor(0.0958, grad_fn=<SubBackward0>), time elapsed: 782.789468050003 seconds\n",
      "Step 8400, loss: tensor(0.0932, grad_fn=<SubBackward0>), time elapsed: 792.1591455936432 seconds\n",
      "Step 8500, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 801.4999225139618 seconds\n",
      "Step 8600, loss: tensor(0.0930, grad_fn=<SubBackward0>), time elapsed: 811.0387618541718 seconds\n",
      "Step 8700, loss: tensor(0.0923, grad_fn=<SubBackward0>), time elapsed: 820.4191043376923 seconds\n",
      "Step 8800, loss: tensor(0.0917, grad_fn=<SubBackward0>), time elapsed: 829.8140435218811 seconds\n",
      "Step 8900, loss: tensor(0.0944, grad_fn=<SubBackward0>), time elapsed: 839.3382797241211 seconds\n",
      "Step 9000, loss: tensor(0.0933, grad_fn=<SubBackward0>), time elapsed: 848.7447216510773 seconds\n",
      "Step 9100, loss: tensor(0.0937, grad_fn=<SubBackward0>), time elapsed: 858.0989527702332 seconds\n",
      "Step 9200, loss: tensor(0.0939, grad_fn=<SubBackward0>), time elapsed: 867.65864610672 seconds\n",
      "Step 9300, loss: tensor(0.0921, grad_fn=<SubBackward0>), time elapsed: 877.1015582084656 seconds\n",
      "Step 9400, loss: tensor(0.0897, grad_fn=<SubBackward0>), time elapsed: 886.6864681243896 seconds\n",
      "Step 9500, loss: tensor(0.0927, grad_fn=<SubBackward0>), time elapsed: 896.0207240581512 seconds\n",
      "Step 9600, loss: tensor(0.0916, grad_fn=<SubBackward0>), time elapsed: 905.359343290329 seconds\n",
      "Step 9700, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 914.9023642539978 seconds\n",
      "Step 9800, loss: tensor(0.0916, grad_fn=<SubBackward0>), time elapsed: 924.2368400096893 seconds\n",
      "Step 9900, loss: tensor(0.0892, grad_fn=<SubBackward0>), time elapsed: 933.7809391021729 seconds\n",
      "Step 10000, loss: tensor(0.0907, grad_fn=<SubBackward0>), time elapsed: 943.4166815280914 seconds\n",
      "Step 10100, loss: tensor(0.0954, grad_fn=<SubBackward0>), time elapsed: 952.7991795539856 seconds\n",
      "Step 10200, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 962.1836986541748 seconds\n",
      "Step 10300, loss: tensor(0.0912, grad_fn=<SubBackward0>), time elapsed: 971.7476704120636 seconds\n",
      "Step 10400, loss: tensor(0.0943, grad_fn=<SubBackward0>), time elapsed: 981.0904359817505 seconds\n",
      "Step 10500, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 990.4872906208038 seconds\n",
      "Step 10600, loss: tensor(0.0917, grad_fn=<SubBackward0>), time elapsed: 1000.0177924633026 seconds\n",
      "Step 10700, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 1009.3899381160736 seconds\n",
      "Step 10800, loss: tensor(0.0921, grad_fn=<SubBackward0>), time elapsed: 1018.775386095047 seconds\n",
      "Step 10900, loss: tensor(0.0905, grad_fn=<SubBackward0>), time elapsed: 1028.3381593227386 seconds\n",
      "Step 11000, loss: tensor(0.0926, grad_fn=<SubBackward0>), time elapsed: 1037.7349824905396 seconds\n",
      "Step 11100, loss: tensor(0.0924, grad_fn=<SubBackward0>), time elapsed: 1047.086309671402 seconds\n",
      "Step 11200, loss: tensor(0.0912, grad_fn=<SubBackward0>), time elapsed: 1056.6430547237396 seconds\n",
      "Step 11300, loss: tensor(0.0932, grad_fn=<SubBackward0>), time elapsed: 1066.0103814601898 seconds\n",
      "Step 11400, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 1075.523976802826 seconds\n",
      "Step 11500, loss: tensor(0.0910, grad_fn=<SubBackward0>), time elapsed: 1084.840045928955 seconds\n",
      "Step 11600, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 1094.191647052765 seconds\n",
      "Step 11700, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 1103.7235264778137 seconds\n",
      "Step 11800, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 1113.1198241710663 seconds\n",
      "Step 11900, loss: tensor(0.0916, grad_fn=<SubBackward0>), time elapsed: 1122.5451276302338 seconds\n",
      "Step 12000, loss: tensor(0.0940, grad_fn=<SubBackward0>), time elapsed: 1132.1004710197449 seconds\n",
      "Step 12100, loss: tensor(0.0903, grad_fn=<SubBackward0>), time elapsed: 1141.439923286438 seconds\n",
      "Step 12200, loss: tensor(0.0938, grad_fn=<SubBackward0>), time elapsed: 1150.774953842163 seconds\n",
      "Step 12300, loss: tensor(0.0927, grad_fn=<SubBackward0>), time elapsed: 1160.2767088413239 seconds\n",
      "Step 12400, loss: tensor(0.0942, grad_fn=<SubBackward0>), time elapsed: 1169.5977461338043 seconds\n",
      "Step 12500, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 1178.881177186966 seconds\n",
      "Step 12600, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 1188.4014024734497 seconds\n",
      "Step 12700, loss: tensor(0.0930, grad_fn=<SubBackward0>), time elapsed: 1197.7057893276215 seconds\n",
      "Step 12800, loss: tensor(0.0941, grad_fn=<SubBackward0>), time elapsed: 1207.0192556381226 seconds\n",
      "Step 12900, loss: tensor(0.0919, grad_fn=<SubBackward0>), time elapsed: 1216.5073664188385 seconds\n",
      "Step 13000, loss: tensor(0.0892, grad_fn=<SubBackward0>), time elapsed: 1225.8540420532227 seconds\n",
      "Step 13100, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 1235.2729721069336 seconds\n",
      "Step 13200, loss: tensor(0.0931, grad_fn=<SubBackward0>), time elapsed: 1245.0005025863647 seconds\n",
      "Step 13300, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 1254.493420124054 seconds\n",
      "Step 13400, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 1263.8393290042877 seconds\n",
      "Step 13500, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 1273.344960451126 seconds\n",
      "Step 13600, loss: tensor(0.0925, grad_fn=<SubBackward0>), time elapsed: 1282.7159564495087 seconds\n",
      "Step 13700, loss: tensor(0.0927, grad_fn=<SubBackward0>), time elapsed: 1292.2557106018066 seconds\n",
      "Step 13800, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 1301.5775706768036 seconds\n",
      "Step 13900, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 1310.9370460510254 seconds\n",
      "Step 14000, loss: tensor(0.0912, grad_fn=<SubBackward0>), time elapsed: 1320.4717965126038 seconds\n",
      "Step 14100, loss: tensor(0.0928, grad_fn=<SubBackward0>), time elapsed: 1329.8471639156342 seconds\n",
      "Step 14200, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 1339.211490392685 seconds\n",
      "Step 14300, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 1348.7170085906982 seconds\n",
      "Step 14400, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 1358.0577392578125 seconds\n",
      "Step 14500, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 1367.3973495960236 seconds\n",
      "Step 14600, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 1376.953073501587 seconds\n",
      "Step 14700, loss: tensor(0.0926, grad_fn=<SubBackward0>), time elapsed: 1386.2582671642303 seconds\n",
      "Step 14800, loss: tensor(0.0925, grad_fn=<SubBackward0>), time elapsed: 1395.5812866687775 seconds\n",
      "Step 14900, loss: tensor(0.0918, grad_fn=<SubBackward0>), time elapsed: 1405.0941245555878 seconds\n",
      "Step 15000, loss: tensor(0.0939, grad_fn=<SubBackward0>), time elapsed: 1414.4169278144836 seconds\n",
      "Step 15100, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 1423.7405750751495 seconds\n",
      "Step 15200, loss: tensor(0.0896, grad_fn=<SubBackward0>), time elapsed: 1433.2647957801819 seconds\n",
      "Step 15300, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 1442.6580476760864 seconds\n",
      "Step 15400, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 1452.0175297260284 seconds\n",
      "Step 15500, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 1461.5532824993134 seconds\n",
      "Step 15600, loss: tensor(0.0928, grad_fn=<SubBackward0>), time elapsed: 1470.9179270267487 seconds\n",
      "Step 15700, loss: tensor(0.0933, grad_fn=<SubBackward0>), time elapsed: 1480.237800836563 seconds\n",
      "Step 15800, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 1489.7526915073395 seconds\n",
      "Step 15900, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 1499.1194801330566 seconds\n",
      "Step 16000, loss: tensor(0.0922, grad_fn=<SubBackward0>), time elapsed: 1508.6874577999115 seconds\n",
      "Step 16100, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 1518.013659954071 seconds\n",
      "Step 16200, loss: tensor(0.0927, grad_fn=<SubBackward0>), time elapsed: 1527.3824799060822 seconds\n",
      "Step 16300, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 1536.9369218349457 seconds\n",
      "Step 16400, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 1546.3005917072296 seconds\n",
      "Step 16500, loss: tensor(0.0915, grad_fn=<SubBackward0>), time elapsed: 1555.6601161956787 seconds\n",
      "Step 16600, loss: tensor(0.0930, grad_fn=<SubBackward0>), time elapsed: 1565.2449703216553 seconds\n",
      "Step 16700, loss: tensor(0.0929, grad_fn=<SubBackward0>), time elapsed: 1574.616775751114 seconds\n",
      "Step 16800, loss: tensor(0.0911, grad_fn=<SubBackward0>), time elapsed: 1583.9765000343323 seconds\n",
      "Step 16900, loss: tensor(0.0948, grad_fn=<SubBackward0>), time elapsed: 1593.4902348518372 seconds\n",
      "Step 17000, loss: tensor(0.0927, grad_fn=<SubBackward0>), time elapsed: 1602.8506391048431 seconds\n",
      "Step 17100, loss: tensor(0.0923, grad_fn=<SubBackward0>), time elapsed: 1612.2173972129822 seconds\n",
      "Step 17200, loss: tensor(0.0918, grad_fn=<SubBackward0>), time elapsed: 1621.8157041072845 seconds\n",
      "Step 17300, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 1631.2011847496033 seconds\n",
      "Step 17400, loss: tensor(0.0927, grad_fn=<SubBackward0>), time elapsed: 1640.5586071014404 seconds\n",
      "Step 17500, loss: tensor(0.0886, grad_fn=<SubBackward0>), time elapsed: 1650.1182897090912 seconds\n",
      "Step 17600, loss: tensor(0.0932, grad_fn=<SubBackward0>), time elapsed: 1659.4566977024078 seconds\n",
      "Step 17700, loss: tensor(0.0931, grad_fn=<SubBackward0>), time elapsed: 1668.815324306488 seconds\n",
      "Step 17800, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 1678.346025466919 seconds\n",
      "Step 17900, loss: tensor(0.0929, grad_fn=<SubBackward0>), time elapsed: 1687.6771919727325 seconds\n",
      "Step 18000, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 1697.0159163475037 seconds\n",
      "Step 18100, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 1706.5535471439362 seconds\n",
      "Step 18200, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 1715.9432051181793 seconds\n",
      "Step 18300, loss: tensor(0.0952, grad_fn=<SubBackward0>), time elapsed: 1725.534303188324 seconds\n",
      "Step 18400, loss: tensor(0.0941, grad_fn=<SubBackward0>), time elapsed: 1735.2470598220825 seconds\n",
      "Step 18500, loss: tensor(0.0933, grad_fn=<SubBackward0>), time elapsed: 1744.6546704769135 seconds\n",
      "Step 18600, loss: tensor(0.0923, grad_fn=<SubBackward0>), time elapsed: 1754.1768190860748 seconds\n",
      "Step 18700, loss: tensor(0.0925, grad_fn=<SubBackward0>), time elapsed: 1763.4766852855682 seconds\n",
      "Step 18800, loss: tensor(0.0918, grad_fn=<SubBackward0>), time elapsed: 1772.8068358898163 seconds\n",
      "Step 18900, loss: tensor(0.0961, grad_fn=<SubBackward0>), time elapsed: 1782.3189432621002 seconds\n",
      "Step 19000, loss: tensor(0.0915, grad_fn=<SubBackward0>), time elapsed: 1791.657573223114 seconds\n",
      "Step 19100, loss: tensor(0.0954, grad_fn=<SubBackward0>), time elapsed: 1801.0439279079437 seconds\n",
      "Step 19200, loss: tensor(0.0912, grad_fn=<SubBackward0>), time elapsed: 1810.602819442749 seconds\n",
      "Step 19300, loss: tensor(0.0958, grad_fn=<SubBackward0>), time elapsed: 1819.9768676757812 seconds\n",
      "Step 19400, loss: tensor(0.0949, grad_fn=<SubBackward0>), time elapsed: 1829.391727924347 seconds\n",
      "Step 19500, loss: tensor(0.0927, grad_fn=<SubBackward0>), time elapsed: 1839.0472881793976 seconds\n",
      "Step 19600, loss: tensor(0.0929, grad_fn=<SubBackward0>), time elapsed: 1848.4192814826965 seconds\n",
      "Step 19700, loss: tensor(0.0901, grad_fn=<SubBackward0>), time elapsed: 1857.8184044361115 seconds\n",
      "Step 19800, loss: tensor(0.0910, grad_fn=<SubBackward0>), time elapsed: 1867.396833896637 seconds\n",
      "Step 19900, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 1876.7604761123657 seconds\n",
      "Step 20000, loss: tensor(0.0940, grad_fn=<SubBackward0>), time elapsed: 1886.1199691295624 seconds\n",
      "Step 20100, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 1895.6379489898682 seconds\n",
      "Step 20200, loss: tensor(0.0931, grad_fn=<SubBackward0>), time elapsed: 1904.9807534217834 seconds\n",
      "Step 20300, loss: tensor(0.0903, grad_fn=<SubBackward0>), time elapsed: 1914.30162191391 seconds\n",
      "Step 20400, loss: tensor(0.0922, grad_fn=<SubBackward0>), time elapsed: 1923.8457045555115 seconds\n",
      "Step 20500, loss: tensor(0.0937, grad_fn=<SubBackward0>), time elapsed: 1933.201026916504 seconds\n",
      "Step 20600, loss: tensor(0.0932, grad_fn=<SubBackward0>), time elapsed: 1942.5969576835632 seconds\n",
      "Step 20700, loss: tensor(0.0931, grad_fn=<SubBackward0>), time elapsed: 1952.1327767372131 seconds\n",
      "Step 20800, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 1961.5089778900146 seconds\n",
      "Step 20900, loss: tensor(0.0916, grad_fn=<SubBackward0>), time elapsed: 1970.9585165977478 seconds\n",
      "Step 21000, loss: tensor(0.0930, grad_fn=<SubBackward0>), time elapsed: 1980.7408668994904 seconds\n",
      "Step 21100, loss: tensor(0.0895, grad_fn=<SubBackward0>), time elapsed: 1990.258533000946 seconds\n",
      "Step 21200, loss: tensor(0.0929, grad_fn=<SubBackward0>), time elapsed: 1999.8228602409363 seconds\n",
      "Step 21300, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 2009.2520081996918 seconds\n",
      "Step 21400, loss: tensor(0.0925, grad_fn=<SubBackward0>), time elapsed: 2018.596966266632 seconds\n",
      "Step 21500, loss: tensor(0.0960, grad_fn=<SubBackward0>), time elapsed: 2028.1424612998962 seconds\n",
      "Step 21600, loss: tensor(0.0928, grad_fn=<SubBackward0>), time elapsed: 2037.4989256858826 seconds\n",
      "Step 21700, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 2046.9359493255615 seconds\n",
      "Step 21800, loss: tensor(0.0925, grad_fn=<SubBackward0>), time elapsed: 2056.4969465732574 seconds\n",
      "Step 21900, loss: tensor(0.0928, grad_fn=<SubBackward0>), time elapsed: 2065.87863945961 seconds\n",
      "Step 22000, loss: tensor(0.0905, grad_fn=<SubBackward0>), time elapsed: 2075.2710847854614 seconds\n",
      "Step 22100, loss: tensor(0.0929, grad_fn=<SubBackward0>), time elapsed: 2084.8635017871857 seconds\n",
      "Step 22200, loss: tensor(0.0917, grad_fn=<SubBackward0>), time elapsed: 2094.3391683101654 seconds\n",
      "Step 22300, loss: tensor(0.0915, grad_fn=<SubBackward0>), time elapsed: 2103.7975647449493 seconds\n",
      "Step 22400, loss: tensor(0.0918, grad_fn=<SubBackward0>), time elapsed: 2113.7004206180573 seconds\n",
      "Step 22500, loss: tensor(0.0924, grad_fn=<SubBackward0>), time elapsed: 2123.095170021057 seconds\n",
      "Step 22600, loss: tensor(0.0926, grad_fn=<SubBackward0>), time elapsed: 2132.4882340431213 seconds\n",
      "Step 22700, loss: tensor(0.0903, grad_fn=<SubBackward0>), time elapsed: 2142.2921662330627 seconds\n",
      "Step 22800, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 2151.684809446335 seconds\n",
      "Step 22900, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 2161.126447916031 seconds\n",
      "Step 23000, loss: tensor(0.0930, grad_fn=<SubBackward0>), time elapsed: 2170.737595319748 seconds\n",
      "Step 23100, loss: tensor(0.0945, grad_fn=<SubBackward0>), time elapsed: 2180.1294627189636 seconds\n",
      "Step 23200, loss: tensor(0.0939, grad_fn=<SubBackward0>), time elapsed: 2189.5168302059174 seconds\n",
      "Step 23300, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 2199.115090608597 seconds\n",
      "Step 23400, loss: tensor(0.0912, grad_fn=<SubBackward0>), time elapsed: 2208.5467393398285 seconds\n",
      "Step 23500, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 2217.95396733284 seconds\n",
      "Step 23600, loss: tensor(0.0929, grad_fn=<SubBackward0>), time elapsed: 2229.217712163925 seconds\n",
      "Step 23700, loss: tensor(0.0934, grad_fn=<SubBackward0>), time elapsed: 2239.0527164936066 seconds\n",
      "Step 23800, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 2248.633392572403 seconds\n",
      "Step 23900, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 2258.33958363533 seconds\n",
      "Step 24000, loss: tensor(0.0940, grad_fn=<SubBackward0>), time elapsed: 2267.7371711730957 seconds\n",
      "Step 24100, loss: tensor(0.0924, grad_fn=<SubBackward0>), time elapsed: 2277.3632447719574 seconds\n",
      "Step 24200, loss: tensor(0.0905, grad_fn=<SubBackward0>), time elapsed: 2286.814801454544 seconds\n",
      "Step 24300, loss: tensor(0.0929, grad_fn=<SubBackward0>), time elapsed: 2297.0724108219147 seconds\n",
      "Step 24400, loss: tensor(0.0941, grad_fn=<SubBackward0>), time elapsed: 2308.432219982147 seconds\n",
      "Step 24500, loss: tensor(0.0940, grad_fn=<SubBackward0>), time elapsed: 2318.015563726425 seconds\n",
      "Step 24600, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 2327.657596349716 seconds\n",
      "Step 24700, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 2337.475964784622 seconds\n",
      "Step 24800, loss: tensor(0.0936, grad_fn=<SubBackward0>), time elapsed: 2347.2022545337677 seconds\n",
      "Step 24900, loss: tensor(0.0915, grad_fn=<SubBackward0>), time elapsed: 2356.8503136634827 seconds\n",
      "Step 25000, loss: tensor(0.0903, grad_fn=<SubBackward0>), time elapsed: 2366.6421024799347 seconds\n",
      "Step 25100, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 2376.231795310974 seconds\n",
      "Step 25200, loss: tensor(0.0910, grad_fn=<SubBackward0>), time elapsed: 2385.8623678684235 seconds\n",
      "Step 25300, loss: tensor(0.0905, grad_fn=<SubBackward0>), time elapsed: 2395.6590588092804 seconds\n",
      "Step 25400, loss: tensor(0.0949, grad_fn=<SubBackward0>), time elapsed: 2405.225417137146 seconds\n",
      "Step 25500, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 2414.9025733470917 seconds\n",
      "Step 25600, loss: tensor(0.0898, grad_fn=<SubBackward0>), time elapsed: 2424.739999294281 seconds\n",
      "Step 25700, loss: tensor(0.0901, grad_fn=<SubBackward0>), time elapsed: 2434.3131239414215 seconds\n",
      "Step 25800, loss: tensor(0.0912, grad_fn=<SubBackward0>), time elapsed: 2444.397935628891 seconds\n",
      "Step 25900, loss: tensor(0.0933, grad_fn=<SubBackward0>), time elapsed: 2454.321152448654 seconds\n",
      "Step 26000, loss: tensor(0.0899, grad_fn=<SubBackward0>), time elapsed: 2463.8550293445587 seconds\n",
      "Step 26100, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 2473.392587661743 seconds\n",
      "Step 26200, loss: tensor(0.0924, grad_fn=<SubBackward0>), time elapsed: 2483.059629917145 seconds\n",
      "Step 26300, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 2492.5147547721863 seconds\n",
      "Step 26400, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 2502.0178096294403 seconds\n",
      "Step 26500, loss: tensor(0.0923, grad_fn=<SubBackward0>), time elapsed: 2511.7040407657623 seconds\n",
      "Step 26600, loss: tensor(0.0927, grad_fn=<SubBackward0>), time elapsed: 2521.224351167679 seconds\n",
      "Step 26700, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 2530.7409315109253 seconds\n",
      "Step 26800, loss: tensor(0.0912, grad_fn=<SubBackward0>), time elapsed: 2540.410908937454 seconds\n",
      "Step 26900, loss: tensor(0.0951, grad_fn=<SubBackward0>), time elapsed: 2549.851989030838 seconds\n",
      "Step 27000, loss: tensor(0.0924, grad_fn=<SubBackward0>), time elapsed: 2559.280600309372 seconds\n",
      "Step 27100, loss: tensor(0.0899, grad_fn=<SubBackward0>), time elapsed: 2568.8857781887054 seconds\n",
      "Step 27200, loss: tensor(0.0937, grad_fn=<SubBackward0>), time elapsed: 2578.347178697586 seconds\n",
      "Step 27300, loss: tensor(0.0953, grad_fn=<SubBackward0>), time elapsed: 2587.981659889221 seconds\n",
      "Step 27400, loss: tensor(0.0922, grad_fn=<SubBackward0>), time elapsed: 2597.597248315811 seconds\n",
      "Step 27500, loss: tensor(0.0931, grad_fn=<SubBackward0>), time elapsed: 2606.9859488010406 seconds\n",
      "Step 27600, loss: tensor(0.0955, grad_fn=<SubBackward0>), time elapsed: 2616.553846359253 seconds\n",
      "Step 27700, loss: tensor(0.0917, grad_fn=<SubBackward0>), time elapsed: 2625.944905281067 seconds\n",
      "Step 27800, loss: tensor(0.0926, grad_fn=<SubBackward0>), time elapsed: 2635.3286011219025 seconds\n",
      "Step 27900, loss: tensor(0.0946, grad_fn=<SubBackward0>), time elapsed: 2644.894254207611 seconds\n",
      "Step 28000, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 2654.278772354126 seconds\n",
      "Step 28100, loss: tensor(0.0943, grad_fn=<SubBackward0>), time elapsed: 2663.65967464447 seconds\n",
      "Step 28200, loss: tensor(0.0940, grad_fn=<SubBackward0>), time elapsed: 2673.194842815399 seconds\n",
      "Step 28300, loss: tensor(0.0938, grad_fn=<SubBackward0>), time elapsed: 2682.5596714019775 seconds\n",
      "Step 28400, loss: tensor(0.0921, grad_fn=<SubBackward0>), time elapsed: 2691.976261615753 seconds\n",
      "Step 28500, loss: tensor(0.0895, grad_fn=<SubBackward0>), time elapsed: 2701.5286977291107 seconds\n",
      "Step 28600, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 2710.9410405158997 seconds\n",
      "Step 28700, loss: tensor(0.0932, grad_fn=<SubBackward0>), time elapsed: 2720.359080553055 seconds\n",
      "Step 28800, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 2729.9381306171417 seconds\n",
      "Step 28900, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 2739.3210604190826 seconds\n",
      "Step 29000, loss: tensor(0.0925, grad_fn=<SubBackward0>), time elapsed: 2748.878499507904 seconds\n",
      "Step 29100, loss: tensor(0.0917, grad_fn=<SubBackward0>), time elapsed: 2758.474622964859 seconds\n",
      "Step 29200, loss: tensor(0.0922, grad_fn=<SubBackward0>), time elapsed: 2767.838541507721 seconds\n",
      "Step 29300, loss: tensor(0.0939, grad_fn=<SubBackward0>), time elapsed: 2777.2771859169006 seconds\n",
      "Step 29400, loss: tensor(0.0942, grad_fn=<SubBackward0>), time elapsed: 2786.8772101402283 seconds\n",
      "Step 29500, loss: tensor(0.0919, grad_fn=<SubBackward0>), time elapsed: 2796.442137479782 seconds\n",
      "Step 29600, loss: tensor(0.0919, grad_fn=<SubBackward0>), time elapsed: 2805.8175704479218 seconds\n",
      "Step 29700, loss: tensor(0.0916, grad_fn=<SubBackward0>), time elapsed: 2815.4527354240417 seconds\n",
      "Step 29800, loss: tensor(0.0948, grad_fn=<SubBackward0>), time elapsed: 2824.844213962555 seconds\n",
      "Step 29900, loss: tensor(0.0922, grad_fn=<SubBackward0>), time elapsed: 2834.214989900589 seconds\n",
      "Step 30000, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 2843.863052845001 seconds\n",
      "Step 30100, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 2853.288506269455 seconds\n",
      "Step 30200, loss: tensor(0.0922, grad_fn=<SubBackward0>), time elapsed: 2862.730546236038 seconds\n",
      "Step 30300, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 2872.319447994232 seconds\n",
      "Step 30400, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 2881.7032685279846 seconds\n",
      "Step 30500, loss: tensor(0.0923, grad_fn=<SubBackward0>), time elapsed: 2891.111752271652 seconds\n",
      "Step 30600, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 2900.718427181244 seconds\n",
      "Step 30700, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 2910.1035492420197 seconds\n",
      "Step 30800, loss: tensor(0.0952, grad_fn=<SubBackward0>), time elapsed: 2919.457789182663 seconds\n",
      "Step 30900, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 2929.0193812847137 seconds\n",
      "Step 31000, loss: tensor(0.0952, grad_fn=<SubBackward0>), time elapsed: 2938.4030849933624 seconds\n",
      "Step 31100, loss: tensor(0.0901, grad_fn=<SubBackward0>), time elapsed: 2947.8111515045166 seconds\n",
      "Step 31200, loss: tensor(0.0939, grad_fn=<SubBackward0>), time elapsed: 2957.397873401642 seconds\n",
      "Step 31300, loss: tensor(0.0930, grad_fn=<SubBackward0>), time elapsed: 2966.800629377365 seconds\n",
      "Step 31400, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 2976.424607515335 seconds\n",
      "Step 31500, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 2985.802941799164 seconds\n",
      "Step 31600, loss: tensor(0.0905, grad_fn=<SubBackward0>), time elapsed: 2995.282547235489 seconds\n",
      "Step 31700, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 3005.051743030548 seconds\n",
      "Step 31800, loss: tensor(0.0938, grad_fn=<SubBackward0>), time elapsed: 3014.4909434318542 seconds\n",
      "Step 31900, loss: tensor(0.0915, grad_fn=<SubBackward0>), time elapsed: 3023.9605841636658 seconds\n",
      "Step 32000, loss: tensor(0.0915, grad_fn=<SubBackward0>), time elapsed: 3033.6159055233 seconds\n",
      "Step 32100, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 3043.067157268524 seconds\n",
      "Step 32200, loss: tensor(0.0919, grad_fn=<SubBackward0>), time elapsed: 3052.685481786728 seconds\n",
      "Step 32300, loss: tensor(0.0937, grad_fn=<SubBackward0>), time elapsed: 3062.314158678055 seconds\n",
      "Step 32400, loss: tensor(0.0915, grad_fn=<SubBackward0>), time elapsed: 3071.7980518341064 seconds\n",
      "Step 32500, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 3081.2343983650208 seconds\n",
      "Step 32600, loss: tensor(0.0907, grad_fn=<SubBackward0>), time elapsed: 3090.935685157776 seconds\n",
      "Step 32700, loss: tensor(0.0922, grad_fn=<SubBackward0>), time elapsed: 3100.4099276065826 seconds\n",
      "Step 32800, loss: tensor(0.0905, grad_fn=<SubBackward0>), time elapsed: 3109.839286327362 seconds\n",
      "Step 32900, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 3119.502531528473 seconds\n",
      "Step 33000, loss: tensor(0.0930, grad_fn=<SubBackward0>), time elapsed: 3128.929987192154 seconds\n",
      "Step 33100, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 3138.3479385375977 seconds\n",
      "Step 33200, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 3147.985092163086 seconds\n",
      "Step 33300, loss: tensor(0.0923, grad_fn=<SubBackward0>), time elapsed: 3157.438855409622 seconds\n",
      "Step 33400, loss: tensor(0.0906, grad_fn=<SubBackward0>), time elapsed: 3166.9898834228516 seconds\n",
      "Step 33500, loss: tensor(0.0928, grad_fn=<SubBackward0>), time elapsed: 3176.846305370331 seconds\n",
      "Step 33600, loss: tensor(0.0901, grad_fn=<SubBackward0>), time elapsed: 3186.2621455192566 seconds\n",
      "Step 33700, loss: tensor(0.0939, grad_fn=<SubBackward0>), time elapsed: 3195.709465265274 seconds\n",
      "Step 33800, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 3205.4002301692963 seconds\n",
      "Step 33900, loss: tensor(0.0944, grad_fn=<SubBackward0>), time elapsed: 3214.827815771103 seconds\n",
      "Step 34000, loss: tensor(0.0918, grad_fn=<SubBackward0>), time elapsed: 3224.2881259918213 seconds\n",
      "Step 34100, loss: tensor(0.0927, grad_fn=<SubBackward0>), time elapsed: 3233.967697620392 seconds\n",
      "Step 34200, loss: tensor(0.0932, grad_fn=<SubBackward0>), time elapsed: 3243.5154893398285 seconds\n",
      "Step 34300, loss: tensor(0.0923, grad_fn=<SubBackward0>), time elapsed: 3253.0409841537476 seconds\n",
      "Step 34400, loss: tensor(0.0915, grad_fn=<SubBackward0>), time elapsed: 3262.7492933273315 seconds\n",
      "Step 34500, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 3272.247275829315 seconds\n",
      "Step 34600, loss: tensor(0.0911, grad_fn=<SubBackward0>), time elapsed: 3281.7009892463684 seconds\n",
      "Step 34700, loss: tensor(0.0923, grad_fn=<SubBackward0>), time elapsed: 3291.3392634391785 seconds\n",
      "Step 34800, loss: tensor(0.0910, grad_fn=<SubBackward0>), time elapsed: 3300.832160949707 seconds\n",
      "Step 34900, loss: tensor(0.0911, grad_fn=<SubBackward0>), time elapsed: 3310.40443110466 seconds\n",
      "Step 35000, loss: tensor(0.0939, grad_fn=<SubBackward0>), time elapsed: 3320.2425186634064 seconds\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(1.0381, grad_fn=<SubBackward0>), time elapsed: 0.13550353050231934 seconds\n",
      "Step 100, loss: tensor(0.9064, grad_fn=<SubBackward0>), time elapsed: 9.928170204162598 seconds\n",
      "Step 200, loss: tensor(0.7214, grad_fn=<SubBackward0>), time elapsed: 20.40116858482361 seconds\n",
      "Step 300, loss: tensor(0.5537, grad_fn=<SubBackward0>), time elapsed: 29.764251947402954 seconds\n",
      "Step 400, loss: tensor(0.4448, grad_fn=<SubBackward0>), time elapsed: 39.09861445426941 seconds\n",
      "Step 500, loss: tensor(0.3744, grad_fn=<SubBackward0>), time elapsed: 48.60478687286377 seconds\n",
      "Step 600, loss: tensor(0.3265, grad_fn=<SubBackward0>), time elapsed: 57.915910959243774 seconds\n",
      "Step 700, loss: tensor(0.2854, grad_fn=<SubBackward0>), time elapsed: 67.21732592582703 seconds\n",
      "Step 800, loss: tensor(0.2522, grad_fn=<SubBackward0>), time elapsed: 76.69308614730835 seconds\n",
      "Step 900, loss: tensor(0.2315, grad_fn=<SubBackward0>), time elapsed: 86.03167152404785 seconds\n",
      "Step 1000, loss: tensor(0.2146, grad_fn=<SubBackward0>), time elapsed: 95.37729263305664 seconds\n",
      "Step 1100, loss: tensor(0.1983, grad_fn=<SubBackward0>), time elapsed: 104.84180092811584 seconds\n",
      "Step 1200, loss: tensor(0.1915, grad_fn=<SubBackward0>), time elapsed: 114.19943809509277 seconds\n",
      "Step 1300, loss: tensor(0.1809, grad_fn=<SubBackward0>), time elapsed: 123.5226800441742 seconds\n",
      "Step 1400, loss: tensor(0.1743, grad_fn=<SubBackward0>), time elapsed: 132.9985225200653 seconds\n",
      "Step 1500, loss: tensor(0.1687, grad_fn=<SubBackward0>), time elapsed: 142.37783288955688 seconds\n",
      "Step 1600, loss: tensor(0.1638, grad_fn=<SubBackward0>), time elapsed: 152.08909916877747 seconds\n",
      "Step 1700, loss: tensor(0.1595, grad_fn=<SubBackward0>), time elapsed: 161.4641900062561 seconds\n",
      "Step 1800, loss: tensor(0.1562, grad_fn=<SubBackward0>), time elapsed: 170.89576864242554 seconds\n",
      "Step 1900, loss: tensor(0.1513, grad_fn=<SubBackward0>), time elapsed: 180.4495496749878 seconds\n",
      "Step 2000, loss: tensor(0.1481, grad_fn=<SubBackward0>), time elapsed: 189.84175086021423 seconds\n",
      "Step 2100, loss: tensor(0.1454, grad_fn=<SubBackward0>), time elapsed: 199.29263019561768 seconds\n",
      "Step 2200, loss: tensor(0.1447, grad_fn=<SubBackward0>), time elapsed: 208.88594007492065 seconds\n",
      "Step 2300, loss: tensor(0.1414, grad_fn=<SubBackward0>), time elapsed: 218.30352640151978 seconds\n",
      "Step 2400, loss: tensor(0.1395, grad_fn=<SubBackward0>), time elapsed: 227.7159960269928 seconds\n",
      "Step 2500, loss: tensor(0.1375, grad_fn=<SubBackward0>), time elapsed: 237.30295372009277 seconds\n",
      "Step 2600, loss: tensor(0.1375, grad_fn=<SubBackward0>), time elapsed: 246.91512966156006 seconds\n",
      "Step 2700, loss: tensor(0.1359, grad_fn=<SubBackward0>), time elapsed: 256.3139910697937 seconds\n",
      "Step 2800, loss: tensor(0.1343, grad_fn=<SubBackward0>), time elapsed: 265.88532853126526 seconds\n",
      "Step 2900, loss: tensor(0.1336, grad_fn=<SubBackward0>), time elapsed: 275.2800042629242 seconds\n",
      "Step 3000, loss: tensor(0.1324, grad_fn=<SubBackward0>), time elapsed: 284.6641285419464 seconds\n",
      "Step 3100, loss: tensor(0.1321, grad_fn=<SubBackward0>), time elapsed: 294.21933913230896 seconds\n",
      "Step 3200, loss: tensor(0.1303, grad_fn=<SubBackward0>), time elapsed: 303.5787434577942 seconds\n",
      "Step 3300, loss: tensor(0.1286, grad_fn=<SubBackward0>), time elapsed: 313.08209109306335 seconds\n",
      "Step 3400, loss: tensor(0.1289, grad_fn=<SubBackward0>), time elapsed: 322.4580662250519 seconds\n",
      "Step 3500, loss: tensor(0.1282, grad_fn=<SubBackward0>), time elapsed: 331.87319707870483 seconds\n",
      "Step 3600, loss: tensor(0.1280, grad_fn=<SubBackward0>), time elapsed: 341.5607535839081 seconds\n",
      "Step 3700, loss: tensor(0.1273, grad_fn=<SubBackward0>), time elapsed: 350.8596336841583 seconds\n",
      "Step 3800, loss: tensor(0.1279, grad_fn=<SubBackward0>), time elapsed: 360.2080411911011 seconds\n",
      "Step 3900, loss: tensor(0.1258, grad_fn=<SubBackward0>), time elapsed: 369.6826181411743 seconds\n",
      "Step 4000, loss: tensor(0.1259, grad_fn=<SubBackward0>), time elapsed: 379.0472753047943 seconds\n",
      "Step 4100, loss: tensor(0.1240, grad_fn=<SubBackward0>), time elapsed: 388.36990904808044 seconds\n",
      "Step 4200, loss: tensor(0.1246, grad_fn=<SubBackward0>), time elapsed: 397.8398153781891 seconds\n",
      "Step 4300, loss: tensor(0.1264, grad_fn=<SubBackward0>), time elapsed: 407.1497781276703 seconds\n",
      "Step 4400, loss: tensor(0.1235, grad_fn=<SubBackward0>), time elapsed: 416.5010139942169 seconds\n",
      "Step 4500, loss: tensor(0.1242, grad_fn=<SubBackward0>), time elapsed: 425.99010133743286 seconds\n",
      "Step 4600, loss: tensor(0.1222, grad_fn=<SubBackward0>), time elapsed: 435.3554456233978 seconds\n",
      "Step 4700, loss: tensor(0.1213, grad_fn=<SubBackward0>), time elapsed: 444.95960545539856 seconds\n",
      "Step 4800, loss: tensor(0.1221, grad_fn=<SubBackward0>), time elapsed: 454.52259945869446 seconds\n",
      "Step 4900, loss: tensor(0.1202, grad_fn=<SubBackward0>), time elapsed: 463.8156657218933 seconds\n",
      "Step 5000, loss: tensor(0.1210, grad_fn=<SubBackward0>), time elapsed: 473.3367705345154 seconds\n",
      "Step 5100, loss: tensor(0.1194, grad_fn=<SubBackward0>), time elapsed: 482.61751103401184 seconds\n",
      "Step 5200, loss: tensor(0.1175, grad_fn=<SubBackward0>), time elapsed: 491.95624256134033 seconds\n",
      "Step 5300, loss: tensor(0.1194, grad_fn=<SubBackward0>), time elapsed: 501.45556139945984 seconds\n",
      "Step 5400, loss: tensor(0.1164, grad_fn=<SubBackward0>), time elapsed: 510.78253149986267 seconds\n",
      "Step 5500, loss: tensor(0.1154, grad_fn=<SubBackward0>), time elapsed: 520.1312873363495 seconds\n",
      "Step 5600, loss: tensor(0.1148, grad_fn=<SubBackward0>), time elapsed: 529.6160261631012 seconds\n",
      "Step 5700, loss: tensor(0.1141, grad_fn=<SubBackward0>), time elapsed: 539.1574919223785 seconds\n",
      "Step 5800, loss: tensor(0.1149, grad_fn=<SubBackward0>), time elapsed: 548.6889960765839 seconds\n",
      "Step 5900, loss: tensor(0.1103, grad_fn=<SubBackward0>), time elapsed: 558.2669398784637 seconds\n",
      "Step 6000, loss: tensor(0.1110, grad_fn=<SubBackward0>), time elapsed: 567.6791617870331 seconds\n",
      "Step 6100, loss: tensor(0.1091, grad_fn=<SubBackward0>), time elapsed: 577.0528035163879 seconds\n",
      "Step 6200, loss: tensor(0.1100, grad_fn=<SubBackward0>), time elapsed: 586.6309757232666 seconds\n",
      "Step 6300, loss: tensor(0.1092, grad_fn=<SubBackward0>), time elapsed: 596.0454936027527 seconds\n",
      "Step 6400, loss: tensor(0.1083, grad_fn=<SubBackward0>), time elapsed: 605.3421747684479 seconds\n",
      "Step 6500, loss: tensor(0.1091, grad_fn=<SubBackward0>), time elapsed: 614.8942773342133 seconds\n",
      "Step 6600, loss: tensor(0.1098, grad_fn=<SubBackward0>), time elapsed: 624.3708069324493 seconds\n",
      "Step 6700, loss: tensor(0.1073, grad_fn=<SubBackward0>), time elapsed: 633.7409892082214 seconds\n",
      "Step 6800, loss: tensor(0.1044, grad_fn=<SubBackward0>), time elapsed: 643.2891137599945 seconds\n",
      "Step 6900, loss: tensor(0.1076, grad_fn=<SubBackward0>), time elapsed: 652.6918711662292 seconds\n",
      "Step 7000, loss: tensor(0.1047, grad_fn=<SubBackward0>), time elapsed: 662.2604649066925 seconds\n",
      "Step 7100, loss: tensor(0.1065, grad_fn=<SubBackward0>), time elapsed: 671.6374707221985 seconds\n",
      "Step 7200, loss: tensor(0.1046, grad_fn=<SubBackward0>), time elapsed: 681.0572171211243 seconds\n",
      "Step 7300, loss: tensor(0.1057, grad_fn=<SubBackward0>), time elapsed: 690.6391644477844 seconds\n",
      "Step 7400, loss: tensor(0.1051, grad_fn=<SubBackward0>), time elapsed: 700.243809223175 seconds\n",
      "Step 7500, loss: tensor(0.1040, grad_fn=<SubBackward0>), time elapsed: 709.5704326629639 seconds\n",
      "Step 7600, loss: tensor(0.1013, grad_fn=<SubBackward0>), time elapsed: 719.0525233745575 seconds\n",
      "Step 7700, loss: tensor(0.1018, grad_fn=<SubBackward0>), time elapsed: 728.3842344284058 seconds\n",
      "Step 7800, loss: tensor(0.1050, grad_fn=<SubBackward0>), time elapsed: 737.6982128620148 seconds\n",
      "Step 7900, loss: tensor(0.1033, grad_fn=<SubBackward0>), time elapsed: 747.1856141090393 seconds\n",
      "Step 8000, loss: tensor(0.1021, grad_fn=<SubBackward0>), time elapsed: 756.4863467216492 seconds\n",
      "Step 8100, loss: tensor(0.1017, grad_fn=<SubBackward0>), time elapsed: 765.8170130252838 seconds\n",
      "Step 8200, loss: tensor(0.1022, grad_fn=<SubBackward0>), time elapsed: 775.315135717392 seconds\n",
      "Step 8300, loss: tensor(0.1027, grad_fn=<SubBackward0>), time elapsed: 784.6936733722687 seconds\n",
      "Step 8400, loss: tensor(0.1012, grad_fn=<SubBackward0>), time elapsed: 793.9909601211548 seconds\n",
      "Step 8500, loss: tensor(0.1022, grad_fn=<SubBackward0>), time elapsed: 803.5041916370392 seconds\n",
      "Step 8600, loss: tensor(0.1000, grad_fn=<SubBackward0>), time elapsed: 812.8297734260559 seconds\n",
      "Step 8700, loss: tensor(0.1015, grad_fn=<SubBackward0>), time elapsed: 822.3219242095947 seconds\n",
      "Step 8800, loss: tensor(0.0996, grad_fn=<SubBackward0>), time elapsed: 831.6274089813232 seconds\n",
      "Step 8900, loss: tensor(0.0990, grad_fn=<SubBackward0>), time elapsed: 840.9397051334381 seconds\n",
      "Step 9000, loss: tensor(0.0992, grad_fn=<SubBackward0>), time elapsed: 850.6024694442749 seconds\n",
      "Step 9100, loss: tensor(0.1000, grad_fn=<SubBackward0>), time elapsed: 859.9258735179901 seconds\n",
      "Step 9200, loss: tensor(0.0986, grad_fn=<SubBackward0>), time elapsed: 869.2770013809204 seconds\n",
      "Step 9300, loss: tensor(0.0989, grad_fn=<SubBackward0>), time elapsed: 878.7555129528046 seconds\n",
      "Step 9400, loss: tensor(0.0996, grad_fn=<SubBackward0>), time elapsed: 888.0472095012665 seconds\n",
      "Step 9500, loss: tensor(0.0991, grad_fn=<SubBackward0>), time elapsed: 897.5624804496765 seconds\n",
      "Step 9600, loss: tensor(0.0981, grad_fn=<SubBackward0>), time elapsed: 907.1944708824158 seconds\n",
      "Step 9700, loss: tensor(0.0985, grad_fn=<SubBackward0>), time elapsed: 916.5769338607788 seconds\n",
      "Step 9800, loss: tensor(0.0967, grad_fn=<SubBackward0>), time elapsed: 925.9578680992126 seconds\n",
      "Step 9900, loss: tensor(0.0991, grad_fn=<SubBackward0>), time elapsed: 935.528228521347 seconds\n",
      "Step 10000, loss: tensor(0.0977, grad_fn=<SubBackward0>), time elapsed: 944.8568551540375 seconds\n",
      "Step 10100, loss: tensor(0.0977, grad_fn=<SubBackward0>), time elapsed: 954.2012751102448 seconds\n",
      "Step 10200, loss: tensor(0.0993, grad_fn=<SubBackward0>), time elapsed: 963.7622759342194 seconds\n",
      "Step 10300, loss: tensor(0.0973, grad_fn=<SubBackward0>), time elapsed: 973.1162631511688 seconds\n",
      "Step 10400, loss: tensor(0.0952, grad_fn=<SubBackward0>), time elapsed: 982.4747860431671 seconds\n",
      "Step 10500, loss: tensor(0.0987, grad_fn=<SubBackward0>), time elapsed: 992.0877192020416 seconds\n",
      "Step 10600, loss: tensor(0.0960, grad_fn=<SubBackward0>), time elapsed: 1001.4728934764862 seconds\n",
      "Step 10700, loss: tensor(0.0988, grad_fn=<SubBackward0>), time elapsed: 1011.0654945373535 seconds\n",
      "Step 10800, loss: tensor(0.0972, grad_fn=<SubBackward0>), time elapsed: 1020.4250047206879 seconds\n",
      "Step 10900, loss: tensor(0.0972, grad_fn=<SubBackward0>), time elapsed: 1029.7975749969482 seconds\n",
      "Step 11000, loss: tensor(0.0952, grad_fn=<SubBackward0>), time elapsed: 1039.3127000331879 seconds\n",
      "Step 11100, loss: tensor(0.0932, grad_fn=<SubBackward0>), time elapsed: 1048.6773285865784 seconds\n",
      "Step 11200, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 1058.0643599033356 seconds\n",
      "Step 11300, loss: tensor(0.0935, grad_fn=<SubBackward0>), time elapsed: 1067.5739214420319 seconds\n",
      "Step 11400, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 1076.9104897975922 seconds\n",
      "Step 11500, loss: tensor(0.0923, grad_fn=<SubBackward0>), time elapsed: 1086.3035428524017 seconds\n",
      "Step 11600, loss: tensor(0.0905, grad_fn=<SubBackward0>), time elapsed: 1095.8464877605438 seconds\n",
      "Step 11700, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 1105.2608261108398 seconds\n",
      "Step 11800, loss: tensor(0.0910, grad_fn=<SubBackward0>), time elapsed: 1114.672800540924 seconds\n",
      "Step 11900, loss: tensor(0.0924, grad_fn=<SubBackward0>), time elapsed: 1124.2043280601501 seconds\n",
      "Step 12000, loss: tensor(0.0907, grad_fn=<SubBackward0>), time elapsed: 1133.536244392395 seconds\n",
      "Step 12100, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 1142.8951778411865 seconds\n",
      "Step 12200, loss: tensor(0.0899, grad_fn=<SubBackward0>), time elapsed: 1152.5989747047424 seconds\n",
      "Step 12300, loss: tensor(0.0911, grad_fn=<SubBackward0>), time elapsed: 1161.9692327976227 seconds\n",
      "Step 12400, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 1171.3782947063446 seconds\n",
      "Step 12500, loss: tensor(0.0911, grad_fn=<SubBackward0>), time elapsed: 1181.1936304569244 seconds\n",
      "Step 12600, loss: tensor(0.0898, grad_fn=<SubBackward0>), time elapsed: 1190.542474746704 seconds\n",
      "Step 12700, loss: tensor(0.0924, grad_fn=<SubBackward0>), time elapsed: 1199.894136428833 seconds\n",
      "Step 12800, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 1209.4037296772003 seconds\n",
      "Step 12900, loss: tensor(0.0897, grad_fn=<SubBackward0>), time elapsed: 1218.7248497009277 seconds\n",
      "Step 13000, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 1228.2183763980865 seconds\n",
      "Step 13100, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 1237.5738654136658 seconds\n",
      "Step 13200, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 1246.9233765602112 seconds\n",
      "Step 13300, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 1256.4161508083344 seconds\n",
      "Step 13400, loss: tensor(0.0896, grad_fn=<SubBackward0>), time elapsed: 1265.7175948619843 seconds\n",
      "Step 13500, loss: tensor(0.0910, grad_fn=<SubBackward0>), time elapsed: 1275.0712344646454 seconds\n",
      "Step 13600, loss: tensor(0.0895, grad_fn=<SubBackward0>), time elapsed: 1284.5833821296692 seconds\n",
      "Step 13700, loss: tensor(0.0891, grad_fn=<SubBackward0>), time elapsed: 1293.944883108139 seconds\n",
      "Step 13800, loss: tensor(0.0897, grad_fn=<SubBackward0>), time elapsed: 1303.3085696697235 seconds\n",
      "Step 13900, loss: tensor(0.0904, grad_fn=<SubBackward0>), time elapsed: 1312.8297102451324 seconds\n",
      "Step 14000, loss: tensor(0.0881, grad_fn=<SubBackward0>), time elapsed: 1322.1887526512146 seconds\n",
      "Step 14100, loss: tensor(0.0879, grad_fn=<SubBackward0>), time elapsed: 1331.572829246521 seconds\n",
      "Step 14200, loss: tensor(0.0887, grad_fn=<SubBackward0>), time elapsed: 1341.0425035953522 seconds\n",
      "Step 14300, loss: tensor(0.0885, grad_fn=<SubBackward0>), time elapsed: 1350.3615741729736 seconds\n",
      "Step 14400, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 1359.6917793750763 seconds\n",
      "Step 14500, loss: tensor(0.0882, grad_fn=<SubBackward0>), time elapsed: 1369.196004152298 seconds\n",
      "Step 14600, loss: tensor(0.0892, grad_fn=<SubBackward0>), time elapsed: 1378.5482203960419 seconds\n",
      "Step 14700, loss: tensor(0.0899, grad_fn=<SubBackward0>), time elapsed: 1387.88795876503 seconds\n",
      "Step 14800, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 1397.4016551971436 seconds\n",
      "Step 14900, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 1406.7552452087402 seconds\n",
      "Step 15000, loss: tensor(0.0895, grad_fn=<SubBackward0>), time elapsed: 1416.0318410396576 seconds\n",
      "Step 15100, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 1425.5348036289215 seconds\n",
      "Step 15200, loss: tensor(0.0897, grad_fn=<SubBackward0>), time elapsed: 1434.8606085777283 seconds\n",
      "Step 15300, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 1444.3919365406036 seconds\n",
      "Step 15400, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 1453.9037373065948 seconds\n",
      "Step 15500, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 1463.2247126102448 seconds\n",
      "Step 15600, loss: tensor(0.0892, grad_fn=<SubBackward0>), time elapsed: 1472.6958882808685 seconds\n",
      "Step 15700, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 1482.0380866527557 seconds\n",
      "Step 15800, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 1491.3335282802582 seconds\n",
      "Step 15900, loss: tensor(0.0886, grad_fn=<SubBackward0>), time elapsed: 1500.8181443214417 seconds\n",
      "Step 16000, loss: tensor(0.0891, grad_fn=<SubBackward0>), time elapsed: 1510.1312141418457 seconds\n",
      "Step 16100, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 1519.4891567230225 seconds\n",
      "Step 16200, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 1529.019525527954 seconds\n",
      "Step 16300, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 1538.3430316448212 seconds\n",
      "Step 16400, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 1547.6603457927704 seconds\n",
      "Step 16500, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 1557.1085147857666 seconds\n",
      "Step 16600, loss: tensor(0.0885, grad_fn=<SubBackward0>), time elapsed: 1566.3773550987244 seconds\n",
      "Step 16700, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 1575.7204372882843 seconds\n",
      "Step 16800, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 1585.2594566345215 seconds\n",
      "Step 16900, loss: tensor(0.0879, grad_fn=<SubBackward0>), time elapsed: 1594.5931386947632 seconds\n",
      "Step 17000, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 1603.9397323131561 seconds\n",
      "Step 17100, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 1613.482048034668 seconds\n",
      "Step 17200, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 1622.8415775299072 seconds\n",
      "Step 17300, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 1632.379348039627 seconds\n",
      "Step 17400, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 1641.9666647911072 seconds\n",
      "Step 17500, loss: tensor(0.0881, grad_fn=<SubBackward0>), time elapsed: 1651.2948379516602 seconds\n",
      "Step 17600, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 1660.791344165802 seconds\n",
      "Step 17700, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 1670.117119550705 seconds\n",
      "Step 17800, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 1679.4471068382263 seconds\n",
      "Step 17900, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 1688.9327034950256 seconds\n",
      "Step 18000, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 1698.276796579361 seconds\n",
      "Step 18100, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 1707.5973544120789 seconds\n",
      "Step 18200, loss: tensor(0.0885, grad_fn=<SubBackward0>), time elapsed: 1717.108161687851 seconds\n",
      "Step 18300, loss: tensor(0.0886, grad_fn=<SubBackward0>), time elapsed: 1726.4886140823364 seconds\n",
      "Step 18400, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 1735.8451602458954 seconds\n",
      "Step 18500, loss: tensor(0.0891, grad_fn=<SubBackward0>), time elapsed: 1745.377403974533 seconds\n",
      "Step 18600, loss: tensor(0.0902, grad_fn=<SubBackward0>), time elapsed: 1754.8353419303894 seconds\n",
      "Step 18700, loss: tensor(0.0888, grad_fn=<SubBackward0>), time elapsed: 1764.1892263889313 seconds\n",
      "Step 18800, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 1773.7151732444763 seconds\n",
      "Step 18900, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 1783.051781654358 seconds\n",
      "Step 19000, loss: tensor(0.0895, grad_fn=<SubBackward0>), time elapsed: 1792.3886499404907 seconds\n",
      "Step 19100, loss: tensor(0.0874, grad_fn=<SubBackward0>), time elapsed: 1801.919269323349 seconds\n",
      "Step 19200, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 1811.2652406692505 seconds\n",
      "Step 19300, loss: tensor(0.0877, grad_fn=<SubBackward0>), time elapsed: 1820.6465291976929 seconds\n",
      "Step 19400, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 1830.1645004749298 seconds\n",
      "Step 19500, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 1839.4830386638641 seconds\n",
      "Step 19600, loss: tensor(0.0877, grad_fn=<SubBackward0>), time elapsed: 1848.8278365135193 seconds\n",
      "Step 19700, loss: tensor(0.0869, grad_fn=<SubBackward0>), time elapsed: 1858.3513085842133 seconds\n",
      "Step 19800, loss: tensor(0.0877, grad_fn=<SubBackward0>), time elapsed: 1867.687643289566 seconds\n",
      "Step 19900, loss: tensor(0.0879, grad_fn=<SubBackward0>), time elapsed: 1877.0089974403381 seconds\n",
      "Step 20000, loss: tensor(0.0859, grad_fn=<SubBackward0>), time elapsed: 1886.5410468578339 seconds\n",
      "Step 20100, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 1895.907958984375 seconds\n",
      "Step 20200, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 1905.6725435256958 seconds\n",
      "Step 20300, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 1914.9941818714142 seconds\n",
      "Step 20400, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 1924.3493921756744 seconds\n",
      "Step 20500, loss: tensor(0.0879, grad_fn=<SubBackward0>), time elapsed: 1933.9039494991302 seconds\n",
      "Step 20600, loss: tensor(0.0874, grad_fn=<SubBackward0>), time elapsed: 1943.258374929428 seconds\n",
      "Step 20700, loss: tensor(0.0863, grad_fn=<SubBackward0>), time elapsed: 1952.6158299446106 seconds\n",
      "Step 20800, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 1962.1767885684967 seconds\n",
      "Step 20900, loss: tensor(0.0879, grad_fn=<SubBackward0>), time elapsed: 1971.5335586071014 seconds\n",
      "Step 21000, loss: tensor(0.0888, grad_fn=<SubBackward0>), time elapsed: 1981.1728715896606 seconds\n",
      "Step 21100, loss: tensor(0.0881, grad_fn=<SubBackward0>), time elapsed: 1990.9627854824066 seconds\n",
      "Step 21200, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 2000.3461363315582 seconds\n",
      "Step 21300, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 2009.8223156929016 seconds\n",
      "Step 21400, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 2019.4988205432892 seconds\n",
      "Step 21500, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 2028.8344085216522 seconds\n",
      "Step 21600, loss: tensor(0.0855, grad_fn=<SubBackward0>), time elapsed: 2038.2085847854614 seconds\n",
      "Step 21700, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 2047.7688100337982 seconds\n",
      "Step 21800, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 2057.331207513809 seconds\n",
      "Step 21900, loss: tensor(0.0862, grad_fn=<SubBackward0>), time elapsed: 2066.756742477417 seconds\n",
      "Step 22000, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 2076.312808275223 seconds\n",
      "Step 22100, loss: tensor(0.0885, grad_fn=<SubBackward0>), time elapsed: 2085.6816849708557 seconds\n",
      "Step 22200, loss: tensor(0.0891, grad_fn=<SubBackward0>), time elapsed: 2095.087388277054 seconds\n",
      "Step 22300, loss: tensor(0.0861, grad_fn=<SubBackward0>), time elapsed: 2104.6303250789642 seconds\n",
      "Step 22400, loss: tensor(0.0877, grad_fn=<SubBackward0>), time elapsed: 2113.981330394745 seconds\n",
      "Step 22500, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 2123.501745223999 seconds\n",
      "Step 22600, loss: tensor(0.0887, grad_fn=<SubBackward0>), time elapsed: 2133.0505352020264 seconds\n",
      "Step 22700, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 2142.399432659149 seconds\n",
      "Step 22800, loss: tensor(0.0882, grad_fn=<SubBackward0>), time elapsed: 2151.7419381141663 seconds\n",
      "Step 22900, loss: tensor(0.0881, grad_fn=<SubBackward0>), time elapsed: 2161.297512769699 seconds\n",
      "Step 23000, loss: tensor(0.0851, grad_fn=<SubBackward0>), time elapsed: 2170.639855146408 seconds\n",
      "Step 23100, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 2180.2031598091125 seconds\n",
      "Step 23200, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 2189.5896739959717 seconds\n",
      "Step 23300, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 2198.9758398532867 seconds\n",
      "Step 23400, loss: tensor(0.0900, grad_fn=<SubBackward0>), time elapsed: 2208.525378227234 seconds\n",
      "Step 23500, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 2217.9289751052856 seconds\n",
      "Step 23600, loss: tensor(0.0887, grad_fn=<SubBackward0>), time elapsed: 2227.812617301941 seconds\n",
      "Step 23700, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 2237.5093746185303 seconds\n",
      "Step 23800, loss: tensor(0.0898, grad_fn=<SubBackward0>), time elapsed: 2246.988275527954 seconds\n",
      "Step 23900, loss: tensor(0.0879, grad_fn=<SubBackward0>), time elapsed: 2256.421565055847 seconds\n",
      "Step 24000, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 2266.0645174980164 seconds\n",
      "Step 24100, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 2275.478399515152 seconds\n",
      "Step 24200, loss: tensor(0.0899, grad_fn=<SubBackward0>), time elapsed: 2284.907379388809 seconds\n",
      "Step 24300, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 2295.6903829574585 seconds\n",
      "Step 24400, loss: tensor(0.0882, grad_fn=<SubBackward0>), time elapsed: 2306.3332421779633 seconds\n",
      "Step 24500, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 2315.948395729065 seconds\n",
      "Step 24600, loss: tensor(0.0885, grad_fn=<SubBackward0>), time elapsed: 2325.7083003520966 seconds\n",
      "Step 24700, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 2335.288882255554 seconds\n",
      "Step 24800, loss: tensor(0.0877, grad_fn=<SubBackward0>), time elapsed: 2344.825563430786 seconds\n",
      "Step 24900, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 2355.031594991684 seconds\n",
      "Step 25000, loss: tensor(0.0885, grad_fn=<SubBackward0>), time elapsed: 2364.6310844421387 seconds\n",
      "Step 25100, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 2374.2315514087677 seconds\n",
      "Step 25200, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 2383.9434394836426 seconds\n",
      "Step 25300, loss: tensor(0.0896, grad_fn=<SubBackward0>), time elapsed: 2393.4928181171417 seconds\n",
      "Step 25400, loss: tensor(0.0879, grad_fn=<SubBackward0>), time elapsed: 2403.0815494060516 seconds\n",
      "Step 25500, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 2412.9242799282074 seconds\n",
      "Step 25600, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 2422.41904091835 seconds\n",
      "Step 25700, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 2431.9098660945892 seconds\n",
      "Step 25800, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 2441.557422399521 seconds\n",
      "Step 25900, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 2451.0052382946014 seconds\n",
      "Step 26000, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 2460.4839520454407 seconds\n",
      "Step 26100, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 2470.1231434345245 seconds\n",
      "Step 26200, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 2479.553503513336 seconds\n",
      "Step 26300, loss: tensor(0.0870, grad_fn=<SubBackward0>), time elapsed: 2489.1685662269592 seconds\n",
      "Step 26400, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 2498.578148841858 seconds\n",
      "Step 26500, loss: tensor(0.0879, grad_fn=<SubBackward0>), time elapsed: 2507.9921810626984 seconds\n",
      "Step 26600, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 2517.61901307106 seconds\n",
      "Step 26700, loss: tensor(0.0860, grad_fn=<SubBackward0>), time elapsed: 2527.0798745155334 seconds\n",
      "Step 26800, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 2536.5253987312317 seconds\n",
      "Step 26900, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 2546.3121745586395 seconds\n",
      "Step 27000, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 2555.827393770218 seconds\n",
      "Step 27100, loss: tensor(0.0864, grad_fn=<SubBackward0>), time elapsed: 2565.2969179153442 seconds\n",
      "Step 27200, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 2574.9437925815582 seconds\n",
      "Step 27300, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 2584.399688243866 seconds\n",
      "Step 27400, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 2593.816793680191 seconds\n",
      "Step 27500, loss: tensor(0.0901, grad_fn=<SubBackward0>), time elapsed: 2603.4544599056244 seconds\n",
      "Step 27600, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 2612.92404460907 seconds\n",
      "Step 27700, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 2622.387137889862 seconds\n",
      "Step 27800, loss: tensor(0.0895, grad_fn=<SubBackward0>), time elapsed: 2632.02544093132 seconds\n",
      "Step 27900, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 2641.491229534149 seconds\n",
      "Step 28000, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 2651.0002603530884 seconds\n",
      "Step 28100, loss: tensor(0.0881, grad_fn=<SubBackward0>), time elapsed: 2661.0207290649414 seconds\n",
      "Step 28200, loss: tensor(0.0879, grad_fn=<SubBackward0>), time elapsed: 2670.6017467975616 seconds\n",
      "Step 28300, loss: tensor(0.0891, grad_fn=<SubBackward0>), time elapsed: 2680.200633764267 seconds\n",
      "Step 28400, loss: tensor(0.0903, grad_fn=<SubBackward0>), time elapsed: 2689.955998659134 seconds\n",
      "Step 28500, loss: tensor(0.0856, grad_fn=<SubBackward0>), time elapsed: 2699.501804828644 seconds\n",
      "Step 28600, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 2708.9401955604553 seconds\n",
      "Step 28700, loss: tensor(0.0881, grad_fn=<SubBackward0>), time elapsed: 2718.5220398902893 seconds\n",
      "Step 28800, loss: tensor(0.0898, grad_fn=<SubBackward0>), time elapsed: 2727.9118087291718 seconds\n",
      "Step 28900, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 2737.3119480609894 seconds\n",
      "Step 29000, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 2746.8928575515747 seconds\n",
      "Step 29100, loss: tensor(0.0893, grad_fn=<SubBackward0>), time elapsed: 2756.3296422958374 seconds\n",
      "Step 29200, loss: tensor(0.0887, grad_fn=<SubBackward0>), time elapsed: 2765.8107118606567 seconds\n",
      "Step 29300, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 2775.449299097061 seconds\n",
      "Step 29400, loss: tensor(0.0877, grad_fn=<SubBackward0>), time elapsed: 2784.9156317710876 seconds\n",
      "Step 29500, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 2794.312928676605 seconds\n",
      "Step 29600, loss: tensor(0.0874, grad_fn=<SubBackward0>), time elapsed: 2803.912453174591 seconds\n",
      "Step 29700, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 2813.318693637848 seconds\n",
      "Step 29800, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 2822.741067171097 seconds\n",
      "Step 29900, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 2832.4075973033905 seconds\n",
      "Step 30000, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 2841.8724358081818 seconds\n",
      "Step 30100, loss: tensor(0.0846, grad_fn=<SubBackward0>), time elapsed: 2851.4887144565582 seconds\n",
      "Step 30200, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 2860.9642281532288 seconds\n",
      "Step 30300, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 2870.414088010788 seconds\n",
      "Step 30400, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 2880.075615167618 seconds\n",
      "Step 30500, loss: tensor(0.0888, grad_fn=<SubBackward0>), time elapsed: 2889.8084921836853 seconds\n",
      "Step 30600, loss: tensor(0.0870, grad_fn=<SubBackward0>), time elapsed: 2899.4410915374756 seconds\n",
      "Step 30700, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 2908.994748353958 seconds\n",
      "Step 30800, loss: tensor(0.0869, grad_fn=<SubBackward0>), time elapsed: 2918.3533594608307 seconds\n",
      "Step 30900, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 2927.719215631485 seconds\n",
      "Step 31000, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 2937.2327852249146 seconds\n",
      "Step 31100, loss: tensor(0.0892, grad_fn=<SubBackward0>), time elapsed: 2946.544698238373 seconds\n",
      "Step 31200, loss: tensor(0.0869, grad_fn=<SubBackward0>), time elapsed: 2955.8903126716614 seconds\n",
      "Step 31300, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 2965.6320712566376 seconds\n",
      "Step 31400, loss: tensor(0.0862, grad_fn=<SubBackward0>), time elapsed: 2974.951882839203 seconds\n",
      "Step 31500, loss: tensor(0.0877, grad_fn=<SubBackward0>), time elapsed: 2984.348685503006 seconds\n",
      "Step 31600, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 2993.9053716659546 seconds\n",
      "Step 31700, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 3003.244818210602 seconds\n",
      "Step 31800, loss: tensor(0.0866, grad_fn=<SubBackward0>), time elapsed: 3012.590809106827 seconds\n",
      "Step 31900, loss: tensor(0.0888, grad_fn=<SubBackward0>), time elapsed: 3022.1232573986053 seconds\n",
      "Step 32000, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 3031.4238724708557 seconds\n",
      "Step 32100, loss: tensor(0.0898, grad_fn=<SubBackward0>), time elapsed: 3040.765324115753 seconds\n",
      "Step 32200, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 3050.2901017665863 seconds\n",
      "Step 32300, loss: tensor(0.0897, grad_fn=<SubBackward0>), time elapsed: 3059.6305096149445 seconds\n",
      "Step 32400, loss: tensor(0.0872, grad_fn=<SubBackward0>), time elapsed: 3068.9261515140533 seconds\n",
      "Step 32500, loss: tensor(0.0901, grad_fn=<SubBackward0>), time elapsed: 3078.505887746811 seconds\n",
      "Step 32600, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 3087.8477585315704 seconds\n",
      "Step 32700, loss: tensor(0.0879, grad_fn=<SubBackward0>), time elapsed: 3097.226560115814 seconds\n",
      "Step 32800, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 3106.8885588645935 seconds\n",
      "Step 32900, loss: tensor(0.0901, grad_fn=<SubBackward0>), time elapsed: 3116.2298567295074 seconds\n",
      "Step 33000, loss: tensor(0.0891, grad_fn=<SubBackward0>), time elapsed: 3125.606187582016 seconds\n",
      "Step 33100, loss: tensor(0.0874, grad_fn=<SubBackward0>), time elapsed: 3135.1911432743073 seconds\n",
      "Step 33200, loss: tensor(0.0908, grad_fn=<SubBackward0>), time elapsed: 3144.775992870331 seconds\n",
      "Step 33300, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 3154.4443576335907 seconds\n",
      "Step 33400, loss: tensor(0.0888, grad_fn=<SubBackward0>), time elapsed: 3164.2518334388733 seconds\n",
      "Step 33500, loss: tensor(0.0880, grad_fn=<SubBackward0>), time elapsed: 3173.6163725852966 seconds\n",
      "Step 33600, loss: tensor(0.0887, grad_fn=<SubBackward0>), time elapsed: 3182.997927904129 seconds\n",
      "Step 33700, loss: tensor(0.0884, grad_fn=<SubBackward0>), time elapsed: 3192.5767471790314 seconds\n",
      "Step 33800, loss: tensor(0.0887, grad_fn=<SubBackward0>), time elapsed: 3202.0244812965393 seconds\n",
      "Step 33900, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 3211.4225895404816 seconds\n",
      "Step 34000, loss: tensor(0.0875, grad_fn=<SubBackward0>), time elapsed: 3221.023960828781 seconds\n",
      "Step 34100, loss: tensor(0.0860, grad_fn=<SubBackward0>), time elapsed: 3230.4336977005005 seconds\n",
      "Step 34200, loss: tensor(0.0873, grad_fn=<SubBackward0>), time elapsed: 3239.809866666794 seconds\n",
      "Step 34300, loss: tensor(0.0879, grad_fn=<SubBackward0>), time elapsed: 3249.38193154335 seconds\n",
      "Step 34400, loss: tensor(0.0883, grad_fn=<SubBackward0>), time elapsed: 3258.8582179546356 seconds\n",
      "Step 34500, loss: tensor(0.0894, grad_fn=<SubBackward0>), time elapsed: 3268.397004842758 seconds\n",
      "Step 34600, loss: tensor(0.0889, grad_fn=<SubBackward0>), time elapsed: 3277.9983744621277 seconds\n",
      "Step 34700, loss: tensor(0.0878, grad_fn=<SubBackward0>), time elapsed: 3287.3995637893677 seconds\n",
      "Step 34800, loss: tensor(0.0865, grad_fn=<SubBackward0>), time elapsed: 3296.8051784038544 seconds\n",
      "Step 34900, loss: tensor(0.0858, grad_fn=<SubBackward0>), time elapsed: 3306.393721818924 seconds\n",
      "Step 35000, loss: tensor(0.0876, grad_fn=<SubBackward0>), time elapsed: 3315.778660297394 seconds\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Step 0, loss: tensor(1.2546, grad_fn=<SubBackward0>), time elapsed: 0.11431241035461426 seconds\n",
      "Step 100, loss: tensor(1.1465, grad_fn=<SubBackward0>), time elapsed: 9.792124271392822 seconds\n",
      "Step 200, loss: tensor(0.9336, grad_fn=<SubBackward0>), time elapsed: 19.560147523880005 seconds\n",
      "Step 300, loss: tensor(0.7300, grad_fn=<SubBackward0>), time elapsed: 28.902970552444458 seconds\n",
      "Step 400, loss: tensor(0.5926, grad_fn=<SubBackward0>), time elapsed: 38.410836696624756 seconds\n",
      "Step 500, loss: tensor(0.4733, grad_fn=<SubBackward0>), time elapsed: 47.72416353225708 seconds\n",
      "Step 600, loss: tensor(0.3857, grad_fn=<SubBackward0>), time elapsed: 57.06675338745117 seconds\n",
      "Step 700, loss: tensor(0.3306, grad_fn=<SubBackward0>), time elapsed: 66.58718585968018 seconds\n",
      "Step 800, loss: tensor(0.2843, grad_fn=<SubBackward0>), time elapsed: 75.92578434944153 seconds\n",
      "Step 900, loss: tensor(0.2491, grad_fn=<SubBackward0>), time elapsed: 85.2475700378418 seconds\n",
      "Step 1000, loss: tensor(0.2168, grad_fn=<SubBackward0>), time elapsed: 94.73636031150818 seconds\n",
      "Step 1100, loss: tensor(0.1939, grad_fn=<SubBackward0>), time elapsed: 104.1026041507721 seconds\n",
      "Step 1200, loss: tensor(0.1754, grad_fn=<SubBackward0>), time elapsed: 113.45541000366211 seconds\n",
      "Step 1300, loss: tensor(0.1626, grad_fn=<SubBackward0>), time elapsed: 122.95873093605042 seconds\n",
      "Step 1400, loss: tensor(0.1517, grad_fn=<SubBackward0>), time elapsed: 132.34131264686584 seconds\n",
      "Step 1500, loss: tensor(0.1443, grad_fn=<SubBackward0>), time elapsed: 141.7258539199829 seconds\n",
      "Step 1600, loss: tensor(0.1385, grad_fn=<SubBackward0>), time elapsed: 151.27393507957458 seconds\n",
      "Step 1700, loss: tensor(0.1330, grad_fn=<SubBackward0>), time elapsed: 160.77831268310547 seconds\n",
      "Step 1800, loss: tensor(0.1261, grad_fn=<SubBackward0>), time elapsed: 170.179514169693 seconds\n",
      "Step 1900, loss: tensor(0.1236, grad_fn=<SubBackward0>), time elapsed: 179.69351840019226 seconds\n",
      "Step 2000, loss: tensor(0.1183, grad_fn=<SubBackward0>), time elapsed: 189.10492086410522 seconds\n",
      "Step 2100, loss: tensor(0.1159, grad_fn=<SubBackward0>), time elapsed: 198.6124677658081 seconds\n",
      "Step 2200, loss: tensor(0.1128, grad_fn=<SubBackward0>), time elapsed: 207.9897267818451 seconds\n",
      "Step 2300, loss: tensor(0.1105, grad_fn=<SubBackward0>), time elapsed: 217.35539746284485 seconds\n",
      "Step 2400, loss: tensor(0.1081, grad_fn=<SubBackward0>), time elapsed: 226.95787453651428 seconds\n",
      "Step 2500, loss: tensor(0.1071, grad_fn=<SubBackward0>), time elapsed: 236.33094358444214 seconds\n",
      "Step 2600, loss: tensor(0.1058, grad_fn=<SubBackward0>), time elapsed: 245.6768445968628 seconds\n",
      "Step 2700, loss: tensor(0.1034, grad_fn=<SubBackward0>), time elapsed: 255.2242088317871 seconds\n",
      "Step 2800, loss: tensor(0.1012, grad_fn=<SubBackward0>), time elapsed: 264.5920379161835 seconds\n",
      "Step 2900, loss: tensor(0.1013, grad_fn=<SubBackward0>), time elapsed: 273.92631220817566 seconds\n",
      "Step 3000, loss: tensor(0.1017, grad_fn=<SubBackward0>), time elapsed: 283.4826159477234 seconds\n",
      "Step 3100, loss: tensor(0.1013, grad_fn=<SubBackward0>), time elapsed: 292.8986265659332 seconds\n",
      "Step 3200, loss: tensor(0.1010, grad_fn=<SubBackward0>), time elapsed: 302.31032252311707 seconds\n",
      "Step 3300, loss: tensor(0.0989, grad_fn=<SubBackward0>), time elapsed: 311.908607006073 seconds\n",
      "Step 3400, loss: tensor(0.0983, grad_fn=<SubBackward0>), time elapsed: 321.3433425426483 seconds\n",
      "Step 3500, loss: tensor(0.0981, grad_fn=<SubBackward0>), time elapsed: 330.7424306869507 seconds\n",
      "Step 3600, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 340.31462597846985 seconds\n",
      "Step 3700, loss: tensor(0.0976, grad_fn=<SubBackward0>), time elapsed: 349.734384059906 seconds\n",
      "Step 3800, loss: tensor(0.0969, grad_fn=<SubBackward0>), time elapsed: 359.2462999820709 seconds\n",
      "Step 3900, loss: tensor(0.0975, grad_fn=<SubBackward0>), time elapsed: 368.58489775657654 seconds\n",
      "Step 4000, loss: tensor(0.0976, grad_fn=<SubBackward0>), time elapsed: 377.95467472076416 seconds\n",
      "Step 4100, loss: tensor(0.0956, grad_fn=<SubBackward0>), time elapsed: 387.45806527137756 seconds\n",
      "Step 4200, loss: tensor(0.0954, grad_fn=<SubBackward0>), time elapsed: 396.8260226249695 seconds\n",
      "Step 4300, loss: tensor(0.0955, grad_fn=<SubBackward0>), time elapsed: 406.21473240852356 seconds\n",
      "Step 4400, loss: tensor(0.0965, grad_fn=<SubBackward0>), time elapsed: 415.72437834739685 seconds\n",
      "Step 4500, loss: tensor(0.0953, grad_fn=<SubBackward0>), time elapsed: 425.0795056819916 seconds\n",
      "Step 4600, loss: tensor(0.0946, grad_fn=<SubBackward0>), time elapsed: 434.498459815979 seconds\n",
      "Step 4700, loss: tensor(0.0948, grad_fn=<SubBackward0>), time elapsed: 444.0361239910126 seconds\n",
      "Step 4800, loss: tensor(0.0940, grad_fn=<SubBackward0>), time elapsed: 453.4374084472656 seconds\n",
      "Step 4900, loss: tensor(0.0941, grad_fn=<SubBackward0>), time elapsed: 462.76788544654846 seconds\n",
      "Step 5000, loss: tensor(0.0926, grad_fn=<SubBackward0>), time elapsed: 472.27326107025146 seconds\n",
      "Step 5100, loss: tensor(0.0928, grad_fn=<SubBackward0>), time elapsed: 481.6245069503784 seconds\n",
      "Step 5200, loss: tensor(0.0933, grad_fn=<SubBackward0>), time elapsed: 490.9545159339905 seconds\n",
      "Step 5300, loss: tensor(0.0927, grad_fn=<SubBackward0>), time elapsed: 500.47257375717163 seconds\n",
      "Step 5400, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 509.82789635658264 seconds\n",
      "Step 5500, loss: tensor(0.0905, grad_fn=<SubBackward0>), time elapsed: 519.3375370502472 seconds\n",
      "Step 5600, loss: tensor(0.0920, grad_fn=<SubBackward0>), time elapsed: 528.7001059055328 seconds\n",
      "Step 5700, loss: tensor(0.0914, grad_fn=<SubBackward0>), time elapsed: 538.0721125602722 seconds\n",
      "Step 5800, loss: tensor(0.0913, grad_fn=<SubBackward0>), time elapsed: 547.5786972045898 seconds\n",
      "Step 5900, loss: tensor(0.0909, grad_fn=<SubBackward0>), time elapsed: 556.9466419219971 seconds\n",
      "Step 6000, loss: tensor(0.0885, grad_fn=<SubBackward0>), time elapsed: 566.2768633365631 seconds\n",
      "Step 6100, loss: tensor(0.0890, grad_fn=<SubBackward0>), time elapsed: 575.8030977249146 seconds\n",
      "Step 6200, loss: tensor(0.0870, grad_fn=<SubBackward0>), time elapsed: 585.1334335803986 seconds\n",
      "Step 6300, loss: tensor(0.0887, grad_fn=<SubBackward0>), time elapsed: 594.4898021221161 seconds\n",
      "Step 6400, loss: tensor(0.0869, grad_fn=<SubBackward0>), time elapsed: 603.9952597618103 seconds\n",
      "Step 6500, loss: tensor(0.0860, grad_fn=<SubBackward0>), time elapsed: 613.3413000106812 seconds\n",
      "Step 6600, loss: tensor(0.0868, grad_fn=<SubBackward0>), time elapsed: 622.7173109054565 seconds\n",
      "Step 6700, loss: tensor(0.0862, grad_fn=<SubBackward0>), time elapsed: 632.2478003501892 seconds\n",
      "Step 6800, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 641.6031699180603 seconds\n",
      "Step 6900, loss: tensor(0.0871, grad_fn=<SubBackward0>), time elapsed: 650.8888871669769 seconds\n",
      "Step 7000, loss: tensor(0.0864, grad_fn=<SubBackward0>), time elapsed: 660.2279150485992 seconds\n",
      "Step 7100, loss: tensor(0.0866, grad_fn=<SubBackward0>), time elapsed: 669.3744530677795 seconds\n",
      "Step 7200, loss: tensor(0.0867, grad_fn=<SubBackward0>), time elapsed: 678.6613614559174 seconds\n",
      "Step 7300, loss: tensor(0.0846, grad_fn=<SubBackward0>), time elapsed: 687.7812519073486 seconds\n",
      "Step 7400, loss: tensor(0.0864, grad_fn=<SubBackward0>), time elapsed: 696.9200720787048 seconds\n",
      "Step 7500, loss: tensor(0.0849, grad_fn=<SubBackward0>), time elapsed: 706.2540345191956 seconds\n",
      "Step 7600, loss: tensor(0.0844, grad_fn=<SubBackward0>), time elapsed: 715.5712006092072 seconds\n",
      "Step 7700, loss: tensor(0.0854, grad_fn=<SubBackward0>), time elapsed: 725.2147595882416 seconds\n",
      "Step 7800, loss: tensor(0.0832, grad_fn=<SubBackward0>), time elapsed: 734.9662072658539 seconds\n",
      "Step 7900, loss: tensor(0.0838, grad_fn=<SubBackward0>), time elapsed: 744.3354108333588 seconds\n",
      "Step 8000, loss: tensor(0.0836, grad_fn=<SubBackward0>), time elapsed: 753.6448750495911 seconds\n",
      "Step 8100, loss: tensor(0.0825, grad_fn=<SubBackward0>), time elapsed: 763.100081205368 seconds\n",
      "Step 8200, loss: tensor(0.0831, grad_fn=<SubBackward0>), time elapsed: 772.4347305297852 seconds\n",
      "Step 8300, loss: tensor(0.0831, grad_fn=<SubBackward0>), time elapsed: 781.7272515296936 seconds\n",
      "Step 8400, loss: tensor(0.0843, grad_fn=<SubBackward0>), time elapsed: 791.1963140964508 seconds\n",
      "Step 8500, loss: tensor(0.0825, grad_fn=<SubBackward0>), time elapsed: 800.4796276092529 seconds\n",
      "Step 8600, loss: tensor(0.0833, grad_fn=<SubBackward0>), time elapsed: 809.7598652839661 seconds\n",
      "Step 8700, loss: tensor(0.0851, grad_fn=<SubBackward0>), time elapsed: 819.1986000537872 seconds\n",
      "Step 8800, loss: tensor(0.0827, grad_fn=<SubBackward0>), time elapsed: 828.4788722991943 seconds\n",
      "Step 8900, loss: tensor(0.0818, grad_fn=<SubBackward0>), time elapsed: 837.8258063793182 seconds\n",
      "Step 9000, loss: tensor(0.0850, grad_fn=<SubBackward0>), time elapsed: 847.2831189632416 seconds\n",
      "Step 9100, loss: tensor(0.0828, grad_fn=<SubBackward0>), time elapsed: 856.5781643390656 seconds\n",
      "Step 9200, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 866.1002857685089 seconds\n",
      "Step 9300, loss: tensor(0.0828, grad_fn=<SubBackward0>), time elapsed: 875.3972268104553 seconds\n",
      "Step 9400, loss: tensor(0.0842, grad_fn=<SubBackward0>), time elapsed: 884.6983246803284 seconds\n",
      "Step 9500, loss: tensor(0.0821, grad_fn=<SubBackward0>), time elapsed: 894.2035632133484 seconds\n",
      "Step 9600, loss: tensor(0.0801, grad_fn=<SubBackward0>), time elapsed: 903.4924430847168 seconds\n",
      "Step 9700, loss: tensor(0.0818, grad_fn=<SubBackward0>), time elapsed: 912.7976341247559 seconds\n",
      "Step 9800, loss: tensor(0.0843, grad_fn=<SubBackward0>), time elapsed: 922.3018970489502 seconds\n",
      "Step 9900, loss: tensor(0.0827, grad_fn=<SubBackward0>), time elapsed: 931.6760833263397 seconds\n",
      "Step 10000, loss: tensor(0.0821, grad_fn=<SubBackward0>), time elapsed: 941.0231413841248 seconds\n",
      "Step 10100, loss: tensor(0.0826, grad_fn=<SubBackward0>), time elapsed: 950.544380903244 seconds\n",
      "Step 10200, loss: tensor(0.0804, grad_fn=<SubBackward0>), time elapsed: 959.9347925186157 seconds\n",
      "Step 10300, loss: tensor(0.0830, grad_fn=<SubBackward0>), time elapsed: 969.2965459823608 seconds\n",
      "Step 10400, loss: tensor(0.0826, grad_fn=<SubBackward0>), time elapsed: 978.8309645652771 seconds\n",
      "Step 10500, loss: tensor(0.0831, grad_fn=<SubBackward0>), time elapsed: 988.1990413665771 seconds\n",
      "Step 10600, loss: tensor(0.0834, grad_fn=<SubBackward0>), time elapsed: 997.5396173000336 seconds\n",
      "Step 10700, loss: tensor(0.0821, grad_fn=<SubBackward0>), time elapsed: 1007.0557453632355 seconds\n",
      "Step 10800, loss: tensor(0.0824, grad_fn=<SubBackward0>), time elapsed: 1016.4568388462067 seconds\n",
      "Step 10900, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 1025.7996492385864 seconds\n",
      "Step 11000, loss: tensor(0.0805, grad_fn=<SubBackward0>), time elapsed: 1035.327006816864 seconds\n",
      "Step 11100, loss: tensor(0.0811, grad_fn=<SubBackward0>), time elapsed: 1044.6771113872528 seconds\n",
      "Step 11200, loss: tensor(0.0807, grad_fn=<SubBackward0>), time elapsed: 1054.2305128574371 seconds\n",
      "Step 11300, loss: tensor(0.0795, grad_fn=<SubBackward0>), time elapsed: 1063.796492099762 seconds\n",
      "Step 11400, loss: tensor(0.0825, grad_fn=<SubBackward0>), time elapsed: 1073.1768250465393 seconds\n",
      "Step 11500, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 1082.673944234848 seconds\n",
      "Step 11600, loss: tensor(0.0817, grad_fn=<SubBackward0>), time elapsed: 1092.0187425613403 seconds\n",
      "Step 11700, loss: tensor(0.0815, grad_fn=<SubBackward0>), time elapsed: 1101.34277176857 seconds\n",
      "Step 11800, loss: tensor(0.0820, grad_fn=<SubBackward0>), time elapsed: 1110.8566739559174 seconds\n",
      "Step 11900, loss: tensor(0.0821, grad_fn=<SubBackward0>), time elapsed: 1120.1680223941803 seconds\n",
      "Step 12000, loss: tensor(0.0814, grad_fn=<SubBackward0>), time elapsed: 1129.4547035694122 seconds\n",
      "Step 12100, loss: tensor(0.0819, grad_fn=<SubBackward0>), time elapsed: 1138.997707605362 seconds\n",
      "Step 12200, loss: tensor(0.0817, grad_fn=<SubBackward0>), time elapsed: 1148.3449184894562 seconds\n",
      "Step 12300, loss: tensor(0.0789, grad_fn=<SubBackward0>), time elapsed: 1157.7008969783783 seconds\n",
      "Step 12400, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 1167.2106885910034 seconds\n",
      "Step 12500, loss: tensor(0.0809, grad_fn=<SubBackward0>), time elapsed: 1176.595211982727 seconds\n",
      "Step 12600, loss: tensor(0.0817, grad_fn=<SubBackward0>), time elapsed: 1185.9638731479645 seconds\n",
      "Step 12700, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 1195.4549236297607 seconds\n",
      "Step 12800, loss: tensor(0.0827, grad_fn=<SubBackward0>), time elapsed: 1204.797714471817 seconds\n",
      "Step 12900, loss: tensor(0.0798, grad_fn=<SubBackward0>), time elapsed: 1214.1227235794067 seconds\n",
      "Step 13000, loss: tensor(0.0808, grad_fn=<SubBackward0>), time elapsed: 1223.577154159546 seconds\n",
      "Step 13100, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 1232.8970515727997 seconds\n",
      "Step 13200, loss: tensor(0.0812, grad_fn=<SubBackward0>), time elapsed: 1242.2385833263397 seconds\n",
      "Step 13300, loss: tensor(0.0810, grad_fn=<SubBackward0>), time elapsed: 1251.753719329834 seconds\n",
      "Step 13400, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 1261.1049571037292 seconds\n",
      "Step 13500, loss: tensor(0.0806, grad_fn=<SubBackward0>), time elapsed: 1270.6494145393372 seconds\n",
      "Step 13600, loss: tensor(0.0809, grad_fn=<SubBackward0>), time elapsed: 1280.0074212551117 seconds\n",
      "Step 13700, loss: tensor(0.0806, grad_fn=<SubBackward0>), time elapsed: 1289.3771600723267 seconds\n",
      "Step 13800, loss: tensor(0.0808, grad_fn=<SubBackward0>), time elapsed: 1298.9222695827484 seconds\n",
      "Step 13900, loss: tensor(0.0810, grad_fn=<SubBackward0>), time elapsed: 1308.2596728801727 seconds\n",
      "Step 14000, loss: tensor(0.0806, grad_fn=<SubBackward0>), time elapsed: 1317.5791282653809 seconds\n",
      "Step 14100, loss: tensor(0.0815, grad_fn=<SubBackward0>), time elapsed: 1327.0935645103455 seconds\n",
      "Step 14200, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 1336.4146194458008 seconds\n",
      "Step 14300, loss: tensor(0.0800, grad_fn=<SubBackward0>), time elapsed: 1345.7614011764526 seconds\n",
      "Step 14400, loss: tensor(0.0802, grad_fn=<SubBackward0>), time elapsed: 1355.2929894924164 seconds\n",
      "Step 14500, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 1364.6574354171753 seconds\n",
      "Step 14600, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 1374.015235900879 seconds\n",
      "Step 14700, loss: tensor(0.0791, grad_fn=<SubBackward0>), time elapsed: 1383.5299291610718 seconds\n",
      "Step 14800, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 1392.9135196208954 seconds\n",
      "Step 14900, loss: tensor(0.0794, grad_fn=<SubBackward0>), time elapsed: 1402.24813246727 seconds\n",
      "Step 15000, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 1411.7574644088745 seconds\n",
      "Step 15100, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 1421.1367247104645 seconds\n",
      "Step 15200, loss: tensor(0.0799, grad_fn=<SubBackward0>), time elapsed: 1430.4972774982452 seconds\n",
      "Step 15300, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 1440.0453834533691 seconds\n",
      "Step 15400, loss: tensor(0.0790, grad_fn=<SubBackward0>), time elapsed: 1449.44584274292 seconds\n",
      "Step 15500, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 1458.7981922626495 seconds\n",
      "Step 15600, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 1468.3458952903748 seconds\n",
      "Step 15700, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 1477.738867521286 seconds\n",
      "Step 15800, loss: tensor(0.0770, grad_fn=<SubBackward0>), time elapsed: 1487.3225462436676 seconds\n",
      "Step 15900, loss: tensor(0.0795, grad_fn=<SubBackward0>), time elapsed: 1496.6977574825287 seconds\n",
      "Step 16000, loss: tensor(0.0786, grad_fn=<SubBackward0>), time elapsed: 1506.0905539989471 seconds\n",
      "Step 16100, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 1515.6142272949219 seconds\n",
      "Step 16200, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 1524.959678888321 seconds\n",
      "Step 16300, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 1534.2670574188232 seconds\n",
      "Step 16400, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 1543.8162834644318 seconds\n",
      "Step 16500, loss: tensor(0.0777, grad_fn=<SubBackward0>), time elapsed: 1553.1340680122375 seconds\n",
      "Step 16600, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 1562.5258295536041 seconds\n",
      "Step 16700, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 1572.0574214458466 seconds\n",
      "Step 16800, loss: tensor(0.0781, grad_fn=<SubBackward0>), time elapsed: 1581.5255765914917 seconds\n",
      "Step 16900, loss: tensor(0.0780, grad_fn=<SubBackward0>), time elapsed: 1591.459323167801 seconds\n",
      "Step 17000, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 1601.1296219825745 seconds\n",
      "Step 17100, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 1610.5401391983032 seconds\n",
      "Step 17200, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 1619.8975598812103 seconds\n",
      "Step 17300, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 1629.4161880016327 seconds\n",
      "Step 17400, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 1639.4764280319214 seconds\n",
      "Step 17500, loss: tensor(0.0795, grad_fn=<SubBackward0>), time elapsed: 1648.6662397384644 seconds\n",
      "Step 17600, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 1658.0410251617432 seconds\n",
      "Step 17700, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 1667.3868477344513 seconds\n",
      "Step 17800, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 1676.699176311493 seconds\n",
      "Step 17900, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 1686.1104464530945 seconds\n",
      "Step 18000, loss: tensor(0.0784, grad_fn=<SubBackward0>), time elapsed: 1695.3463006019592 seconds\n",
      "Step 18100, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 1704.7857298851013 seconds\n",
      "Step 18200, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 1715.2417969703674 seconds\n",
      "Step 18300, loss: tensor(0.0781, grad_fn=<SubBackward0>), time elapsed: 1724.4741609096527 seconds\n",
      "Step 18400, loss: tensor(0.0786, grad_fn=<SubBackward0>), time elapsed: 1733.9086096286774 seconds\n",
      "Step 18500, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 1743.1203784942627 seconds\n",
      "Step 18600, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 1752.4541239738464 seconds\n",
      "Step 18700, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 1761.9088380336761 seconds\n",
      "Step 18800, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 1771.100299835205 seconds\n",
      "Step 18900, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 1780.298822402954 seconds\n",
      "Step 19000, loss: tensor(0.0793, grad_fn=<SubBackward0>), time elapsed: 1790.078438282013 seconds\n",
      "Step 19100, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 1799.3128070831299 seconds\n",
      "Step 19200, loss: tensor(0.0782, grad_fn=<SubBackward0>), time elapsed: 1808.4940729141235 seconds\n",
      "Step 19300, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 1818.2033824920654 seconds\n",
      "Step 19400, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 1827.3801352977753 seconds\n",
      "Step 19500, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 1836.5390820503235 seconds\n",
      "Step 19600, loss: tensor(0.0779, grad_fn=<SubBackward0>), time elapsed: 1845.9050691127777 seconds\n",
      "Step 19700, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 1855.0840060710907 seconds\n",
      "Step 19800, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 1864.8782579898834 seconds\n",
      "Step 19900, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 1874.265602350235 seconds\n",
      "Step 20000, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 1883.4612488746643 seconds\n",
      "Step 20100, loss: tensor(0.0770, grad_fn=<SubBackward0>), time elapsed: 1892.8662676811218 seconds\n",
      "Step 20200, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 1902.5896937847137 seconds\n",
      "Step 20300, loss: tensor(0.0770, grad_fn=<SubBackward0>), time elapsed: 1912.059832572937 seconds\n",
      "Step 20400, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 1921.8323352336884 seconds\n",
      "Step 20500, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 1931.3549654483795 seconds\n",
      "Step 20600, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 1940.7303097248077 seconds\n",
      "Step 20700, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 1950.3528826236725 seconds\n",
      "Step 20800, loss: tensor(0.0766, grad_fn=<SubBackward0>), time elapsed: 1960.178145647049 seconds\n",
      "Step 20900, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 1969.5597739219666 seconds\n",
      "Step 21000, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 1979.1141612529755 seconds\n",
      "Step 21100, loss: tensor(0.0783, grad_fn=<SubBackward0>), time elapsed: 1988.735261440277 seconds\n",
      "Step 21200, loss: tensor(0.0785, grad_fn=<SubBackward0>), time elapsed: 1998.426262140274 seconds\n",
      "Step 21300, loss: tensor(0.0769, grad_fn=<SubBackward0>), time elapsed: 2008.0156915187836 seconds\n",
      "Step 21400, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 2017.722136259079 seconds\n",
      "Step 21500, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 2027.1753942966461 seconds\n",
      "Step 21600, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 2036.7807970046997 seconds\n",
      "Step 21700, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 2046.2550539970398 seconds\n",
      "Step 21800, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 2055.8140738010406 seconds\n",
      "Step 21900, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 2065.502720117569 seconds\n",
      "Step 22000, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 2075.081447124481 seconds\n",
      "Step 22100, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 2084.5596611499786 seconds\n",
      "Step 22200, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 2094.357081890106 seconds\n",
      "Step 22300, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 2103.8467416763306 seconds\n",
      "Step 22400, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 2113.800532102585 seconds\n",
      "Step 22500, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 2123.576102256775 seconds\n",
      "Step 22600, loss: tensor(0.0723, grad_fn=<SubBackward0>), time elapsed: 2133.054608821869 seconds\n",
      "Step 22700, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 2142.4952812194824 seconds\n",
      "Step 22800, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 2152.1242303848267 seconds\n",
      "Step 22900, loss: tensor(0.0774, grad_fn=<SubBackward0>), time elapsed: 2161.574607849121 seconds\n",
      "Step 23000, loss: tensor(0.0772, grad_fn=<SubBackward0>), time elapsed: 2170.997240304947 seconds\n",
      "Step 23100, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 2180.6071586608887 seconds\n",
      "Step 23200, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 2190.2239441871643 seconds\n",
      "Step 23300, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 2199.6596245765686 seconds\n",
      "Step 23400, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 2209.3014974594116 seconds\n",
      "Step 23500, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 2218.7758922576904 seconds\n",
      "Step 23600, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 2228.4031343460083 seconds\n",
      "Step 23700, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 2239.376643896103 seconds\n",
      "Step 23800, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 2249.6978278160095 seconds\n",
      "Step 23900, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 2259.678768157959 seconds\n",
      "Step 24000, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 2269.9434361457825 seconds\n",
      "Step 24100, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 2279.507396697998 seconds\n",
      "Step 24200, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 2289.137949705124 seconds\n",
      "Step 24300, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 2298.5587453842163 seconds\n",
      "Step 24400, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 2308.7774510383606 seconds\n",
      "Step 24500, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 2318.5560154914856 seconds\n",
      "Step 24600, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 2328.135847568512 seconds\n",
      "Step 24700, loss: tensor(0.0756, grad_fn=<SubBackward0>), time elapsed: 2337.761042356491 seconds\n",
      "Step 24800, loss: tensor(0.0770, grad_fn=<SubBackward0>), time elapsed: 2347.496942281723 seconds\n",
      "Step 24900, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 2356.998236179352 seconds\n",
      "Step 25000, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 2366.51642370224 seconds\n",
      "Step 25100, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 2376.173983335495 seconds\n",
      "Step 25200, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 2385.6565742492676 seconds\n",
      "Step 25300, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 2395.178593158722 seconds\n",
      "Step 25400, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 2404.825924873352 seconds\n",
      "Step 25500, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 2414.2938771247864 seconds\n",
      "Step 25600, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 2423.755285024643 seconds\n",
      "Step 25700, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 2433.4173493385315 seconds\n",
      "Step 25800, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 2442.892815351486 seconds\n",
      "Step 25900, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 2452.336519241333 seconds\n",
      "Step 26000, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 2462.0088658332825 seconds\n",
      "Step 26100, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 2471.4806747436523 seconds\n",
      "Step 26200, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 2480.994123697281 seconds\n",
      "Step 26300, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 2490.65868639946 seconds\n",
      "Step 26400, loss: tensor(0.0735, grad_fn=<SubBackward0>), time elapsed: 2500.117765903473 seconds\n",
      "Step 26500, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 2509.5571308135986 seconds\n",
      "Step 26600, loss: tensor(0.0733, grad_fn=<SubBackward0>), time elapsed: 2519.1792607307434 seconds\n",
      "Step 26700, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 2528.613830089569 seconds\n",
      "Step 26800, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 2538.2194640636444 seconds\n",
      "Step 26900, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 2547.627653837204 seconds\n",
      "Step 27000, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 2556.996976852417 seconds\n",
      "Step 27100, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 2566.5908823013306 seconds\n",
      "Step 27200, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 2575.9784772396088 seconds\n",
      "Step 27300, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 2585.3589968681335 seconds\n",
      "Step 27400, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 2594.9579408168793 seconds\n",
      "Step 27500, loss: tensor(0.0737, grad_fn=<SubBackward0>), time elapsed: 2604.361414909363 seconds\n",
      "Step 27600, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 2613.7584557533264 seconds\n",
      "Step 27700, loss: tensor(0.0727, grad_fn=<SubBackward0>), time elapsed: 2623.3349776268005 seconds\n",
      "Step 27800, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 2632.756927013397 seconds\n",
      "Step 27900, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 2642.1997649669647 seconds\n",
      "Step 28000, loss: tensor(0.0778, grad_fn=<SubBackward0>), time elapsed: 2651.8209047317505 seconds\n",
      "Step 28100, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 2661.2502975463867 seconds\n",
      "Step 28200, loss: tensor(0.0720, grad_fn=<SubBackward0>), time elapsed: 2670.7055463790894 seconds\n",
      "Step 28300, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 2680.3467502593994 seconds\n",
      "Step 28400, loss: tensor(0.0763, grad_fn=<SubBackward0>), time elapsed: 2689.7823901176453 seconds\n",
      "Step 28500, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 2699.1939220428467 seconds\n",
      "Step 28600, loss: tensor(0.0751, grad_fn=<SubBackward0>), time elapsed: 2708.8463883399963 seconds\n",
      "Step 28700, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 2718.245561361313 seconds\n",
      "Step 28800, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 2727.6509165763855 seconds\n",
      "Step 28900, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 2737.279435634613 seconds\n",
      "Step 29000, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 2746.6555037498474 seconds\n",
      "Step 29100, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 2756.056922674179 seconds\n",
      "Step 29200, loss: tensor(0.0776, grad_fn=<SubBackward0>), time elapsed: 2765.6740329265594 seconds\n",
      "Step 29300, loss: tensor(0.0762, grad_fn=<SubBackward0>), time elapsed: 2775.0969336032867 seconds\n",
      "Step 29400, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 2784.4981229305267 seconds\n",
      "Step 29500, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 2794.103768348694 seconds\n",
      "Step 29600, loss: tensor(0.0745, grad_fn=<SubBackward0>), time elapsed: 2803.33726477623 seconds\n",
      "Step 29700, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 2812.514360666275 seconds\n",
      "Step 29800, loss: tensor(0.0718, grad_fn=<SubBackward0>), time elapsed: 2821.9181644916534 seconds\n",
      "Step 29900, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 2831.1470630168915 seconds\n",
      "Step 30000, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 2840.35595035553 seconds\n",
      "Step 30100, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 2849.803636789322 seconds\n",
      "Step 30200, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 2859.050917863846 seconds\n",
      "Step 30300, loss: tensor(0.0771, grad_fn=<SubBackward0>), time elapsed: 2868.3280367851257 seconds\n",
      "Step 30400, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 2877.7666549682617 seconds\n",
      "Step 30500, loss: tensor(0.0757, grad_fn=<SubBackward0>), time elapsed: 2887.050389289856 seconds\n",
      "Step 30600, loss: tensor(0.0767, grad_fn=<SubBackward0>), time elapsed: 2896.493323802948 seconds\n",
      "Step 30700, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 2906.2106833457947 seconds\n",
      "Step 30800, loss: tensor(0.0731, grad_fn=<SubBackward0>), time elapsed: 2915.433480501175 seconds\n",
      "Step 30900, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 2924.8911633491516 seconds\n",
      "Step 31000, loss: tensor(0.0768, grad_fn=<SubBackward0>), time elapsed: 2934.0869557857513 seconds\n",
      "Step 31100, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 2943.2823605537415 seconds\n",
      "Step 31200, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 2952.6956264972687 seconds\n",
      "Step 31300, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 2961.8793892860413 seconds\n",
      "Step 31400, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 2971.100833415985 seconds\n",
      "Step 31500, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 2980.504523038864 seconds\n",
      "Step 31600, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 2989.7039864063263 seconds\n",
      "Step 31700, loss: tensor(0.0752, grad_fn=<SubBackward0>), time elapsed: 2998.89600110054 seconds\n",
      "Step 31800, loss: tensor(0.0741, grad_fn=<SubBackward0>), time elapsed: 3008.3100550174713 seconds\n",
      "Step 31900, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 3017.910886526108 seconds\n",
      "Step 32000, loss: tensor(0.0754, grad_fn=<SubBackward0>), time elapsed: 3027.0922186374664 seconds\n",
      "Step 32100, loss: tensor(0.0773, grad_fn=<SubBackward0>), time elapsed: 3036.4690754413605 seconds\n",
      "Step 32200, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 3045.6849133968353 seconds\n",
      "Step 32300, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 3054.8577587604523 seconds\n",
      "Step 32400, loss: tensor(0.0761, grad_fn=<SubBackward0>), time elapsed: 3064.199666261673 seconds\n",
      "Step 32500, loss: tensor(0.0734, grad_fn=<SubBackward0>), time elapsed: 3073.459955215454 seconds\n",
      "Step 32600, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 3082.664083003998 seconds\n",
      "Step 32700, loss: tensor(0.0749, grad_fn=<SubBackward0>), time elapsed: 3092.065689563751 seconds\n",
      "Step 32800, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 3101.296298265457 seconds\n",
      "Step 32900, loss: tensor(0.0759, grad_fn=<SubBackward0>), time elapsed: 3110.514778137207 seconds\n",
      "Step 33000, loss: tensor(0.0743, grad_fn=<SubBackward0>), time elapsed: 3120.2416632175446 seconds\n",
      "Step 33100, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 3129.4412035942078 seconds\n",
      "Step 33200, loss: tensor(0.0747, grad_fn=<SubBackward0>), time elapsed: 3138.6629667282104 seconds\n",
      "Step 33300, loss: tensor(0.0765, grad_fn=<SubBackward0>), time elapsed: 3148.095927476883 seconds\n",
      "Step 33400, loss: tensor(0.0746, grad_fn=<SubBackward0>), time elapsed: 3157.3551185131073 seconds\n",
      "Step 33500, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 3166.5724489688873 seconds\n",
      "Step 33600, loss: tensor(0.0738, grad_fn=<SubBackward0>), time elapsed: 3175.9578976631165 seconds\n",
      "Step 33700, loss: tensor(0.0739, grad_fn=<SubBackward0>), time elapsed: 3185.3037559986115 seconds\n",
      "Step 33800, loss: tensor(0.0748, grad_fn=<SubBackward0>), time elapsed: 3194.5090873241425 seconds\n",
      "Step 33900, loss: tensor(0.0775, grad_fn=<SubBackward0>), time elapsed: 3203.8938748836517 seconds\n",
      "Step 34000, loss: tensor(0.0742, grad_fn=<SubBackward0>), time elapsed: 3213.0932540893555 seconds\n",
      "Step 34100, loss: tensor(0.0740, grad_fn=<SubBackward0>), time elapsed: 3222.39289188385 seconds\n",
      "Step 34200, loss: tensor(0.0758, grad_fn=<SubBackward0>), time elapsed: 3233.089903116226 seconds\n",
      "Step 34300, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 3242.2973012924194 seconds\n",
      "Step 34400, loss: tensor(0.0723, grad_fn=<SubBackward0>), time elapsed: 3251.498788356781 seconds\n",
      "Step 34500, loss: tensor(0.0753, grad_fn=<SubBackward0>), time elapsed: 3260.9170632362366 seconds\n",
      "Step 34600, loss: tensor(0.0755, grad_fn=<SubBackward0>), time elapsed: 3270.1392419338226 seconds\n",
      "Step 34700, loss: tensor(0.0732, grad_fn=<SubBackward0>), time elapsed: 3279.387528657913 seconds\n",
      "Step 34800, loss: tensor(0.0750, grad_fn=<SubBackward0>), time elapsed: 3288.7947738170624 seconds\n",
      "Step 34900, loss: tensor(0.0729, grad_fn=<SubBackward0>), time elapsed: 3297.982239484787 seconds\n",
      "Step 35000, loss: tensor(0.0760, grad_fn=<SubBackward0>), time elapsed: 3307.386048555374 seconds\n"
     ]
    }
   ],
   "source": [
    "#train on diffused data\n",
    "n, na = 4, 1 # number of data and ancilla qubits\n",
    "T = 25 # number of diffusion steps\n",
    "L = 6 # layer of backward PQC\n",
    "Ndata = 3000 # number of data in the training data set\n",
    "epochs = 35001\n",
    "\n",
    "\n",
    " # number of training epochs\n",
    "\n",
    "#gen random samples at t = T\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_T = diffModel.HaarSampleGeneration(Ndata, seed = 22)\n",
    "\n",
    "#load diffusion process\n",
    "states_diff = np.load('states_diff.npy')\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "model.set_diffusionSet(states_diff)\n",
    "\n",
    "params_total = []\n",
    "loss_hist_total = []\n",
    "\n",
    "for t in range(T - 1, -1, -1):\n",
    "    params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "    for tt in range(t+1, T):\n",
    "        print(tt)\n",
    "        params_tot[tt] = np.load('paramsandloss_squaremodel/params_t%d.npy'%tt)\n",
    "    \n",
    "    params, loss_hist = Training_t(model, t, inputs_T, params_tot, Ndata, epochs)\n",
    "\n",
    "    np.save('paramsandloss_squaremodel/params_t%d'%t, params.detach().numpy())\n",
    "    np.save('paramsandloss_squaremodel/loss_t%d'%t, loss_hist.detach().numpy())\n",
    "    \n",
    "    #params_total = np.append(params_total, params, axis = 1)\n",
    "    #loss_hist_total = np.append(loss_hist_total, loss_hist, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save total parameters\n",
    "n, na = 4, 1\n",
    "T = 25\n",
    "L = 6\n",
    "Ndata = 3000\n",
    "epochs = 35000 + 1\n",
    "\n",
    "params_tot = np.zeros((T, 2*(n+na)*L))\n",
    "loss_tot = np.zeros((T, epochs))\n",
    "f0_tot = np.zeros((T, epochs))\n",
    "\n",
    "for t in range(T):\n",
    "    params_tot[t] = np.load('paramsandloss_squaremodel/params_t%d.npy'%t)\n",
    "    loss_tot[t] = np.load('paramsandloss_squaremodel/loss_t%d.npy'%t)\n",
    "    \n",
    "\n",
    "np.save(\"params_total_3000Ndata_35kEpochs\", params_tot)\n",
    "np.save(\"loss_tot_3000Ndata_35kEpochs\", loss_tot)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGiCAYAAAB+sGhNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkLklEQVR4nO3dfXBU9aH/8c8GyAYqCaTkCZAHEwzPkATBjXdIrNGIlDEzd6ilToMp0NomMyCOlXQ6ckXtttciOreUh+Fielsp1paHW58wBgOjBJCQjDxYRlJKrkw2wQJLiGWB7Pf3h7+mRrIhgZxN+Ob9mjl/5Lvf7zkfz+zw8eye3XUZY4wAALBYRHcHAADAaZQdAMB6lB0AwHqUHQDAepQdAMB6lB0AwHqUHQDAepQdAMB6lB0AwHqUHQDAeo6V3ZkzZ/Twww8rOjpagwYN0oIFC3ThwoV212RnZ8vlcrXaHn30UaciAgB6CZdT3405a9Ys1dXVad26dbp8+bIKCgp0xx13aNOmTSHXZGdn6/bbb9eKFStaxgYMGKDo6OhrHu/KlSv65JNPWo3FxsYqIoKLVwC4GQSDQZ05c6bV2JgxY9S3b98b37lxwNGjR40k8+GHH7aMvfXWW8blcplTp06FXJeVlWUWL158Q8dkY2NjY7NnO3r06HV1wlc5ctlTUVGhQYMGadq0aS1jOTk5ioiI0L59+9pd+8orr2jIkCGaOHGiiouL9fnnn7c7PxAI6Pz589d8iRQA0Ht1wbXh1Xw+n+Lj41sfqG9fxcbGyufzhVz3ne98RyNHjtTQoUP10Ucf6cknn9SxY8e0ZcuWkGu8Xq+efvrpLssOALBQZy4Dn3zyyWtecn788cfmueeeM7fffvtV6+Pi4syvf/3rDh+vrKzMSDLHjx8POefixYvG7/eb/fv3d/vlNhsbGxtb125d9TJmp67sHn/8cT3yyCPtzrntttuUmJiohoaGVuNXrlzRmTNnlJiY2OHjzZgxQ5J0/PhxJScntznH7XbL7XZrxIgRVz1WWlqqwYMHd/h4uD5ffrka4ZGSktLdEXqVxx57rLsj9AqNjY1atmxZq7HY2Ngu2Xenyi4uLk5xcXHXnOfxeHTu3DlVVlYqIyNDkrRz504Fg8GWAuuI6upqSVJSUtI157Z11+XgwYP19a9/vcPHA24WXXJ3GjqsI3eEwxlddUe9IzeojBs3Tvfff78WLVqk/fv364MPPlBRUZG+/e1va+jQoZKkU6dOaezYsdq/f78kqaamRs8884wqKyv1t7/9Tf/7v/+r/Px8zZw5U5MnT3YiJgCgl3DsQ2ivvPKKxo4dq3vuuUcPPPCA/u3f/k3r169vefzy5cs6duxYy92WkZGRevfdd3Xfffdp7Nixevzxx/Xv//7v+vOf/+xURABAL+HYayGxsbHtfoB81KhRMl/6PPutt96qXbt2ORUHANCL8fUiAADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOs5XnarV6/WqFGjFBUVpRkzZmj//v3tzn/ttdc0duxYRUVFadKkSXrzzTedjggAsJyjZffqq69q6dKlWr58uQ4ePKgpU6YoNzdXDQ0Nbc7fs2eP5s2bpwULFqiqqkp5eXnKy8vT4cOHnYwJALCco2X3wgsvaNGiRSooKND48eO1du1aDRgwQBs3bmxz/ksvvaT7779fTzzxhMaNG6dnnnlG6enp+tWvfuVkTACA5Rwru0uXLqmyslI5OTn/OlhEhHJyclRRUdHmmoqKilbzJSk3NzfkfEkKBAI6f/68GhsbuyY4AMA6jpXdZ599pubmZiUkJLQaT0hIkM/na3ONz+fr1HxJ8nq9iomJUXJy8o2HBgBY6aa/G7O4uFh+v181NTXdHQUA0EP1dWrHQ4YMUZ8+fVRfX99qvL6+XomJiW2uSUxM7NR8SXK73XK73QoEAjceGgBgJceu7CIjI5WRkaGysrKWsWAwqLKyMnk8njbXeDyeVvMlqbS0NOR8AAA6wrErO0launSp5s+fr2nTpmn69Ol68cUX1dTUpIKCAklSfn6+hg0bJq/XK0lavHixsrKytHLlSs2ePVubN2/WgQMHtH79eidjAgAs52jZPfTQQzp9+rSeeuop+Xw+TZ06VW+//XbLTSi1tbWKiPjXxWVmZqY2bdqkn/70p/rJT36iMWPGaNu2bZo4caKTMQEAlnO07CSpqKhIRUVFbT5WXl5+1djcuXM1d+5ch1MBAHqTm/5uTAAAroWyAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYz/GyW716tUaNGqWoqCjNmDFD+/fvDzm3pKRELper1RYVFeV0RACA5Rwtu1dffVVLly7V8uXLdfDgQU2ZMkW5ublqaGgIuSY6Olp1dXUt28mTJ52MCADoBRwtuxdeeEGLFi1SQUGBxo8fr7Vr12rAgAHauHFjyDUul0uJiYktW0JCgpMRAQC9QF+ndnzp0iVVVlaquLi4ZSwiIkI5OTmqqKgIue7ChQsaOXKkgsGg0tPT9bOf/UwTJkwIOT8QCCgQCKixsfGqx65cuaIrV67c2H8Irum9997r7gi9zte+9rXujtCrNDc3d3eEXuHs2bOO7duxK7vPPvtMzc3NV12ZJSQkyOfztbkmNTVVGzdu1Pbt2/W73/1OwWBQmZmZ+vTTT0Mex+v1KiYmRsnJyV2aHwBgjx51N6bH41F+fr6mTp2qrKwsbdmyRXFxcVq3bl3INcXFxfL7/aqpqQljUgDAzcSxlzGHDBmiPn36qL6+vtV4fX29EhMTO7SPfv36KS0tTcePHw85x+12y+12KxAI3FBeAIC9HLuyi4yMVEZGhsrKylrGgsGgysrK5PF4OrSP5uZmHTp0SElJSU7FBAD0Ao5d2UnS0qVLNX/+fE2bNk3Tp0/Xiy++qKamJhUUFEiS8vPzNWzYMHm9XknSihUrdOeddyolJUXnzp3T888/r5MnT2rhwoVOxgQAWM7RsnvooYd0+vRpPfXUU/L5fJo6darefvvtlptWamtrFRHxr4vLs2fPatGiRfL5fBo8eLAyMjK0Z88ejR8/3smYAADLuYwxprtDdIXTp08rPj6+1djevXv19a9/vZsS9R7t3S0LZ/DRg/DiowfhcfbsWT3wwAOtxhoaGhQXF3fD++5Rd2MCAOAEyg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPUfLbvfu3ZozZ46GDh0ql8ulbdu2XXNNeXm50tPT5Xa7lZKSopKSEicjAgB6AUfLrqmpSVOmTNHq1as7NP/EiROaPXu27r77blVXV2vJkiVauHChduzY4WRMAIDl+jq581mzZmnWrFkdnr927VqNHj1aK1eulCSNGzdO77//vlatWqXc3Nw21wQCAQUCATU2NnZJZgCAfXrUe3YVFRXKyclpNZabm6uKioqQa7xer2JiYpScnOx0PADATapHlZ3P51NCQkKrsYSEBJ0/f17/+Mc/2lxTXFwsv9+vmpqacEQEANyEHH0ZMxzcbrfcbrcCgUB3RwEA9FA96souMTFR9fX1rcbq6+sVHR2t/v37d1MqAMDNrkeVncfjUVlZWaux0tJSeTyebkoEALCBo2V34cIFVVdXq7q6WtIXHy2orq5WbW2tpC/eb8vPz2+Z/+ijj+qvf/2rfvzjH+svf/mLfv3rX+sPf/iDHnvsMSdjAgAs52jZHThwQGlpaUpLS5MkLV26VGlpaXrqqackSXV1dS3FJ0mjR4/WG2+8odLSUk2ZMkUrV67Uhg0bQn7sAACAjnD0BpXs7GwZY0I+3ta3o2RnZ6uqqsrBVACA3qZHvWcHAIATKDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUcLbvdu3drzpw5Gjp0qFwul7Zt29bu/PLycrlcrqs2n8/nZEwAgOUcLbumpiZNmTJFq1ev7tS6Y8eOqa6urmWLj493KCEAoDfo6+TOZ82apVmzZnV6XXx8vAYNGtShuYFAQIFAQI2NjZ0+DgCgd3C07K7X1KlTFQgENHHiRP3Hf/yH7rrrrpBzvV6vnn766TYf+9GPfqR+/fo5FRP/X35+fndH6HX69+/f3RF6lYKCgu6O0CucPn3asX33qBtUkpKStHbtWv3pT3/Sn/70J916663Kzs7WwYMHQ64pLi6W3+9XTU1NGJMCAG4mPerKLjU1VampqS1/Z2ZmqqamRqtWrdJvf/vbNte43W653W4FAoFwxQQA3GR61JVdW6ZPn67jx493dwwAwE2sx5dddXW1kpKSujsGAOAm5ujLmBcuXGh1VXbixAlVV1crNjZWI0aMUHFxsU6dOqX/+Z//kSS9+OKLGj16tCZMmKCLFy9qw4YN2rlzp9555x0nYwIALOdo2R04cEB33313y99Lly6VJM2fP18lJSWqq6tTbW1ty+OXLl3S448/rlOnTmnAgAGaPHmy3n333Vb7AACgs1zGGNPdIbrC6dOnr/rweXp6Oh89CAM+ehB+fPQgvPjoQXi09e94Q0OD4uLibnjfPf49OwAAbhRlBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwnqNl5/V6dccdd2jgwIGKj49XXl6ejh07ds11r732msaOHauoqChNmjRJb775ppMxAQCWc7Tsdu3apcLCQu3du1elpaW6fPmy7rvvPjU1NYVcs2fPHs2bN08LFixQVVWV8vLylJeXp8OHDzsZFQBgMZcxxoTrYKdPn1Z8fLx27dqlmTNntjnnoYceUlNTk15//fWWsTvvvFNTp07V2rVrr7nvL0tPT1e/fv26JjxCys/P7+4IvU7//v27O0KvUlBQ0N0ReoW2/h1vaGhQXFzcDe87rO/Z+f1+SVJsbGzIORUVFcrJyWk1lpubq4qKijbnBwIBnT9/Xo2NjV0XFABglbCVXTAY1JIlS3TXXXdp4sSJIef5fD4lJCS0GktISJDP52tzvtfrVUxMjJKTk7s0LwDAHmEru8LCQh0+fFibN2/u0v0WFxfL7/erpqamS/cLALBH33AcpKioSK+//rp2796t4cOHtzs3MTFR9fX1rcbq6+uVmJjY5ny32y23261AINBleQEAdnH0ys4Yo6KiIm3dulU7d+7U6NGjr7nG4/GorKys1Vhpaak8Ho9TMQEAlnP0yq6wsFCbNm3S9u3bNXDgwJb33WJiYlruJsvPz9ewYcPk9XolSYsXL1ZWVpZWrlyp2bNna/PmzTpw4IDWr1/vZFQAgMUcvbJbs2aN/H6/srOzlZSU1LK9+uqrLXNqa2tVV1fX8ndmZqY2bdqk9evXa8qUKfrjH/+obdu2tXtTCwAA7XH0yq4jH+ErLy+/amzu3LmaO3euA4kAAL0R340JALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsJ6jZef1enXHHXdo4MCBio+PV15eno4dO9bumpKSErlcrlZbVFSUkzEBAJZztOx27dqlwsJC7d27V6Wlpbp8+bLuu+8+NTU1tbsuOjpadXV1LdvJkyedjAkAsFxfJ3f+9ttvt/q7pKRE8fHxqqys1MyZM0Ouc7lcSkxMdDIaAKAXcbTsvsrv90uSYmNj25134cIFjRw5UsFgUOnp6frZz36mCRMmtDk3EAgoEAiosbHxqse+9a1vaeDAgTceHO169tlnuztCr1NRUdHdEXqVd955p7sj9Arnzp1zbN9hu0ElGAxqyZIluuuuuzRx4sSQ81JTU7Vx40Zt375dv/vd7xQMBpWZmalPP/20zfler1cxMTFKTk52KjoA4CYXtrIrLCzU4cOHtXnz5nbneTwe5efna+rUqcrKytKWLVsUFxendevWtTm/uLhYfr9fNTU1TsQGAFggLC9jFhUV6fXXX9fu3bs1fPjwTq3t16+f0tLSdPz48TYfd7vdcrvdCgQCXREVAGAhR6/sjDEqKirS1q1btXPnTo0ePbrT+2hubtahQ4eUlJTkQEIAQG/g6JVdYWGhNm3apO3bt2vgwIHy+XySpJiYGPXv31+SlJ+fr2HDhsnr9UqSVqxYoTvvvFMpKSk6d+6cnn/+eZ08eVILFy50MioAwGKOlt2aNWskSdnZ2a3GX375ZT3yyCOSpNraWkVE/OsC8+zZs1q0aJF8Pp8GDx6sjIwM7dmzR+PHj3cyKgDAYo6WnTHmmnPKy8tb/b1q1SqtWrXKoUQAgN6I78YEAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWM/RsluzZo0mT56s6OhoRUdHy+Px6K233mp3zWuvvaaxY8cqKipKkyZN0ptvvulkRABAL+Bo2Q0fPlw///nPVVlZqQMHDugb3/iGHnzwQR05cqTN+Xv27NG8efO0YMECVVVVKS8vT3l5eTp8+LCTMQEAlnO07ObMmaMHHnhAY8aM0e23367nnntOt9xyi/bu3dvm/Jdeekn333+/nnjiCY0bN07PPPOM0tPT9atf/crJmAAAy4XtPbvm5mZt3rxZTU1N8ng8bc6pqKhQTk5Oq7Hc3FxVVFSE3G8gEND58+fV2NjYpXkBAPZwvOwOHTqkW265RW63W48++qi2bt2q8ePHtznX5/MpISGh1VhCQoJ8Pl/I/Xu9XsXExCg5OblLcwMA7OF42aWmpqq6ulr79u3TD3/4Q82fP19Hjx7tsv0XFxfL7/erpqamy/YJALBLX6cPEBkZqZSUFElSRkaGPvzwQ7300ktat27dVXMTExNVX1/faqy+vl6JiYkh9+92u+V2uxUIBLo2OADAGmH/nF0wGAxZTB6PR2VlZa3GSktLQ77HBwBARzh6ZVdcXKxZs2ZpxIgRamxs1KZNm1ReXq4dO3ZIkvLz8zVs2DB5vV5J0uLFi5WVlaWVK1dq9uzZ2rx5sw4cOKD169c7GRMAYDlHy66hoUH5+fmqq6tTTEyMJk+erB07dujee++VJNXW1ioi4l8Xl5mZmdq0aZN++tOf6ic/+YnGjBmjbdu2aeLEiU7GBABYztGy++///u92Hy8vL79qbO7cuZo7d65DiQAAvRHfjQkAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALBeXyd3vmbNGq1Zs0Z/+9vfJEkTJkzQU089pVmzZrU5v6SkRAUFBa3G3G63Ll68eM1jBYPBq8YuXLjQ+dDotObm5u6O0Ov8/e9/7+4Ivcq5c+e6O0KvcP78+avG2vq3/Xo4WnbDhw/Xz3/+c40ZM0bGGP3mN7/Rgw8+qKqqKk2YMKHNNdHR0Tp27FjL3y6Xq0PHOnPmzFVjzz777PUFB3q4jIyM7o4AhMWZM2eUkJBww/txtOzmzJnT6u/nnntOa9as0d69e0OWncvlUmJiYoePEQgEFAgEuIoDAIQUtvfsmpubtXnzZjU1Ncnj8YScd+HCBY0cOVK33nqrHnzwQR05cqTd/Xq9XsXExGj69OldHRkAYAvjsI8++sh87WtfM3369DExMTHmjTfeCDl3z5495je/+Y2pqqoy5eXl5pvf/KaJjo42//d//xdyzcWLF43f7zf79+83ktjY2NjYLNqOHj3aJV3kMsYYOejSpUuqra2V3+/XH//4R23YsEG7du3S+PHjr7n28uXLGjdunObNm6dnnnmm3blXrlzRJ598ogsXLmj69Onav3+/RowYoYiIm+OG08bGRiUnJ6umpkYDBw7s7jidcrNmJ3d4kTv8brbswWBQZ86cafXveFpamvr2vfF33Bwvu6/KyclRcnKy1q1b16H5c+fOVd++ffX73/++Q/PPnz+vmJgY+f1+RUdH30jUsLpZc0s3b3Zyhxe5w+9mze5E7rBf9gSDQQUCgQ7NbW5u1qFDh5SUlORwKgCAzRy9G7O4uFizZs3SiBEj1NjYqE2bNqm8vFw7duyQJOXn52vYsGHyer2SpBUrVujOO+9USkqKzp07p+eff14nT57UwoULO3xMt9ut5cuXy+12O/Lf5JSbNbd082Ynd3iRO/xu1uxO5Hb0ZcwFCxaorKxMdXV1iomJ0eTJk/Xkk0/q3nvvlSRlZ2dr1KhRKikpkSQ99thj2rJli3w+nwYPHqyMjAw9++yzSktLcyoiAKAXCPt7dgAAhNvNcasiAAA3gLIDAFiPsgMAWI+yAwBYz4qyO3PmjB5++GFFR0dr0KBBWrBgwTW/GDo7O1sul6vV9uijjzqac/Xq1Ro1apSioqI0Y8YM7d+/v935r732msaOHauoqChNmjRJb775pqP52tOZ7CUlJVed26ioqDCmlXbv3q05c+Zo6NChcrlc2rZt2zXXlJeXKz09XW63WykpKS13CYdbZ7OXl5dfdb5dLpd8Pl94AuuL76i94447NHDgQMXHxysvL6/Vr5eE0t3P8evJ3ROe39IXP6E2efJkRUdHKzo6Wh6PR2+99Va7a7r7fEudz91V59uKsnv44Yd15MgRlZaW6vXXX9fu3bv1/e9//5rrFi1apLq6upbtP//zPx3L+Oqrr2rp0qVavny5Dh48qClTpig3N1cNDQ1tzt+zZ4/mzZunBQsWqKqqSnl5ecrLy9Phw4cdyxhKZ7NLX/xU05fP7cmTJ8OYWGpqatKUKVO0evXqDs0/ceKEZs+erbvvvlvV1dVasmSJFi5c2PKZ0HDqbPZ/OnbsWKtzHh8f71DCq+3atUuFhYXau3evSktLdfnyZd13331qamoKuaYnPMevJ7fU/c9v6V8/oVZZWakDBw7oG9/4Rrtfnt8Tzvf15Ja66Hx3yTdsdqOjR48aSebDDz9sGXvrrbeMy+Uyp06dCrkuKyvLLF68OAwJvzB9+nRTWFjY8ndzc7MZOnSo8Xq9bc7/1re+ZWbPnt1qbMaMGeYHP/iBoznb0tnsL7/8somJiQlTumuTZLZu3drunB//+MdmwoQJrcYeeughk5ub62Cya+tI9vfee89IMmfPng1Lpo5oaGgwksyuXbtCzulJz/F/6kjunvb8/rLBgwebDRs2tPlYTzzf/9Re7q463zf9lV1FRYUGDRqkadOmtYzl5OQoIiJC+/bta3ftK6+8oiFDhmjixIkqLi7W559/7kjGS5cuqbKyUjk5OS1jERERysnJUUVFRZtrKioqWs2XpNzc3JDznXI92aXO/1RTd+sp5/tGTJ06VUlJSbr33nv1wQcfdGsWv98vSYqNjQ05pyee847klnre87sjP6HWE8+3Uz/91hZHvy4sHHw+31Uv1/Tt21exsbHtvmfxne98RyNHjtTQoUP10Ucf6cknn9SxY8e0ZcuWLs/42Wefqbm5+apf201ISNBf/vKXNtf4fL4254fzfRjp+rKnpqZq48aNmjx5svx+v375y18qMzNTR44c0fDhw8MRu9NCne/z58/rH//4h/r3799Nya4tKSlJa9eu1bRp0xQIBLRhwwZlZ2dr3759Sk9PD3ueYDCoJUuW6K677tLEiRNDzuspz/F/6mjunvT8PnTokDwejy5evKhbbrlFW7duDfmLMj3pfHcmd1ed7x5bdsuWLdMvfvGLdud8/PHH173/L7+nN2nSJCUlJemee+5RTU2NkpOTr3u/kDweT6v/S8vMzNS4ceO0bt26a/5UEzovNTVVqampLX9nZmaqpqZGq1at0m9/+9uw5yksLNThw4f1/vvvh/3YN6KjuXvS8zs1NVXV1dUtP6E2f/78Dv+EWnfqTO6uOt89tuwef/xxPfLII+3Oue2225SYmHjVjRJXrlzRmTNnlJiY2OHjzZgxQ5J0/PjxLi+7IUOGqE+fPqqvr281Xl9fHzJjYmJip+Y75Xqyf1W/fv2Ulpam48ePOxGxS4Q639HR0T36qi6U6dOnd0vZFBUVtdwkdq3/6+4pz3Gpc7m/qjuf35GRkUpJSZEkZWRk6MMPP9RLL73U5k+o9aTz3ZncX3W957vHvmcXFxensWPHtrtFRkbK4/Ho3LlzqqysbFm7c+dOBYPBlgLriOrqakly5OeEIiMjlZGRobKyspaxYDCosrKykK9TezyeVvMlqbS0tN3XtZ1wPdm/6mb4qaaecr67SnV1dVjPtzFGRUVF2rp1q3bu3KnRo0dfc01POOfXk/uretLzu72fUOsJ5zuUsPz02w3f4tID3H///SYtLc3s27fPvP/++2bMmDFm3rx5LY9/+umnJjU11ezbt88YY8zx48fNihUrzIEDB8yJEyfM9u3bzW233WZmzpzpWMbNmzcbt9ttSkpKzNGjR833v/99M2jQIOPz+Ywxxnz3u981y5Yta5n/wQcfmL59+5pf/vKX5uOPPzbLly83/fr1M4cOHXIsY1dlf/rpp82OHTtMTU2NqaysNN/+9rdNVFSUOXLkSNgyNzY2mqqqKlNVVWUkmRdeeMFUVVWZkydPGmOMWbZsmfnud7/bMv+vf/2rGTBggHniiSfMxx9/bFavXm369Olj3n777bBlvt7sq1atMtu2bTOffPKJOXTokFm8eLGJiIgw7777btgy//CHPzQxMTGmvLzc1NXVtWyff/55y5ye+By/ntw94fltzBfPg127dpkTJ06Yjz76yCxbtsy4XC7zzjvvtJm7J5zv68ndVefbirL7+9//bubNm2duueUWEx0dbQoKCkxjY2PL4ydOnDCSzHvvvWeMMaa2ttbMnDnTxMbGGrfbbVJSUswTTzxh/H6/ozn/67/+y4wYMcJERkaa6dOnm71797Y8lpWVZebPn99q/h/+8Adz++23m8jISDNhwgTzxhtvOJqvPZ3JvmTJkpa5CQkJ5oEHHjAHDx4Ma95/3o7/1e2fOefPn2+ysrKuWjN16lQTGRlpbrvtNvPyyy+HNfOXc3Qm+y9+8QuTnJxsoqKiTGxsrMnOzjY7d+4Ma+a28kpqdQ574nP8enL3hOe3McZ873vfMyNHjjSRkZEmLi7O3HPPPS2F0VZuY7r/fBvT+dxddb75iR8AgPV67Ht2AAB0FcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGC9/wfAht7yuUVnZQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create random 16 pixel data\n",
    "Ndata = 3000\n",
    "n = 4\n",
    "random_images = np.zeros((Ndata, 4, 4))\n",
    "amplitude_vals = np.zeros(Ndata)\n",
    "\n",
    "for i in range(0, Ndata):\n",
    "    random_images[i] = np.random.rand(4,4)\n",
    "    amplitude_vals[i] = np.sum(random_images[i] ** 2) ** 0.5\n",
    "\n",
    "plt.imshow(random_images[1], cmap = 'grey', interpolation = 'nearest')\n",
    "np.save(\"random_images\", random_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amplitude encode random image data into qubits\n",
    "random_images_qubits = np.zeros((Ndata, 2**n)) + 1j * np.zeros((Ndata, 2**n))\n",
    "for i in range(0, Ndata):\n",
    "    random_images_qubits[i] = np.ravel(random_images[i] / amplitude_vals[i] + 0j)\n",
    "\n",
    "#print(random_images_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run trained model on random image data\n",
    "n, na = 4, 1\n",
    "T = 25\n",
    "L = 6\n",
    "Ndata = 3000\n",
    "\n",
    "params_tot = np.load('params_total_3000Ndata_35kEpochs.npy')\n",
    "\n",
    "diffModel = DiffusionModel(n, T, Ndata)\n",
    "inputs_te = diffModel.HaarSampleGeneration(Ndata, seed=22)\n",
    "\n",
    "model = QDDPM(n=n, na=na, T=T, L=L)\n",
    "\n",
    "\n",
    "#data_te = model.backDataGeneration(test_data_T20, params_tot, Ndata)[:, :, :2**n].numpy()\n",
    "data_te = model.backDataGeneration(torch.from_numpy(random_images_qubits), params_tot, Ndata)[:, :, :2**n].numpy()\n",
    "\n",
    "np.save(\"test_backwardsgen\", data_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from backwards_gen decode the qubit state to get the nxn image\n",
    "backwards_gen = np.load('test_backwardsgen.npy')\n",
    "dim = 4#int((final_output_flattened.size) ** 0.5)\n",
    "final_output_nxn = np.zeros((T + 1, Ndata, dim, dim))\n",
    "\n",
    "for z in range(0, T + 1):\n",
    "    final_output_flattened = backwards_gen[z]\n",
    "    final_output_flattened = np.abs(final_output_flattened)\n",
    "    #multiplier = np.max(final_output_flattened)\n",
    "    for i in range(0, np.size(amplitude_vals)):\n",
    "        final_output_flattened[:][i] *= amplitude_vals[i]\n",
    "    #final_output_flattened /= multiplier\n",
    "\n",
    "    for i in range(0, dim):\n",
    "        for j in range(0, dim):\n",
    "            for nth_data in range(0, Ndata):\n",
    "                final_output_nxn[z][nth_data][i][j] = final_output_flattened[nth_data][(i*dim) + j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.77142553e-03-8.93511227e-04j  3.26634268e-03+1.45356078e-03j\n",
      "  -2.90996837e-03-2.50547333e-03j  2.55328231e-03+5.21663751e-04j\n",
      "   1.45792392e-05+2.00932985e-03j  1.76505989e-03-2.43030125e-04j\n",
      "  -3.87607794e-03+3.29236686e-03j -5.95670310e-04-3.15215182e-03j\n",
      "  -1.28148939e-03-1.62163924e-03j  2.69731879e-03-4.16143557e-05j\n",
      "  -1.88057544e-03+7.03340629e-04j -9.76602081e-04+3.44157405e-03j\n",
      "  -2.52061314e-03+1.92547147e-03j  2.23761162e-04-1.59270130e-03j\n",
      "  -4.07991931e-03+6.59427233e-03j  1.19948725e-03-4.06151079e-03j]\n",
      " [ 4.81839845e-04+1.53439760e-04j -4.39706771e-03+2.99442769e-03j\n",
      "   1.69902353e-03+1.37351651e-03j  2.05553346e-03+1.89198251e-03j\n",
      "  -1.96455978e-03+5.86898113e-03j -1.40663318e-03-1.29554968e-03j\n",
      "  -8.40923283e-04-8.14967789e-04j -1.39848946e-03-2.01846357e-03j\n",
      "  -4.09379601e-03+5.50284004e-03j -2.28072191e-03-3.21183424e-03j\n",
      "  -1.25396159e-03-1.29060901e-03j  3.93927301e-04-1.73161423e-03j\n",
      "  -9.85943479e-04+3.52418749e-04j -2.93649198e-03+2.60916725e-03j\n",
      "   1.44929055e-03+7.02407215e-06j  2.94597237e-03+9.89025692e-04j]\n",
      " [ 2.93238345e-03+2.84877699e-03j  3.11554526e-03+3.09193041e-03j\n",
      "   1.81721163e-03+4.11159778e-03j -7.13753398e-04+3.70339351e-03j\n",
      "   3.08290380e-03+4.38013813e-03j  5.83863584e-04-1.95713760e-03j\n",
      "  -2.98104831e-03+1.76891964e-03j -4.01041703e-03+2.31944607e-03j\n",
      "   6.26919884e-03+3.67920147e-03j  3.00483801e-03-8.33682774e-04j\n",
      "  -2.63116992e-04-4.21398028e-04j  1.06827938e-03+1.03113218e-03j\n",
      "   2.21117190e-03+1.89785985e-03j -4.59071860e-04+4.85439412e-03j\n",
      "   3.18164378e-03+9.97393625e-04j  5.00873849e-03+2.80134205e-04j]\n",
      " [ 2.42486200e-03+6.32845564e-04j -1.12297166e-04+2.97616026e-03j\n",
      "  -1.44281669e-03+1.01985934e-03j  2.08679377e-03+4.34487330e-04j\n",
      "  -1.03323802e-03+3.19160544e-03j -3.80656868e-03+2.04703584e-03j\n",
      "   3.65852145e-04-2.19732802e-03j  3.42131352e-05+1.72767241e-03j\n",
      "  -4.19997331e-03-2.30675749e-03j -1.71670545e-05+1.20445588e-06j\n",
      "  -1.65064970e-03+5.10040845e-04j  3.55706364e-03+5.26617700e-03j\n",
      "   7.65552570e-04+8.46837554e-03j -1.70983165e-03+1.98498322e-03j\n",
      "  -7.07119121e-04+4.44806647e-03j  2.16346998e-05+4.15661791e-03j]\n",
      " [-1.50801335e-03-6.23414933e-04j  1.18378003e-03+4.67572501e-03j\n",
      "   1.98824145e-03+3.64755141e-03j  1.85330911e-03+3.30950925e-03j\n",
      "  -1.10306975e-03+4.19250969e-03j  3.38382903e-03-5.65908966e-04j\n",
      "  -2.77483440e-03+6.78767858e-04j  6.98358286e-04+3.89378471e-03j\n",
      "   3.01659270e-03-1.39641645e-03j -3.90149420e-03+3.22281470e-04j\n",
      "  -3.94710340e-04+8.27713462e-04j -7.28474755e-04+6.12316234e-03j\n",
      "   1.26165512e-03+1.05649675e-03j  1.98843563e-03+6.40245480e-03j\n",
      "  -3.07402923e-03-2.49345484e-03j  2.94648204e-03+7.09206506e-04j]\n",
      " [ 6.59022620e-03-1.92406971e-03j  9.64921433e-03-2.48318305e-04j\n",
      "   2.35593785e-03+1.58820464e-03j  2.49821297e-03+4.25945548e-03j\n",
      "   1.58544956e-03+2.11723801e-03j -6.37196586e-04-1.03338342e-03j\n",
      "  -3.84878396e-04-2.27159867e-03j  2.14381446e-03+1.20646192e-03j\n",
      "  -3.00884509e-04+4.03748825e-03j  1.70794677e-03+1.53358665e-03j\n",
      "  -3.29631194e-03-2.93129833e-05j  6.08754810e-03+2.86247325e-03j\n",
      "  -4.45848593e-04+3.54836439e-03j  1.55157162e-04-1.14647119e-04j\n",
      "   4.15818160e-03+2.23260582e-03j -1.42520366e-04-3.79819423e-03j]\n",
      " [-4.41860233e-04+3.58576491e-03j -6.78073755e-03+2.20436580e-03j\n",
      "  -5.42500115e-04+6.22470514e-04j -4.52410342e-04-6.23217318e-03j\n",
      "  -1.50005834e-03-6.85312320e-03j  3.84172122e-03+1.45006017e-03j\n",
      "   2.95048463e-04-6.77928503e-04j -7.29242433e-03-2.77212425e-03j\n",
      "  -2.45860312e-03+6.58587785e-04j  2.29644822e-03+2.88216281e-03j\n",
      "   1.11697777e-03+9.50626854e-04j -7.74102891e-03+4.45209397e-03j\n",
      "  -1.72264641e-03-1.90250331e-03j -5.56548825e-03-5.58790169e-04j\n",
      "   3.01535032e-03-4.85390890e-03j  4.04476002e-03+1.21904258e-03j]\n",
      " [-5.75122051e-03+1.12237129e-02j  2.91722361e-03-1.14608062e-02j\n",
      "   2.07965821e-03-2.38267845e-03j -5.12870785e-04+4.75417299e-04j\n",
      "  -4.32887813e-03-3.98134906e-03j  5.18230209e-03+1.79963757e-03j\n",
      "   6.73229457e-04-1.37625006e-03j  3.69360321e-03-2.68730614e-03j\n",
      "   3.72016709e-03-2.08507432e-03j -8.94812681e-03+4.29215282e-03j\n",
      "   8.26477364e-04-2.73921830e-03j  1.72392302e-03-1.67877495e-03j\n",
      "   2.64323130e-03-5.42351650e-03j -7.16582872e-03-2.69499258e-03j\n",
      "  -2.93412851e-03-5.58119966e-03j -6.31794857e-04+3.34635912e-03j]\n",
      " [-4.02732473e-03+6.64395327e-03j -5.74434700e-04-2.52162782e-03j\n",
      "   2.16268934e-03+7.26707978e-04j  7.95796700e-03+7.28034694e-03j\n",
      "   4.89461934e-03-3.73500353e-03j -2.42010318e-03+4.80783056e-04j\n",
      "   3.03732231e-03-3.91236786e-03j  3.56289605e-03-9.22952779e-03j\n",
      "  -2.98588624e-04-3.20702349e-03j -1.87096756e-03-4.27360833e-03j\n",
      "  -1.89584144e-03-4.97287232e-03j -1.45222375e-03+1.69221987e-03j\n",
      "   2.90791295e-03-4.02003014e-03j  2.67446553e-03-1.60717661e-03j\n",
      "   4.23279544e-03-1.86460488e-03j  2.36827647e-03+9.60414391e-03j]\n",
      " [-3.39196576e-03-4.91540274e-03j  3.90617963e-04+1.31875323e-03j\n",
      "  -3.54936207e-03-9.90527682e-03j  7.62172416e-03-8.32528062e-03j\n",
      "   4.16039443e-03+3.99492308e-03j -3.42853321e-03-6.41181413e-03j\n",
      "  -2.64372746e-03+1.09678728e-03j -2.09895871e-03-1.07143251e-02j\n",
      "  -7.47489650e-03+6.51928177e-03j  1.50359119e-03+2.42008455e-03j\n",
      "   9.71361937e-04-1.44863094e-03j  1.51332788e-04-1.01791052e-02j\n",
      "   2.08100514e-03+7.75471097e-04j -3.83999571e-03-4.66697291e-03j\n",
      "   7.37688364e-03-7.21586915e-03j  2.71145045e-03+7.73672480e-03j]\n",
      " [ 4.31205239e-03+1.97390025e-03j  2.63285288e-03+7.92525057e-03j\n",
      "   8.43502581e-04+1.30061973e-02j -1.05105657e-02+4.64155572e-03j\n",
      "   1.61443884e-03-1.07552460e-03j -3.40432714e-04+1.03797037e-02j\n",
      "   1.22788912e-02-1.26352010e-03j  4.47818497e-03+6.84914971e-03j\n",
      "  -5.60872070e-03-9.86023224e-04j  3.65945906e-03-7.53500406e-03j\n",
      "   2.03746627e-03+4.84465715e-03j  2.69565801e-03+1.41029321e-02j\n",
      "  -1.21902982e-02-2.29442748e-03j  4.04248573e-03+9.39586293e-03j\n",
      "  -7.43908982e-04-2.69957731e-04j -5.03078941e-03+7.12825125e-03j]\n",
      " [ 1.01472565e-03+7.17097288e-03j -2.09654937e-03+2.57770461e-03j\n",
      "  -2.80450587e-03-6.75559556e-03j -5.39970817e-03-1.14827715e-02j\n",
      "   4.00617486e-03+3.65209329e-04j  1.96572603e-03+1.24499230e-02j\n",
      "   8.10871460e-03-2.78548012e-03j  7.08588213e-03+1.61100067e-02j\n",
      "   6.64855493e-03+1.56855881e-02j -5.38028590e-03-8.83529708e-03j\n",
      "   2.23749946e-03+1.60667882e-03j  8.19277484e-03+5.42478915e-03j\n",
      "   3.47045506e-03+1.28929950e-02j -5.58104971e-03+4.99341823e-03j\n",
      "  -2.50167126e-04-2.97547760e-03j  3.18330294e-03+1.62928924e-03j]\n",
      " [-2.19192449e-03+4.73462307e-04j  1.31456042e-02-4.71274043e-03j\n",
      "   1.48834509e-03+6.59088232e-03j  1.08428695e-03+6.08442258e-03j\n",
      "   4.26859641e-03+6.48515997e-03j -1.30972071e-02+1.08944881e-03j\n",
      "  -2.72766361e-03-1.26326531e-02j -4.44299169e-03-9.03550943e-04j\n",
      "  -5.99802658e-03+5.52721834e-03j  5.17659541e-03-2.91514862e-03j\n",
      "   1.31248450e-03-8.36038962e-03j  1.96093898e-02+1.40773160e-02j\n",
      "   1.54931669e-03-1.08717419e-02j  7.91887433e-05+5.96470106e-03j\n",
      "  -1.49988933e-02-9.90010332e-03j  1.90325722e-03-1.25629809e-02j]\n",
      " [-3.09496303e-03+3.65084922e-03j  1.80411246e-02-1.99558400e-03j\n",
      "   1.24672167e-02-1.55584980e-02j  9.98124667e-03+2.74664024e-03j\n",
      "   2.25420073e-02+4.77223843e-03j -1.78156011e-02+5.18761016e-03j\n",
      "   7.77193764e-03+1.42150512e-02j  8.05387273e-03+8.30611680e-03j\n",
      "   8.75695050e-03+1.23664699e-02j -3.15793487e-03-2.37204321e-03j\n",
      "  -1.47699863e-02+2.41464889e-03j  1.39150042e-02-6.29419461e-03j\n",
      "   1.87730975e-03-4.87266900e-03j  1.66632235e-02+1.11496891e-03j\n",
      "   3.75375384e-03-1.71652762e-03j -1.32439751e-02-6.14536041e-03j]\n",
      " [-1.45629728e-02+1.47790299e-03j  6.45108521e-04+1.34935556e-02j\n",
      "   8.28391779e-03+1.58929154e-02j -1.74789894e-02+5.88463247e-03j\n",
      "   2.48669386e-02+7.76148215e-03j -1.04476158e-02-5.95229724e-03j\n",
      "  -3.30183259e-03+3.60792875e-02j  4.95923683e-03+1.43738906e-03j\n",
      "   1.46859800e-02+1.47963930e-02j -1.46443853e-02+2.88782082e-02j\n",
      "   8.31065327e-03+1.35283936e-02j -1.13754496e-02+2.05525588e-02j\n",
      "  -2.20570266e-02+1.98234916e-02j  2.29053069e-02+8.92451033e-04j\n",
      "  -1.07592866e-02+3.23769194e-03j  1.99300479e-02+1.30746895e-02j]\n",
      " [ 7.52830226e-03+4.65448387e-03j  6.07376918e-03-1.21682219e-03j\n",
      "   4.86657955e-03+1.02619326e-03j  1.92784779e-02+1.39533088e-03j\n",
      "   1.26754623e-02+3.72328190e-03j  8.87678191e-03-2.22066641e-02j\n",
      "   2.53255405e-02+9.16443206e-03j  4.86447150e-03-4.65818681e-02j\n",
      "   1.85967120e-03-3.41000292e-03j  2.77915411e-02+7.08617829e-03j\n",
      "   1.12801129e-02+3.44868307e-03j  2.18097754e-02-1.78768393e-02j\n",
      "   3.81442951e-03+9.33610182e-03j -9.43908747e-03+1.89085640e-02j\n",
      "   2.54787002e-02-3.09051815e-02j  2.48644128e-03-2.92114895e-02j]\n",
      " [-1.81102790e-02+1.54969851e-02j -1.65883880e-02+1.76302455e-02j\n",
      "   2.41790414e-02-4.06886414e-02j  9.58857127e-03-8.29595665e-04j\n",
      "  -2.15477273e-02+6.09072521e-02j  2.05726805e-03+3.19679715e-02j\n",
      "  -4.39152727e-03-4.58402932e-03j -1.10298144e-02-2.81159207e-02j\n",
      "   1.50042558e-02-2.09852327e-02j  2.04266817e-03+1.91205665e-02j\n",
      "  -2.28917543e-02+2.44858675e-02j -3.64148840e-02+1.16783762e-02j\n",
      "   3.58516758e-04-1.34865632e-02j  2.32244004e-03-3.74124050e-02j\n",
      "   5.70854992e-02-4.76083811e-03j -2.00251788e-02-4.27453732e-03j]\n",
      " [ 2.55152136e-02+2.97690220e-02j  7.79262837e-03+7.84958899e-03j\n",
      "  -6.52315654e-03-2.96134818e-02j -3.43104005e-02+3.24252062e-03j\n",
      "   4.00893241e-02+7.81427044e-03j -1.42495995e-02+2.02328973e-02j\n",
      "   1.25569822e-02-2.84695793e-02j  2.29722336e-02-1.98514871e-02j\n",
      "  -1.92239713e-02+6.17634878e-02j -3.16945761e-02-1.46337710e-02j\n",
      "  -1.57978199e-02+2.33395975e-02j -2.01120656e-02+1.78022832e-02j\n",
      "  -3.20000648e-02+1.80096682e-02j -2.38292124e-02-2.78406851e-02j\n",
      "   8.56541935e-03+2.13481467e-02j  1.52265802e-02-3.45788933e-02j]\n",
      " [-6.44956306e-02-2.33680066e-02j -7.48750865e-02-1.89908240e-02j\n",
      "   2.64783483e-02+1.50799127e-02j -6.59895502e-03+2.22624689e-02j\n",
      "  -2.76800785e-02-8.53768289e-02j  3.77378426e-02-3.42992954e-02j\n",
      "  -1.20890578e-02+4.37704031e-04j  1.25672901e-02+7.87503868e-02j\n",
      "   3.44444737e-02+6.07027113e-02j -8.66104811e-02+1.43918470e-01j\n",
      "   1.37202606e-01+4.84884195e-02j -5.95155135e-02-6.26782980e-03j\n",
      "   1.47220837e-02-9.47018806e-03j -5.69733791e-02+6.55706897e-02j\n",
      "  -1.95501633e-02+4.69516329e-02j -7.90253002e-03-3.60598639e-02j]\n",
      " [ 1.56371713e-01+7.92783722e-02j  2.87894253e-02+3.18660587e-02j\n",
      "  -5.98108098e-02-6.48894161e-02j -1.88088827e-02-9.83487219e-02j\n",
      "  -3.03701926e-02+8.72253329e-02j  5.55969961e-02+1.07894517e-01j\n",
      "  -2.32749302e-02+3.30580026e-02j -3.54502164e-02-1.16246557e-02j\n",
      "   8.91703553e-03+2.97661535e-02j  7.93288574e-02-3.18909157e-03j\n",
      "  -1.37733579e-01-3.46618146e-02j  8.71892720e-02-1.11029387e-01j\n",
      "   5.65818176e-02-5.49891256e-02j -4.85157482e-02+4.10321765e-02j\n",
      "  -9.98523012e-02-7.87679330e-02j  1.25479877e-01-4.61204350e-03j]\n",
      " [ 4.17298600e-02+1.90972611e-01j -8.33552554e-02+3.59627344e-02j\n",
      "   1.00888032e-02-5.86412661e-02j  2.41184384e-02-6.23647347e-02j\n",
      "   1.31038297e-02+9.89577174e-03j -1.21600322e-01-4.87683015e-03j\n",
      "  -5.23581393e-02-1.13546185e-01j -3.23231705e-02-8.79379436e-02j\n",
      "   3.24503809e-01+9.70542356e-02j -1.39825732e-01-5.68303354e-02j\n",
      "  -7.33915018e-03-1.55298799e-01j  5.10339811e-02+5.60659952e-02j\n",
      "   5.12014478e-02-3.43183950e-02j  1.76123083e-01-1.91254824e-01j\n",
      "  -2.24538982e-01-4.61486541e-02j  3.92443351e-02+5.91849908e-02j]\n",
      " [ 2.83800792e-02-3.15957993e-01j -8.87150988e-02+1.26891183e-02j\n",
      "  -4.49739173e-02-8.79871920e-02j  2.54904956e-01-1.20674809e-02j\n",
      "  -1.29587144e-01-1.65855765e-01j -3.39478850e-01-1.83458254e-01j\n",
      "  -7.26524889e-02-3.72072197e-02j -8.26617517e-03-1.74852684e-01j\n",
      "   9.30370241e-02+5.46394736e-02j  2.66964622e-02-4.86389548e-02j\n",
      "  -1.43399844e-02-6.36726916e-02j -3.18153091e-02-2.81417876e-01j\n",
      "   6.41216934e-02+2.02231988e-01j  1.61124259e-01-1.34298772e-01j\n",
      "   1.05630189e-01-1.03896178e-01j  1.42330244e-01+1.78905085e-01j]\n",
      " [ 2.65311226e-02+1.00482054e-01j -1.78747684e-01+3.51225063e-02j\n",
      "  -1.39119387e-01+8.46616998e-02j -2.07042500e-01-2.31695618e-03j\n",
      "   1.85163263e-02+2.19397038e-01j -2.56081700e-01-2.37847548e-02j\n",
      "  -1.06238827e-01-1.19209513e-02j -8.08278620e-02-6.06057942e-02j\n",
      "  -2.27889791e-01-2.05416679e-01j  2.96577156e-01+6.27414584e-02j\n",
      "   1.33485749e-01-1.74210161e-01j -6.73727095e-02-2.81655136e-02j\n",
      "  -7.83970654e-02+2.23271996e-01j  1.86853901e-01-5.96781727e-03j\n",
      "  -1.20508634e-01-2.12752283e-01j  2.20843121e-01-4.10417430e-02j]\n",
      " [ 4.55125272e-02-5.08594047e-03j  1.77806299e-02-2.78113000e-02j\n",
      "   7.90492669e-02+8.88721496e-02j -2.67325103e-01+1.49735566e-02j\n",
      "   2.59569410e-04-1.35917068e-01j -3.97748221e-03+4.90709618e-02j\n",
      "  -5.31700382e-04+1.21508269e-02j  2.56761700e-01-2.60508686e-01j\n",
      "  -1.33518159e-01+1.73120908e-02j  3.51917267e-01+2.08739728e-01j\n",
      "   1.58266664e-01+1.42722815e-01j -1.07436180e-01+1.47885131e-02j\n",
      "   1.45785436e-01+2.88431030e-02j -1.53947279e-01-1.43137842e-01j\n",
      "   1.60273556e-02-1.24380909e-01j  4.28119972e-02+3.75507921e-01j]\n",
      " [ 3.11905265e-01+2.59642918e-02j  1.89354211e-01+1.51338711e-01j\n",
      "  -1.45306632e-01+1.03660360e-01j  4.66293003e-03-4.52672876e-02j\n",
      "  -1.21161103e-01-8.25444087e-02j -3.23839456e-01-1.99339509e-01j\n",
      "   4.98073176e-03+5.91823421e-02j -3.01450002e-03-9.19647887e-02j\n",
      "   1.51351780e-01+1.33261591e-01j -9.89862159e-02-7.61754587e-02j\n",
      "   1.90144494e-01+1.01071019e-02j -9.89895239e-02-1.22129858e-01j\n",
      "   1.14909738e-01-9.41480398e-02j  1.48688957e-01-1.16513763e-02j\n",
      "   8.09163079e-02+4.03571963e-01j -1.19203314e-01+1.57347471e-01j]\n",
      " [ 2.18085408e-01+0.00000000e+00j  2.15409234e-01+0.00000000e+00j\n",
      "   2.17281327e-01+0.00000000e+00j  2.17963472e-01+0.00000000e+00j\n",
      "   2.19255492e-01+0.00000000e+00j  2.17635125e-01+0.00000000e+00j\n",
      "   2.17695802e-01+0.00000000e+00j  2.16980338e-01+0.00000000e+00j\n",
      "   2.17047155e-01+0.00000000e+00j  2.18230218e-01+0.00000000e+00j\n",
      "   2.20929906e-01+0.00000000e+00j  2.16133460e-01+0.00000000e+00j\n",
      "   2.15835199e-01+0.00000000e+00j  2.17307225e-01+0.00000000e+00j\n",
      "   2.14117244e-01+0.00000000e+00j  2.15545386e-01+0.00000000e+00j]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'T')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Will change to showing loss instead of mean square error\n",
    "#Xout[T][Ndata][2**n]\n",
    "#backwards_gen[T][Ndata][2**n]\n",
    "mse_calc = np.zeros((T + 1, 16))\n",
    "avg_backwards_vector = np.sum(backwards_gen, axis = 1)/Ndata\n",
    "print(avg_backwards_vector)\n",
    "\n",
    "orig_vals = np.sum((np.load('training_data.npy')), axis = 0) / Ndata\n",
    "#print(orig_vals)\n",
    "for i in range(0, T + 1):\n",
    "    temp1 = 0\n",
    "    for j in range(0, 2**n):\n",
    "        temp1 += (np.abs(orig_vals[j] - avg_backwards_vector[i][j]))**2\n",
    "    #print(temp1)\n",
    "    mse_calc[i] = temp1/ 16\n",
    "\n",
    "#mse_calc = np.abs(np.sum(mse_calc, axis = 1)) / 16\n",
    "\n",
    "plt.plot(range(0, T + 1), mse_calc)\n",
    "plt.title(\"Mean Square Error between Forward and Backward Diffusion\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.xlabel(\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x2000 with 50 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize average output of the model\n",
    "#final_output_nxn[T][Ndata][4][4]\n",
    "#print_this_1 is average output of model\n",
    "fig, axs = plt.subplots(10, 5, figsize = (10, 20))\n",
    "num1 = np.max(np.abs(avg_backwards_vector))\n",
    "print_this = np.abs(avg_backwards_vector) /num1\n",
    "print_this_1 = np.zeros((4,4))\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 4):\n",
    "        print_this_1[i][j] = print_this[0][4*i + j]\n",
    "\n",
    "\n",
    "for z in range(0, 50):\n",
    "    #print(final_output_nxn[z])\n",
    "    axs[int(z / 5)][z % 5].imshow(\n",
    "        final_output_nxn[0][z]\n",
    "        #print_this_1\n",
    "        , cmap = 'grey', interpolation = 'nearest')\n",
    "    plt.title('Average Output at T = %d'%0)\n",
    "\n",
    "#plt.suptitle('Backwards Diffusion At T = 0')\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
